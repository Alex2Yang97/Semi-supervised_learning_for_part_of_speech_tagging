{"cells":[{"cell_type":"markdown","metadata":{"id":"F_LX9XAD32So"},"source":["# Load Data"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15082,"status":"ok","timestamp":1673592076677,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"},"user_tz":300},"id":"C3soh1b03deD","outputId":"7e9ae505-7502-4382-94be-65a6ab167099"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.8/dist-packages (0.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.26.49)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n","Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.49 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.49)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaleido in /usr/local/lib/python3.8/dist-packages (0.2.1)\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics\n","! pip install -U kaleido"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7629,"status":"ok","timestamp":1673592084299,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"},"user_tz":300},"id":"jIFha6OOht8L","outputId":"c7f2e9bc-08f1-44c1-e619-07ce29dfde1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","from utils import read_conll_file, read_data, filter_tag, create_sub_dir, read_unlabeled_data\n","from utils import TAG2IDX, IDX2TAG, DATA_DIR, POS_FINE_DIR, UNLABELED_DIR\n","from utils import MODEL_DIR, INT_RESULT_DIR, METRICS_DIR, RESULT_DIR, PLOT_TAGS_DIR\n","from utils import (wsj_train_word_lst, wsj_train_tag_lst, \n","                   wsj_dev_word_lst, wsj_dev_tag_lst,\n","                   wsj_test_word_lst, wsj_test_tag_lst)\n","\n","from build_model import PosDataset, UnlabeledDataset, Net, DEVICE, TOKENIZER\n","from build_model import pad, train_one_epoch, eval\n","\n","from analysis import save_sns_fig, analysis_output, make_plot_metric\n","\n","from create_pseudo_data import gen_pseudo_data_by_unlabel\n","\n","import os\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","from tqdm import tqdm_notebook as tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer, BertModel\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","torch.manual_seed(0)\n","\n","import time\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0ZDK1-UU6I5K","executionInfo":{"status":"ok","timestamp":1673592084300,"user_tz":300,"elapsed":24,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"}}},"outputs":[],"source":["def run_scratch_nonfixed(domain, top_percent, threshold=0.1, max_loop=100, lr=0.0001):\n","\n","  print(\"=========================================================\")\n","  print(\"Create directories\")\n","  (sub_model_dir, sub_metrics_dir, sub_result_dir, \n","   sub_int_res_dir) = create_sub_dir(domain, method_name=\"Scratch_nonfixed_self_learning\")\n","\n","  print(\"=========================================================\")\n","  print(\"Load data\")\n","  time1 = time.time()\n","\n","  ul_domain_file = os.path.join(UNLABELED_DIR, f\"gweb-{domain}.unlabeled.txt\")\n","\n","  domain_dir = os.path.join(POS_FINE_DIR, f\"{domain}\")\n","  domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","  domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")\n","\n","  domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","  domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","\n","  domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","  domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","  test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_iter = data.DataLoader(\n","      dataset=dev_dataset,\n","      batch_size=8,\n","      shuffle=True,\n","      num_workers=1,\n","      collate_fn=pad)\n","  test_iter = data.DataLoader(\n","      dataset=test_dataset,\n","      batch_size=8,\n","      shuffle=False,\n","      num_workers=1,\n","      collate_fn=pad)\n","  \n","  time2 = time.time()\n","  print(\" Running time:\", time2 - time1)\n","\n","  # =========================================================\n","  avg_domain_prec_lst = []\n","  avg_domain_rec_lst = []\n","  avg_domain_f1_lst = []\n","  avg_domain_acc_lst = []\n","\n","  micro_domain_prec_lst = []\n","  micro_domain_rec_lst = []\n","  micro_domain_f1_lst = []\n","  micro_domain_acc_lst = []\n","\n","  macro_domain_prec_lst = []\n","  macro_domain_rec_lst = []\n","  macro_domain_f1_lst = []\n","  macro_domain_acc_lst = []\n","\n","  prob_lst = []\n","\n","  print(\"=========================================================\")\n","  print(\"Start Self-training\")\n","\n","  loop_i = 0\n","\n","  domain_unlabeled_data = read_unlabeled_data(ul_domain_file, max_unlabeled=100_000)\n","  \n","  time3 = time.time()\n","  print(\" Running time:\", time3 - time2)\n","\n","  print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","  topn = int(top_percent * len(domain_unlabeled_data))\n","  print(\"  The number of sentences in top n\", topn)\n","\n","  last_top_sen = set()\n","  top_words = domain_unlabeled_data[:topn]\n","  new_top_sen = set([tuple(sen) for sen in top_words])\n","\n","  print(\"Stop condition\", int(threshold * topn))\n","  while len(new_top_sen.difference(last_top_sen)) > int(threshold * topn) and loop_i < max_loop:\n","    print(\"Difference\", len(new_top_sen.difference(last_top_sen)))\n","    time4 = time.time()\n","\n","    loop_i += 1\n","    print(\"\\nLoop\", loop_i)\n","    \n","\n","    # =========================================================\n","    # Load model\n","    if loop_i == 1:\n","      model_name = [name for name in os.listdir(MODEL_DIR) if \"base_model_\" in name][0]\n","      model_file = os.path.join(MODEL_DIR, model_name)\n","    else:\n","      model_name = [name for name in os.listdir(sub_model_dir) if f\"model-top{top_percent}-threshold{threshold}-loop{loop_i-1}-lr{lr}\" in name][0]\n","      model_file = os.path.join(sub_model_dir, model_name)\n","    \n","    print(model_file)\n","\n","    model = Net(vocab_size=len(TAG2IDX))\n","    model.to(DEVICE)\n","    model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(model_file))\n","\n","    # =========================================================\n","    # Performance on test dataset\n","\n","    output_res_file = os.path.join(sub_result_dir, f\"top{top_percent}-threshold{threshold}-loop{loop_i}-lr{lr}.txt\")\n","    \n","    (test_prec_avg, test_rec_avg, test_f1_avg, acc_avg, \n","      test_prec_micro, test_rec_micro, test_f1_micro, test_acc_micro, \n","      test_prec_macro, test_rec_macro, test_f1_macro, test_acc_macro) = eval(\n","        model, test_iter, save_output=True, output_file=output_res_file)\n","    \n","    time5 = time.time()\n","    print(\" Running time:\", time5 - time4)\n","\n","    avg_domain_prec_lst.append(test_prec_avg.item())\n","    avg_domain_rec_lst.append(test_rec_avg.item())\n","    avg_domain_f1_lst.append(test_f1_avg.item())\n","    avg_domain_acc_lst.append(acc_avg.item())\n","\n","    micro_domain_prec_lst.append(test_prec_micro.item())\n","    micro_domain_rec_lst.append(test_rec_micro.item())\n","    micro_domain_f1_lst.append(test_f1_micro.item())\n","    micro_domain_acc_lst.append(test_acc_micro.item())\n","\n","    macro_domain_prec_lst.append(test_prec_macro.item())\n","    macro_domain_rec_lst.append(test_rec_macro.item())\n","    macro_domain_f1_lst.append(test_f1_macro.item())\n","    macro_domain_acc_lst.append(test_acc_macro.item())\n","\n","    csv_file_name = os.path.join(sub_result_dir, f\"top{top_percent}-threshold{threshold}-loop{loop_i}-lr{lr}.csv\")\n","    output_plot_name = os.path.join(sub_result_dir, f\"top{top_percent}-threshold{threshold}-loop{loop_i}-lr{lr}.png\")\n","\n","    _ = analysis_output(\n","        output_res_file, csvsave=True, pngsave=True, \n","        csv_file_name=csv_file_name, output_plot_name=output_plot_name, \n","        figtitle=f\"{domain}-top{top_percent}-loop{loop_i} Test: Accuracy for each tag\")\n","\n","    print(\"=========================================================\")\n","    print(\"Generate new train dataset\")\n","\n","    print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","    ul_domain_dataset = UnlabeledDataset(domain_unlabeled_data)\n","    unlabel_iter = data.DataLoader(\n","        dataset=ul_domain_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad\n","        )\n","\n","    # Save analysis outputs for intermediate results\n","    output_int_res_file = os.path.join(sub_int_res_dir, f\"top{top_percent}-threshold{threshold}-loop{loop_i}-lr{lr}.txt\")\n","\n","    time6 = time.time()\n","    print(\" Running time:\", time6 - time5)\n","\n","    (top_words, top_pseudo_tags, top_prob, \n","     remain_words, remain_pseudo_tags, remain_prob)= gen_pseudo_data_by_unlabel(\n","          model, unlabel_iter, topn, save_output=True, output_file=output_int_res_file)\n","    \n","    last_top_sen = new_top_sen\n","    new_top_sen = set([tuple(sen) for sen in top_words])\n","\n","    time7 = time.time()\n","    print(\" Running time:\", time7 - time6)\n","    \n","    # Save top_prob\n","    prob_lst.append(top_prob)\n","\n","    new_train_dataset = PosDataset(\n","        wsj_train_word_lst + top_words, \n","        wsj_train_tag_lst + top_pseudo_tags)\n","    new_train_iter = data.DataLoader(\n","        dataset=new_train_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad)\n","    \n","    print(\"=========================================================\")\n","    print(\"Build new model\")\n","    model = Net(vocab_size=len(TAG2IDX))\n","    model.to(DEVICE)\n","    model = nn.DataParallel(model)\n","\n","    time8 = time.time()\n","    print(\" Running time:\", time8 - time7)\n","\n","    print(\"=========================================================\")\n","    print(\"Self training for epochs\")\n","\n","    epoch_number = 0\n","    EPOCHS = 3\n","    best_vloss = 1_000_000.\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    for epoch in range(EPOCHS):\n","        print('  EPOCH {}:'.format(epoch_number + 1))\n","\n","        model.train(True)\n","        avg_loss = train_one_epoch(model, new_train_iter, optimizer, loss_fn, epoch_number)\n","\n","        model.train(False)\n","\n","        running_vloss = 0.0\n","        for i, vbatch in enumerate(dev_iter):\n","          words, x, is_heads, tags, y, seqlens = vbatch\n","\n","          logits, y, _ = model(x, y)\n","          logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","          y = y.view(-1)  # (N*T,)\n","          \n","          vloss = loss_fn(logits, y)\n","          running_vloss += vloss\n","\n","        avg_vloss = running_vloss / (i + 1)\n","        print('  LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = os.path.join(sub_model_dir, f'model-top{top_percent}-threshold{threshold}-loop{loop_i}-lr{lr}')\n","            torch.save(model.state_dict(), model_path)\n","\n","        epoch_number += 1\n","  \n","  time9 = time.time()\n","  print(\" Running time:\", time9 - time8)\n","  \n","  print(\"=========================================================\")\n","  print(\"Save metrics and probability list\")\n","\n","  metrics_df = pd.DataFrame({\n","      \"avg_domain_prec_lst\": avg_domain_prec_lst,\n","      \"avg_domain_rec_lst\": avg_domain_rec_lst,\n","      \"avg_domain_f1_lst\": avg_domain_f1_lst,\n","      \"avg_domain_acc_lst\": avg_domain_acc_lst,\n","\n","      \"micro_domain_prec_lst\": micro_domain_prec_lst,\n","      \"micro_domain_rec_lst\": micro_domain_rec_lst,\n","      \"micro_domain_f1_lst\": micro_domain_f1_lst,\n","      \"micro_domain_acc_lst\": micro_domain_acc_lst,\n","\n","      \"macro_domain_prec_lst\": macro_domain_prec_lst,\n","      \"macro_domain_rec_lst\": macro_domain_rec_lst,\n","      \"macro_domain_f1_lst\": macro_domain_f1_lst,\n","      \"macro_domain_acc_lst\": macro_domain_acc_lst\n","\n","  })\n","\n","  metrics_df.to_csv(os.path.join(sub_metrics_dir, f\"metrics_df-top{top_percent}-threshold{threshold}-lr{lr}.csv\"), index=False)\n","  make_plot_metric(metrics_df, sub_metrics_dir, name=f\"top{top_percent}-threshold{threshold}-lr{lr}\")\n","\n","  prob_df = pd.DataFrame({})\n","  for i in range(len(prob_lst)):\n","    prob_df[i+1] = prob_lst[i]\n","  prob_df.to_csv(os.path.join(sub_metrics_dir, f\"prob_df-top{top_percent}-threshold{threshold}-lr{lr}.csv\"), index=False)\n","  \n","  time10 = time.time()\n","  print(\" Running time:\", time10 - time9)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"f5pQmdTS6I20","executionInfo":{"status":"ok","timestamp":1673592084300,"user_tz":300,"elapsed":22,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"}}},"outputs":[],"source":["top_percent_lst = [0.05, 0.1, 0.2] #0.01, 0.02\n","lr_lst = [0.0001, 0.00001]\n","threshold = 0.1\n","DOMAIN_LST = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1Wkbj2FykGF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f0804b0-03fb-4e31-cc15-45929f779bd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","$$$ Run answers, top_percent 0.2, max_loop 5\n","=========================================================\n","Create directories\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/metrics/Scratch_nonfixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/result/Scratch_nonfixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/intermediate_result/Scratch_nonfixed_self_learning/answers\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 0.32220983505249023\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n"," Running time: 12.105831384658813\n","  The number of unlabeled data 27260\n","  The number of sentences in top n 5452\n","Stop condition 545\n","Difference 5297\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"," Running time: 16.148019552230835\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5433440208435059\n"," Running time: 116.51800632476807\n","=========================================================\n","Build new model\n"," Running time: 5.377196311950684\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11397754787560552\n","  batch 1000 loss: 0.05359295746963471\n","  batch 1500 loss: 0.04673237058427185\n","  batch 2000 loss: 0.05138401884492487\n","  batch 2500 loss: 0.04491995071433485\n","  batch 3000 loss: 0.043193309362977744\n","  batch 3500 loss: 0.04112710344139486\n","  batch 4000 loss: 0.041740663410164414\n","  LOSS train 0.041740663410164414 valid 0.46199506521224976\n","  EPOCH 2:\n","  batch 500 loss: 0.035584585225209596\n","  batch 1000 loss: 0.03525310193374753\n","  batch 1500 loss: 0.034203961056657135\n","  batch 2000 loss: 0.037238275649026036\n","  batch 2500 loss: 0.03730603828607127\n","  batch 3000 loss: 0.036507105854339895\n","  batch 3500 loss: 0.0359900135754142\n","  batch 4000 loss: 0.03533574974164367\n","  LOSS train 0.03533574974164367 valid 0.4634626805782318\n","  EPOCH 3:\n","  batch 500 loss: 0.03119793995888904\n","  batch 1000 loss: 0.03314087778283283\n","  batch 1500 loss: 0.031018324034754186\n","  batch 2000 loss: 0.03185755080031231\n","  batch 2500 loss: 0.032031011565588415\n","  batch 3000 loss: 0.034252957119606436\n","  batch 3500 loss: 0.03301683787256479\n","  batch 4000 loss: 0.03089906725147739\n","  LOSS train 0.03089906725147739 valid 0.5093327760696411\n","Difference 4151\n","\n","Loop 2\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers/model-top0.2-threshold0.1-loop1-lr0.0001\n"," Running time: 12.39421820640564\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.0434489250183105\n"," Running time: 116.75120639801025\n","=========================================================\n","Build new model\n"," Running time: 6.061410188674927\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11658453890308738\n","  batch 1000 loss: 0.05262653128150851\n","  batch 1500 loss: 0.05132424358464777\n","  batch 2000 loss: 0.04616963535360992\n","  batch 2500 loss: 0.04432472094614059\n","  batch 3000 loss: 0.04164602676872164\n","  batch 3500 loss: 0.04370233345683664\n","  batch 4000 loss: 0.04759643685352057\n","  LOSS train 0.04759643685352057 valid 0.46513986587524414\n","  EPOCH 2:\n","  batch 500 loss: 0.03350131300650537\n","  batch 1000 loss: 0.04869425837416202\n","  batch 1500 loss: 0.03880105574335903\n","  batch 2000 loss: 0.034908133608521894\n","  batch 2500 loss: 0.04166262198099867\n","  batch 3000 loss: 0.033043830676935615\n","  batch 3500 loss: 0.033460837191436436\n","  batch 4000 loss: 0.03419609724311158\n","  LOSS train 0.03419609724311158 valid 0.46546056866645813\n","  EPOCH 3:\n","  batch 500 loss: 0.0289754602748435\n","  batch 1000 loss: 0.02861971093993634\n","  batch 1500 loss: 0.02832898990646936\n","  batch 2000 loss: 0.03319068048801273\n","  batch 2500 loss: 0.03417896602489054\n","  batch 3000 loss: 0.034820413868874314\n","  batch 3500 loss: 0.032548255883157254\n","  batch 4000 loss: 0.037873295991681516\n","  LOSS train 0.037873295991681516 valid 0.49940672516822815\n","Difference 1664\n","\n","Loop 3\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers/model-top0.2-threshold0.1-loop2-lr0.0001\n"," Running time: 12.227141380310059\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5580346584320068\n"," Running time: 118.05778479576111\n","=========================================================\n","Build new model\n"," Running time: 5.528901100158691\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11436662669107318\n","  batch 1000 loss: 0.050499198561534286\n","  batch 1500 loss: 0.04920313380565494\n","  batch 2000 loss: 0.045490191324148326\n","  batch 2500 loss: 0.04427992876805365\n","  batch 3000 loss: 0.045588689097203317\n","  batch 3500 loss: 0.045368029123172164\n","  batch 4000 loss: 0.045277613147627564\n","  LOSS train 0.045277613147627564 valid 0.41839566826820374\n","  EPOCH 2:\n","  batch 500 loss: 0.033292361781932414\n","  batch 1000 loss: 0.034130390416830775\n","  batch 1500 loss: 0.03929449459724128\n","  batch 2000 loss: 0.040172018059529364\n","  batch 2500 loss: 0.03641058730892837\n","  batch 3000 loss: 0.04504725671932101\n","  batch 3500 loss: 0.04048501273011789\n","  batch 4000 loss: 0.035975938701070846\n","  LOSS train 0.035975938701070846 valid 0.44471073150634766\n","  EPOCH 3:\n","  batch 500 loss: 0.027572820769855753\n","  batch 1000 loss: 0.030436774680041707\n","  batch 1500 loss: 0.033469987560994924\n","  batch 2000 loss: 0.03441757817286998\n","  batch 2500 loss: 0.033634145571384576\n","  batch 3000 loss: 0.03268077762518078\n","  batch 3500 loss: 0.033585981456097216\n","  batch 4000 loss: 0.03408538504689932\n","  LOSS train 0.03408538504689932 valid 0.4951631724834442\n","Difference 1508\n","\n","Loop 4\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers/model-top0.2-threshold0.1-loop3-lr0.0001\n"," Running time: 12.273337841033936\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.0364601612091064\n"," Running time: 117.67681193351746\n","=========================================================\n","Build new model\n"," Running time: 5.62389063835144\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11625445080548524\n","  batch 1000 loss: 0.055088167063891885\n","  batch 1500 loss: 0.0458232891028747\n","  batch 2000 loss: 0.04494161063060165\n","  batch 2500 loss: 0.04661939737852663\n","  batch 3000 loss: 0.04675624052621424\n","  batch 3500 loss: 0.0439518701871857\n","  batch 4000 loss: 0.04638892781734467\n","  LOSS train 0.04638892781734467 valid 0.4471878111362457\n","  EPOCH 2:\n","  batch 500 loss: 0.03299021092429757\n","  batch 1000 loss: 0.03459455548413098\n","  batch 1500 loss: 0.04011464328877628\n","  batch 2000 loss: 0.0425031723389402\n","  batch 2500 loss: 0.0407033087015152\n","  batch 3000 loss: 0.0371760545601137\n","  batch 3500 loss: 0.033677813818212596\n","  batch 4000 loss: 0.03945006389077753\n","  LOSS train 0.03945006389077753 valid 0.4703048765659332\n","  EPOCH 3:\n","  batch 500 loss: 0.0273151902416721\n","  batch 1000 loss: 0.030904586595948785\n","  batch 1500 loss: 0.03168089267425239\n","  batch 2000 loss: 0.03537100899266079\n","  batch 2500 loss: 0.030974193238653244\n","  batch 3000 loss: 0.03305269912071526\n","  batch 3500 loss: 0.033023179868236184\n","  batch 4000 loss: 0.032539861313533036\n","  LOSS train 0.032539861313533036 valid 0.49520090222358704\n","Difference 1371\n","\n","Loop 5\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers/model-top0.2-threshold0.1-loop4-lr0.0001\n"," Running time: 12.20162320137024\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.0727884769439697\n"," Running time: 116.65887379646301\n","=========================================================\n","Build new model\n"," Running time: 5.559818267822266\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11615181494504213\n","  batch 1000 loss: 0.04931308815814555\n","  batch 1500 loss: 0.04747289416287094\n","  batch 2000 loss: 0.046688526537269355\n","  batch 2500 loss: 0.044750086953863503\n","  batch 3000 loss: 0.04534862562827766\n","  batch 3500 loss: 0.05846908426657319\n","  batch 4000 loss: 0.04460753121320158\n","  LOSS train 0.04460753121320158 valid 0.4604392349720001\n","  EPOCH 2:\n","  batch 500 loss: 0.03371088759787381\n","  batch 1000 loss: 0.03974726321687922\n","  batch 1500 loss: 0.03622650213912129\n","  batch 2000 loss: 0.03355863009858877\n","  batch 2500 loss: 0.035531206520972775\n","  batch 3000 loss: 0.0358008219297044\n","  batch 3500 loss: 0.03529479195922613\n","  batch 4000 loss: 0.03561149847321212\n","  LOSS train 0.03561149847321212 valid 0.4802187383174896\n","  EPOCH 3:\n","  batch 500 loss: 0.030011281425366177\n","  batch 1000 loss: 0.030983511425554753\n","  batch 1500 loss: 0.03516969957575202\n","  batch 2000 loss: 0.035897125259507445\n","  batch 2500 loss: 0.044560419239802286\n","  batch 3000 loss: 0.03528854100499302\n","  batch 3500 loss: 0.03441216733935289\n","  batch 4000 loss: 0.041680278453044596\n","  LOSS train 0.041680278453044596 valid 0.49984586238861084\n"," Running time: 1673.773715019226\n","=========================================================\n","Save metrics and probability list\n"," Running time: 4.686326026916504\n","\n","$$$ Run answers, top_percent 0.2, max_loop 5\n","=========================================================\n","Create directories\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 0.166123628616333\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n"," Running time: 6.457507848739624\n","  The number of unlabeled data 27260\n","  The number of sentences in top n 5452\n","Stop condition 545\n","Difference 5297\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"," Running time: 12.175304174423218\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5852508544921875\n"," Running time: 117.33872938156128\n","=========================================================\n","Build new model\n"," Running time: 5.420264959335327\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.3832809674888849\n","  batch 1000 loss: 0.06901314421370626\n","  batch 1500 loss: 0.051830705350264905\n","  batch 2000 loss: 0.04560650802962482\n","  batch 2500 loss: 0.040194726307876405\n","  batch 3000 loss: 0.03964542774297297\n","  batch 3500 loss: 0.03572344633657485\n","  batch 4000 loss: 0.03436350898537785\n","  LOSS train 0.03436350898537785 valid 0.3736555874347687\n","  EPOCH 2:\n","  batch 500 loss: 0.029129432257264852\n","  batch 1000 loss: 0.02806641590455547\n","  batch 1500 loss: 0.030235884621739387\n","  batch 2000 loss: 0.02863621960254386\n","  batch 2500 loss: 0.0266353109437041\n","  batch 3000 loss: 0.027236149203032255\n","  batch 3500 loss: 0.02731283052545041\n","  batch 4000 loss: 0.026213882461190222\n","  LOSS train 0.026213882461190222 valid 0.3843909800052643\n","  EPOCH 3:\n","  batch 500 loss: 0.021333586635999383\n","  batch 1000 loss: 0.021642954034730794\n","  batch 1500 loss: 0.020651675033383072\n","  batch 2000 loss: 0.02236457072244957\n","  batch 2500 loss: 0.021879579173400998\n","  batch 3000 loss: 0.021220382049679756\n","  batch 3500 loss: 0.02063952791900374\n","  batch 4000 loss: 0.022325572524219753\n","  LOSS train 0.022325572524219753 valid 0.4049600660800934\n","Difference 4150\n","\n","Loop 2\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_nonfixed_self_learning/answers/model-top0.2-threshold0.1-loop1-lr1e-05\n"," Running time: 12.331877708435059\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.089857816696167\n"," Running time: 117.33016467094421\n","=========================================================\n","Build new model\n"," Running time: 5.4877049922943115\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.3789322749748826\n","  batch 1000 loss: 0.06532288164831698\n","  batch 1500 loss: 0.049002340618520976\n","  batch 2000 loss: 0.0442129785772413\n","  batch 2500 loss: 0.04223530627135187\n","  batch 3000 loss: 0.03767516123503446\n","  batch 3500 loss: 0.03666160646080971\n","  batch 4000 loss: 0.03484485617186874\n","  LOSS train 0.03484485617186874 valid 0.38952186703681946\n","  EPOCH 2:\n","  batch 500 loss: 0.028806081128772348\n","  batch 1000 loss: 0.027078470698324964\n","  batch 1500 loss: 0.02649745607888326\n","  batch 2000 loss: 0.029509195884689688\n","  batch 2500 loss: 0.027258918434847148\n","  batch 3000 loss: 0.02737411339301616\n","  batch 3500 loss: 0.028108912102878095\n","  batch 4000 loss: 0.026644044908462092\n","  LOSS train 0.026644044908462092 valid 0.40210631489753723\n","  EPOCH 3:\n","  batch 500 loss: 0.020406059567350893\n","  batch 1000 loss: 0.021403700487688183\n","  batch 1500 loss: 0.020681704203365372\n"]}],"source":["for top_percent in top_percent_lst[::-1]:\n","  for domain in DOMAIN_LST:\n","    for lr in lr_lst:\n","      max_loop = int(1 / top_percent)\n","      print(f\"\\n$$$ Run {domain}, top_percent {top_percent}, max_loop {max_loop}\")\n","      # run_scratch_nonfixed(domain, top_percent, threshold=0.1, max_loop=max_loop, lr=lr)\n","\n","      sub_metrics_dir = os.path.join(METRICS_DIR, \"Scratch_nonfixed_self_learning\", domain)\n","      if os.path.exists(os.path.join(sub_metrics_dir, f\"metrics_df-top{top_percent}-threshold{threshold}-lr{lr}.csv\")):\n","        print(\"Already run\")\n","      else:\n","        run_scratch_nonfixed(domain, top_percent, threshold=0.1, max_loop=max_loop, lr=lr)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ngux9I-orPX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEACZ4DaGGRA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}