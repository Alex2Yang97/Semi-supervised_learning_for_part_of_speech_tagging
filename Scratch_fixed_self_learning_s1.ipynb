{"cells":[{"cell_type":"markdown","metadata":{"id":"F_LX9XAD32So"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10455,"status":"ok","timestamp":1673591988547,"user":{"displayName":"Alex Yang","userId":"09551439996217845755"},"user_tz":300},"id":"C3soh1b03deD","outputId":"744abb3f-10b7-4c9e-dd92-40c04e01ebf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.8/dist-packages (0.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.26.49)\n","Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.49 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.49)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaleido in /usr/local/lib/python3.8/dist-packages (0.2.1)\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics\n","! pip install -U kaleido"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","outputId":"d426ecdb-4dd5-45b4-9f06-909f6ebf1423","executionInfo":{"status":"ok","timestamp":1673591997062,"user_tz":300,"elapsed":8520,"user":{"displayName":"Alex Yang","userId":"09551439996217845755"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","from utils import read_conll_file, read_data, filter_tag, create_sub_dir, read_unlabeled_data\n","from utils import TAG2IDX, IDX2TAG, DATA_DIR, POS_FINE_DIR, UNLABELED_DIR\n","from utils import MODEL_DIR, INT_RESULT_DIR, METRICS_DIR, RESULT_DIR, PLOT_TAGS_DIR\n","from utils import (wsj_train_word_lst, wsj_train_tag_lst, \n","                   wsj_dev_word_lst, wsj_dev_tag_lst,\n","                   wsj_test_word_lst, wsj_test_tag_lst)\n","\n","from build_model import PosDataset, UnlabeledDataset, Net, DEVICE, TOKENIZER\n","from build_model import pad, train_one_epoch, eval\n","\n","from analysis import save_sns_fig, analysis_output, make_plot_metric\n","\n","from create_pseudo_data import gen_pseudo_data_by_unlabel\n","\n","import os\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","from tqdm import tqdm_notebook as tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer, BertModel\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","torch.manual_seed(0)\n","\n","import time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P63L-h-28T_a"},"outputs":[],"source":["def run_scratch_fixed(domain, top_percent, lr=0.0001):\n","\n","  print(\"=========================================================\")\n","  print(\"Create directories\")\n","  (sub_model_dir, sub_metrics_dir, sub_result_dir, \n","   sub_int_res_dir) = create_sub_dir(domain, method_name=\"Scratch_fixed_self_learning\")\n","\n","  print(\"=========================================================\")\n","  print(\"Load data\")\n","  time1 = time.time()\n","\n","  ul_domain_file = os.path.join(UNLABELED_DIR, f\"gweb-{domain}.unlabeled.txt\")\n","\n","  domain_dir = os.path.join(POS_FINE_DIR, f\"{domain}\")\n","  domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","  domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")\n","\n","  domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","  domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","\n","  domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","  domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","  test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_iter = data.DataLoader(\n","      dataset=dev_dataset,\n","      batch_size=8,\n","      shuffle=True,\n","      num_workers=1,\n","      collate_fn=pad)\n","  test_iter = data.DataLoader(\n","      dataset=test_dataset,\n","      batch_size=8,\n","      shuffle=False,\n","      num_workers=1,\n","      collate_fn=pad)\n","  \n","  time2 = time.time()\n","  print(\" Running time:\", time2 - time1)\n","\n","  # =========================================================\n","  avg_domain_prec_lst = []\n","  avg_domain_rec_lst = []\n","  avg_domain_f1_lst = []\n","  avg_domain_acc_lst = []\n","\n","  micro_domain_prec_lst = []\n","  micro_domain_rec_lst = []\n","  micro_domain_f1_lst = []\n","  micro_domain_acc_lst = []\n","\n","  macro_domain_prec_lst = []\n","  macro_domain_rec_lst = []\n","  macro_domain_f1_lst = []\n","  macro_domain_acc_lst = []\n","\n","  prob_lst = []\n","\n","  print(\"=========================================================\")\n","  print(\"Start Self-training\")\n","\n","  loop_i = 0\n","\n","  domain_unlabeled_data = read_unlabeled_data(ul_domain_file, max_unlabeled=100_000)\n","  \n","  time3 = time.time()\n","  print(\" Running time:\", time3 - time2)\n","\n","  print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","  topn = int(top_percent * len(domain_unlabeled_data))\n","  print(\"  The number of sentences in top n\", topn)\n","\n","  while len(domain_unlabeled_data) >= topn:\n","    \n","    time4 = time.time()\n","\n","    loop_i += 1\n","    print(\"\\nLoop\", loop_i)\n","\n","    # =========================================================\n","    # Load model\n","    if loop_i == 1:\n","      model_name = [name for name in os.listdir(MODEL_DIR) if \"base_model_\" in name][0]\n","      model_file = os.path.join(MODEL_DIR, model_name)\n","    else:\n","      model_name = [name for name in os.listdir(sub_model_dir) if f\"model-top{top_percent}-loop{loop_i-1}-lr{lr}\" in name][0]\n","      model_file = os.path.join(sub_model_dir, model_name)\n","    \n","    print(model_file)\n","\n","    model = Net(vocab_size=len(TAG2IDX))\n","    model.to(DEVICE)\n","    model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(model_file))\n","\n","    # =========================================================\n","    # Performance on test dataset\n","\n","    output_res_file = os.path.join(sub_result_dir, f\"top{top_percent}-loop{loop_i}-lr{lr}.txt\")\n","    \n","    (test_prec_avg, test_rec_avg, test_f1_avg, acc_avg, \n","      test_prec_micro, test_rec_micro, test_f1_micro, test_acc_micro, \n","      test_prec_macro, test_rec_macro, test_f1_macro, test_acc_macro) = eval(\n","        model, test_iter, save_output=True, output_file=output_res_file)\n","    \n","    time5 = time.time()\n","    print(\" Running time:\", time5 - time4)\n","\n","    avg_domain_prec_lst.append(test_prec_avg.item())\n","    avg_domain_rec_lst.append(test_rec_avg.item())\n","    avg_domain_f1_lst.append(test_f1_avg.item())\n","    avg_domain_acc_lst.append(acc_avg.item())\n","\n","    micro_domain_prec_lst.append(test_prec_micro.item())\n","    micro_domain_rec_lst.append(test_rec_micro.item())\n","    micro_domain_f1_lst.append(test_f1_micro.item())\n","    micro_domain_acc_lst.append(test_acc_micro.item())\n","\n","    macro_domain_prec_lst.append(test_prec_macro.item())\n","    macro_domain_rec_lst.append(test_rec_macro.item())\n","    macro_domain_f1_lst.append(test_f1_macro.item())\n","    macro_domain_acc_lst.append(test_acc_macro.item())\n","\n","    csv_file_name = os.path.join(sub_result_dir, f\"top{top_percent}-loop{loop_i}-lr{lr}.csv\")\n","    output_plot_name = os.path.join(sub_result_dir, f\"top{top_percent}-loop{loop_i}-lr{lr}.png\")\n","\n","    _ = analysis_output(\n","        output_res_file, csvsave=True, pngsave=True, \n","        csv_file_name=csv_file_name, output_plot_name=output_plot_name, \n","        figtitle=f\"{domain}-top{top_percent}-loop{loop_i} Test: Accuracy for each tag\")\n","\n","    print(\"=========================================================\")\n","    print(\"Generate new train dataset\")\n","\n","    print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","    ul_domain_dataset = UnlabeledDataset(domain_unlabeled_data)\n","    unlabel_iter = data.DataLoader(\n","        dataset=ul_domain_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad\n","        )\n","\n","    # Save analysis outputs for intermediate results\n","    output_int_res_file = os.path.join(sub_int_res_dir, f\"top{top_percent}-loop{loop_i}-lr{lr}.txt\")\n","\n","    time6 = time.time()\n","    print(\" Running time:\", time6 - time5)\n","\n","    (top_words, top_pseudo_tags, top_prob, \n","     remain_words, remain_pseudo_tags, remain_prob)= gen_pseudo_data_by_unlabel(\n","          model, unlabel_iter, topn, save_output=True, output_file=output_int_res_file)\n","    \n","    time7 = time.time()\n","    print(\" Running time:\", time7 - time6)\n","    \n","    # Save top_prob\n","    prob_lst.append(top_prob)\n","\n","    # Add wsj and top_words together\n","    new_train_dataset = PosDataset(\n","        wsj_train_word_lst + top_words, \n","        wsj_train_tag_lst + top_pseudo_tags)\n","    new_train_iter = data.DataLoader(\n","        dataset=new_train_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad)\n","    \n","    # Only use remain data as new training dataset\n","    domain_unlabeled_data = remain_words\n","\n","    print(\"=========================================================\")\n","    print(\"Build new model\")\n","    model = Net(vocab_size=len(TAG2IDX))\n","    model.to(DEVICE)\n","    model = nn.DataParallel(model)\n","\n","    time8 = time.time()\n","    print(\" Running time:\", time8 - time7)\n","\n","    print(\"=========================================================\")\n","    print(\"Self training for epochs\")\n","\n","    epoch_number = 0\n","    EPOCHS = 3\n","\n","    best_vloss = 1_000_000.\n","\n","    optimizer = optim.Adam(model.parameters(), lr = lr)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    for epoch in range(EPOCHS):\n","        print('  EPOCH {}:'.format(epoch_number + 1))\n","\n","        model.train(True)\n","        avg_loss = train_one_epoch(model, new_train_iter, optimizer, loss_fn, epoch_number)\n","\n","        model.train(False)\n","\n","        running_vloss = 0.0\n","        for i, vbatch in enumerate(dev_iter):\n","          words, x, is_heads, tags, y, seqlens = vbatch\n","\n","          logits, y, _ = model(x, y)\n","          logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","          y = y.view(-1)  # (N*T,)\n","          \n","          vloss = loss_fn(logits, y)\n","          running_vloss += vloss\n","\n","        avg_vloss = running_vloss / (i + 1)\n","        print('  LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = os.path.join(sub_model_dir, f'model-top{top_percent}-loop{loop_i}-lr{lr}')\n","            torch.save(model.state_dict(), model_path)\n","\n","        epoch_number += 1\n","  \n","  time9 = time.time()\n","  print(\" Running time:\", time9 - time8)\n","  \n","  print(\"=========================================================\")\n","  print(\"Save metrics and probability list\")\n","\n","  metrics_df = pd.DataFrame({\n","      \"avg_domain_prec_lst\": avg_domain_prec_lst,\n","      \"avg_domain_rec_lst\": avg_domain_rec_lst,\n","      \"avg_domain_f1_lst\": avg_domain_f1_lst,\n","      \"avg_domain_acc_lst\": avg_domain_acc_lst,\n","\n","      \"micro_domain_prec_lst\": micro_domain_prec_lst,\n","      \"micro_domain_rec_lst\": micro_domain_rec_lst,\n","      \"micro_domain_f1_lst\": micro_domain_f1_lst,\n","      \"micro_domain_acc_lst\": micro_domain_acc_lst,\n","\n","      \"macro_domain_prec_lst\": macro_domain_prec_lst,\n","      \"macro_domain_rec_lst\": macro_domain_rec_lst,\n","      \"macro_domain_f1_lst\": macro_domain_f1_lst,\n","      \"macro_domain_acc_lst\": macro_domain_acc_lst\n","\n","  })\n","\n","  metrics_df.to_csv(os.path.join(sub_metrics_dir, f\"metrics_df-top{top_percent}-lr{lr}.csv\"), index=False)\n","  make_plot_metric(metrics_df, sub_metrics_dir, name=f\"top{top_percent}-lr{lr}\")\n","\n","  prob_df = pd.DataFrame({})\n","  for i in range(len(prob_lst)):\n","    prob_df[i+1] = prob_lst[i]\n","  prob_df.to_csv(os.path.join(sub_metrics_dir, f\"prob_df-top{top_percent}-lr{lr}.csv\"), index=False)\n","  \n","  time10 = time.time()\n","  print(\" Running time:\", time10 - time9)\n"]},{"cell_type":"code","source":["top_percent_lst = [0.05, 0.1, 0.2] # 0.01, 0.02\n","lr_lst = [0.0001, 0.00001]\n","DOMAIN_LST = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"jzSH-aAA321u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for top_percent in top_percent_lst[::-1]:\n","  for domain in DOMAIN_LST:\n","    for lr in lr_lst:\n","      print(f\"\\n$$$ Run {domain}, top_percent {top_percent}, lr {lr}\")\n","      # run_scratch_fixed(domain, top_percent, lr=lr)\n","\n","      sub_metrics_dir = os.path.join(METRICS_DIR, \"Scratch_fixed_self_learning\", domain)\n","      if os.path.exists(os.path.join(sub_metrics_dir, f\"metrics_df-top{top_percent}-lr{lr}.csv\")):\n","        print(\"Already run\")\n","      else:\n","        run_scratch_fixed(domain, top_percent, lr=lr)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AggaJ0T3L9S","outputId":"e79ef07e-dd5f-40b9-8d38-67ed3c76b0ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","$$$ Run answers, top_percent 0.2, lr 0.0001\n","=========================================================\n","Create directories\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_fixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/metrics/Scratch_fixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/result/Scratch_fixed_self_learning/answers\n","Create /content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/intermediate_result/Scratch_fixed_self_learning/answers\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 0.1477653980255127\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n"," Running time: 7.688755512237549\n","  The number of unlabeled data 27260\n","  The number of sentences in top n 5452\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"," Running time: 19.30111312866211\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5359768867492676\n"," Running time: 117.89409947395325\n","=========================================================\n","Build new model\n"," Running time: 6.559118986129761\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.11397754787560552\n","  batch 1000 loss: 0.05359295746963471\n","  batch 1500 loss: 0.04673237058427185\n","  batch 2000 loss: 0.05138401884492487\n","  batch 2500 loss: 0.04491995071433485\n","  batch 3000 loss: 0.043193309362977744\n","  batch 3500 loss: 0.04112710344139486\n","  batch 4000 loss: 0.041740663410164414\n","  LOSS train 0.041740663410164414 valid 0.46199506521224976\n","  EPOCH 2:\n","  batch 500 loss: 0.035584585225209596\n","  batch 1000 loss: 0.03525310193374753\n","  batch 1500 loss: 0.034203961056657135\n","  batch 2000 loss: 0.037238275649026036\n","  batch 2500 loss: 0.03730603828607127\n","  batch 3000 loss: 0.036507105854339895\n","  batch 3500 loss: 0.0359900135754142\n","  batch 4000 loss: 0.03533574974164367\n","  LOSS train 0.03533574974164367 valid 0.4634626805782318\n","  EPOCH 3:\n","  batch 500 loss: 0.03119793995888904\n","  batch 1000 loss: 0.03314087778283283\n","  batch 1500 loss: 0.031018324034754186\n","  batch 2000 loss: 0.03185755080031231\n","  batch 2500 loss: 0.032031011565588415\n","  batch 3000 loss: 0.034252957119606436\n","  batch 3500 loss: 0.03301683787256479\n","  batch 4000 loss: 0.03089906725147739\n","  LOSS train 0.03089906725147739 valid 0.5093327760696411\n","\n","Loop 2\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_fixed_self_learning/answers/model-top0.2-loop1-lr0.0001\n"," Running time: 12.994616985321045\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 21808\n"," Running time: 0.9682385921478271\n"," Running time: 99.89577722549438\n","=========================================================\n","Build new model\n"," Running time: 6.7704758644104\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.1196511036902666\n","  batch 1000 loss: 0.053125706461258235\n","  batch 1500 loss: 0.04847918497305363\n","  batch 2000 loss: 0.04732458090595901\n","  batch 2500 loss: 0.04521986602433026\n","  batch 3000 loss: 0.04470205891504884\n","  batch 3500 loss: 0.04573154789861292\n","  batch 4000 loss: 0.04960632947646081\n","  LOSS train 0.04960632947646081 valid 0.45902130007743835\n","  EPOCH 2:\n","  batch 500 loss: 0.03776538989134133\n","  batch 1000 loss: 0.034938786650076506\n","  batch 1500 loss: 0.03781935854442418\n","  batch 2000 loss: 0.034922082394361495\n","  batch 2500 loss: 0.03539060068130493\n","  batch 3000 loss: 0.035835992410313336\n","  batch 3500 loss: 0.03757997092511505\n","  batch 4000 loss: 0.04022241776622832\n","  LOSS train 0.04022241776622832 valid 0.5328629612922668\n","  EPOCH 3:\n","  batch 500 loss: 0.03141982468031347\n","  batch 1000 loss: 0.02930659073404968\n","  batch 1500 loss: 0.03289110503951088\n","  batch 2000 loss: 0.03343246611300856\n","  batch 2500 loss: 0.035266780529171225\n","  batch 3000 loss: 0.032838176930323246\n","  batch 3500 loss: 0.03354436971945688\n","  batch 4000 loss: 0.03587914285063744\n","  LOSS train 0.03587914285063744 valid 0.49880579113960266\n","\n","Loop 3\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_fixed_self_learning/answers/model-top0.2-loop2-lr0.0001\n"," Running time: 12.852628946304321\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 16356\n"," Running time: 0.5486018657684326\n"," Running time: 82.3698558807373\n","=========================================================\n","Build new model\n"," Running time: 6.767683267593384\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.1164377396479249\n","  batch 1000 loss: 0.056305234150961045\n","  batch 1500 loss: 0.050332736486569045\n","  batch 2000 loss: 0.050704868787899614\n","  batch 2500 loss: 0.047332845462486145\n","  batch 3000 loss: 0.04530668115243316\n","  batch 3500 loss: 0.04651594696752727\n","  batch 4000 loss: 0.04631532091833651\n","  LOSS train 0.04631532091833651 valid 0.4526674449443817\n","  EPOCH 2:\n","  batch 500 loss: 0.037101239287294446\n","  batch 1000 loss: 0.0390590315554291\n","  batch 1500 loss: 0.03758495879080147\n","  batch 2000 loss: 0.0346504970099777\n","  batch 2500 loss: 0.039218885868787766\n","  batch 3000 loss: 0.03728374590678141\n","  batch 3500 loss: 0.039406849880237134\n","  batch 4000 loss: 0.04301048958674073\n","  LOSS train 0.04301048958674073 valid 0.46719250082969666\n","  EPOCH 3:\n","  batch 500 loss: 0.03747956849727779\n","  batch 1000 loss: 0.036676720752380786\n","  batch 1500 loss: 0.03067894285917282\n","  batch 2000 loss: 0.031999054848216475\n","  batch 2500 loss: 0.04638750697486103\n","  batch 3000 loss: 0.03278004640620202\n","  batch 3500 loss: 0.03666753709502518\n","  batch 4000 loss: 0.03691346748173237\n","  LOSS train 0.03691346748173237 valid 0.5100849866867065\n","\n","Loop 4\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_fixed_self_learning/answers/model-top0.2-loop3-lr0.0001\n"," Running time: 12.948111057281494\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 10904\n"," Running time: 0.5509748458862305\n"," Running time: 61.234121561050415\n","=========================================================\n","Build new model\n"," Running time: 6.751364231109619\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.12616013623401523\n","  batch 1000 loss: 0.059048807341605426\n","  batch 1500 loss: 0.05073163609812036\n","  batch 2000 loss: 0.05504314011242241\n","  batch 2500 loss: 0.047529915479943154\n","  batch 3000 loss: 0.055357038097456096\n","  batch 3500 loss: 0.04680389206111431\n","  batch 4000 loss: 0.045169936632737516\n","  LOSS train 0.045169936632737516 valid 0.47103381156921387\n","  EPOCH 2:\n","  batch 500 loss: 0.03939528035605326\n","  batch 1000 loss: 0.03706698752287775\n","  batch 1500 loss: 0.03846177780721337\n","  batch 2000 loss: 0.039111614203080534\n","  batch 2500 loss: 0.04042562442459166\n","  batch 3000 loss: 0.0403276766045019\n","  batch 3500 loss: 0.03786396178137511\n","  batch 4000 loss: 0.04036444391310215\n","  LOSS train 0.04036444391310215 valid 0.5159174799919128\n","  EPOCH 3:\n","  batch 500 loss: 0.036753498429898175\n","  batch 1000 loss: 0.03402953008795157\n","  batch 1500 loss: 0.031004932320211082\n","  batch 2000 loss: 0.031160270871594547\n","  batch 2500 loss: 0.03361387009220198\n","  batch 3000 loss: 0.03351579580083489\n","  batch 3500 loss: 0.03362848463375121\n","  batch 4000 loss: 0.035603449287824336\n","  LOSS train 0.035603449287824336 valid 0.5648666620254517\n","\n","Loop 5\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Scratch_fixed_self_learning/answers/model-top0.2-loop4-lr0.0001\n"," Running time: 13.190307140350342\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 5452\n"," Running time: 0.5338468551635742\n"," Running time: 34.59554123878479\n","=========================================================\n","Build new model\n"," Running time: 6.392419338226318\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.13495790745690464\n","  batch 1000 loss: 0.06440057444013655\n","  batch 1500 loss: 0.06163657555356622\n","  batch 2000 loss: 0.05664104327652603\n","  batch 2500 loss: 0.053180590951815244\n","  batch 3000 loss: 0.05751282182335853\n","  batch 3500 loss: 0.057029685635119676\n","  batch 4000 loss: 0.05483371479436755\n","  LOSS train 0.05483371479436755 valid 0.4588232636451721\n","  EPOCH 2:\n","  batch 500 loss: 0.04220603059418499\n","  batch 1000 loss: 0.04352336958609521\n","  batch 1500 loss: 0.045216726178303364\n","  batch 2000 loss: 0.04615827306732535\n","  batch 2500 loss: 0.04750586345419288\n","  batch 3000 loss: 0.04423296469077468\n","  batch 3500 loss: 0.04926537302322686\n","  batch 4000 loss: 0.04364640950364992\n","  LOSS train 0.04364640950364992 valid 0.46827083826065063\n","  EPOCH 3:\n","  batch 500 loss: 0.033432084031868724\n","  batch 1000 loss: 0.03951499779243022\n","  batch 1500 loss: 0.040912345603108406\n","  batch 2000 loss: 0.03668165040994063\n","  batch 2500 loss: 0.037231494535692036\n","  batch 3000 loss: 0.039305531696416436\n","  batch 3500 loss: 0.04033293773885816\n","  batch 4000 loss: 0.04070530447736383\n","  LOSS train 0.04070530447736383 valid 0.49339181184768677\n"," Running time: 1826.8416104316711\n","=========================================================\n","Save metrics and probability list\n"," Running time: 4.652610778808594\n","\n","$$$ Run answers, top_percent 0.2, lr 1e-05\n","=========================================================\n","Create directories\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 0.160064697265625\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n"," Running time: 6.304811954498291\n","  The number of unlabeled data 27260\n","  The number of sentences in top n 5452\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"," Running time: 12.957901239395142\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.8913090229034424\n"," Running time: 116.35874247550964\n","=========================================================\n","Build new model\n"," Running time: 6.369295358657837\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.3796251455545425\n","  batch 1000 loss: 0.06789757923223078\n","  batch 1500 loss: 0.051123243506997824\n","  batch 2000 loss: 0.04496182081196457\n","  batch 2500 loss: 0.04003956968523562\n","  batch 3000 loss: 0.039831315607763824\n","  batch 3500 loss: 0.036226216157898304\n","  batch 4000 loss: 0.03469065363053232\n","  LOSS train 0.03469065363053232 valid 0.3758832812309265\n","  EPOCH 2:\n","  batch 500 loss: 0.02885871983598918\n","  batch 1000 loss: 0.027800447289366276\n","  batch 1500 loss: 0.030349682153668255\n","  batch 2000 loss: 0.028746670907363294\n","  batch 2500 loss: 0.027095909000374375\n","  batch 3000 loss: 0.027289685755735264\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hsgFuM2dxWv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaHwOUUqshdR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}