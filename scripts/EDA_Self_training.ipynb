{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1667502029032,"user_tz":240,"elapsed":3642,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"04d3a494-e8b9-4189-aaaf-5e50cdaaad6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.26.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.1)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.1->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.1->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.1->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1667502031076,"user_tz":240,"elapsed":2059,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"dfc19af9-818f-42b6-ce90-e1cbff820495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os"],"metadata":{"id":"JAp_R4aG3nuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","answer_dir = os.path.join(data_dir, \"pos_fine\", \"answers\")\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","labeled_dir = os.path.join(data_dir, \"unlabeled\")\n","\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"id":"CPKysG3i3nsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import codecs"],"metadata":{"id":"9zR0GZJA3np5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)"],"metadata":{"id":"WKWluD5y3nnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")"],"metadata":{"id":"bOIJvUda3xt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst = []\n","wsj_train_tag_lst = []\n","wsj_tags = []\n","for word, tag in read_conll_file(wsj_train_file):\n","  wsj_train_word_lst.append(word)\n","  wsj_train_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj train\", len(wsj_train_word_lst))\n","\n","wsj_dev_word_lst = []\n","wsj_dev_tag_lst = []\n","for word, tag in read_conll_file(wsj_dev_file):\n","  wsj_dev_word_lst.append(word)\n","  wsj_dev_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj dev\", len(wsj_dev_word_lst))\n","print(\"The number of tags in wsj\", len(set(wsj_tags)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-_9MiDj3xrl","executionInfo":{"status":"ok","timestamp":1667502032249,"user_tz":240,"elapsed":1175,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"eca5da83-aca6-4332-fff8-4d11396c1146"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of sentences in wsj train 30060\n","The number of sentences in wsj dev 1336\n","The number of tags in wsj 48\n"]}]},{"cell_type":"code","source":["import random"],"metadata":{"id":"MPyeN0066vBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(0)\n","random.shuffle(wsj_train_word_lst)\n","random.seed(0)\n","random.shuffle(wsj_train_tag_lst)"],"metadata":{"id":"9N8fDl0u72zK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labeled_train_words = wsj_train_word_lst[:10000]\n","labeled_train_tags = wsj_train_tag_lst[:10000]\n","unlabeled_words = wsj_train_word_lst[10000:]\n","unlabeled_tags = wsj_train_tag_lst[10000:]\n","\n","print(len(labeled_train_words))\n","print(len(unlabeled_words))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKLOMMXw6j7U","executionInfo":{"status":"ok","timestamp":1667502032249,"user_tz":240,"elapsed":8,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"1b1f285a-4b34-480e-af60-0bcb840187a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000\n","20060\n"]}]},{"cell_type":"code","source":["wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1667503338702,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"86aaa58a-cd28-4e5e-afbf-c42122fae826"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = PosDataset(labeled_train_words, labeled_train_tags)\n","eval_dataset = PosDataset(wsj_dev_word_lst, wsj_dev_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, train_iter, optimizer, criterion)\n","eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0x3gRfi9iyA","executionInfo":{"status":"ok","timestamp":1667502207614,"user_tz":240,"elapsed":158470,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"af0e8020-ce73-4eaf-b3eb-d4082e5d93c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 4.019284725189209\n","step: 10, loss: 1.9670449495315552\n","step: 20, loss: 0.6764383912086487\n","step: 30, loss: 0.3844396770000458\n","step: 40, loss: 0.2357845902442932\n","step: 50, loss: 0.2136392891407013\n","step: 60, loss: 0.1777462214231491\n","step: 70, loss: 0.18430738151073456\n","step: 80, loss: 0.3058522641658783\n","step: 90, loss: 0.18708693981170654\n","step: 100, loss: 0.11203416436910629\n","step: 110, loss: 0.16677045822143555\n","step: 120, loss: 0.09839129447937012\n","step: 130, loss: 0.16833922266960144\n","step: 140, loss: 0.1866477131843567\n","step: 150, loss: 0.07283345609903336\n","step: 160, loss: 0.1110125333070755\n","step: 170, loss: 0.09983836114406586\n","step: 180, loss: 0.241097092628479\n","step: 190, loss: 0.16661380231380463\n","step: 200, loss: 0.09665440768003464\n","step: 210, loss: 0.1402125060558319\n","step: 220, loss: 0.11045082658529282\n","step: 230, loss: 0.05402184650301933\n","step: 240, loss: 0.08019950240850449\n","step: 250, loss: 0.12376444041728973\n","step: 260, loss: 0.10420562326908112\n","step: 270, loss: 0.10999877005815506\n","step: 280, loss: 0.07000099867582321\n","step: 290, loss: 0.13231854140758514\n","step: 300, loss: 0.07952697575092316\n","step: 310, loss: 0.16092036664485931\n","step: 320, loss: 0.1160336285829544\n","step: 330, loss: 0.07448500394821167\n","step: 340, loss: 0.12721200287342072\n","step: 350, loss: 0.02583962306380272\n","step: 360, loss: 0.07353852689266205\n","step: 370, loss: 0.16229824721813202\n","step: 380, loss: 0.19464467465877533\n","step: 390, loss: 0.176726296544075\n","step: 400, loss: 0.01647094264626503\n","step: 410, loss: 0.10997572541236877\n","step: 420, loss: 0.20114612579345703\n","step: 430, loss: 0.08247566223144531\n","step: 440, loss: 0.07770255208015442\n","step: 450, loss: 0.16778245568275452\n","step: 460, loss: 0.1062714159488678\n","step: 470, loss: 0.04151427000761032\n","step: 480, loss: 0.1562637984752655\n","step: 490, loss: 0.10398241132497787\n","step: 500, loss: 0.04368767887353897\n","step: 510, loss: 0.08450552821159363\n","step: 520, loss: 0.09388559311628342\n","step: 530, loss: 0.08558634668588638\n","step: 540, loss: 0.06570535153150558\n","step: 550, loss: 0.06732361763715744\n","step: 560, loss: 0.19128897786140442\n","step: 570, loss: 0.10834477096796036\n","step: 580, loss: 0.07500334829092026\n","step: 590, loss: 0.2737663984298706\n","step: 600, loss: 0.06224588304758072\n","step: 610, loss: 0.06047200784087181\n","step: 620, loss: 0.08545931428670883\n","step: 630, loss: 0.1543591469526291\n","step: 640, loss: 0.1284322887659073\n","step: 650, loss: 0.02605040930211544\n","step: 660, loss: 0.052862029522657394\n","step: 670, loss: 0.10959797352552414\n","step: 680, loss: 0.08782732486724854\n","step: 690, loss: 0.08673478662967682\n","step: 700, loss: 0.06575392186641693\n","step: 710, loss: 0.033472754061222076\n","step: 720, loss: 0.12638311088085175\n","step: 730, loss: 0.10781493782997131\n","step: 740, loss: 0.1099638044834137\n","step: 750, loss: 0.05191550403833389\n","step: 760, loss: 0.06626521050930023\n","step: 770, loss: 0.14932356774806976\n","step: 780, loss: 0.060302674770355225\n","step: 790, loss: 0.10281608998775482\n","step: 800, loss: 0.10186617076396942\n","step: 810, loss: 0.07995850592851639\n","step: 820, loss: 0.14388498663902283\n","step: 830, loss: 0.050953563302755356\n","step: 840, loss: 0.10387080907821655\n","step: 850, loss: 0.08825480937957764\n","step: 860, loss: 0.11605742573738098\n","step: 870, loss: 0.10652133822441101\n","step: 880, loss: 0.0518566258251667\n","step: 890, loss: 0.1571395844221115\n","step: 900, loss: 0.06565584242343903\n","step: 910, loss: 0.17765840888023376\n","step: 920, loss: 0.07974459230899811\n","step: 930, loss: 0.20654946565628052\n","step: 940, loss: 0.1295657604932785\n","step: 950, loss: 0.19280162453651428\n","step: 960, loss: 0.040730852633714676\n","step: 970, loss: 0.06362409889698029\n","step: 980, loss: 0.07183173298835754\n","step: 990, loss: 0.07262839376926422\n","step: 1000, loss: 0.1601548045873642\n","step: 1010, loss: 0.052714716643095016\n","step: 1020, loss: 0.1046467199921608\n","step: 1030, loss: 0.06323140859603882\n","step: 1040, loss: 0.06957120448350906\n","step: 1050, loss: 0.07527944445610046\n","step: 1060, loss: 0.0246713999658823\n","step: 1070, loss: 0.11350086331367493\n","step: 1080, loss: 0.06156669929623604\n","step: 1090, loss: 0.04292606934905052\n","step: 1100, loss: 0.046942658722400665\n","step: 1110, loss: 0.04366784170269966\n","step: 1120, loss: 0.18827110528945923\n","step: 1130, loss: 0.0893082246184349\n","step: 1140, loss: 0.093312107026577\n","step: 1150, loss: 0.06491299718618393\n","step: 1160, loss: 0.06962490826845169\n","step: 1170, loss: 0.06448338180780411\n","step: 1180, loss: 0.11935977637767792\n","step: 1190, loss: 0.09605085104703903\n","step: 1200, loss: 0.07637272775173187\n","step: 1210, loss: 0.09789557009935379\n","step: 1220, loss: 0.036747269332408905\n","step: 1230, loss: 0.045470867305994034\n","step: 1240, loss: 0.06948147714138031\n","acc=0.97\n"]}]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file1 = os.path.join(model_dir, \"base_model1.pt\")"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), model_file1)"],"metadata":{"id":"MgMq01rh9ism"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_file2 = os.path.join(model_dir, \"base_model2.pt\")"],"metadata":{"id":"aU34zQwL4n04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model, model_file2)"],"metadata":{"id":"A2AFZjnh4hB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["base_model1 = Net(vocab_size=len(tag2idx))\n","base_model1.to(device)\n","base_model1 = nn.DataParallel(base_model1)"],"metadata":{"id":"hAD3Wd574v6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model1.load_state_dict(torch.load(model_file1))\n","eval(base_model1, test_iter)"],"metadata":{"id":"oHjILhhK9M3A","executionInfo":{"status":"ok","timestamp":1667502444184,"user_tz":240,"elapsed":6856,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2e913fb-aff2-467b-bded-2049a7a6c9e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n"]}]},{"cell_type":"code","source":["base_model2 = torch.load(model_file2)\n","eval(base_model2, test_iter)"],"metadata":{"id":"xYFb8zAp9M0D","executionInfo":{"status":"ok","timestamp":1667502516469,"user_tz":240,"elapsed":6750,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7786930a-2385-4296-d5f2-bf4777af5904"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n"]}]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["unlabeled_dataset = PosDataset(unlabeled_words, unlabeled_tags)\n","\n","unlabeled_iter = data.DataLoader(dataset=unlabeled_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)"],"metadata":{"id":"DYCxNf1k5YFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval(base_model1, unlabeled_iter)"],"metadata":{"id":"Pqjg7y9u9MvV","executionInfo":{"status":"ok","timestamp":1667502840237,"user_tz":240,"elapsed":82776,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1837ee1e-f85c-4406-a762-954c0f53d40b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n"]}]},{"cell_type":"markdown","source":["## Produce pseduo-labels"],"metadata":{"id":"FgvIECR8AHOv"}},{"cell_type":"code","source":["model.eval()\n","\n","Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","LLD = []\n","new_x_lst = []\n","new_y_lst = []\n","i = 0\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(unlabeled_iter):\n","\n","      words, x, is_heads, tags, y, seqlens = batch\n","\n","      logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","      # Save prediction as new training dataset\n","      softmax_value = torch.softmax(logits, dim=2)\n","      max_prob = torch.amax(softmax_value, dim=2)\n","      lld = torch.prod(max_prob, 1)\n","      LLD.extend(lld)\n","\n","      new_x_lst.extend(x.tolist())\n","      new_y_lst.extend(y_hat.tolist())\n","\n","      Words.extend(words)\n","      Is_heads.extend(is_heads)\n","      Tags.extend(tags)\n","      Y.extend(y.numpy().tolist())\n","      Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","      # if i==20:\n","      #   break"],"metadata":{"id":"Tq8DyhZz9Msv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(LLD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pff8slbkXVOE","executionInfo":{"status":"ok","timestamp":1667510470811,"user_tz":240,"elapsed":1,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"5ea4c9e3-d8cb-440d-b675-fea35abfeb72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20060"]},"metadata":{},"execution_count":201}]},{"cell_type":"code","source":["ind = list(range(len(LLD)))\n","ind = [x for _, x in sorted(zip(LLD, ind), reverse=True)]"],"metadata":{"id":"_xPlS9KHMNPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["select_ind = ind[: 2000]\n","not_select_ind = ind[2000:]\n","\n","new_train_x = [new_x_lst[i] for i in select_ind]\n","new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","remain_train_y = [new_y_lst[i] for i in not_select_ind]"],"metadata":{"id":"kYeLIaD6M1sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(new_train_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAO9koSL2Cid","executionInfo":{"status":"ok","timestamp":1667518452244,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"73b85b95-25e1-4144-e316-1d181e2a5a8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":263}]},{"cell_type":"code","source":["# y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","# y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])"],"metadata":{"id":"iv-iXprU9Mgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## calc metric\n","# y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","# y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","# acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","# print(\"classification_report\", classification_report(y_true, y_pred))"],"metadata":{"id":"TSpHEvy-7qHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen"],"metadata":{"id":"M_Dfl9vW7qFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens"],"metadata":{"id":"tazNmMoPgAEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_train_dataset = PosDataset_new(new_train_x, new_train_y)\n","\n","new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad_new)"],"metadata":{"id":"aD4gN8kR7qC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, batch_iter in enumerate(new_train_iter):\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_JYUK0Qqq6y","executionInfo":{"status":"ok","timestamp":1667519061676,"user_tz":240,"elapsed":8,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"da230f98-8fcc-47fc-f816-e4e648b88ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["41\n","8\n","8\n","[39, 34, 39, 36, 36, 41, 39, 40]\n","56\n","8\n","8\n","[44, 34, 40, 34, 39, 44, 56, 47]\n"]}]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"BUErqWSi7qAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"6PJrl9qR7p9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_new(model, new_train_iter, optimizer, criterion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lBuUT9P7p7D","executionInfo":{"status":"ok","timestamp":1667519408076,"user_tz":240,"elapsed":30886,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"8e569764-912a-4b38-c2b1-ed7f2907760b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.015222178772091866\n","step: 10, loss: 0.008685328997671604\n","step: 20, loss: 0.071180559694767\n","step: 30, loss: 0.010449478402733803\n","step: 40, loss: 0.03320135921239853\n","step: 50, loss: 0.016082294285297394\n","step: 60, loss: 0.03760402277112007\n","step: 70, loss: 0.00989021547138691\n","step: 80, loss: 0.042467646300792694\n","step: 90, loss: 0.021140089258551598\n","step: 100, loss: 0.015271157026290894\n","step: 110, loss: 0.028544079512357712\n","step: 120, loss: 0.055811602622270584\n","step: 130, loss: 0.01912704110145569\n","step: 140, loss: 0.018367575481534004\n","step: 150, loss: 0.01601603627204895\n","step: 160, loss: 0.01235022209584713\n","step: 170, loss: 0.020826663821935654\n","step: 180, loss: 0.017045238986611366\n","step: 190, loss: 0.012764754705131054\n","step: 200, loss: 0.06760004907846451\n","step: 210, loss: 0.058981966227293015\n","step: 220, loss: 0.021801872178912163\n","step: 230, loss: 0.019672349095344543\n","step: 240, loss: 0.03009956143796444\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"l9sQ8lGr7p4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2DEMIx2r3xf2"},"execution_count":null,"outputs":[]}]}