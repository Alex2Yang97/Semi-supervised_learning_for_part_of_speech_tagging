{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1669699896593,"user_tz":300,"elapsed":26235,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"0e986c79-4421-4a27-b0c4-1c42760782c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 27.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Collecting boto3\n","  Downloading boto3-1.26.17-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 42.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 3.4 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.17\n","  Downloading botocore-1.29.17-py3-none-any.whl (10.0 MB)\n","\u001b[K     |████████████████████████████████| 10.0 MB 50.6 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 51.4 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 60.4 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.17 botocore-1.29.17 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 12.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.10.3\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from utils import read_conll_file, read_data\n","\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1669729847602,"user_tz":300,"elapsed":29951037,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"0ede4576-79c1-425b-afe5-04197c406bcf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"CPKysG3i3nsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KqectYYC30N","executionInfo":{"status":"ok","timestamp":1669729855641,"user_tz":300,"elapsed":7641,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"5783f09b-cb25-4f8a-d8c0-f52e8178cda3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1669729855642,"user_tz":300,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"bd8ce5b4-a14a-40a3-89b6-89aed70e425c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669729859776,"user_tz":300,"elapsed":3264,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"47c6d9f5-3a4e-48a3-a552-02299bd65d2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 279609.26B/s]\n"]}]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How to choose weight method?\n","# https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f#60ad\n","def eval(model, iterator, average=\"weighted\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","\n","    pred_lst = []\n","    true_lst = []\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            for s in y_hat.cpu().numpy().tolist():\n","              pred_lst.extend(s)\n","            for s in y.numpy().tolist():\n","              true_lst.extend(s)\n","\n","    precision_value = multiclass_precision(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    recall_value = multiclass_recall(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    f1_value = multiclass_f1_score(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    acc = multiclass_accuracy(\n","        torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","        average=average)    \n","\n","\n","    return precision_value, recall_value, f1_value, acc"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669729906847,"user_tz":300,"elapsed":47075,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"4066c9bf-f4eb-46fe-e9cb-f8010758e2d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:33<00:00, 11941927.73B/s]\n"]}]},{"cell_type":"code","source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"],"metadata":{"id":"B0x3gRfi9iyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# torch.save(model.state_dict(), model_file)"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value = eval(model, test_iter)\n","print(wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAD3Wd574v6Q","executionInfo":{"status":"ok","timestamp":1669729931214,"user_tz":300,"elapsed":24388,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"610221d1-31bb-44d9-bd09-a6f0d02784b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9771) tensor(0.9743) tensor(0.9751) tensor(0.9743)\n"]}]},{"cell_type":"code","source":["wsj_precision_value, wsj_recall_value, wsj_f1_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2M6AkbHMD6VJ","executionInfo":{"status":"ok","timestamp":1669729931218,"user_tz":300,"elapsed":29,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"9971c322-6636-4036-b52e-494a19b716e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.9771), tensor(0.9743), tensor(0.9751))"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"],"metadata":{"id":"agIHM1TmEYl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"mGm3QLNcD8bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain = \"emails\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"],"metadata":{"id":"fwvivWyzEHOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHAOs-fdEHMO","executionInfo":{"status":"ok","timestamp":1669729933465,"user_tz":300,"elapsed":2269,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"41a13f42-d7b5-41a3-af59-162733e9027f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 2450\n","The number of tags 49\n","The number of samples: 2450\n","The number of tags 48\n","after filter tag 2427\n","after filter tag 2402\n"]}]},{"cell_type":"code","source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []\n","domain_acc_value_lst = []"],"metadata":{"id":"3JGnpmRNHHmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)\n","domain_acc_value_lst.append(domain_acc_value)"],"metadata":{"id":"cHb8ZM-VjG-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"S1HrHo0LEhWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","  acc_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          # When calculating the length of sentences, ignore <pad>\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob.tolist())\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","          # Calculate the accuracy for each sentences, ignore 0\n","          batch_acc = multiclass_accuracy(\n","              torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags), \n","              ignore_index=0, average=\"micro\", multidim_average=\"samplewise\")\n","          acc_lst.extend(batch_acc.tolist())\n","          \n","\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob.tolist())\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","          # Calculate the accuracy for each sentences, ignore 0\n","          batch_acc = multiclass_accuracy(\n","              torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags), \n","              ignore_index=0, average=\"micro\", multidim_average=\"samplewise\")\n","          acc_lst.extend(batch_acc.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","  prob_lst = [prob for prob, _ in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn] # The index of topn sentences\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","\n","  new_prob = prob_lst[: topn]\n","  remain_prob = prob_lst[topn: ]\n","  new_acc = [acc_lst[i] for i in select_ind]\n","  remain_acc = [acc_lst[i] for i in not_select_ind]\n","\n","\n","  return new_train_x, new_train_y, remain_train_x, remain_train_y, new_acc, remain_acc, new_prob, remain_prob"],"metadata":{"id":"vzWOF5sCGjgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc_lst = []\n","prob_lst = []\n","\n","threshold = 0.01\n","top_percent = 0.6\n","topn = int(top_percent * len(domain_dev_word_lst))\n","\n","print(\"topn:\", topn)\n","print(\"stop:\", int(threshold * topn))\n","\n","i = 0\n","last_top_sen = set()\n","top_words = domain_dev_word_lst[:topn]\n","new_top_sen = set([tuple(sen) for sen in top_words])\n","\n","while len(new_top_sen.difference(last_top_sen)) > int(threshold * topn):\n","  i += 1\n","  print(\"\\nLoop\", i)\n","\n","  domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","  domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  if i == 1:\n","    last_top_sen = set()\n","  else:\n","    last_top_sen = new_top_sen\n","\n","  top_words_ids, top_tags_ids, remain_words, remain_tags, new_acc, remain_acc, new_prob, remain_prob = gen_pseudo_data(model, domain_dev_iter, topn)\n","  \n","  new_top_sen = set([tuple(sen) for sen in top_words_ids])\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    words = []\n","    tags = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        tags.append(tag_ids[k])\n","    top_words.append(words)\n","    top_tags.append(tags)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","  \n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)\n","  domain_acc_value_lst.append(domain_acc_value)\n","\n","  acc_lst.append(new_acc)\n","  prob_lst.append(new_prob)\n","\n","\n","  print(\"Difference\", len(new_top_sen.difference(last_top_sen)))\n","\n","  if i==2:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zl3VMmnVVD-8","outputId":"5ef70d68-bca6-4492-ed28-758c8b9350e1","executionInfo":{"status":"ok","timestamp":1669730969098,"user_tz":300,"elapsed":1027918,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["topn: 1456\n","stop: 14\n","\n","Loop 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9611403942108154\n","step: 10, loss: 1.7096995115280151\n","step: 20, loss: 0.8377414345741272\n","step: 30, loss: 0.5990095138549805\n","step: 40, loss: 0.5547073483467102\n","step: 50, loss: 0.22327964007854462\n","step: 60, loss: 0.14814500510692596\n","step: 70, loss: 0.1611810028553009\n","step: 80, loss: 0.10375607013702393\n","step: 90, loss: 0.10618849843740463\n","step: 100, loss: 0.22247950732707977\n","step: 110, loss: 0.2027372419834137\n","step: 120, loss: 0.19710686802864075\n","step: 130, loss: 0.12244416028261185\n","step: 140, loss: 0.20871517062187195\n","step: 150, loss: 0.2001812905073166\n","step: 160, loss: 0.1427653580904007\n","step: 170, loss: 0.09805649518966675\n","step: 180, loss: 0.30391034483909607\n","step: 190, loss: 0.14604218304157257\n","step: 200, loss: 0.091248519718647\n","step: 210, loss: 0.09923768043518066\n","step: 220, loss: 0.22695060074329376\n","step: 230, loss: 0.18331557512283325\n","step: 240, loss: 0.03736533969640732\n","step: 250, loss: 0.07287833094596863\n","step: 260, loss: 0.3279063105583191\n","step: 270, loss: 0.1349021941423416\n","step: 280, loss: 0.08314988762140274\n","step: 290, loss: 0.11834179610013962\n","step: 300, loss: 0.2170383632183075\n","step: 310, loss: 0.15833649039268494\n","step: 320, loss: 0.2958666980266571\n","step: 330, loss: 0.21646955609321594\n","step: 340, loss: 0.15840214490890503\n","step: 350, loss: 0.08410371840000153\n","step: 360, loss: 0.17214927077293396\n","step: 370, loss: 0.04097815975546837\n","step: 380, loss: 0.1750565767288208\n","step: 390, loss: 0.05195529758930206\n","step: 400, loss: 0.053307775408029556\n","step: 410, loss: 0.0713973194360733\n","step: 420, loss: 0.11958909034729004\n","step: 430, loss: 0.1217462345957756\n","step: 440, loss: 0.074069082736969\n","step: 450, loss: 0.1501128375530243\n","step: 460, loss: 0.19833023846149445\n","step: 470, loss: 0.1623334437608719\n","step: 480, loss: 0.32055240869522095\n","step: 490, loss: 0.042154207825660706\n","step: 500, loss: 0.11187044531106949\n","step: 510, loss: 0.05709860846400261\n","step: 520, loss: 0.1740933507680893\n","step: 530, loss: 0.11361078172922134\n","step: 540, loss: 0.08668834716081619\n","step: 550, loss: 0.1962698996067047\n","step: 560, loss: 0.11499179899692535\n","step: 570, loss: 0.07093098014593124\n","step: 580, loss: 0.17860345542430878\n","step: 590, loss: 0.06443631649017334\n","step: 600, loss: 0.12141269445419312\n","step: 610, loss: 0.075187548995018\n","step: 620, loss: 0.11419207602739334\n","step: 630, loss: 0.10502313822507858\n","step: 640, loss: 0.035829029977321625\n","step: 650, loss: 0.1170165166258812\n","step: 660, loss: 0.1525469273328781\n","step: 670, loss: 0.17080093920230865\n","step: 680, loss: 0.049327000975608826\n","step: 690, loss: 0.10784818977117538\n","step: 700, loss: 0.10891704261302948\n","step: 710, loss: 0.11334402859210968\n","step: 720, loss: 0.14699876308441162\n","step: 730, loss: 0.17433053255081177\n","step: 740, loss: 0.10623950511217117\n","step: 750, loss: 0.07135241478681564\n","step: 760, loss: 0.19229742884635925\n","step: 770, loss: 0.15212216973304749\n","step: 780, loss: 0.11708629131317139\n","step: 790, loss: 0.03356892615556717\n","step: 800, loss: 0.12656916677951813\n","step: 810, loss: 0.0854974091053009\n","step: 820, loss: 0.13589881360530853\n","step: 830, loss: 0.12722760438919067\n","step: 840, loss: 0.09561740607023239\n","step: 850, loss: 0.0656658262014389\n","step: 860, loss: 0.06394150108098984\n","step: 870, loss: 0.17589569091796875\n","step: 880, loss: 0.14017575979232788\n","step: 890, loss: 0.13932499289512634\n","step: 900, loss: 0.08786740899085999\n","step: 910, loss: 0.2005137801170349\n","step: 920, loss: 0.12027718126773834\n","step: 930, loss: 0.09203747659921646\n","step: 940, loss: 0.11127914488315582\n","step: 950, loss: 0.1000293493270874\n","step: 960, loss: 0.05954974517226219\n","step: 970, loss: 0.18324722349643707\n","step: 980, loss: 0.05690877512097359\n","step: 990, loss: 0.13025009632110596\n","step: 1000, loss: 0.05079977959394455\n","step: 1010, loss: 0.059837356209754944\n","step: 1020, loss: 0.07696912437677383\n","step: 1030, loss: 0.15610074996948242\n","step: 1040, loss: 0.0886225700378418\n","step: 1050, loss: 0.06169717386364937\n","step: 1060, loss: 0.10173862427473068\n","step: 1070, loss: 0.1688433289527893\n","step: 1080, loss: 0.15022455155849457\n","step: 1090, loss: 0.11506335437297821\n","step: 1100, loss: 0.12052789330482483\n","step: 1110, loss: 0.13028793036937714\n","step: 1120, loss: 0.08828359097242355\n","step: 1130, loss: 0.057216573506593704\n","step: 1140, loss: 0.1056460365653038\n","step: 1150, loss: 0.11768779158592224\n","step: 1160, loss: 0.12304241955280304\n","step: 1170, loss: 0.05640081688761711\n","step: 1180, loss: 0.10982205718755722\n","step: 1190, loss: 0.13803894817829132\n","step: 1200, loss: 0.09022177010774612\n","step: 1210, loss: 0.17676429450511932\n","step: 1220, loss: 0.14678633213043213\n","step: 1230, loss: 0.1137363538146019\n","step: 1240, loss: 0.10955126583576202\n","step: 1250, loss: 0.17576013505458832\n","step: 1260, loss: 0.06460288166999817\n","step: 1270, loss: 0.08124425262212753\n","step: 1280, loss: 0.18842490017414093\n","step: 1290, loss: 0.07156579196453094\n","step: 1300, loss: 0.0937318503856659\n","step: 1310, loss: 0.11791589111089706\n","step: 1320, loss: 0.11213573813438416\n","step: 1330, loss: 0.1205972358584404\n","step: 1340, loss: 0.03315503150224686\n","step: 1350, loss: 0.14561480283737183\n","step: 1360, loss: 0.15071678161621094\n","step: 1370, loss: 0.029452364891767502\n","step: 1380, loss: 0.09767710417509079\n","step: 1390, loss: 0.076642245054245\n","step: 1400, loss: 0.15758204460144043\n","step: 1410, loss: 0.13805146515369415\n","step: 1420, loss: 0.07613072544336319\n","step: 1430, loss: 0.05883702263236046\n","step: 1440, loss: 0.1042267456650734\n","step: 1450, loss: 0.16791637241840363\n","step: 1460, loss: 0.04826176539063454\n","step: 1470, loss: 0.07920809835195541\n","step: 1480, loss: 0.0743391141295433\n","step: 1490, loss: 0.049769796431064606\n","step: 1500, loss: 0.047898177057504654\n","step: 1510, loss: 0.08062388747930527\n","step: 1520, loss: 0.10725688934326172\n","step: 1530, loss: 0.06688559055328369\n","step: 1540, loss: 0.13912233710289001\n","step: 1550, loss: 0.086558997631073\n","step: 1560, loss: 0.05844670161604881\n","step: 1570, loss: 0.11915401369333267\n","step: 1580, loss: 0.08559437096118927\n","step: 1590, loss: 0.12891361117362976\n","step: 1600, loss: 0.08711180835962296\n","step: 1610, loss: 0.0840524435043335\n","step: 1620, loss: 0.09146672487258911\n","step: 1630, loss: 0.04335285350680351\n","step: 1640, loss: 0.07346858084201813\n","step: 1650, loss: 0.03314713016152382\n","step: 1660, loss: 0.050633322447538376\n","step: 1670, loss: 0.1576581448316574\n","step: 1680, loss: 0.07752248644828796\n","step: 1690, loss: 0.16311639547348022\n","step: 1700, loss: 0.07036234438419342\n","step: 1710, loss: 0.15161600708961487\n","step: 1720, loss: 0.11863342672586441\n","step: 1730, loss: 0.0611092671751976\n","step: 1740, loss: 0.08749964833259583\n","step: 1750, loss: 0.11345864832401276\n","step: 1760, loss: 0.13009566068649292\n","step: 1770, loss: 0.024168388918042183\n","step: 1780, loss: 0.17369122803211212\n","step: 1790, loss: 0.08613026142120361\n","step: 1800, loss: 0.09106644988059998\n","step: 1810, loss: 0.10271885246038437\n","step: 1820, loss: 0.09590812772512436\n","step: 1830, loss: 0.1629074513912201\n","step: 1840, loss: 0.08956541121006012\n","step: 1850, loss: 0.08100575953722\n","step: 1860, loss: 0.1098313257098198\n","step: 1870, loss: 0.06564163416624069\n","step: 1880, loss: 0.09985116869211197\n","step: 1890, loss: 0.06693653762340546\n","step: 1900, loss: 0.09704606980085373\n","step: 1910, loss: 0.04858006164431572\n","step: 1920, loss: 0.10064967721700668\n","step: 1930, loss: 0.06562398374080658\n","step: 1940, loss: 0.08715289831161499\n","step: 1950, loss: 0.06435675919055939\n","step: 1960, loss: 0.14429287612438202\n","step: 1970, loss: 0.05516020208597183\n","step: 1980, loss: 0.08646221458911896\n","step: 1990, loss: 0.09747224301099777\n","step: 2000, loss: 0.1867786943912506\n","step: 2010, loss: 0.09857943654060364\n","step: 2020, loss: 0.1790909469127655\n","step: 2030, loss: 0.023266009986400604\n","step: 2040, loss: 0.12789615988731384\n","step: 2050, loss: 0.11991304904222488\n","step: 2060, loss: 0.049014072865247726\n","step: 2070, loss: 0.18624982237815857\n","step: 2080, loss: 0.022510696202516556\n","step: 2090, loss: 0.20056363940238953\n","step: 2100, loss: 0.12725605070590973\n","step: 2110, loss: 0.14217382669448853\n","step: 2120, loss: 0.03421344608068466\n","step: 2130, loss: 0.03772210329771042\n","step: 2140, loss: 0.05190170928835869\n","step: 2150, loss: 0.06566954404115677\n","step: 2160, loss: 0.09650162607431412\n","step: 2170, loss: 0.23588977754116058\n","step: 2180, loss: 0.23280774056911469\n","step: 2190, loss: 0.10114236176013947\n","step: 2200, loss: 0.1558285355567932\n","step: 2210, loss: 0.0662354826927185\n","step: 2220, loss: 0.11460897326469421\n","step: 2230, loss: 0.13232634961605072\n","step: 2240, loss: 0.12287422269582748\n","step: 2250, loss: 0.19775235652923584\n","step: 2260, loss: 0.14264270663261414\n","step: 2270, loss: 0.19640454649925232\n","step: 2280, loss: 0.1189047172665596\n","step: 2290, loss: 0.1125854030251503\n","step: 2300, loss: 0.07784778624773026\n","step: 2310, loss: 0.03589077293872833\n","step: 2320, loss: 0.1286129504442215\n","step: 2330, loss: 0.06751982122659683\n","step: 2340, loss: 0.16683168709278107\n","step: 2350, loss: 0.12961764633655548\n","step: 2360, loss: 0.044570572674274445\n","step: 2370, loss: 0.06042135879397392\n","step: 2380, loss: 0.13657847046852112\n","step: 2390, loss: 0.07328365743160248\n","step: 2400, loss: 0.06136597692966461\n","step: 2410, loss: 0.06741301715373993\n","step: 2420, loss: 0.08711253851652145\n","step: 2430, loss: 0.08946990966796875\n","step: 2440, loss: 0.39912205934524536\n","step: 2450, loss: 0.08723458647727966\n","step: 2460, loss: 0.1450587660074234\n","step: 2470, loss: 0.1853960156440735\n","step: 2480, loss: 0.10709337145090103\n","step: 2490, loss: 0.4344973862171173\n","step: 2500, loss: 0.12028208374977112\n","step: 2510, loss: 0.22775161266326904\n","step: 2520, loss: 0.07484054565429688\n","step: 2530, loss: 0.10556305199861526\n","step: 2540, loss: 0.0915653258562088\n","step: 2550, loss: 0.13261012732982635\n","step: 2560, loss: 0.0799613669514656\n","step: 2570, loss: 0.19729207456111908\n","step: 2580, loss: 0.1433636099100113\n","step: 2590, loss: 0.23665091395378113\n","step: 2600, loss: 0.08891420811414719\n","step: 2610, loss: 0.09108790755271912\n","step: 2620, loss: 0.113950215280056\n","step: 2630, loss: 0.01772182434797287\n","step: 2640, loss: 0.05549832433462143\n","step: 2650, loss: 0.04101906716823578\n","step: 2660, loss: 0.049821678549051285\n","step: 2670, loss: 0.10454326868057251\n","step: 2680, loss: 0.10114255547523499\n","step: 2690, loss: 0.06415384262800217\n","step: 2700, loss: 0.08427173644304276\n","step: 2710, loss: 0.14353987574577332\n","step: 2720, loss: 0.052151959389448166\n","step: 2730, loss: 0.14166733622550964\n","step: 2740, loss: 0.09600361436605453\n","step: 2750, loss: 0.05131056159734726\n","step: 2760, loss: 0.11179926246404648\n","step: 2770, loss: 0.16548074781894684\n","step: 2780, loss: 0.12573997676372528\n","step: 2790, loss: 0.026633113622665405\n","step: 2800, loss: 0.0620577298104763\n","step: 2810, loss: 0.0646447017788887\n","step: 2820, loss: 0.10017158091068268\n","step: 2830, loss: 0.02807389758527279\n","step: 2840, loss: 0.07314694672822952\n","step: 2850, loss: 0.09551045298576355\n","step: 2860, loss: 0.16860704123973846\n","step: 2870, loss: 0.17296242713928223\n","step: 2880, loss: 0.0692569687962532\n","step: 2890, loss: 0.07077432423830032\n","step: 2900, loss: 0.09606269747018814\n","step: 2910, loss: 0.11517591029405594\n","step: 2920, loss: 0.08793877065181732\n","step: 2930, loss: 0.10160357505083084\n","step: 2940, loss: 0.043166130781173706\n","step: 2950, loss: 0.05358267202973366\n","step: 2960, loss: 0.05752001702785492\n","step: 2970, loss: 0.14344274997711182\n","step: 2980, loss: 0.09660910069942474\n","step: 2990, loss: 0.1285666525363922\n","step: 3000, loss: 0.06577885150909424\n","step: 3010, loss: 0.09684621542692184\n","step: 3020, loss: 0.15699274837970734\n","step: 3030, loss: 0.14117445051670074\n","step: 3040, loss: 0.08156778663396835\n","step: 3050, loss: 0.045334283262491226\n","step: 3060, loss: 0.07568240165710449\n","step: 3070, loss: 0.10601527988910675\n","step: 3080, loss: 0.08876781910657883\n","step: 3090, loss: 0.043270595371723175\n","step: 3100, loss: 0.07690684497356415\n","step: 3110, loss: 0.05424857884645462\n","step: 3120, loss: 0.09333451837301254\n","step: 3130, loss: 0.17950262129306793\n","step: 3140, loss: 0.04577825963497162\n","step: 3150, loss: 0.02916146256029606\n","step: 3160, loss: 0.07413601875305176\n","step: 3170, loss: 0.032858479768037796\n","step: 3180, loss: 0.07505582273006439\n","step: 3190, loss: 0.07844186574220657\n","step: 3200, loss: 0.08049201220273972\n","step: 3210, loss: 0.05876930430531502\n","step: 3220, loss: 0.09384690970182419\n","step: 3230, loss: 0.08597778528928757\n","step: 3240, loss: 0.06142423674464226\n","step: 3250, loss: 0.07536137849092484\n","step: 3260, loss: 0.0652252659201622\n","step: 3270, loss: 0.0468437634408474\n","step: 3280, loss: 0.13434116542339325\n","step: 3290, loss: 0.060183752328157425\n","step: 3300, loss: 0.052142851054668427\n","step: 3310, loss: 0.02929014340043068\n","step: 3320, loss: 0.076222725212574\n","step: 3330, loss: 0.12652042508125305\n","step: 3340, loss: 0.09957272559404373\n","step: 3350, loss: 0.118935726583004\n","step: 3360, loss: 0.04274701699614525\n","step: 3370, loss: 0.04207845777273178\n","step: 3380, loss: 0.07568636536598206\n","step: 3390, loss: 0.13116572797298431\n","step: 3400, loss: 0.09593136608600616\n","step: 3410, loss: 0.07705574482679367\n","step: 3420, loss: 0.12655258178710938\n","step: 3430, loss: 0.046609409153461456\n","step: 3440, loss: 0.09866564720869064\n","step: 3450, loss: 0.12987473607063293\n","step: 3460, loss: 0.08086937665939331\n","step: 3470, loss: 0.2341187298297882\n","step: 3480, loss: 0.12824933230876923\n","step: 3490, loss: 0.07289627939462662\n","step: 3500, loss: 0.10010531544685364\n","step: 3510, loss: 0.05943969264626503\n","step: 3520, loss: 0.05921110138297081\n","step: 3530, loss: 0.05081496387720108\n","step: 3540, loss: 0.21390503644943237\n","step: 3550, loss: 0.05591652914881706\n","step: 3560, loss: 0.07160034775733948\n","step: 3570, loss: 0.030861547216773033\n","step: 3580, loss: 0.10210227966308594\n","step: 3590, loss: 0.03655831515789032\n","step: 3600, loss: 0.030283700674772263\n","step: 3610, loss: 0.08607373386621475\n","step: 3620, loss: 0.043818868696689606\n","step: 3630, loss: 0.12691402435302734\n","step: 3640, loss: 0.07906598597764969\n","step: 3650, loss: 0.05566725879907608\n","step: 3660, loss: 0.13325583934783936\n","step: 3670, loss: 0.036481138318777084\n","step: 3680, loss: 0.06394881010055542\n","step: 3690, loss: 0.10422663390636444\n","step: 3700, loss: 0.07816001772880554\n","step: 3710, loss: 0.1067710593342781\n","step: 3720, loss: 0.06330907344818115\n","step: 3730, loss: 0.060858651995658875\n","step: 3740, loss: 0.11681763827800751\n","step: 3750, loss: 0.044493790715932846\n","step: 3760, loss: 0.08518624305725098\n","step: 3770, loss: 0.07338409125804901\n","step: 3780, loss: 0.10427790880203247\n","step: 3790, loss: 0.03930851072072983\n","step: 3800, loss: 0.1720690280199051\n","step: 3810, loss: 0.0813111737370491\n","step: 3820, loss: 0.03674165904521942\n","step: 3830, loss: 0.10858923196792603\n","step: 3840, loss: 0.0821114182472229\n","step: 3850, loss: 0.03127254173159599\n","step: 3860, loss: 0.08035942167043686\n","step: 3870, loss: 0.03724265098571777\n","step: 3880, loss: 0.03747924044728279\n","step: 3890, loss: 0.025412827730178833\n","step: 3900, loss: 0.0786372721195221\n","step: 3910, loss: 0.05754278227686882\n","step: 3920, loss: 0.08583539724349976\n","step: 3930, loss: 0.006483647506684065\n","Difference 1433\n","\n","Loop 2\n","Train from scratch...\n","step: 0, loss: 4.023037910461426\n","step: 10, loss: 1.9026225805282593\n","step: 20, loss: 0.8376230597496033\n","step: 30, loss: 0.4282348155975342\n","step: 40, loss: 0.317433625459671\n","step: 50, loss: 0.14848388731479645\n","step: 60, loss: 0.26646292209625244\n","step: 70, loss: 0.15962114930152893\n","step: 80, loss: 0.2822115421295166\n","step: 90, loss: 0.3364154100418091\n","step: 100, loss: 0.19703207910060883\n","step: 110, loss: 0.2081746757030487\n","step: 120, loss: 0.22448620200157166\n","step: 130, loss: 0.07754936814308167\n","step: 140, loss: 0.09051302075386047\n","step: 150, loss: 0.21084192395210266\n","step: 160, loss: 0.18889115750789642\n","step: 170, loss: 0.12223078310489655\n","step: 180, loss: 0.16235142946243286\n","step: 190, loss: 0.13496312499046326\n","step: 200, loss: 0.07470055669546127\n","step: 210, loss: 0.2076326459646225\n","step: 220, loss: 0.15098188817501068\n","step: 230, loss: 0.1429077833890915\n","step: 240, loss: 0.09910186380147934\n","step: 250, loss: 0.1347576230764389\n","step: 260, loss: 0.14169271290302277\n","step: 270, loss: 0.09965336322784424\n","step: 280, loss: 0.08540874719619751\n","step: 290, loss: 0.15361586213111877\n","step: 300, loss: 0.06905944645404816\n","step: 310, loss: 0.1498498171567917\n","step: 320, loss: 0.10676945745944977\n","step: 330, loss: 0.11718832701444626\n","step: 340, loss: 0.0852939784526825\n","step: 350, loss: 0.06449539959430695\n","step: 360, loss: 0.11070320755243301\n","step: 370, loss: 0.1362110674381256\n","step: 380, loss: 0.06413515657186508\n","step: 390, loss: 0.201212540268898\n","step: 400, loss: 0.15175378322601318\n","step: 410, loss: 0.09731974452733994\n","step: 420, loss: 0.04690191522240639\n","step: 430, loss: 0.09622101485729218\n","step: 440, loss: 0.07208610326051712\n","step: 450, loss: 0.25537294149398804\n","step: 460, loss: 0.13502152264118195\n","step: 470, loss: 0.09103825688362122\n","step: 480, loss: 0.06849156320095062\n","step: 490, loss: 0.049717437475919724\n","step: 500, loss: 0.0523868203163147\n","step: 510, loss: 0.09639693051576614\n","step: 520, loss: 0.08500833064317703\n","step: 530, loss: 0.12121052294969559\n","step: 540, loss: 0.215254545211792\n","step: 550, loss: 0.0678466409444809\n","step: 560, loss: 0.13343840837478638\n","step: 570, loss: 0.24066413938999176\n","step: 580, loss: 0.15519928932189941\n","step: 590, loss: 0.028776412829756737\n","step: 600, loss: 0.1107444167137146\n","step: 610, loss: 0.10426651686429977\n","step: 620, loss: 0.08039897680282593\n","step: 630, loss: 0.022313890978693962\n","step: 640, loss: 0.09535948187112808\n","step: 650, loss: 0.13087992370128632\n","step: 660, loss: 0.09953037649393082\n","step: 670, loss: 0.08005881309509277\n","step: 680, loss: 0.1039360761642456\n","step: 690, loss: 0.17851166427135468\n","step: 700, loss: 0.1418679654598236\n","step: 710, loss: 0.06803277879953384\n","step: 720, loss: 0.07496108114719391\n","step: 730, loss: 0.157895028591156\n","step: 740, loss: 0.08356935530900955\n","step: 750, loss: 0.08263329416513443\n","step: 760, loss: 0.04945424944162369\n","step: 770, loss: 0.1305500566959381\n","step: 780, loss: 0.196305513381958\n","step: 790, loss: 0.06133129075169563\n","step: 800, loss: 0.09632608294487\n","step: 810, loss: 0.13179872930049896\n","step: 820, loss: 0.2105632871389389\n","step: 830, loss: 0.14217200875282288\n","step: 840, loss: 0.06509383022785187\n","step: 850, loss: 0.19762283563613892\n","step: 860, loss: 0.1073932871222496\n","step: 870, loss: 0.06555268913507462\n","step: 880, loss: 0.09930938482284546\n","step: 890, loss: 0.023434527218341827\n","step: 900, loss: 0.3227905333042145\n","step: 910, loss: 0.059376075863838196\n","step: 920, loss: 0.08488483726978302\n","step: 930, loss: 0.1174260750412941\n","step: 940, loss: 0.10112002491950989\n","step: 950, loss: 0.28281018137931824\n","step: 960, loss: 0.15871058404445648\n","step: 970, loss: 0.06434459984302521\n","step: 980, loss: 0.18557249009609222\n","step: 990, loss: 0.025267601013183594\n","step: 1000, loss: 0.014214739203453064\n","step: 1010, loss: 0.04655271768569946\n","step: 1020, loss: 0.19159336388111115\n","step: 1030, loss: 0.0636136531829834\n","step: 1040, loss: 0.08300212025642395\n","step: 1050, loss: 0.08102249354124069\n","step: 1060, loss: 0.05844474211335182\n","step: 1070, loss: 0.0647234246134758\n","step: 1080, loss: 0.056168168783187866\n","step: 1090, loss: 0.09449253976345062\n","step: 1100, loss: 0.09897663444280624\n","step: 1110, loss: 0.07046495378017426\n","step: 1120, loss: 0.09169736504554749\n","step: 1130, loss: 0.08088752627372742\n","step: 1140, loss: 0.1083928719162941\n","step: 1150, loss: 0.10259722918272018\n","step: 1160, loss: 0.08865126967430115\n","step: 1170, loss: 0.10729357600212097\n","step: 1180, loss: 0.04330357536673546\n","step: 1190, loss: 0.09033572673797607\n","step: 1200, loss: 0.14495953917503357\n","step: 1210, loss: 0.029100941494107246\n","step: 1220, loss: 0.11298397183418274\n","step: 1230, loss: 0.04031934589147568\n","step: 1240, loss: 0.16936272382736206\n","step: 1250, loss: 0.035327862948179245\n","step: 1260, loss: 0.09563931077718735\n","step: 1270, loss: 0.11272338777780533\n","step: 1280, loss: 0.19850975275039673\n","step: 1290, loss: 0.07959157228469849\n","step: 1300, loss: 0.11036669462919235\n","step: 1310, loss: 0.1906016618013382\n","step: 1320, loss: 0.044609375298023224\n","step: 1330, loss: 0.12281522899866104\n","step: 1340, loss: 0.08532985299825668\n","step: 1350, loss: 0.0985817089676857\n","step: 1360, loss: 0.16192342340946198\n","step: 1370, loss: 0.14080898463726044\n","step: 1380, loss: 0.12691576778888702\n","step: 1390, loss: 0.1368417739868164\n","step: 1400, loss: 0.17334844172000885\n","step: 1410, loss: 0.04438256844878197\n","step: 1420, loss: 0.05626317858695984\n","step: 1430, loss: 0.07240165770053864\n","step: 1440, loss: 0.13958513736724854\n","step: 1450, loss: 0.10188542306423187\n","step: 1460, loss: 0.10420306026935577\n","step: 1470, loss: 0.13075228035449982\n","step: 1480, loss: 0.08680036664009094\n","step: 1490, loss: 0.2307136207818985\n","step: 1500, loss: 0.1456208974123001\n","step: 1510, loss: 0.10587761551141739\n","step: 1520, loss: 0.082852803170681\n","step: 1530, loss: 0.044053174555301666\n","step: 1540, loss: 0.06628160178661346\n","step: 1550, loss: 0.028055807575583458\n","step: 1560, loss: 0.05707419291138649\n","step: 1570, loss: 0.049172792583703995\n","step: 1580, loss: 0.08211681991815567\n","step: 1590, loss: 0.0779753103852272\n","step: 1600, loss: 0.07053081691265106\n","step: 1610, loss: 0.08137425035238266\n","step: 1620, loss: 0.07009102404117584\n","step: 1630, loss: 0.056870367377996445\n","step: 1640, loss: 0.05194248631596565\n","step: 1650, loss: 0.12861771881580353\n","step: 1660, loss: 0.12767904996871948\n","step: 1670, loss: 0.07805090397596359\n","step: 1680, loss: 0.08116122335195541\n","step: 1690, loss: 0.05784916132688522\n","step: 1700, loss: 0.10184191167354584\n","step: 1710, loss: 0.17034593224525452\n","step: 1720, loss: 0.12220809608697891\n","step: 1730, loss: 0.08989784866571426\n","step: 1740, loss: 0.04113175347447395\n","step: 1750, loss: 0.09381220489740372\n","step: 1760, loss: 0.06019905209541321\n","step: 1770, loss: 0.12828657031059265\n","step: 1780, loss: 0.11890994012355804\n","step: 1790, loss: 0.055851273238658905\n","step: 1800, loss: 0.05703539773821831\n","step: 1810, loss: 0.036728933453559875\n","step: 1820, loss: 0.1778714805841446\n","step: 1830, loss: 0.09883999079465866\n","step: 1840, loss: 0.14481940865516663\n","step: 1850, loss: 0.03454527631402016\n","step: 1860, loss: 0.0903983861207962\n","step: 1870, loss: 0.07541736215353012\n","step: 1880, loss: 0.15345269441604614\n","step: 1890, loss: 0.1004408523440361\n","step: 1900, loss: 0.1141374409198761\n","step: 1910, loss: 0.10510475188493729\n","step: 1920, loss: 0.08719617128372192\n","step: 1930, loss: 0.15946584939956665\n","step: 1940, loss: 0.08123812824487686\n","step: 1950, loss: 0.1250121146440506\n","step: 1960, loss: 0.08539152145385742\n","step: 1970, loss: 0.12282601743936539\n","step: 1980, loss: 0.12170613557100296\n","step: 1990, loss: 0.06760403513908386\n","step: 2000, loss: 0.07769172638654709\n","step: 2010, loss: 0.12541671097278595\n","step: 2020, loss: 0.10388997942209244\n","step: 2030, loss: 0.10935607552528381\n","step: 2040, loss: 0.09991072863340378\n","step: 2050, loss: 0.0866689458489418\n","step: 2060, loss: 0.08521972596645355\n","step: 2070, loss: 0.21349795162677765\n","step: 2080, loss: 0.16279847919940948\n","step: 2090, loss: 0.12558147311210632\n","step: 2100, loss: 0.025414153933525085\n","step: 2110, loss: 0.05222630128264427\n","step: 2120, loss: 0.06701011955738068\n","step: 2130, loss: 0.09472686052322388\n","step: 2140, loss: 0.0683240219950676\n","step: 2150, loss: 0.1094699278473854\n","step: 2160, loss: 0.09125056862831116\n","step: 2170, loss: 0.09052639454603195\n","step: 2180, loss: 0.11994610726833344\n","step: 2190, loss: 0.16014021635055542\n","step: 2200, loss: 0.09297611564397812\n","step: 2210, loss: 0.03523079678416252\n","step: 2220, loss: 0.0849677175283432\n","step: 2230, loss: 0.12114770710468292\n","step: 2240, loss: 0.05816328898072243\n","step: 2250, loss: 0.19554434716701508\n","step: 2260, loss: 0.11079304665327072\n","step: 2270, loss: 0.059090785682201385\n","step: 2280, loss: 0.07878609746694565\n","step: 2290, loss: 0.15644299983978271\n","step: 2300, loss: 0.09830118715763092\n","step: 2310, loss: 0.11984845250844955\n","step: 2320, loss: 0.15810492634773254\n","step: 2330, loss: 0.054916031658649445\n","step: 2340, loss: 0.10985779017210007\n","step: 2350, loss: 0.05301913246512413\n","step: 2360, loss: 0.09240575134754181\n","step: 2370, loss: 0.0719488114118576\n","step: 2380, loss: 0.06299234926700592\n","step: 2390, loss: 0.04241731017827988\n","step: 2400, loss: 0.12399596720933914\n","step: 2410, loss: 0.11141671240329742\n","step: 2420, loss: 0.1063387542963028\n","step: 2430, loss: 0.1494450569152832\n","step: 2440, loss: 0.06797623634338379\n","step: 2450, loss: 0.08099543303251266\n","step: 2460, loss: 0.0734385997056961\n","step: 2470, loss: 0.12844568490982056\n","step: 2480, loss: 0.16980306804180145\n","step: 2490, loss: 0.11890289932489395\n","step: 2500, loss: 0.08655745536088943\n","step: 2510, loss: 0.128396138548851\n","step: 2520, loss: 0.11247271299362183\n","step: 2530, loss: 0.08192238956689835\n","step: 2540, loss: 0.2638646364212036\n","step: 2550, loss: 0.05177640542387962\n","step: 2560, loss: 0.0678430050611496\n","step: 2570, loss: 0.06747619807720184\n","step: 2580, loss: 0.10767322033643723\n","step: 2590, loss: 0.06306304037570953\n","step: 2600, loss: 0.16501477360725403\n","step: 2610, loss: 0.06279817223548889\n","step: 2620, loss: 0.028670480474829674\n","step: 2630, loss: 0.04278136044740677\n","step: 2640, loss: 0.06192571297287941\n","step: 2650, loss: 0.14791224896907806\n","step: 2660, loss: 0.11542671918869019\n","step: 2670, loss: 0.05523276329040527\n","step: 2680, loss: 0.1013602614402771\n","step: 2690, loss: 0.07682083547115326\n","step: 2700, loss: 0.017956074327230453\n","step: 2710, loss: 0.11889144033193588\n","step: 2720, loss: 0.09820695966482162\n","step: 2730, loss: 0.1313898116350174\n","step: 2740, loss: 0.02068421058356762\n","step: 2750, loss: 0.08018728345632553\n","step: 2760, loss: 0.08637027442455292\n","step: 2770, loss: 0.09036421775817871\n","step: 2780, loss: 0.10525918006896973\n","step: 2790, loss: 0.03462671488523483\n","step: 2800, loss: 0.15224555134773254\n","step: 2810, loss: 0.1860506236553192\n","step: 2820, loss: 0.046656347811222076\n","step: 2830, loss: 0.12848816812038422\n","step: 2840, loss: 0.06680864095687866\n","step: 2850, loss: 0.09835662692785263\n","step: 2860, loss: 0.03578122332692146\n","step: 2870, loss: 0.1274159699678421\n","step: 2880, loss: 0.09154638648033142\n","step: 2890, loss: 0.09822249412536621\n","step: 2900, loss: 0.03785925731062889\n","step: 2910, loss: 0.1239205077290535\n","step: 2920, loss: 0.14595726132392883\n","step: 2930, loss: 0.06267417967319489\n","step: 2940, loss: 0.04810479283332825\n","step: 2950, loss: 0.10562868416309357\n","step: 2960, loss: 0.14192286133766174\n","step: 2970, loss: 0.04432667791843414\n","step: 2980, loss: 0.08340224623680115\n","step: 2990, loss: 0.016989421099424362\n","step: 3000, loss: 0.15353205800056458\n","step: 3010, loss: 0.03745047003030777\n","step: 3020, loss: 0.09962285310029984\n","step: 3030, loss: 0.1535813808441162\n","step: 3040, loss: 0.06533868610858917\n","step: 3050, loss: 0.09649509936571121\n","step: 3060, loss: 0.1060536801815033\n","step: 3070, loss: 0.09866393357515335\n","step: 3080, loss: 0.05897928401827812\n","step: 3090, loss: 0.06283999979496002\n","step: 3100, loss: 0.030231034383177757\n","step: 3110, loss: 0.04629908874630928\n","step: 3120, loss: 0.0915437713265419\n","step: 3130, loss: 0.08816474676132202\n","step: 3140, loss: 0.15107832849025726\n","step: 3150, loss: 0.041664984077215195\n","step: 3160, loss: 0.02093237265944481\n","step: 3170, loss: 0.15065719187259674\n","step: 3180, loss: 0.09667613357305527\n","step: 3190, loss: 0.03547956049442291\n","step: 3200, loss: 0.05387582257390022\n","step: 3210, loss: 0.058182451874017715\n","step: 3220, loss: 0.0761776939034462\n","step: 3230, loss: 0.11677441000938416\n","step: 3240, loss: 0.08192799985408783\n","step: 3250, loss: 0.058253783732652664\n","step: 3260, loss: 0.023225238546729088\n","step: 3270, loss: 0.16747866570949554\n","step: 3280, loss: 0.09848833084106445\n","step: 3290, loss: 0.0520760715007782\n","step: 3300, loss: 0.07037317007780075\n","step: 3310, loss: 0.07641153782606125\n","step: 3320, loss: 0.10130831599235535\n","step: 3330, loss: 0.05658446624875069\n","step: 3340, loss: 0.030161108821630478\n","step: 3350, loss: 0.0484684556722641\n","step: 3360, loss: 0.13810279965400696\n","step: 3370, loss: 0.07502789050340652\n","step: 3380, loss: 0.09498557448387146\n","step: 3390, loss: 0.24648094177246094\n","step: 3400, loss: 0.04541489854454994\n","step: 3410, loss: 0.12019399553537369\n","step: 3420, loss: 0.11457549780607224\n","step: 3430, loss: 0.08861501514911652\n","step: 3440, loss: 0.04398011416196823\n","step: 3450, loss: 0.0880354791879654\n","step: 3460, loss: 0.05996861681342125\n","step: 3470, loss: 0.1153927594423294\n","step: 3480, loss: 0.04260959103703499\n","step: 3490, loss: 0.11772438883781433\n","step: 3500, loss: 0.10619064420461655\n","step: 3510, loss: 0.022400179877877235\n","step: 3520, loss: 0.16920533776283264\n","step: 3530, loss: 0.08782964199781418\n","step: 3540, loss: 0.16928772628307343\n","step: 3550, loss: 0.07579950988292694\n","step: 3560, loss: 0.12561307847499847\n","step: 3570, loss: 0.24556584656238556\n","step: 3580, loss: 0.10821836441755295\n","step: 3590, loss: 0.03131217882037163\n","step: 3600, loss: 0.11777157336473465\n","step: 3610, loss: 0.12139356881380081\n","step: 3620, loss: 0.0820365697145462\n","step: 3630, loss: 0.06613312661647797\n","step: 3640, loss: 0.08099649846553802\n","step: 3650, loss: 0.07229289412498474\n","step: 3660, loss: 0.09156807512044907\n","step: 3670, loss: 0.08434920012950897\n","step: 3680, loss: 0.1080627590417862\n","step: 3690, loss: 0.07179590314626694\n","step: 3700, loss: 0.10301876813173294\n","step: 3710, loss: 0.08709706366062164\n","step: 3720, loss: 0.01797475293278694\n","step: 3730, loss: 0.07699985802173615\n","step: 3740, loss: 0.192225843667984\n","step: 3750, loss: 0.0765223279595375\n","step: 3760, loss: 0.09034573286771774\n","step: 3770, loss: 0.04452905431389809\n","step: 3780, loss: 0.08220605552196503\n","step: 3790, loss: 0.031703315675258636\n","step: 3800, loss: 0.11918724328279495\n","step: 3810, loss: 0.18437956273555756\n","step: 3820, loss: 0.1042080670595169\n","step: 3830, loss: 0.05445795878767967\n","step: 3840, loss: 0.037565678358078\n","step: 3850, loss: 0.057006318122148514\n","step: 3860, loss: 0.0732172355055809\n","step: 3870, loss: 0.0427035391330719\n","step: 3880, loss: 0.04462813958525658\n","step: 3890, loss: 0.15540584921836853\n","step: 3900, loss: 0.08239948004484177\n","step: 3910, loss: 0.10830771923065186\n","step: 3920, loss: 0.08516515791416168\n","step: 3930, loss: 0.08727241307497025\n","Difference 1306\n"]}]},{"cell_type":"code","source":["print(domain_precision_value_lst)\n","print(domain_recall_value_lst)\n","print(domain_f1_value_lst)\n","print(domain_acc_value_lst)\n","\n","print(acc_lst)\n","print(prob_lst)"],"metadata":{"id":"W-t80AQ7wb4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"g4ZuWMYTojmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"],"metadata":{"id":"jJxcl1cLn7rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"2Ngux9I-orPX","executionInfo":{"status":"ok","timestamp":1669730972026,"user_tz":300,"elapsed":1284,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/","height":542},"outputId":"b2847ec5-c771-4248-e268-3c3ff7d9b7cb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"c4bd2f4a-5a86-4d2f-a48b-723cb78d7330\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c4bd2f4a-5a86-4d2f-a48b-723cb78d7330\")) {                    Plotly.newPlot(                        \"c4bd2f4a-5a86-4d2f-a48b-723cb78d7330\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2],\"xaxis\":\"x\",\"y\":[0.9122195839881897,0.9224042892456055,0.9226545095443726],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2],\"xaxis\":\"x\",\"y\":[0.9050920009613037,0.9078368544578552,0.9050920009613037],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2],\"xaxis\":\"x\",\"y\":[0.9023036956787109,0.9081002473831177,0.9057203531265259],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c4bd2f4a-5a86-4d2f-a48b-723cb78d7330');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"uEACZ4DaGGRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"O2DFUhCDorSf","executionInfo":{"status":"ok","timestamp":1668116215977,"user_tz":300,"elapsed":995,"user":{"displayName":"Zheng Wu","userId":"01190949623920087412"}},"outputId":"7f39a8f8-220a-4930-8f04-d14c44fe2152"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"b1b1c453-8c4a-4fd0-97f8-11fb284592fb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b1b1c453-8c4a-4fd0-97f8-11fb284592fb\")) {                    Plotly.newPlot(                        \"b1b1c453-8c4a-4fd0-97f8-11fb284592fb\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],\"xaxis\":\"x\",\"y\":[0.8149770917992446,0.787162347107309,0.8007413494963384,0.7818423788455493,0.8169540726170971,0.7857853580214625,0.7951625860586435,0.7928418332562367,0.8079142356743158,0.8057454208799963,0.7730580799805111,0.7721888456648999,0.783876736142003,0.8157896094782606,0.8122934539977998,0.7956948256483566,0.8155514223113872,0.7943292718213456,0.8056764443686836,0.8103274438386009,0.8046654586212678,0.7837218348816499,0.7842776630517582,0.815831903287871,0.7632632444671331,0.8052006995624871,0.7938599579904194],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],\"xaxis\":\"x\",\"y\":[0.8263669527156633,0.7985081569503201,0.8049504833888047,0.7886884136982024,0.8153922016991278,0.8055932070892023,0.7904048467234799,0.7902589691714964,0.8054297587716377,0.8294103026430311,0.7759278432524409,0.8232545580828771,0.8136427284771007,0.8050740063729277,0.8372759211027732,0.7953340116638705,0.8275545437326359,0.811794723133119,0.8161215875448494,0.8299756725988467,0.7971686042913824,0.7995056849702782,0.801737243279009,0.8358335257419224,0.8103046730433128,0.8038117354143534,0.8228633640116804],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],\"xaxis\":\"x\",\"y\":[0.7903634316049043,0.7768367246495933,0.7749776960832627,0.7574053587360632,0.785884346972887,0.7777079854366554,0.7649574331375284,0.7681709640369782,0.7849515320789118,0.7969772158372213,0.7541348390004601,0.778288484683667,0.7807595005850504,0.7885157495507905,0.8062150034988945,0.7752331609945883,0.8035894908555179,0.7888715437456876,0.7820441559046298,0.7983561072078335,0.7758636965596218,0.7728378566631627,0.7745888548644229,0.8044292536279628,0.7732149732644567,0.7863999759737169,0.7834516182623232],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b1b1c453-8c4a-4fd0-97f8-11fb284592fb');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]}]}