{"cells":[{"cell_type":"markdown","metadata":{"id":"F_LX9XAD32So"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11740,"status":"ok","timestamp":1669402421167,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"C3soh1b03deD","outputId":"0d69606c-bb69-4db9-f08e-ca7c292913fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 31.0 MB/s \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.26.16-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 62.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting botocore<1.30.0,>=1.29.16\n","  Downloading botocore-1.29.16-py3-none-any.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 49.2 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 68.6 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.16->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.16->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 70.1 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.16 botocore-1.29.16 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-JWWLtCFnK5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XvA751OFiEB"},"outputs":[],"source":["import os\n","import codecs\n","\n","def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)\n","\n","\n","def read_data(data_file):\n","    word_lst = []\n","    tag_lst = []\n","    tags = []\n","    for word, tag in read_conll_file(data_file):\n","        word_lst.append(word)\n","        tag_lst.append(tag)\n","        tags.extend(tag)\n","    print(\"The number of samples:\", len(word_lst))\n","    print(\"The number of tags\", len(set(tags)))\n","    return word_lst, tag_lst, list(set(tags))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20691,"status":"ok","timestamp":1669402470608,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"jIFha6OOht8L","outputId":"3bae0e6f-a412-4a6c-a619-92e6b79238ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","\n","\n","data_dir = \"/content/drive/MyDrive/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Capstone/model\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPKysG3i3nsR"},"outputs":[],"source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55njl5OrqKWk"},"outputs":[],"source":["import os\n","import codecs\n","\n","\n","def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)\n","\n","\n","def read_data(data_file):\n","    word_lst = []\n","    tag_lst = []\n","    tags = []\n","    for word, tag in read_conll_file(data_file):\n","        word_lst.append(word)\n","        tag_lst.append(tag)\n","        tags.extend(tag)\n","    print(\"The number of samples:\", len(word_lst))\n","    print(\"The number of tags\", len(set(tags)))\n","    return word_lst, tag_lst, list(set(tags))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHsUvJCIF0zx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5500,"status":"ok","timestamp":1669402476105,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"0KqectYYC30N","outputId":"7734c50b-05f7-4b00-fd70-492ce89c05e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}],"source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1715,"status":"ok","timestamp":1669402477817,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"3merONNCukFt","outputId":"c2582cd5-0305-4699-d31a-0c33b791215f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669402477817,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"CgySDvNl3xkh","outputId":"af5b137d-9bd2-434e-83b5-99a07ba33a8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["49\n"]}],"source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"]},{"cell_type":"markdown","metadata":{"id":"V9yUXS679IFc"},"source":["# Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nIm4vqm3xiL"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zleK0sd96JRp"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2917,"status":"ok","timestamp":1669402483360,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"6fRrkkC26JP_","outputId":"eb14434b-6a0c-4804-fe61-f53fd02ee793"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 213450/213450 [00:00<00:00, 236677.35B/s]\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpKgRRbK6JMr"},"outputs":[],"source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZ_JndBu6LjA"},"outputs":[],"source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mthfoFt6JFJ"},"outputs":[],"source":["from pytorch_pretrained_bert import BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6ydlTI16JCz"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeD_19uq6JAd"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW4KvG4x6I91"},"outputs":[],"source":["def eval(model, iterator, average=\"macro\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n","    precision_value = precision_score(y_true, y_pred, average=average)\n","    recall_value = recall_score(y_true, y_pred, average=average)\n","    f1_value = f1_score(y_true, y_pred, average=average)\n","\n","    return precision_value, recall_value, f1_value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45503,"status":"ok","timestamp":1669402528853,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"0ZDK1-UU6I5K","outputId":"51ed779f-3eac-41f6-f6d5-9b4f94b95ab5"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 404400730/404400730 [00:32<00:00, 12309664.81B/s]\n"]}],"source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5pQmdTS6I20"},"outputs":[],"source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0x3gRfi9iyA"},"outputs":[],"source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"]},{"cell_type":"markdown","metadata":{"id":"6fVp31VJ5U64"},"source":["# Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtVeE3zd04C8"},"outputs":[],"source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# torch.save(model.state_dict(), model_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNelrj19HGno"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"cyvpy9QH4sQd"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20412,"status":"ok","timestamp":1669402949315,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"hAD3Wd574v6Q","outputId":"744e71d3-efb0-4dac-aa2d-44e037d3b44f"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc=0.97\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       178\n","           2       1.00      1.00      1.00       352\n","           3       1.00      1.00      1.00      2000\n","           4       1.00      1.00      1.00        60\n","           5       1.00      1.00      1.00        60\n","           6       1.00      1.00      1.00      1613\n","           7       1.00      1.00      1.00       223\n","           9       1.00      0.99      1.00       935\n","          10       0.98      1.00      0.99      1266\n","          11       0.99      1.00      0.99      3309\n","          12       1.00      1.00      1.00        46\n","          13       1.00      0.20      0.33        20\n","          14       1.00      0.99      1.00       511\n","          15       0.97      0.99      0.98      4250\n","          16       0.97      0.89      0.93      2423\n","          17       0.96      0.93      0.94       139\n","          18       0.92      0.93      0.93        73\n","          19       1.00      0.75      0.86         4\n","          20       1.00      0.99      1.00       413\n","          22       0.98      0.98      0.98      5545\n","          23       0.99      0.96      0.97      4133\n","          24       0.21      0.71      0.32        45\n","          25       0.99      0.98      0.98      2316\n","          26       0.71      1.00      0.83        15\n","          27       0.99      0.99      0.99       373\n","          28       1.00      0.99      0.99       766\n","          29       0.99      1.00      1.00       357\n","          30       0.95      0.91      0.93      1405\n","          31       0.88      0.85      0.86        82\n","          32       0.85      0.89      0.87        37\n","          33       0.85      0.87      0.86       126\n","          34       0.75      0.82      0.78        11\n","          35       0.99      0.99      0.99       588\n","          36       0.88      1.00      0.93         7\n","          37       0.98      0.98      0.98      1124\n","          38       0.99      0.96      0.98      1162\n","          39       0.93      0.97      0.95       608\n","          40       0.82      0.99      0.90       829\n","          41       0.97      0.98      0.97       563\n","          42       0.98      0.99      0.99       873\n","          43       0.93      0.97      0.95       191\n","          44       1.00      1.00      1.00        90\n","          45       1.00      1.00      1.00        14\n","          46       1.00      0.99      0.99        89\n","          48       1.00      1.00      1.00       366\n","\n","    accuracy                           0.97     39590\n","   macro avg       0.94      0.94      0.93     39590\n","weighted avg       0.98      0.97      0.98     39590\n","\n"]}],"source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value = eval(model, test_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669402949315,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"2M6AkbHMD6VJ","outputId":"b70d0654-6a35-4537-ccf4-e9b6f6393434"},"outputs":[{"data":{"text/plain":["(0.9417027899389416, 0.9425258210459151, 0.9318435575593024)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["wsj_precision_value, wsj_recall_value, wsj_f1_value"]},{"cell_type":"markdown","metadata":{"id":"reoycWJi5azd"},"source":["# Self Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"agIHM1TmEYl3"},"outputs":[],"source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGm3QLNcD8bU"},"outputs":[],"source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwvivWyzEHOi"},"outputs":[],"source":["domain = \"emails\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1540,"status":"ok","timestamp":1669402950843,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"UHAOs-fdEHMO","outputId":"71727c70-7216-407d-ddc0-54a1f4ff418f"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of samples: 2450\n","The number of tags 49\n","The number of samples: 2450\n","The number of tags 48\n","after filter tag 2427\n","after filter tag 2402\n"]}],"source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JGnpmRNHHmz"},"outputs":[],"source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []"]},{"cell_type":"code","source":[],"metadata":{"id":"wC5gohxgq8hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8989,"status":"ok","timestamp":1669402959830,"user":{"displayName":"yue zonghan","userId":"12845997756417594752"},"user_tz":300},"id":"cHb8ZM-VjG-7","outputId":"6463b5ee-bdac-4f76-c4f0-7e421cfa5fcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.94      0.86        35\n","           2       0.87      0.52      0.65        77\n","           3       1.00      0.79      0.88      1030\n","           4       1.00      0.84      0.91       291\n","           5       0.91      0.84      0.87       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.98       689\n","          10       0.93      0.98      0.96       901\n","          11       0.96      1.00      0.98      2111\n","          12       0.98      0.96      0.97        47\n","          13       0.60      0.46      0.52        13\n","          14       0.28      1.00      0.44        43\n","          15       0.93      0.98      0.95      2778\n","          16       0.90      0.80      0.85      1151\n","          17       0.91      0.95      0.93        41\n","          18       0.94      1.00      0.97        32\n","          19       0.68      0.57      0.62        40\n","          20       1.00      0.96      0.98       584\n","          21       0.00      0.00      0.00        52\n","          22       0.93      0.75      0.83      4175\n","          23       0.73      0.95      0.83      2253\n","          24       0.37      0.68      0.48        44\n","          25       0.87      0.92      0.89       888\n","          26       1.00      0.78      0.88         9\n","          27       0.71      0.99      0.82        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.81      0.86      0.83      1136\n","          31       0.55      0.63      0.59        19\n","          32       1.00      0.75      0.86         8\n","          33       0.75      0.97      0.84        86\n","          34       0.19      0.38      0.25        32\n","          35       0.97      1.00      0.98       474\n","          36       1.00      0.14      0.24       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.98      0.96      0.97       404\n","          39       0.97      0.93      0.95       485\n","          40       0.90      0.98      0.94       573\n","          41       0.93      0.94      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.94      0.89      0.92       152\n","          44       0.87      0.92      0.90        75\n","          46       0.96      0.99      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.79      0.80      0.78     28417\n","weighted avg       0.91      0.91      0.90     28417\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1HrHo0LEhWN"},"outputs":[],"source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzWOF5sCGjgC"},"outputs":[],"source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","          \n","          # Rank by LLD\n","          # lld = torch.prod(max_prob, 1)\n","          # LLD.extend(lld)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn]\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","\n","  return new_train_x, new_train_y, remain_train_x, remain_train_y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"zl3VMmnVVD-8","outputId":"6ab2730a-2df3-4352-ee0d-d493fb494af8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Loop 1\n","Train from scratch...\n","step: 0, loss: 4.036646842956543\n","step: 10, loss: 2.025042772293091\n","step: 20, loss: 0.7598252892494202\n","step: 30, loss: 0.38657280802726746\n","step: 40, loss: 0.20626065135002136\n","step: 50, loss: 0.3390898108482361\n","step: 60, loss: 0.22518108785152435\n","step: 70, loss: 0.2822517156600952\n","step: 80, loss: 0.15507148206233978\n","step: 90, loss: 0.21990534663200378\n","step: 100, loss: 0.2163505107164383\n","step: 110, loss: 0.17872563004493713\n","step: 120, loss: 0.1065683662891388\n","step: 130, loss: 0.14126498997211456\n","step: 140, loss: 0.0858919769525528\n","step: 150, loss: 0.12172899395227432\n","step: 160, loss: 0.14077410101890564\n","step: 170, loss: 0.11176017671823502\n","step: 180, loss: 0.1498749852180481\n","step: 190, loss: 0.05982294678688049\n","step: 200, loss: 0.11438219249248505\n","step: 210, loss: 0.1178639829158783\n","step: 220, loss: 0.09368232637643814\n","step: 230, loss: 0.07843580096960068\n","step: 240, loss: 0.06928496062755585\n","step: 250, loss: 0.10336120426654816\n","step: 260, loss: 0.19269971549510956\n","step: 270, loss: 0.11329326033592224\n","step: 280, loss: 0.1206568256020546\n","step: 290, loss: 0.22482189536094666\n","step: 300, loss: 0.12289758771657944\n","step: 310, loss: 0.0959017276763916\n","step: 320, loss: 0.0781879872083664\n","step: 330, loss: 0.1245792880654335\n","step: 340, loss: 0.07984928786754608\n","step: 350, loss: 0.1598571538925171\n","step: 360, loss: 0.1899379938840866\n","step: 370, loss: 0.06883639097213745\n","step: 380, loss: 0.17338606715202332\n","step: 390, loss: 0.0903920829296112\n","step: 400, loss: 0.2046608179807663\n","step: 410, loss: 0.06630569696426392\n","step: 420, loss: 0.12299631536006927\n","step: 430, loss: 0.20552310347557068\n","step: 440, loss: 0.06830732524394989\n","step: 450, loss: 0.08715558052062988\n","step: 460, loss: 0.056254904717206955\n","step: 470, loss: 0.08904749900102615\n","step: 480, loss: 0.08176547288894653\n","step: 490, loss: 0.1675068438053131\n","step: 500, loss: 0.08102810382843018\n","step: 510, loss: 0.1700776070356369\n","step: 520, loss: 0.11776430159807205\n","step: 530, loss: 0.09016650170087814\n","step: 540, loss: 0.10448647290468216\n","step: 550, loss: 0.08936086297035217\n","step: 560, loss: 0.06819207221269608\n","step: 570, loss: 0.09126592427492142\n","step: 580, loss: 0.09309769421815872\n","step: 590, loss: 0.022907059639692307\n","step: 600, loss: 0.0699848085641861\n","step: 610, loss: 0.05870833992958069\n","step: 620, loss: 0.05367228016257286\n","step: 630, loss: 0.11115942150354385\n","step: 640, loss: 0.0801171213388443\n","step: 650, loss: 0.1526401787996292\n","step: 660, loss: 0.06891927123069763\n","step: 670, loss: 0.06385968625545502\n","step: 680, loss: 0.19710782170295715\n","step: 690, loss: 0.053139932453632355\n","step: 700, loss: 0.11552206426858902\n","step: 710, loss: 0.11212554574012756\n","step: 720, loss: 0.056406985968351364\n","step: 730, loss: 0.04414013773202896\n","step: 740, loss: 0.24414199590682983\n","step: 750, loss: 0.13082998991012573\n","step: 760, loss: 0.17046934366226196\n","step: 770, loss: 0.13275526463985443\n","step: 780, loss: 0.11394895613193512\n","step: 790, loss: 0.13839390873908997\n","step: 800, loss: 0.09279867261648178\n","step: 810, loss: 0.1009935587644577\n","step: 820, loss: 0.15938925743103027\n","step: 830, loss: 0.07455538958311081\n","step: 840, loss: 0.140216663479805\n","step: 850, loss: 0.1262824386358261\n","step: 860, loss: 0.04136550799012184\n","step: 870, loss: 0.10158059746026993\n","step: 880, loss: 0.08138644695281982\n","step: 890, loss: 0.07691627740859985\n","step: 900, loss: 0.08060096949338913\n","step: 910, loss: 0.08299554139375687\n","step: 920, loss: 0.06619616597890854\n","step: 930, loss: 0.13611121475696564\n","step: 940, loss: 0.08532946556806564\n","step: 950, loss: 0.054510001093149185\n","step: 960, loss: 0.14635193347930908\n","step: 970, loss: 0.1002502366900444\n","step: 980, loss: 0.15724670886993408\n","step: 990, loss: 0.16516269743442535\n","step: 1000, loss: 0.03713079169392586\n","step: 1010, loss: 0.16322802007198334\n","step: 1020, loss: 0.04028395563364029\n","step: 1030, loss: 0.08689345419406891\n","step: 1040, loss: 0.24184918403625488\n","step: 1050, loss: 0.07891117036342621\n","step: 1060, loss: 0.132819265127182\n","step: 1070, loss: 0.11606123298406601\n","step: 1080, loss: 0.06866312026977539\n","step: 1090, loss: 0.17491407692432404\n","step: 1100, loss: 0.10185099393129349\n","step: 1110, loss: 0.14467790722846985\n","step: 1120, loss: 0.10934644937515259\n","step: 1130, loss: 0.0664498582482338\n","step: 1140, loss: 0.041180677711963654\n","step: 1150, loss: 0.1589885950088501\n","step: 1160, loss: 0.17741115391254425\n","step: 1170, loss: 0.1048617735505104\n","step: 1180, loss: 0.06214837729930878\n","step: 1190, loss: 0.07569701224565506\n","step: 1200, loss: 0.06399490684270859\n","step: 1210, loss: 0.15350854396820068\n","step: 1220, loss: 0.04597252979874611\n","step: 1230, loss: 0.07947245240211487\n","step: 1240, loss: 0.0632864385843277\n","step: 1250, loss: 0.0798601433634758\n","step: 1260, loss: 0.1150628924369812\n","step: 1270, loss: 0.14463868737220764\n","step: 1280, loss: 0.026329634711146355\n","step: 1290, loss: 0.1514071226119995\n","step: 1300, loss: 0.1277136504650116\n","step: 1310, loss: 0.10422968119382858\n","step: 1320, loss: 0.0424296036362648\n","step: 1330, loss: 0.07071182876825333\n","step: 1340, loss: 0.06054198741912842\n","step: 1350, loss: 0.12635739147663116\n","step: 1360, loss: 0.14816907048225403\n","step: 1370, loss: 0.03651672974228859\n","step: 1380, loss: 0.14299628138542175\n","step: 1390, loss: 0.06279007345438004\n","step: 1400, loss: 0.07222294807434082\n","step: 1410, loss: 0.0660969614982605\n","step: 1420, loss: 0.0970938503742218\n","step: 1430, loss: 0.10499139130115509\n","step: 1440, loss: 0.036118634045124054\n","step: 1450, loss: 0.20100612938404083\n","step: 1460, loss: 0.03565109893679619\n","step: 1470, loss: 0.1391681283712387\n","step: 1480, loss: 0.06327465176582336\n","step: 1490, loss: 0.055968962609767914\n","step: 1500, loss: 0.1456674337387085\n","step: 1510, loss: 0.05916544795036316\n","step: 1520, loss: 0.23171839118003845\n","step: 1530, loss: 0.10838279873132706\n","step: 1540, loss: 0.08786056190729141\n","step: 1550, loss: 0.0745195522904396\n","step: 1560, loss: 0.08819784224033356\n","step: 1570, loss: 0.0671805590391159\n","step: 1580, loss: 0.12258965522050858\n","step: 1590, loss: 0.07129382342100143\n","step: 1600, loss: 0.11757026612758636\n","step: 1610, loss: 0.030850449576973915\n","step: 1620, loss: 0.2476460486650467\n","step: 1630, loss: 0.15596675872802734\n","step: 1640, loss: 0.0548061728477478\n","step: 1650, loss: 0.18725284934043884\n","step: 1660, loss: 0.13024847209453583\n","step: 1670, loss: 0.09382110089063644\n","step: 1680, loss: 0.09199751168489456\n","step: 1690, loss: 0.09068398177623749\n","step: 1700, loss: 0.03881179541349411\n","step: 1710, loss: 0.04855511710047722\n","step: 1720, loss: 0.025920161977410316\n","step: 1730, loss: 0.1781681925058365\n","step: 1740, loss: 0.1282721608877182\n","step: 1750, loss: 0.09665180742740631\n","step: 1760, loss: 0.02602790854871273\n","step: 1770, loss: 0.06789951771497726\n","step: 1780, loss: 0.06587038934230804\n","step: 1790, loss: 0.053201839327812195\n","step: 1800, loss: 0.056416116654872894\n","step: 1810, loss: 0.1005340963602066\n","step: 1820, loss: 0.1265905201435089\n","step: 1830, loss: 0.0978037640452385\n","step: 1840, loss: 0.16077405214309692\n","step: 1850, loss: 0.06953448057174683\n","step: 1860, loss: 0.08116934448480606\n","step: 1870, loss: 0.1625622808933258\n","step: 1880, loss: 0.0946536734700203\n","step: 1890, loss: 0.0895419716835022\n","step: 1900, loss: 0.07958244532346725\n","step: 1910, loss: 0.06712494790554047\n","step: 1920, loss: 0.09454405307769775\n","step: 1930, loss: 0.10888320952653885\n","step: 1940, loss: 0.09685824811458588\n","step: 1950, loss: 0.06803211569786072\n","step: 1960, loss: 0.056043900549411774\n","step: 1970, loss: 0.16346727311611176\n","step: 1980, loss: 0.030048394575715065\n","step: 1990, loss: 0.05619606375694275\n","step: 2000, loss: 0.09444354474544525\n","step: 2010, loss: 0.16458189487457275\n","step: 2020, loss: 0.09589004516601562\n","step: 2030, loss: 0.18718720972537994\n","step: 2040, loss: 0.0726129338145256\n","step: 2050, loss: 0.0792064517736435\n","step: 2060, loss: 0.057241007685661316\n","step: 2070, loss: 0.09816817194223404\n","step: 2080, loss: 0.14417821168899536\n","step: 2090, loss: 0.10756596177816391\n","step: 2100, loss: 0.07714372873306274\n","step: 2110, loss: 0.07516061514616013\n","step: 2120, loss: 0.1084352657198906\n","step: 2130, loss: 0.08016343414783478\n","step: 2140, loss: 0.15516725182533264\n","step: 2150, loss: 0.10565244406461716\n","step: 2160, loss: 0.11021535098552704\n","step: 2170, loss: 0.07019712030887604\n","step: 2180, loss: 0.05880090966820717\n","step: 2190, loss: 0.025174448266625404\n","step: 2200, loss: 0.14596779644489288\n","step: 2210, loss: 0.013091959990561008\n","step: 2220, loss: 0.18217110633850098\n","step: 2230, loss: 0.10974935442209244\n","step: 2240, loss: 0.0654296725988388\n","step: 2250, loss: 0.10701967030763626\n","step: 2260, loss: 0.03768208622932434\n","step: 2270, loss: 0.06074130907654762\n","step: 2280, loss: 0.12978805601596832\n","step: 2290, loss: 0.07113981992006302\n","step: 2300, loss: 0.061697714030742645\n","step: 2310, loss: 0.024345163255929947\n","step: 2320, loss: 0.0788729190826416\n","step: 2330, loss: 0.06594864279031754\n","step: 2340, loss: 0.09546665847301483\n","step: 2350, loss: 0.10674872249364853\n","step: 2360, loss: 0.03970377892255783\n","step: 2370, loss: 0.08735965937376022\n","step: 2380, loss: 0.22549264132976532\n","step: 2390, loss: 0.08947369456291199\n","step: 2400, loss: 0.057516735047101974\n","step: 2410, loss: 0.14369603991508484\n","step: 2420, loss: 0.04487992450594902\n","step: 2430, loss: 0.038678839802742004\n","step: 2440, loss: 0.11251749843358994\n","step: 2450, loss: 0.1040058583021164\n","step: 2460, loss: 0.1879163682460785\n","step: 2470, loss: 0.11701952666044235\n","step: 2480, loss: 0.10006449371576309\n","step: 2490, loss: 0.08506495505571365\n","step: 2500, loss: 0.06601125001907349\n","step: 2510, loss: 0.14276942610740662\n","step: 2520, loss: 0.11083801090717316\n","step: 2530, loss: 0.0527428463101387\n","step: 2540, loss: 0.03044094145298004\n","step: 2550, loss: 0.09934313595294952\n","step: 2560, loss: 0.19045965373516083\n","step: 2570, loss: 0.10193006694316864\n","step: 2580, loss: 0.12001562118530273\n","step: 2590, loss: 0.040177952498197556\n","step: 2600, loss: 0.08038800209760666\n","step: 2610, loss: 0.07355336099863052\n","step: 2620, loss: 0.05844414234161377\n","step: 2630, loss: 0.08769553154706955\n","step: 2640, loss: 0.0372871458530426\n","step: 2650, loss: 0.12135995179414749\n","step: 2660, loss: 0.08968418091535568\n","step: 2670, loss: 0.09697622060775757\n","step: 2680, loss: 0.13057920336723328\n","step: 2690, loss: 0.06589703261852264\n","step: 2700, loss: 0.052855148911476135\n","step: 2710, loss: 0.09145162254571915\n","step: 2720, loss: 0.16244208812713623\n","step: 2730, loss: 0.1414487659931183\n","step: 2740, loss: 0.05254070833325386\n","step: 2750, loss: 0.07996869832277298\n","step: 2760, loss: 0.0481831431388855\n","step: 2770, loss: 0.12360439449548721\n","step: 2780, loss: 0.11494630575180054\n","step: 2790, loss: 0.1417478322982788\n","step: 2800, loss: 0.08240090310573578\n","step: 2810, loss: 0.0823775976896286\n","step: 2820, loss: 0.07059305161237717\n","step: 2830, loss: 0.06446661055088043\n","step: 2840, loss: 0.09095419943332672\n","step: 2850, loss: 0.05005181208252907\n","step: 2860, loss: 0.03209233656525612\n","step: 2870, loss: 0.09212320297956467\n","step: 2880, loss: 0.11070716381072998\n","step: 2890, loss: 0.060017846524715424\n","step: 2900, loss: 0.1141449585556984\n","step: 2910, loss: 0.03971894457936287\n","step: 2920, loss: 0.20653335750102997\n","step: 2930, loss: 0.059726353734731674\n","step: 2940, loss: 0.03497908636927605\n","step: 2950, loss: 0.17025505006313324\n","step: 2960, loss: 0.12373369187116623\n","step: 2970, loss: 0.11247560381889343\n","step: 2980, loss: 0.0534573495388031\n","step: 2990, loss: 0.04376611486077309\n","step: 3000, loss: 0.09097392112016678\n","step: 3010, loss: 0.10318538546562195\n","step: 3020, loss: 0.10522081702947617\n","step: 3030, loss: 0.01933012716472149\n","step: 3040, loss: 0.09425193071365356\n","step: 3050, loss: 0.03871631249785423\n","step: 3060, loss: 0.09908320754766464\n","step: 3070, loss: 0.1382542848587036\n","step: 3080, loss: 0.14306651055812836\n","step: 3090, loss: 0.06814122945070267\n","step: 3100, loss: 0.10091781616210938\n","step: 3110, loss: 0.028303038328886032\n","step: 3120, loss: 0.07187173515558243\n","step: 3130, loss: 0.033776603639125824\n","step: 3140, loss: 0.132277712225914\n","step: 3150, loss: 0.3234192430973053\n","step: 3160, loss: 0.14539691805839539\n","step: 3170, loss: 0.10823588073253632\n","step: 3180, loss: 0.0818648561835289\n","step: 3190, loss: 0.013414225541055202\n","step: 3200, loss: 0.1072709858417511\n","step: 3210, loss: 0.09143394231796265\n","step: 3220, loss: 0.05096496641635895\n","step: 3230, loss: 0.07149752974510193\n","step: 3240, loss: 0.04226558655500412\n","step: 3250, loss: 0.0665513277053833\n","step: 3260, loss: 0.1776796132326126\n","step: 3270, loss: 0.06595418602228165\n","step: 3280, loss: 0.06555327773094177\n","step: 3290, loss: 0.06581757962703705\n","step: 3300, loss: 0.10235527902841568\n","step: 3310, loss: 0.19088275730609894\n","step: 3320, loss: 0.05701921135187149\n","step: 3330, loss: 0.03289062902331352\n","step: 3340, loss: 0.06453964859247208\n","step: 3350, loss: 0.08199010044336319\n","step: 3360, loss: 0.07536904513835907\n","step: 3370, loss: 0.07208441197872162\n","step: 3380, loss: 0.09069079160690308\n","step: 3390, loss: 0.08257760107517242\n","step: 3400, loss: 0.09019008278846741\n","step: 3410, loss: 0.07779177278280258\n","step: 3420, loss: 0.03430488333106041\n","step: 3430, loss: 0.04266887530684471\n","step: 3440, loss: 0.06755036860704422\n","step: 3450, loss: 0.09850600361824036\n","step: 3460, loss: 0.11330881714820862\n","step: 3470, loss: 0.036356985569000244\n","step: 3480, loss: 0.04618191719055176\n","step: 3490, loss: 0.07675138115882874\n","step: 3500, loss: 0.16524623334407806\n","step: 3510, loss: 0.08106333762407303\n","step: 3520, loss: 0.08765710145235062\n","step: 3530, loss: 0.06876395642757416\n","step: 3540, loss: 0.06416415423154831\n","step: 3550, loss: 0.0641351044178009\n","step: 3560, loss: 0.1070294976234436\n","step: 3570, loss: 0.03942444175481796\n","step: 3580, loss: 0.1096409410238266\n","step: 3590, loss: 0.08571454882621765\n","step: 3600, loss: 0.10700613260269165\n","step: 3610, loss: 0.02903168648481369\n","step: 3620, loss: 0.04719359800219536\n","step: 3630, loss: 0.04955889657139778\n","step: 3640, loss: 0.06728894263505936\n","step: 3650, loss: 0.1082395613193512\n","step: 3660, loss: 0.13122281432151794\n","step: 3670, loss: 0.07774180173873901\n","step: 3680, loss: 0.09224984049797058\n","step: 3690, loss: 0.04868055880069733\n","step: 3700, loss: 0.060535937547683716\n","step: 3710, loss: 0.08750046044588089\n","step: 3720, loss: 0.0774836614727974\n","step: 3730, loss: 0.034768808633089066\n","step: 3740, loss: 0.11293426901102066\n","step: 3750, loss: 0.04535374790430069\n","step: 3760, loss: 0.11356038600206375\n","step: 3770, loss: 0.07653557509183884\n","step: 3780, loss: 0.11332674324512482\n","step: 3790, loss: 0.037434644997119904\n","step: 3800, loss: 0.14917664229869843\n","step: 3810, loss: 0.08463181555271149\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        35\n","           2       0.64      0.73      0.68        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.95      0.77      0.85       291\n","           5       0.76      0.84      0.80       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.53      0.94      0.68       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.94      0.97      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.32      0.92      0.47        13\n","          14       0.45      1.00      0.62        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.83      0.84      0.83      1151\n","          17       0.82      1.00      0.90        41\n","          18       1.00      0.94      0.97        32\n","          19       0.75      0.07      0.14        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.93      0.73      0.82      4175\n","          23       0.65      0.97      0.78      2253\n","          24       0.50      0.02      0.04        44\n","          25       0.88      0.90      0.89       888\n","          26       0.78      0.78      0.78         9\n","          27       0.90      1.00      0.95        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.94      0.83      0.88      1136\n","          31       0.44      0.37      0.40        19\n","          32       0.80      1.00      0.89         8\n","          33       0.59      0.99      0.74        86\n","          34       0.21      0.44      0.28        32\n","          35       0.98      0.99      0.99       474\n","          36       1.00      0.11      0.20       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.91      0.98      0.94       404\n","          39       0.97      0.95      0.96       485\n","          40       0.95      0.77      0.85       573\n","          41       0.96      0.93      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.94      0.85      0.89       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.90      0.11      0.20        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.79      0.76     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 488\n","\n","Loop 2\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9660637378692627\n","step: 10, loss: 2.0270564556121826\n","step: 20, loss: 0.8263635039329529\n","step: 30, loss: 0.464794784784317\n","step: 40, loss: 0.39439985156059265\n","step: 50, loss: 0.2372395545244217\n","step: 60, loss: 0.35294830799102783\n","step: 70, loss: 0.27602988481521606\n","step: 80, loss: 0.19601143896579742\n","step: 90, loss: 0.20713412761688232\n","step: 100, loss: 0.1984756737947464\n","step: 110, loss: 0.16856598854064941\n","step: 120, loss: 0.2540299892425537\n","step: 130, loss: 0.1296726018190384\n","step: 140, loss: 0.12365398555994034\n","step: 150, loss: 0.1291559636592865\n","step: 160, loss: 0.22244258224964142\n","step: 170, loss: 0.15750591456890106\n","step: 180, loss: 0.19788070023059845\n","step: 190, loss: 0.12344711273908615\n","step: 200, loss: 0.09306109696626663\n","step: 210, loss: 0.08963422477245331\n","step: 220, loss: 0.12762440741062164\n","step: 230, loss: 0.1617126613855362\n","step: 240, loss: 0.12461451441049576\n","step: 250, loss: 0.08309676498174667\n","step: 260, loss: 0.1303124576807022\n","step: 270, loss: 0.09374240040779114\n","step: 280, loss: 0.17441651225090027\n","step: 290, loss: 0.17634323239326477\n","step: 300, loss: 0.07647953182458878\n","step: 310, loss: 0.05841955915093422\n","step: 320, loss: 0.09934436529874802\n","step: 330, loss: 0.08506256341934204\n","step: 340, loss: 0.1011536717414856\n","step: 350, loss: 0.2138606607913971\n","step: 360, loss: 0.15819817781448364\n","step: 370, loss: 0.09145407378673553\n","step: 380, loss: 0.08218291401863098\n","step: 390, loss: 0.15844114124774933\n","step: 400, loss: 0.1783788502216339\n","step: 410, loss: 0.05649768188595772\n","step: 420, loss: 0.0732048898935318\n","step: 430, loss: 0.11129473894834518\n","step: 440, loss: 0.12695030868053436\n","step: 450, loss: 0.056854888796806335\n","step: 460, loss: 0.08429277688264847\n","step: 470, loss: 0.10075274854898453\n","step: 480, loss: 0.1405046433210373\n","step: 490, loss: 0.11863668262958527\n","step: 500, loss: 0.09908817708492279\n","step: 510, loss: 0.08144766837358475\n","step: 520, loss: 0.10360436886548996\n","step: 530, loss: 0.11982674151659012\n","step: 540, loss: 0.22634932398796082\n","step: 550, loss: 0.20814771950244904\n","step: 560, loss: 0.18617792427539825\n","step: 570, loss: 0.08876502513885498\n","step: 580, loss: 0.06295435130596161\n","step: 590, loss: 0.10194549709558487\n","step: 600, loss: 0.06099015474319458\n","step: 610, loss: 0.18844084441661835\n","step: 620, loss: 0.10050015896558762\n","step: 630, loss: 0.07225804030895233\n","step: 640, loss: 0.0972917303442955\n","step: 650, loss: 0.04666535556316376\n","step: 660, loss: 0.09892375022172928\n","step: 670, loss: 0.1699373722076416\n","step: 680, loss: 0.13416263461112976\n","step: 690, loss: 0.09017860144376755\n","step: 700, loss: 0.10228762775659561\n","step: 710, loss: 0.08618588000535965\n","step: 720, loss: 0.12534113228321075\n","step: 730, loss: 0.028595004230737686\n","step: 740, loss: 0.07827187329530716\n","step: 750, loss: 0.03691734001040459\n","step: 760, loss: 0.06754452735185623\n","step: 770, loss: 0.11548026651144028\n","step: 780, loss: 0.09671542793512344\n","step: 790, loss: 0.0670134499669075\n","step: 800, loss: 0.14055988192558289\n","step: 810, loss: 0.07268008589744568\n","step: 820, loss: 0.06405362486839294\n","step: 830, loss: 0.1633920669555664\n","step: 840, loss: 0.14022420346736908\n","step: 850, loss: 0.08325612545013428\n","step: 860, loss: 0.08152753114700317\n","step: 870, loss: 0.15459801256656647\n","step: 880, loss: 0.0750386193394661\n","step: 890, loss: 0.0994662493467331\n","step: 900, loss: 0.08363590389490128\n","step: 910, loss: 0.10874440521001816\n","step: 920, loss: 0.04221566021442413\n","step: 930, loss: 0.06318529695272446\n","step: 940, loss: 0.1076778769493103\n","step: 950, loss: 0.06762520968914032\n","step: 960, loss: 0.13210704922676086\n","step: 970, loss: 0.2652553915977478\n","step: 980, loss: 0.08083444088697433\n","step: 990, loss: 0.06899212300777435\n","step: 1000, loss: 0.11792762577533722\n","step: 1010, loss: 0.18391482532024384\n","step: 1020, loss: 0.08740599453449249\n","step: 1030, loss: 0.0997185930609703\n","step: 1040, loss: 0.07323611527681351\n","step: 1050, loss: 0.10215608775615692\n","step: 1060, loss: 0.0867399200797081\n","step: 1070, loss: 0.14781685173511505\n","step: 1080, loss: 0.11573651432991028\n","step: 1090, loss: 0.03539678826928139\n","step: 1100, loss: 0.06342756003141403\n","step: 1110, loss: 0.05453774705529213\n","step: 1120, loss: 0.2019629180431366\n","step: 1130, loss: 0.12822940945625305\n","step: 1140, loss: 0.058720774948596954\n","step: 1150, loss: 0.0387752391397953\n","step: 1160, loss: 0.06688294559717178\n","step: 1170, loss: 0.031931888312101364\n","step: 1180, loss: 0.06785722076892853\n","step: 1190, loss: 0.11418528109788895\n","step: 1200, loss: 0.11153466999530792\n","step: 1210, loss: 0.05656407028436661\n","step: 1220, loss: 0.07540106028318405\n","step: 1230, loss: 0.07149463891983032\n","step: 1240, loss: 0.16248442232608795\n","step: 1250, loss: 0.15692657232284546\n","step: 1260, loss: 0.08381941169500351\n","step: 1270, loss: 0.19599902629852295\n","step: 1280, loss: 0.0798693373799324\n","step: 1290, loss: 0.08357328176498413\n","step: 1300, loss: 0.14578799903392792\n","step: 1310, loss: 0.10862524062395096\n","step: 1320, loss: 0.13477365672588348\n","step: 1330, loss: 0.18448427319526672\n","step: 1340, loss: 0.13869836926460266\n","step: 1350, loss: 0.1008686050772667\n","step: 1360, loss: 0.08215542882680893\n","step: 1370, loss: 0.08937318623065948\n","step: 1380, loss: 0.08402004092931747\n","step: 1390, loss: 0.07380735874176025\n","step: 1400, loss: 0.23875340819358826\n","step: 1410, loss: 0.04883059114217758\n","step: 1420, loss: 0.03800324350595474\n","step: 1430, loss: 0.045768581330776215\n","step: 1440, loss: 0.09558690339326859\n","step: 1450, loss: 0.11078112572431564\n","step: 1460, loss: 0.11136464774608612\n","step: 1470, loss: 0.15249964594841003\n","step: 1480, loss: 0.12056655436754227\n","step: 1490, loss: 0.15762022137641907\n","step: 1500, loss: 0.11307433992624283\n","step: 1510, loss: 0.030474474653601646\n","step: 1520, loss: 0.07579824328422546\n","step: 1530, loss: 0.12916229665279388\n","step: 1540, loss: 0.08715497702360153\n","step: 1550, loss: 0.09436888247728348\n","step: 1560, loss: 0.2258760631084442\n","step: 1570, loss: 0.0388040766119957\n","step: 1580, loss: 0.0741703137755394\n","step: 1590, loss: 0.09830939024686813\n","step: 1600, loss: 0.0528535470366478\n","step: 1610, loss: 0.15036115050315857\n","step: 1620, loss: 0.0653921440243721\n","step: 1630, loss: 0.11693178862333298\n","step: 1640, loss: 0.04257715120911598\n","step: 1650, loss: 0.08471966534852982\n","step: 1660, loss: 0.07664522528648376\n","step: 1670, loss: 0.07559846341609955\n","step: 1680, loss: 0.019082464277744293\n","step: 1690, loss: 0.07287263870239258\n","step: 1700, loss: 0.1623489260673523\n","step: 1710, loss: 0.18451471626758575\n","step: 1720, loss: 0.11644759774208069\n","step: 1730, loss: 0.08302023261785507\n","step: 1740, loss: 0.2022770792245865\n","step: 1750, loss: 0.06939691305160522\n","step: 1760, loss: 0.16412663459777832\n","step: 1770, loss: 0.0895165205001831\n","step: 1780, loss: 0.03056352399289608\n","step: 1790, loss: 0.09711962193250656\n","step: 1800, loss: 0.08375821262598038\n","step: 1810, loss: 0.15419797599315643\n","step: 1820, loss: 0.05197109282016754\n","step: 1830, loss: 0.06469808518886566\n","step: 1840, loss: 0.09783689677715302\n","step: 1850, loss: 0.0400511771440506\n","step: 1860, loss: 0.09644558280706406\n","step: 1870, loss: 0.06516330689191818\n","step: 1880, loss: 0.031160704791545868\n","step: 1890, loss: 0.06051058694720268\n","step: 1900, loss: 0.050417013466358185\n","step: 1910, loss: 0.04624931886792183\n","step: 1920, loss: 0.08152882009744644\n","step: 1930, loss: 0.12117412686347961\n","step: 1940, loss: 0.08057273179292679\n","step: 1950, loss: 0.06116415560245514\n","step: 1960, loss: 0.06824863702058792\n","step: 1970, loss: 0.11329517513513565\n","step: 1980, loss: 0.02038494497537613\n","step: 1990, loss: 0.1269736886024475\n","step: 2000, loss: 0.11683443188667297\n","step: 2010, loss: 0.0905003473162651\n","step: 2020, loss: 0.062113549560308456\n","step: 2030, loss: 0.11166194826364517\n","step: 2040, loss: 0.05493948608636856\n","step: 2050, loss: 0.06958775967359543\n","step: 2060, loss: 0.058296628296375275\n","step: 2070, loss: 0.05066003277897835\n","step: 2080, loss: 0.09910870343446732\n","step: 2090, loss: 0.15659472346305847\n","step: 2100, loss: 0.09196986258029938\n","step: 2110, loss: 0.0753885880112648\n","step: 2120, loss: 0.10458419471979141\n","step: 2130, loss: 0.04417361691594124\n","step: 2140, loss: 0.17233777046203613\n","step: 2150, loss: 0.1548483967781067\n","step: 2160, loss: 0.08659268915653229\n","step: 2170, loss: 0.08150295168161392\n","step: 2180, loss: 0.10456165671348572\n","step: 2190, loss: 0.14723877608776093\n","step: 2200, loss: 0.11397148668766022\n","step: 2210, loss: 0.11824195832014084\n","step: 2220, loss: 0.12115409225225449\n","step: 2230, loss: 0.040414322167634964\n","step: 2240, loss: 0.06368391960859299\n","step: 2250, loss: 0.10552152991294861\n","step: 2260, loss: 0.1549198180437088\n","step: 2270, loss: 0.08441409468650818\n","step: 2280, loss: 0.12709274888038635\n","step: 2290, loss: 0.08589650690555573\n","step: 2300, loss: 0.11281270533800125\n","step: 2310, loss: 0.06132350116968155\n","step: 2320, loss: 0.1400558203458786\n","step: 2330, loss: 0.06197551637887955\n","step: 2340, loss: 0.017149170860648155\n","step: 2350, loss: 0.07834883034229279\n","step: 2360, loss: 0.12658807635307312\n","step: 2370, loss: 0.11379778385162354\n","step: 2380, loss: 0.1264352798461914\n","step: 2390, loss: 0.06718707084655762\n","step: 2400, loss: 0.07005982100963593\n","step: 2410, loss: 0.059077970683574677\n","step: 2420, loss: 0.07561995089054108\n","step: 2430, loss: 0.07708512991666794\n","step: 2440, loss: 0.04855760559439659\n","step: 2450, loss: 0.08329290896654129\n","step: 2460, loss: 0.14835749566555023\n","step: 2470, loss: 0.10297848284244537\n","step: 2480, loss: 0.058460116386413574\n","step: 2490, loss: 0.11731734871864319\n","step: 2500, loss: 0.10299348831176758\n","step: 2510, loss: 0.07184028625488281\n","step: 2520, loss: 0.03634923696517944\n","step: 2530, loss: 0.06947147101163864\n","step: 2540, loss: 0.06005582958459854\n","step: 2550, loss: 0.0602712482213974\n","step: 2560, loss: 0.07216747850179672\n","step: 2570, loss: 0.11194202303886414\n","step: 2580, loss: 0.06664756685495377\n","step: 2590, loss: 0.08637715131044388\n","step: 2600, loss: 0.05993672087788582\n","step: 2610, loss: 0.10525894165039062\n","step: 2620, loss: 0.147727832198143\n","step: 2630, loss: 0.18420633673667908\n","step: 2640, loss: 0.07224652171134949\n","step: 2650, loss: 0.05247637256979942\n","step: 2660, loss: 0.049961578100919724\n","step: 2670, loss: 0.054849714040756226\n","step: 2680, loss: 0.21003283560276031\n","step: 2690, loss: 0.1015658900141716\n","step: 2700, loss: 0.22843876481056213\n","step: 2710, loss: 0.06820786744356155\n","step: 2720, loss: 0.1499500721693039\n","step: 2730, loss: 0.12932129204273224\n","step: 2740, loss: 0.1381744146347046\n","step: 2750, loss: 0.09374932944774628\n","step: 2760, loss: 0.12616164982318878\n","step: 2770, loss: 0.06303955614566803\n","step: 2780, loss: 0.1714586466550827\n","step: 2790, loss: 0.07334079593420029\n","step: 2800, loss: 0.13826574385166168\n","step: 2810, loss: 0.0347115658223629\n","step: 2820, loss: 0.1171937957406044\n","step: 2830, loss: 0.08471817523241043\n","step: 2840, loss: 0.14995884895324707\n","step: 2850, loss: 0.07678304612636566\n","step: 2860, loss: 0.06909551471471786\n","step: 2870, loss: 0.09133869409561157\n","step: 2880, loss: 0.12338965386152267\n","step: 2890, loss: 0.06476886570453644\n","step: 2900, loss: 0.0259393360465765\n","step: 2910, loss: 0.037726256996393204\n","step: 2920, loss: 0.07738188654184341\n","step: 2930, loss: 0.07360712438821793\n","step: 2940, loss: 0.10468501597642899\n","step: 2950, loss: 0.05305810272693634\n","step: 2960, loss: 0.08974098414182663\n","step: 2970, loss: 0.0482749342918396\n","step: 2980, loss: 0.04502396658062935\n","step: 2990, loss: 0.1281643509864807\n","step: 3000, loss: 0.11834432184696198\n","step: 3010, loss: 0.14312125742435455\n","step: 3020, loss: 0.04976300150156021\n","step: 3030, loss: 0.08677182346582413\n","step: 3040, loss: 0.12280189245939255\n","step: 3050, loss: 0.07977523654699326\n","step: 3060, loss: 0.07985344529151917\n","step: 3070, loss: 0.09792684018611908\n","step: 3080, loss: 0.14052394032478333\n","step: 3090, loss: 0.2024107426404953\n","step: 3100, loss: 0.08056331425905228\n","step: 3110, loss: 0.14420758187770844\n","step: 3120, loss: 0.07503598183393478\n","step: 3130, loss: 0.09767307341098785\n","step: 3140, loss: 0.08773768693208694\n","step: 3150, loss: 0.10601980984210968\n","step: 3160, loss: 0.12494400143623352\n","step: 3170, loss: 0.1522691547870636\n","step: 3180, loss: 0.04733245447278023\n","step: 3190, loss: 0.09692086279392242\n","step: 3200, loss: 0.02455940470099449\n","step: 3210, loss: 0.08252587914466858\n","step: 3220, loss: 0.0818013921380043\n","step: 3230, loss: 0.14190660417079926\n","step: 3240, loss: 0.02629895880818367\n","step: 3250, loss: 0.033957164734601974\n","step: 3260, loss: 0.03735090047121048\n","step: 3270, loss: 0.06827560067176819\n","step: 3280, loss: 0.13388006389141083\n","step: 3290, loss: 0.06632325053215027\n","step: 3300, loss: 0.052461132407188416\n","step: 3310, loss: 0.07168886065483093\n","step: 3320, loss: 0.08434616774320602\n","step: 3330, loss: 0.0388999804854393\n","step: 3340, loss: 0.11052606254816055\n","step: 3350, loss: 0.06226516515016556\n","step: 3360, loss: 0.12844210863113403\n","step: 3370, loss: 0.11622044444084167\n","step: 3380, loss: 0.09194836020469666\n","step: 3390, loss: 0.01711115427315235\n","step: 3400, loss: 0.048148348927497864\n","step: 3410, loss: 0.05009913817048073\n","step: 3420, loss: 0.1646539866924286\n","step: 3430, loss: 0.1609155535697937\n","step: 3440, loss: 0.10991033166646957\n","step: 3450, loss: 0.12557391822338104\n","step: 3460, loss: 0.1431877166032791\n","step: 3470, loss: 0.028600361198186874\n","step: 3480, loss: 0.1322142332792282\n","step: 3490, loss: 0.028984347358345985\n","step: 3500, loss: 0.06859295070171356\n","step: 3510, loss: 0.07877407968044281\n","step: 3520, loss: 0.053150076419115067\n","step: 3530, loss: 0.03296906501054764\n","step: 3540, loss: 0.07545281201601028\n","step: 3550, loss: 0.03339501470327377\n","step: 3560, loss: 0.08225765079259872\n","step: 3570, loss: 0.0919378250837326\n","step: 3580, loss: 0.1006937325000763\n","step: 3590, loss: 0.029800016433000565\n","step: 3600, loss: 0.12484599649906158\n","step: 3610, loss: 0.06568949669599533\n","step: 3620, loss: 0.06908818334341049\n","step: 3630, loss: 0.11280637234449387\n","step: 3640, loss: 0.06699811667203903\n","step: 3650, loss: 0.10995815694332123\n","step: 3660, loss: 0.08384879678487778\n","step: 3670, loss: 0.06729962676763535\n","step: 3680, loss: 0.06520850211381912\n","step: 3690, loss: 0.042140256613492966\n","step: 3700, loss: 0.08259560167789459\n","step: 3710, loss: 0.06399541348218918\n","step: 3720, loss: 0.04525235667824745\n","step: 3730, loss: 0.07683465629816055\n","step: 3740, loss: 0.04552552476525307\n","step: 3750, loss: 0.06650689989328384\n","step: 3760, loss: 0.05830186605453491\n","step: 3770, loss: 0.04781664162874222\n","step: 3780, loss: 0.02357139065861702\n","step: 3790, loss: 0.11889855563640594\n","step: 3800, loss: 0.04718092456459999\n","step: 3810, loss: 0.02929801680147648\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.83      0.97      0.89        35\n","           2       0.72      0.62      0.67        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.98      0.83      0.90       291\n","           5       0.81      0.82      0.82       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.49      0.94      0.64       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.79      0.98      0.88        47\n","          13       1.00      0.23      0.38        13\n","          14       0.50      1.00      0.67        43\n","          15       0.95      0.97      0.96      2778\n","          16       0.85      0.86      0.86      1151\n","          17       0.93      0.95      0.94        41\n","          18       1.00      0.94      0.97        32\n","          19       0.52      0.62      0.57        40\n","          20       1.00      0.98      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.77      0.86      4175\n","          23       0.77      0.94      0.85      2253\n","          24       0.35      0.43      0.39        44\n","          25       0.82      0.93      0.87       888\n","          26       0.88      0.78      0.82         9\n","          27       0.97      0.96      0.96        69\n","          28       0.99      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.85      0.87      0.86      1136\n","          31       0.52      0.79      0.62        19\n","          32       0.80      1.00      0.89         8\n","          33       0.64      0.97      0.77        86\n","          34       0.24      0.72      0.37        32\n","          35       0.96      0.99      0.98       474\n","          36       0.91      0.18      0.29       182\n","          37       0.87      0.96      0.91      1592\n","          38       0.90      0.98      0.94       404\n","          39       0.97      0.96      0.96       485\n","          40       0.89      0.93      0.91       573\n","          41       0.94      0.92      0.93       841\n","          42       0.98      0.99      0.98       575\n","          43       0.96      0.74      0.83       152\n","          44       0.85      0.92      0.88        75\n","          46       0.94      0.96      0.95        82\n","          48       0.90      0.59      0.72        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.82      0.79     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 466\n","\n","Loop 3\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.856262445449829\n","step: 10, loss: 1.9734201431274414\n","step: 20, loss: 0.7351809740066528\n","step: 30, loss: 0.5117853879928589\n","step: 40, loss: 0.3234884738922119\n","step: 50, loss: 0.2817065417766571\n","step: 60, loss: 0.22018836438655853\n","step: 70, loss: 0.28618109226226807\n","step: 80, loss: 0.2472105175256729\n","step: 90, loss: 0.21570774912834167\n","step: 100, loss: 0.2512865960597992\n","step: 110, loss: 0.26859328150749207\n","step: 120, loss: 0.1237674206495285\n","step: 130, loss: 0.275084912776947\n","step: 140, loss: 0.1105947494506836\n","step: 150, loss: 0.2579306364059448\n","step: 160, loss: 0.214762344956398\n","step: 170, loss: 0.20998643338680267\n","step: 180, loss: 0.14759477972984314\n","step: 190, loss: 0.19216059148311615\n","step: 200, loss: 0.1695946902036667\n","step: 210, loss: 0.19324927031993866\n","step: 220, loss: 0.3201900124549866\n","step: 230, loss: 0.17924188077449799\n","step: 240, loss: 0.22279155254364014\n","step: 250, loss: 0.11815333366394043\n","step: 260, loss: 0.08782553672790527\n","step: 270, loss: 0.08452772349119186\n","step: 280, loss: 0.05670849606394768\n","step: 290, loss: 0.13960720598697662\n","step: 300, loss: 0.15643757581710815\n","step: 310, loss: 0.13038760423660278\n","step: 320, loss: 0.23783954977989197\n","step: 330, loss: 0.1993287354707718\n","step: 340, loss: 0.14040349423885345\n","step: 350, loss: 0.09626660495996475\n","step: 360, loss: 0.09770294278860092\n","step: 370, loss: 0.11736439913511276\n","step: 380, loss: 0.14669014513492584\n","step: 390, loss: 0.08122255653142929\n","step: 400, loss: 0.08785252273082733\n","step: 410, loss: 0.07387088984251022\n","step: 420, loss: 0.11995934695005417\n","step: 430, loss: 0.14078198373317719\n","step: 440, loss: 0.05053924396634102\n","step: 450, loss: 0.10020728409290314\n","step: 460, loss: 0.11361826211214066\n","step: 470, loss: 0.12720368802547455\n","step: 480, loss: 0.056027840822935104\n","step: 490, loss: 0.11146809160709381\n","step: 500, loss: 0.08389028906822205\n","step: 510, loss: 0.06702173501253128\n","step: 520, loss: 0.11864984035491943\n","step: 530, loss: 0.11252991110086441\n","step: 540, loss: 0.09503765404224396\n","step: 550, loss: 0.3290320634841919\n","step: 560, loss: 0.14985644817352295\n","step: 570, loss: 0.17560233175754547\n","step: 580, loss: 0.1464901566505432\n","step: 590, loss: 0.16317807137966156\n","step: 600, loss: 0.10041774809360504\n","step: 610, loss: 0.06624986231327057\n","step: 620, loss: 0.11272143572568893\n","step: 630, loss: 0.12933766841888428\n","step: 640, loss: 0.045997560024261475\n","step: 650, loss: 0.1362352967262268\n","step: 660, loss: 0.05526898428797722\n","step: 670, loss: 0.08044148981571198\n","step: 680, loss: 0.08131391555070877\n","step: 690, loss: 0.15740956366062164\n","step: 700, loss: 0.1239115446805954\n","step: 710, loss: 0.06758084893226624\n","step: 720, loss: 0.10120490938425064\n","step: 730, loss: 0.08781299740076065\n","step: 740, loss: 0.10991831123828888\n","step: 750, loss: 0.08438162505626678\n","step: 760, loss: 0.12358436733484268\n","step: 770, loss: 0.10023069381713867\n","step: 780, loss: 0.12994854152202606\n","step: 790, loss: 0.09855981916189194\n","step: 800, loss: 0.1357044279575348\n","step: 810, loss: 0.0817258358001709\n","step: 820, loss: 0.11571276932954788\n","step: 830, loss: 0.0692872405052185\n","step: 840, loss: 0.13300152122974396\n","step: 850, loss: 0.18172237277030945\n","step: 860, loss: 0.05041298270225525\n","step: 870, loss: 0.13297061622142792\n","step: 880, loss: 0.14636477828025818\n","step: 890, loss: 0.18103644251823425\n","step: 900, loss: 0.12805907428264618\n","step: 910, loss: 0.051367729902267456\n","step: 920, loss: 0.09838920831680298\n","step: 930, loss: 0.05381026118993759\n","step: 940, loss: 0.04408929869532585\n","step: 950, loss: 0.059391189366579056\n","step: 960, loss: 0.16326561570167542\n","step: 970, loss: 0.12342669069766998\n","step: 980, loss: 0.05408133566379547\n","step: 990, loss: 0.03685220703482628\n","step: 1000, loss: 0.09691927582025528\n","step: 1010, loss: 0.15925350785255432\n","step: 1020, loss: 0.11043456941843033\n","step: 1030, loss: 0.05456698313355446\n","step: 1040, loss: 0.08511501550674438\n","step: 1050, loss: 0.04320961982011795\n","step: 1060, loss: 0.17817242443561554\n","step: 1070, loss: 0.092582568526268\n","step: 1080, loss: 0.09873135387897491\n","step: 1090, loss: 0.09018317610025406\n","step: 1100, loss: 0.07169882208108902\n","step: 1110, loss: 0.16654646396636963\n","step: 1120, loss: 0.0903252586722374\n","step: 1130, loss: 0.07530011236667633\n","step: 1140, loss: 0.15306901931762695\n","step: 1150, loss: 0.11498991400003433\n","step: 1160, loss: 0.06506343185901642\n","step: 1170, loss: 0.17493927478790283\n","step: 1180, loss: 0.02707865461707115\n","step: 1190, loss: 0.057050492614507675\n","step: 1200, loss: 0.07148575037717819\n","step: 1210, loss: 0.052443280816078186\n","step: 1220, loss: 0.06218380481004715\n","step: 1230, loss: 0.11036798357963562\n","step: 1240, loss: 0.04917820543050766\n","step: 1250, loss: 0.10425442457199097\n","step: 1260, loss: 0.05882645025849342\n","step: 1270, loss: 0.08430978655815125\n","step: 1280, loss: 0.10262231528759003\n","step: 1290, loss: 0.06586937606334686\n","step: 1300, loss: 0.10605327039957047\n","step: 1310, loss: 0.07480224967002869\n","step: 1320, loss: 0.0798092782497406\n","step: 1330, loss: 0.10204301029443741\n","step: 1340, loss: 0.06561804562807083\n","step: 1350, loss: 0.06293566524982452\n","step: 1360, loss: 0.07771125435829163\n","step: 1370, loss: 0.12131232768297195\n","step: 1380, loss: 0.06793037056922913\n","step: 1390, loss: 0.05881263688206673\n","step: 1400, loss: 0.14093005657196045\n","step: 1410, loss: 0.0774274617433548\n","step: 1420, loss: 0.1598774939775467\n","step: 1430, loss: 0.19414874911308289\n","step: 1440, loss: 0.13737253844738007\n","step: 1450, loss: 0.08943100273609161\n","step: 1460, loss: 0.05230611190199852\n","step: 1470, loss: 0.0849335715174675\n","step: 1480, loss: 0.032028570771217346\n","step: 1490, loss: 0.03625429794192314\n","step: 1500, loss: 0.11801856011152267\n","step: 1510, loss: 0.13288597762584686\n","step: 1520, loss: 0.06548386812210083\n","step: 1530, loss: 0.04554712772369385\n","step: 1540, loss: 0.04918239638209343\n","step: 1550, loss: 0.07534490525722504\n","step: 1560, loss: 0.17417216300964355\n","step: 1570, loss: 0.07538555562496185\n","step: 1580, loss: 0.043853916227817535\n","step: 1590, loss: 0.07342507690191269\n","step: 1600, loss: 0.014459189027547836\n","step: 1610, loss: 0.046009212732315063\n","step: 1620, loss: 0.062198180705308914\n","step: 1630, loss: 0.19425873458385468\n","step: 1640, loss: 0.11270207911729813\n","step: 1650, loss: 0.16247926652431488\n","step: 1660, loss: 0.04393620789051056\n","step: 1670, loss: 0.09327146410942078\n","step: 1680, loss: 0.12236372381448746\n","step: 1690, loss: 0.08969331532716751\n","step: 1700, loss: 0.09920854866504669\n","step: 1710, loss: 0.14325502514839172\n","step: 1720, loss: 0.04705178365111351\n","step: 1730, loss: 0.036942027509212494\n","step: 1740, loss: 0.20499929785728455\n","step: 1750, loss: 0.1278976947069168\n","step: 1760, loss: 0.1189262717962265\n","step: 1770, loss: 0.04342779889702797\n","step: 1780, loss: 0.13381583988666534\n","step: 1790, loss: 0.026967301964759827\n","step: 1800, loss: 0.08552486449480057\n","step: 1810, loss: 0.0520135797560215\n","step: 1820, loss: 0.05671152099967003\n","step: 1830, loss: 0.12620729207992554\n","step: 1840, loss: 0.08209045976400375\n","step: 1850, loss: 0.09932227432727814\n","step: 1860, loss: 0.09904640167951584\n","step: 1870, loss: 0.07496141642332077\n","step: 1880, loss: 0.10795388370752335\n","step: 1890, loss: 0.06267118453979492\n","step: 1900, loss: 0.056912973523139954\n","step: 1910, loss: 0.07107000052928925\n","step: 1920, loss: 0.10935486853122711\n","step: 1930, loss: 0.06664315611124039\n","step: 1940, loss: 0.0931001603603363\n","step: 1950, loss: 0.11893826723098755\n","step: 1960, loss: 0.053284771740436554\n","step: 1970, loss: 0.11252032965421677\n","step: 1980, loss: 0.08002619445323944\n","step: 1990, loss: 0.08689895272254944\n","step: 2000, loss: 0.13343185186386108\n","step: 2010, loss: 0.03608907014131546\n","step: 2020, loss: 0.13662943243980408\n","step: 2030, loss: 0.25307026505470276\n","step: 2040, loss: 0.11181033402681351\n","step: 2050, loss: 0.13591717183589935\n","step: 2060, loss: 0.0957232266664505\n","step: 2070, loss: 0.15256500244140625\n","step: 2080, loss: 0.051362428814172745\n","step: 2090, loss: 0.08043269068002701\n","step: 2100, loss: 0.01607492007315159\n","step: 2110, loss: 0.06468342244625092\n","step: 2120, loss: 0.10958439111709595\n","step: 2130, loss: 0.12344460934400558\n","step: 2140, loss: 0.07382369041442871\n","step: 2150, loss: 0.06603468209505081\n","step: 2160, loss: 0.1331212967634201\n","step: 2170, loss: 0.20118416845798492\n","step: 2180, loss: 0.08281542360782623\n","step: 2190, loss: 0.09430590271949768\n","step: 2200, loss: 0.08265095949172974\n","step: 2210, loss: 0.042183950543403625\n","step: 2220, loss: 0.1018628478050232\n","step: 2230, loss: 0.0682607963681221\n","step: 2240, loss: 0.12815524637699127\n","step: 2250, loss: 0.12462085485458374\n","step: 2260, loss: 0.14016300439834595\n","step: 2270, loss: 0.016720261424779892\n","step: 2280, loss: 0.0472511351108551\n","step: 2290, loss: 0.12411163002252579\n","step: 2300, loss: 0.07920953631401062\n","step: 2310, loss: 0.21634669601917267\n","step: 2320, loss: 0.05940339341759682\n","step: 2330, loss: 0.11339469254016876\n","step: 2340, loss: 0.10383140295743942\n","step: 2350, loss: 0.10804933309555054\n","step: 2360, loss: 0.12575873732566833\n","step: 2370, loss: 0.0944073274731636\n","step: 2380, loss: 0.09473925083875656\n","step: 2390, loss: 0.08425819873809814\n","step: 2400, loss: 0.03869574889540672\n","step: 2410, loss: 0.2088516354560852\n","step: 2420, loss: 0.12044110894203186\n","step: 2430, loss: 0.06598479300737381\n","step: 2440, loss: 0.1270797699689865\n","step: 2450, loss: 0.08462729305028915\n","step: 2460, loss: 0.11747892946004868\n","step: 2470, loss: 0.12039116770029068\n","step: 2480, loss: 0.05941222980618477\n","step: 2490, loss: 0.07549046725034714\n","step: 2500, loss: 0.08515609800815582\n","step: 2510, loss: 0.12753041088581085\n","step: 2520, loss: 0.06254251301288605\n","step: 2530, loss: 0.09919355064630508\n","step: 2540, loss: 0.10536006838083267\n","step: 2550, loss: 0.044191133230924606\n","step: 2560, loss: 0.09054065495729446\n","step: 2570, loss: 0.038325972855091095\n","step: 2580, loss: 0.09415328502655029\n","step: 2590, loss: 0.056236717849969864\n","step: 2600, loss: 0.11993655562400818\n","step: 2610, loss: 0.09499617666006088\n","step: 2620, loss: 0.059820059686899185\n","step: 2630, loss: 0.0294992383569479\n","step: 2640, loss: 0.06071489676833153\n","step: 2650, loss: 0.08129973709583282\n","step: 2660, loss: 0.06143539026379585\n","step: 2670, loss: 0.08956682682037354\n","step: 2680, loss: 0.065582774579525\n","step: 2690, loss: 0.1301952302455902\n","step: 2700, loss: 0.1455499529838562\n","step: 2710, loss: 0.09024941176176071\n","step: 2720, loss: 0.09782596677541733\n","step: 2730, loss: 0.1260298788547516\n","step: 2740, loss: 0.09358879923820496\n","step: 2750, loss: 0.07880128920078278\n","step: 2760, loss: 0.20879095792770386\n","step: 2770, loss: 0.10466470569372177\n","step: 2780, loss: 0.0845227912068367\n","step: 2790, loss: 0.04602989926934242\n","step: 2800, loss: 0.05784301087260246\n","step: 2810, loss: 0.08978579193353653\n","step: 2820, loss: 0.019211014732718468\n","step: 2830, loss: 0.2086278349161148\n","step: 2840, loss: 0.06282293796539307\n","step: 2850, loss: 0.09703979641199112\n","step: 2860, loss: 0.05304085463285446\n","step: 2870, loss: 0.08636337518692017\n","step: 2880, loss: 0.08500724285840988\n","step: 2890, loss: 0.10178698599338531\n","step: 2900, loss: 0.0655018612742424\n","step: 2910, loss: 0.11410678178071976\n","step: 2920, loss: 0.03069000504910946\n","step: 2930, loss: 0.10128182917833328\n","step: 2940, loss: 0.05487411469221115\n","step: 2950, loss: 0.12883061170578003\n","step: 2960, loss: 0.10897349566221237\n","step: 2970, loss: 0.09217715263366699\n","step: 2980, loss: 0.07467127591371536\n","step: 2990, loss: 0.13613495230674744\n","step: 3000, loss: 0.2071104198694229\n","step: 3010, loss: 0.012955468147993088\n","step: 3020, loss: 0.10003624856472015\n","step: 3030, loss: 0.07243183255195618\n","step: 3040, loss: 0.14024895429611206\n","step: 3050, loss: 0.1276206076145172\n","step: 3060, loss: 0.1090189665555954\n","step: 3070, loss: 0.15652437508106232\n","step: 3080, loss: 0.07874184101819992\n","step: 3090, loss: 0.10540880262851715\n","step: 3100, loss: 0.13784708082675934\n","step: 3110, loss: 0.10246765613555908\n","step: 3120, loss: 0.032542768865823746\n","step: 3130, loss: 0.054450493305921555\n","step: 3140, loss: 0.12417388707399368\n","step: 3150, loss: 0.060985222458839417\n","step: 3160, loss: 0.09106481075286865\n","step: 3170, loss: 0.08034361153841019\n","step: 3180, loss: 0.11049506813287735\n","step: 3190, loss: 0.05955735221505165\n","step: 3200, loss: 0.08993589133024216\n","step: 3210, loss: 0.09238363802433014\n","step: 3220, loss: 0.07841809839010239\n","step: 3230, loss: 0.04009990394115448\n","step: 3240, loss: 0.06341394782066345\n","step: 3250, loss: 0.05333584174513817\n","step: 3260, loss: 0.020466839894652367\n","step: 3270, loss: 0.15757425129413605\n","step: 3280, loss: 0.08043775707483292\n","step: 3290, loss: 0.033738501369953156\n","step: 3300, loss: 0.10886761546134949\n","step: 3310, loss: 0.027671096846461296\n","step: 3320, loss: 0.07190132886171341\n","step: 3330, loss: 0.10267708450555801\n","step: 3340, loss: 0.17475257813930511\n","step: 3350, loss: 0.042615510523319244\n","step: 3360, loss: 0.053713638335466385\n","step: 3370, loss: 0.09794347733259201\n","step: 3380, loss: 0.060190849006175995\n","step: 3390, loss: 0.10964806377887726\n","step: 3400, loss: 0.06535845249891281\n","step: 3410, loss: 0.09400273114442825\n","step: 3420, loss: 0.0985398069024086\n","step: 3430, loss: 0.13071395456790924\n","step: 3440, loss: 0.08013597130775452\n","step: 3450, loss: 0.06487374007701874\n","step: 3460, loss: 0.023568635806441307\n","step: 3470, loss: 0.15785157680511475\n","step: 3480, loss: 0.12201203405857086\n","step: 3490, loss: 0.06162937730550766\n","step: 3500, loss: 0.05501949042081833\n","step: 3510, loss: 0.06279346346855164\n","step: 3520, loss: 0.07778774201869965\n","step: 3530, loss: 0.03973754122853279\n","step: 3540, loss: 0.12223606556653976\n","step: 3550, loss: 0.05700836330652237\n","step: 3560, loss: 0.10238964855670929\n","step: 3570, loss: 0.07380444556474686\n","step: 3580, loss: 0.10651793330907822\n","step: 3590, loss: 0.045399293303489685\n","step: 3600, loss: 0.05347716808319092\n","step: 3610, loss: 0.029817545786499977\n","step: 3620, loss: 0.0822858214378357\n","step: 3630, loss: 0.0819275751709938\n","step: 3640, loss: 0.07065938413143158\n","step: 3650, loss: 0.022057387977838516\n","step: 3660, loss: 0.06064189225435257\n","step: 3670, loss: 0.1335192620754242\n","step: 3680, loss: 0.05043549835681915\n","step: 3690, loss: 0.06602407991886139\n","step: 3700, loss: 0.08775044977664948\n","step: 3710, loss: 0.11373955011367798\n","step: 3720, loss: 0.07719767093658447\n","step: 3730, loss: 0.057656046003103256\n","step: 3740, loss: 0.1015053391456604\n","step: 3750, loss: 0.09559224545955658\n","step: 3760, loss: 0.04110930487513542\n","step: 3770, loss: 0.07798344641923904\n","step: 3780, loss: 0.036690711975097656\n","step: 3790, loss: 0.1513737440109253\n","step: 3800, loss: 0.05301640182733536\n","step: 3810, loss: 0.04911879450082779\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.70      1.00      0.82        35\n","           2       0.78      0.81      0.79        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.86      0.87      0.87       291\n","           5       0.93      0.85      0.89       294\n","           6       0.99      0.97      0.98      1570\n","           7       0.58      0.93      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.96      0.98      0.97       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.87      0.98      0.92        47\n","          13       0.11      0.38      0.18        13\n","          14       0.36      1.00      0.52        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.91      1.00      0.96        32\n","          19       1.00      0.23      0.37        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.92      0.75      0.83      4175\n","          23       0.69      0.97      0.81      2253\n","          24       0.18      0.25      0.21        44\n","          25       0.87      0.89      0.88       888\n","          26       0.90      1.00      0.95         9\n","          27       1.00      0.97      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.96      0.82      0.88      1136\n","          31       0.61      0.58      0.59        19\n","          32       1.00      0.62      0.77         8\n","          33       0.59      0.98      0.73        86\n","          34       0.27      0.69      0.39        32\n","          35       0.99      0.98      0.98       474\n","          36       0.86      0.16      0.28       182\n","          37       0.89      0.95      0.92      1592\n","          38       0.98      0.97      0.98       404\n","          39       0.97      0.94      0.95       485\n","          40       0.93      0.92      0.92       573\n","          41       0.95      0.93      0.94       841\n","          42       0.98      0.98      0.98       575\n","          43       0.96      0.93      0.95       152\n","          44       0.91      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.87      0.73      0.79        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.81      0.79     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 448\n","\n","Loop 4\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9929401874542236\n","step: 10, loss: 1.8011258840560913\n","step: 20, loss: 0.7792457342147827\n","step: 30, loss: 0.43431463837623596\n","step: 40, loss: 0.31532877683639526\n","step: 50, loss: 0.3240410387516022\n","step: 60, loss: 0.15017040073871613\n","step: 70, loss: 0.39023250341415405\n","step: 80, loss: 0.3229660987854004\n","step: 90, loss: 0.10904449224472046\n","step: 100, loss: 0.12808051705360413\n","step: 110, loss: 0.12374233454465866\n","step: 120, loss: 0.23023970425128937\n","step: 130, loss: 0.14011384546756744\n","step: 140, loss: 0.20931710302829742\n","step: 150, loss: 0.08408397436141968\n","step: 160, loss: 0.14771296083927155\n","step: 170, loss: 0.1929650753736496\n","step: 180, loss: 0.23879700899124146\n","step: 190, loss: 0.22340290248394012\n","step: 200, loss: 0.09469575434923172\n","step: 210, loss: 0.08287011831998825\n","step: 220, loss: 0.09883993864059448\n","step: 230, loss: 0.09361308813095093\n","step: 240, loss: 0.15433791279792786\n","step: 250, loss: 0.05646607279777527\n","step: 260, loss: 0.11651411652565002\n","step: 270, loss: 0.06391317397356033\n","step: 280, loss: 0.13326339423656464\n","step: 290, loss: 0.12718868255615234\n","step: 300, loss: 0.08816970884799957\n","step: 310, loss: 0.15747888386249542\n","step: 320, loss: 0.05496821179986\n","step: 330, loss: 0.10464271903038025\n","step: 340, loss: 0.09219405055046082\n","step: 350, loss: 0.14623290300369263\n","step: 360, loss: 0.17189739644527435\n","step: 370, loss: 0.05306004732847214\n","step: 380, loss: 0.09577998518943787\n","step: 390, loss: 0.14316798746585846\n","step: 400, loss: 0.12105248123407364\n","step: 410, loss: 0.04829578101634979\n","step: 420, loss: 0.06134580075740814\n","step: 430, loss: 0.07643208652734756\n","step: 440, loss: 0.06548453867435455\n","step: 450, loss: 0.04301770031452179\n","step: 460, loss: 0.052731048315763474\n","step: 470, loss: 0.03362754359841347\n","step: 480, loss: 0.0581800639629364\n","step: 490, loss: 0.157399982213974\n","step: 500, loss: 0.14477355778217316\n","step: 510, loss: 0.06415007263422012\n","step: 520, loss: 0.024178484454751015\n","step: 530, loss: 0.1258210986852646\n","step: 540, loss: 0.1784324049949646\n","step: 550, loss: 0.08634131401777267\n","step: 560, loss: 0.06096018850803375\n","step: 570, loss: 0.1228996142745018\n","step: 580, loss: 0.11186974495649338\n","step: 590, loss: 0.07052267342805862\n","step: 600, loss: 0.1002473384141922\n","step: 610, loss: 0.09343280643224716\n","step: 620, loss: 0.107545405626297\n","step: 630, loss: 0.05986597016453743\n","step: 640, loss: 0.06243167817592621\n","step: 650, loss: 0.11803016811609268\n","step: 660, loss: 0.045961592346429825\n","step: 670, loss: 0.07011982053518295\n","step: 680, loss: 0.07493593543767929\n","step: 690, loss: 0.06783344596624374\n","step: 700, loss: 0.05672510340809822\n","step: 710, loss: 0.09035119414329529\n","step: 720, loss: 0.10351000726222992\n","step: 730, loss: 0.08487260341644287\n","step: 740, loss: 0.23448309302330017\n","step: 750, loss: 0.05416937544941902\n","step: 760, loss: 0.13380993902683258\n","step: 770, loss: 0.1587531566619873\n","step: 780, loss: 0.06989000737667084\n","step: 790, loss: 0.07864422351121902\n","step: 800, loss: 0.04217790439724922\n","step: 810, loss: 0.1537133753299713\n","step: 820, loss: 0.12663231790065765\n","step: 830, loss: 0.13115638494491577\n","step: 840, loss: 0.04623908922076225\n","step: 850, loss: 0.05878746137022972\n","step: 860, loss: 0.0818769633769989\n","step: 870, loss: 0.140424907207489\n","step: 880, loss: 0.10826822370290756\n","step: 890, loss: 0.12558601796627045\n","step: 900, loss: 0.05917663499712944\n","step: 910, loss: 0.06587023288011551\n","step: 920, loss: 0.07902900874614716\n","step: 930, loss: 0.2136734426021576\n","step: 940, loss: 0.09770350158214569\n","step: 950, loss: 0.1378149688243866\n","step: 960, loss: 0.06301009654998779\n","step: 970, loss: 0.06975941359996796\n","step: 980, loss: 0.031032836064696312\n","step: 990, loss: 0.18532024323940277\n","step: 1000, loss: 0.05851078778505325\n","step: 1010, loss: 0.09629642218351364\n","step: 1020, loss: 0.14618130028247833\n","step: 1030, loss: 0.0571000799536705\n","step: 1040, loss: 0.13749822974205017\n","step: 1050, loss: 0.1323845535516739\n","step: 1060, loss: 0.035497356206178665\n","step: 1070, loss: 0.026638807728886604\n","step: 1080, loss: 0.08369137346744537\n","step: 1090, loss: 0.14502499997615814\n","step: 1100, loss: 0.06334993243217468\n","step: 1110, loss: 0.12442285567522049\n","step: 1120, loss: 0.11788278073072433\n","step: 1130, loss: 0.02140060067176819\n","step: 1140, loss: 0.08746256679296494\n","step: 1150, loss: 0.09085475653409958\n","step: 1160, loss: 0.09362436830997467\n","step: 1170, loss: 0.09362298250198364\n","step: 1180, loss: 0.06901859492063522\n","step: 1190, loss: 0.08064275979995728\n","step: 1200, loss: 0.15989279747009277\n","step: 1210, loss: 0.2860633432865143\n","step: 1220, loss: 0.08681539446115494\n","step: 1230, loss: 0.036614954471588135\n","step: 1240, loss: 0.19196240603923798\n","step: 1250, loss: 0.19323121011257172\n","step: 1260, loss: 0.13203276693820953\n","step: 1270, loss: 0.09845523536205292\n","step: 1280, loss: 0.05451112985610962\n","step: 1290, loss: 0.08005663752555847\n","step: 1300, loss: 0.14745336771011353\n","step: 1310, loss: 0.09305236488580704\n","step: 1320, loss: 0.05353565141558647\n","step: 1330, loss: 0.05577198043465614\n","step: 1340, loss: 0.09600794315338135\n","step: 1350, loss: 0.11380163580179214\n","step: 1360, loss: 0.07658927142620087\n","step: 1370, loss: 0.07335855811834335\n","step: 1380, loss: 0.04994754120707512\n","step: 1390, loss: 0.0768347680568695\n","step: 1400, loss: 0.04182656481862068\n","step: 1410, loss: 0.027742957696318626\n","step: 1420, loss: 0.10060147941112518\n","step: 1430, loss: 0.06883049011230469\n","step: 1440, loss: 0.041894227266311646\n","step: 1450, loss: 0.07538207620382309\n","step: 1460, loss: 0.03940146788954735\n","step: 1470, loss: 0.1601950079202652\n","step: 1480, loss: 0.07411346584558487\n","step: 1490, loss: 0.02350742556154728\n","step: 1500, loss: 0.05819350853562355\n","step: 1510, loss: 0.13755032420158386\n","step: 1520, loss: 0.04203959181904793\n","step: 1530, loss: 0.14529526233673096\n","step: 1540, loss: 0.03239819407463074\n","step: 1550, loss: 0.0743432566523552\n","step: 1560, loss: 0.07277537137269974\n","step: 1570, loss: 0.051602210849523544\n","step: 1580, loss: 0.09134478867053986\n","step: 1590, loss: 0.0986224040389061\n","step: 1600, loss: 0.06058390438556671\n","step: 1610, loss: 0.09535444527864456\n","step: 1620, loss: 0.13397015631198883\n","step: 1630, loss: 0.13361771404743195\n","step: 1640, loss: 0.07712302356958389\n","step: 1650, loss: 0.1999456286430359\n","step: 1660, loss: 0.10691744089126587\n","step: 1670, loss: 0.13952034711837769\n","step: 1680, loss: 0.08260006457567215\n","step: 1690, loss: 0.09172294288873672\n","step: 1700, loss: 0.1138543039560318\n","step: 1710, loss: 0.09923134744167328\n","step: 1720, loss: 0.07784659415483475\n","step: 1730, loss: 0.09215177595615387\n","step: 1740, loss: 0.04880819469690323\n","step: 1750, loss: 0.10070663690567017\n","step: 1760, loss: 0.09383916109800339\n","step: 1770, loss: 0.046536631882190704\n","step: 1780, loss: 0.157760351896286\n","step: 1790, loss: 0.12116186320781708\n","step: 1800, loss: 0.08054652065038681\n","step: 1810, loss: 0.12385372072458267\n","step: 1820, loss: 0.043356362730264664\n","step: 1830, loss: 0.06001373380422592\n","step: 1840, loss: 0.21941474080085754\n","step: 1850, loss: 0.12586142122745514\n","step: 1860, loss: 0.08919861912727356\n","step: 1870, loss: 0.1480168104171753\n","step: 1880, loss: 0.07279908657073975\n","step: 1890, loss: 0.07789599150419235\n","step: 1900, loss: 0.03593561053276062\n","step: 1910, loss: 0.06499770283699036\n","step: 1920, loss: 0.14353546500205994\n","step: 1930, loss: 0.07140351086854935\n","step: 1940, loss: 0.04550916701555252\n","step: 1950, loss: 0.07213125377893448\n","step: 1960, loss: 0.02415195293724537\n","step: 1970, loss: 0.09283695369958878\n","step: 1980, loss: 0.14041432738304138\n","step: 1990, loss: 0.06047515571117401\n","step: 2000, loss: 0.06283634901046753\n","step: 2010, loss: 0.095284603536129\n","step: 2020, loss: 0.12595270574092865\n","step: 2030, loss: 0.04163578152656555\n","step: 2040, loss: 0.10536761581897736\n","step: 2050, loss: 0.10054676979780197\n","step: 2060, loss: 0.09773776680231094\n","step: 2070, loss: 0.16690756380558014\n","step: 2080, loss: 0.11916334927082062\n","step: 2090, loss: 0.14187945425510406\n","step: 2100, loss: 0.1529160737991333\n","step: 2110, loss: 0.12423970550298691\n","step: 2120, loss: 0.07707787305116653\n","step: 2130, loss: 0.14106006920337677\n","step: 2140, loss: 0.10665009915828705\n","step: 2150, loss: 0.11271805316209793\n","step: 2160, loss: 0.07831623405218124\n","step: 2170, loss: 0.09550806879997253\n","step: 2180, loss: 0.10870574414730072\n","step: 2190, loss: 0.06346466392278671\n","step: 2200, loss: 0.07937014847993851\n","step: 2210, loss: 0.10535036772489548\n","step: 2220, loss: 0.061610106378793716\n","step: 2230, loss: 0.1110876128077507\n","step: 2240, loss: 0.14389380812644958\n","step: 2250, loss: 0.12147961556911469\n","step: 2260, loss: 0.047770846635103226\n","step: 2270, loss: 0.05091040953993797\n","step: 2280, loss: 0.032344039529561996\n","step: 2290, loss: 0.04958958551287651\n","step: 2300, loss: 0.11421437561511993\n","step: 2310, loss: 0.05909639224410057\n","step: 2320, loss: 0.17108027637004852\n","step: 2330, loss: 0.17231985926628113\n","step: 2340, loss: 0.05366860702633858\n","step: 2350, loss: 0.08272441476583481\n","step: 2360, loss: 0.10645440220832825\n","step: 2370, loss: 0.11459745466709137\n","step: 2380, loss: 0.1806723177433014\n","step: 2390, loss: 0.0528063140809536\n","step: 2400, loss: 0.06161023676395416\n","step: 2410, loss: 0.056019317358732224\n","step: 2420, loss: 0.0881710797548294\n","step: 2430, loss: 0.0592779777944088\n","step: 2440, loss: 0.09972260892391205\n","step: 2450, loss: 0.094655342400074\n","step: 2460, loss: 0.08969531953334808\n","step: 2470, loss: 0.10104349255561829\n","step: 2480, loss: 0.05244957283139229\n","step: 2490, loss: 0.09175889939069748\n","step: 2500, loss: 0.09849881380796432\n","step: 2510, loss: 0.039931394159793854\n","step: 2520, loss: 0.13730254769325256\n","step: 2530, loss: 0.10079869627952576\n","step: 2540, loss: 0.07537954300642014\n","step: 2550, loss: 0.07770264893770218\n","step: 2560, loss: 0.11435656994581223\n","step: 2570, loss: 0.11315704137086868\n","step: 2580, loss: 0.06713427603244781\n","step: 2590, loss: 0.04533186927437782\n","step: 2600, loss: 0.029713431373238564\n","step: 2610, loss: 0.12933211028575897\n","step: 2620, loss: 0.14595963060855865\n","step: 2630, loss: 0.06869249045848846\n","step: 2640, loss: 0.11980739235877991\n","step: 2650, loss: 0.13986921310424805\n","step: 2660, loss: 0.05251694098114967\n","step: 2670, loss: 0.08005170524120331\n","step: 2680, loss: 0.11236532032489777\n","step: 2690, loss: 0.07160217314958572\n","step: 2700, loss: 0.09324674308300018\n","step: 2710, loss: 0.06006685644388199\n","step: 2720, loss: 0.03815138712525368\n","step: 2730, loss: 0.1721823811531067\n","step: 2740, loss: 0.06737790256738663\n","step: 2750, loss: 0.1372731477022171\n","step: 2760, loss: 0.1024390161037445\n","step: 2770, loss: 0.0638096034526825\n","step: 2780, loss: 0.09749195724725723\n","step: 2790, loss: 0.07555459439754486\n","step: 2800, loss: 0.10587679594755173\n","step: 2810, loss: 0.07227589190006256\n","step: 2820, loss: 0.1088850125670433\n","step: 2830, loss: 0.06442991644144058\n","step: 2840, loss: 0.08417302370071411\n","step: 2850, loss: 0.14517201483249664\n","step: 2860, loss: 0.056421712040901184\n","step: 2870, loss: 0.1092456504702568\n","step: 2880, loss: 0.18555276095867157\n","step: 2890, loss: 0.06324209272861481\n","step: 2900, loss: 0.1567181497812271\n","step: 2910, loss: 0.07622633874416351\n","step: 2920, loss: 0.05321396887302399\n","step: 2930, loss: 0.036508623510599136\n","step: 2940, loss: 0.03975338861346245\n","step: 2950, loss: 0.06700842082500458\n","step: 2960, loss: 0.11444120109081268\n","step: 2970, loss: 0.09514588862657547\n","step: 2980, loss: 0.09798675775527954\n","step: 2990, loss: 0.086945541203022\n","step: 3000, loss: 0.10968606173992157\n","step: 3010, loss: 0.09713597595691681\n","step: 3020, loss: 0.18401867151260376\n","step: 3030, loss: 0.15513552725315094\n","step: 3040, loss: 0.07806756347417831\n","step: 3050, loss: 0.1215723305940628\n","step: 3060, loss: 0.12397509813308716\n","step: 3070, loss: 0.1003098413348198\n","step: 3080, loss: 0.10354489088058472\n","step: 3090, loss: 0.1490161120891571\n","step: 3100, loss: 0.0789891853928566\n","step: 3110, loss: 0.12506437301635742\n","step: 3120, loss: 0.03450533375144005\n","step: 3130, loss: 0.04048766940832138\n","step: 3140, loss: 0.22444291412830353\n","step: 3150, loss: 0.07172238081693649\n","step: 3160, loss: 0.11253838241100311\n","step: 3170, loss: 0.06482275575399399\n","step: 3180, loss: 0.09022056311368942\n","step: 3190, loss: 0.12607422471046448\n","step: 3200, loss: 0.060393430292606354\n","step: 3210, loss: 0.0786639153957367\n","step: 3220, loss: 0.09867753088474274\n","step: 3230, loss: 0.1480053812265396\n","step: 3240, loss: 0.11452972143888474\n","step: 3250, loss: 0.046540435403585434\n","step: 3260, loss: 0.1351945847272873\n","step: 3270, loss: 0.04355790093541145\n","step: 3280, loss: 0.08561612665653229\n","step: 3290, loss: 0.0772077739238739\n","step: 3300, loss: 0.03846462070941925\n","step: 3310, loss: 0.21840651333332062\n","step: 3320, loss: 0.12260248512029648\n","step: 3330, loss: 0.06560897827148438\n","step: 3340, loss: 0.07900572568178177\n","step: 3350, loss: 0.09308274835348129\n","step: 3360, loss: 0.10575254261493683\n","step: 3370, loss: 0.19585460424423218\n","step: 3380, loss: 0.0766526386141777\n","step: 3390, loss: 0.12565551698207855\n","step: 3400, loss: 0.051492154598236084\n","step: 3410, loss: 0.08160583674907684\n","step: 3420, loss: 0.04586681351065636\n","step: 3430, loss: 0.05541655048727989\n","step: 3440, loss: 0.06188071146607399\n","step: 3450, loss: 0.13082394003868103\n","step: 3460, loss: 0.034308623522520065\n","step: 3470, loss: 0.09534301608800888\n","step: 3480, loss: 0.05465561896562576\n","step: 3490, loss: 0.028499463573098183\n","step: 3500, loss: 0.10920829325914383\n","step: 3510, loss: 0.06574706733226776\n","step: 3520, loss: 0.03239370882511139\n","step: 3530, loss: 0.012822115793824196\n","step: 3540, loss: 0.10571732372045517\n","step: 3550, loss: 0.02282358892261982\n","step: 3560, loss: 0.13849025964736938\n","step: 3570, loss: 0.1326708048582077\n","step: 3580, loss: 0.04932845011353493\n","step: 3590, loss: 0.09999363869428635\n","step: 3600, loss: 0.03803811967372894\n","step: 3610, loss: 0.049807965755462646\n","step: 3620, loss: 0.12939269840717316\n","step: 3630, loss: 0.10331995040178299\n","step: 3640, loss: 0.07426384091377258\n","step: 3650, loss: 0.11561643332242966\n","step: 3660, loss: 0.04939429461956024\n","step: 3670, loss: 0.06465435773134232\n","step: 3680, loss: 0.08134595304727554\n","step: 3690, loss: 0.0900319293141365\n","step: 3700, loss: 0.08163953572511673\n","step: 3710, loss: 0.04008879140019417\n","step: 3720, loss: 0.042417604476213455\n","step: 3730, loss: 0.0641973465681076\n","step: 3740, loss: 0.033800698816776276\n","step: 3750, loss: 0.0396273098886013\n","step: 3760, loss: 0.10650759935379028\n","step: 3770, loss: 0.06821628659963608\n","step: 3780, loss: 0.10446289926767349\n","step: 3790, loss: 0.13482420146465302\n","step: 3800, loss: 0.03187849000096321\n","step: 3810, loss: 0.029986264184117317\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.69      1.00      0.81        35\n","           2       0.46      0.84      0.59        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.90      0.84      0.87       291\n","           5       0.96      0.84      0.89       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.59      0.94      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.96      0.97      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.25      0.08      0.12        13\n","          14       0.39      1.00      0.56        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.89      0.85      0.87      1151\n","          17       0.95      0.95      0.95        41\n","          18       0.93      0.81      0.87        32\n","          19       0.70      0.35      0.47        40\n","          20       1.00      1.00      1.00       584\n","          21       0.12      0.04      0.06        52\n","          22       0.92      0.73      0.81      4175\n","          23       0.68      0.96      0.80      2253\n","          24       0.32      0.73      0.45        44\n","          25       0.85      0.89      0.87       888\n","          26       0.67      0.89      0.76         9\n","          27       0.89      0.97      0.93        69\n","          28       1.00      0.97      0.98      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.91      0.87      0.89      1136\n","          31       0.65      0.79      0.71        19\n","          32       0.50      0.75      0.60         8\n","          33       0.67      0.93      0.78        86\n","          34       0.21      0.66      0.32        32\n","          35       0.98      1.00      0.99       474\n","          36       0.94      0.17      0.29       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.94      0.97      0.96       404\n","          39       0.94      0.97      0.96       485\n","          40       0.93      0.94      0.93       573\n","          41       0.95      0.93      0.94       841\n","          42       0.98      0.99      0.98       575\n","          43       0.96      0.89      0.92       152\n","          44       0.97      0.92      0.95        75\n","          46       0.98      0.99      0.98        82\n","          48       0.69      0.32      0.43        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 450\n","\n","Loop 5\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9658899307250977\n","step: 10, loss: 1.872463583946228\n","step: 20, loss: 0.605051577091217\n","step: 30, loss: 0.4266273081302643\n","step: 40, loss: 0.2592635750770569\n","step: 50, loss: 0.21919898688793182\n","step: 60, loss: 0.27755704522132874\n","step: 70, loss: 0.14587299525737762\n","step: 80, loss: 0.1999373435974121\n","step: 90, loss: 0.19328512251377106\n","step: 100, loss: 0.23573406040668488\n","step: 110, loss: 0.2153569906949997\n","step: 120, loss: 0.2128649353981018\n","step: 130, loss: 0.1998467743396759\n","step: 140, loss: 0.16430523991584778\n","step: 150, loss: 0.11073765158653259\n","step: 160, loss: 0.24567408859729767\n","step: 170, loss: 0.23478931188583374\n","step: 180, loss: 0.07636728137731552\n","step: 190, loss: 0.20560121536254883\n","step: 200, loss: 0.15440362691879272\n","step: 210, loss: 0.09839971363544464\n","step: 220, loss: 0.15984287858009338\n","step: 230, loss: 0.1384783685207367\n","step: 240, loss: 0.11087741702795029\n","step: 250, loss: 0.1655322015285492\n","step: 260, loss: 0.11686563491821289\n","step: 270, loss: 0.10261361300945282\n","step: 280, loss: 0.07310356944799423\n","step: 290, loss: 0.034298598766326904\n","step: 300, loss: 0.12785576283931732\n","step: 310, loss: 0.08297054469585419\n","step: 320, loss: 0.0963524803519249\n","step: 330, loss: 0.1270006150007248\n","step: 340, loss: 0.052808232605457306\n","step: 350, loss: 0.05724756047129631\n","step: 360, loss: 0.17648790776729584\n","step: 370, loss: 0.061139702796936035\n","step: 380, loss: 0.13642637431621552\n","step: 390, loss: 0.08706634491682053\n","step: 400, loss: 0.2228289246559143\n","step: 410, loss: 0.12646052241325378\n","step: 420, loss: 0.07881693542003632\n","step: 430, loss: 0.11339090764522552\n","step: 440, loss: 0.13941359519958496\n","step: 450, loss: 0.07135381549596786\n","step: 460, loss: 0.09905894845724106\n","step: 470, loss: 0.13909408450126648\n","step: 480, loss: 0.06343070417642593\n","step: 490, loss: 0.20008282363414764\n","step: 500, loss: 0.10543899983167648\n","step: 510, loss: 0.08062086254358292\n","step: 520, loss: 0.1456729620695114\n","step: 530, loss: 0.22808332741260529\n","step: 540, loss: 0.1949804425239563\n","step: 550, loss: 0.08547774702310562\n","step: 560, loss: 0.07756563276052475\n","step: 570, loss: 0.041926924139261246\n","step: 580, loss: 0.1963106095790863\n","step: 590, loss: 0.051585420966148376\n","step: 600, loss: 0.08466920256614685\n","step: 610, loss: 0.13164383172988892\n","step: 620, loss: 0.2049245834350586\n","step: 630, loss: 0.1057216003537178\n","step: 640, loss: 0.09375544637441635\n","step: 650, loss: 0.08443337678909302\n","step: 660, loss: 0.12588782608509064\n","step: 670, loss: 0.06504670530557632\n","step: 680, loss: 0.1387472152709961\n","step: 690, loss: 0.15225589275360107\n","step: 700, loss: 0.10376196354627609\n","step: 710, loss: 0.035002779215574265\n","step: 720, loss: 0.07898937165737152\n","step: 730, loss: 0.11146534234285355\n","step: 740, loss: 0.09934493154287338\n","step: 750, loss: 0.14763888716697693\n","step: 760, loss: 0.1472674459218979\n","step: 770, loss: 0.06678441166877747\n","step: 780, loss: 0.09834115207195282\n","step: 790, loss: 0.11587534099817276\n","step: 800, loss: 0.04572960361838341\n","step: 810, loss: 0.08233938366174698\n","step: 820, loss: 0.1658502072095871\n","step: 830, loss: 0.060417722910642624\n","step: 840, loss: 0.05553070083260536\n","step: 850, loss: 0.04519861191511154\n","step: 860, loss: 0.024813340976834297\n","step: 870, loss: 0.1375494748353958\n","step: 880, loss: 0.07642266154289246\n","step: 890, loss: 0.04690563678741455\n","step: 900, loss: 0.16485275328159332\n","step: 910, loss: 0.18870793282985687\n","step: 920, loss: 0.08874478936195374\n","step: 930, loss: 0.09557542204856873\n","step: 940, loss: 0.12997880578041077\n","step: 950, loss: 0.12496646493673325\n","step: 960, loss: 0.04085914045572281\n","step: 970, loss: 0.1353142112493515\n","step: 980, loss: 0.14494337141513824\n","step: 990, loss: 0.15883485972881317\n","step: 1000, loss: 0.06443873792886734\n","step: 1010, loss: 0.07502159476280212\n","step: 1020, loss: 0.09117821604013443\n","step: 1030, loss: 0.0459129624068737\n","step: 1040, loss: 0.09441010653972626\n","step: 1050, loss: 0.0981435626745224\n","step: 1060, loss: 0.10070126503705978\n","step: 1070, loss: 0.08035582304000854\n","step: 1080, loss: 0.11671042442321777\n","step: 1090, loss: 0.10856738686561584\n","step: 1100, loss: 0.09640934318304062\n","step: 1110, loss: 0.10496324300765991\n","step: 1120, loss: 0.12801863253116608\n","step: 1130, loss: 0.09919380396604538\n","step: 1140, loss: 0.10042048990726471\n","step: 1150, loss: 0.0787290632724762\n","step: 1160, loss: 0.08143079280853271\n","step: 1170, loss: 0.08727146685123444\n","step: 1180, loss: 0.15234732627868652\n","step: 1190, loss: 0.11198455840349197\n","step: 1200, loss: 0.19147025048732758\n","step: 1210, loss: 0.1625942438840866\n","step: 1220, loss: 0.08348429203033447\n","step: 1230, loss: 0.06693320721387863\n","step: 1240, loss: 0.07987906783819199\n","step: 1250, loss: 0.08268208801746368\n","step: 1260, loss: 0.19724281132221222\n","step: 1270, loss: 0.07591496407985687\n","step: 1280, loss: 0.07984559237957001\n","step: 1290, loss: 0.10588793456554413\n","step: 1300, loss: 0.06574150174856186\n","step: 1310, loss: 0.17950859665870667\n","step: 1320, loss: 0.05231791362166405\n","step: 1330, loss: 0.09008566290140152\n","step: 1340, loss: 0.06086825951933861\n","step: 1350, loss: 0.0449049137532711\n","step: 1360, loss: 0.12341950833797455\n","step: 1370, loss: 0.054320670664310455\n","step: 1380, loss: 0.18212196230888367\n","step: 1390, loss: 0.1128765195608139\n","step: 1400, loss: 0.09002683311700821\n","step: 1410, loss: 0.15495668351650238\n","step: 1420, loss: 0.12131654471158981\n","step: 1430, loss: 0.11291597038507462\n","step: 1440, loss: 0.09451106190681458\n","step: 1450, loss: 0.07636971026659012\n","step: 1460, loss: 0.12253156304359436\n","step: 1470, loss: 0.05889756605029106\n","step: 1480, loss: 0.04122228920459747\n","step: 1490, loss: 0.11263438314199448\n","step: 1500, loss: 0.12903936207294464\n","step: 1510, loss: 0.07076562196016312\n","step: 1520, loss: 0.07692631334066391\n","step: 1530, loss: 0.04902531951665878\n","step: 1540, loss: 0.12854228913784027\n","step: 1550, loss: 0.11722338199615479\n","step: 1560, loss: 0.104471355676651\n","step: 1570, loss: 0.07504899799823761\n","step: 1580, loss: 0.11544269323348999\n","step: 1590, loss: 0.11412011831998825\n","step: 1600, loss: 0.03505276143550873\n","step: 1610, loss: 0.0725942924618721\n","step: 1620, loss: 0.0997493788599968\n","step: 1630, loss: 0.03513171523809433\n","step: 1640, loss: 0.12042698264122009\n","step: 1650, loss: 0.19389493763446808\n","step: 1660, loss: 0.060848236083984375\n","step: 1670, loss: 0.14549891650676727\n","step: 1680, loss: 0.08589831739664078\n","step: 1690, loss: 0.0524313822388649\n","step: 1700, loss: 0.09203950315713882\n","step: 1710, loss: 0.0465354360640049\n","step: 1720, loss: 0.08785942941904068\n","step: 1730, loss: 0.0852280855178833\n","step: 1740, loss: 0.11700719594955444\n","step: 1750, loss: 0.11286269128322601\n","step: 1760, loss: 0.14510437846183777\n","step: 1770, loss: 0.09220877289772034\n","step: 1780, loss: 0.1734624058008194\n","step: 1790, loss: 0.05139435827732086\n","step: 1800, loss: 0.06835579872131348\n","step: 1810, loss: 0.10351374000310898\n","step: 1820, loss: 0.04663575813174248\n","step: 1830, loss: 0.09421896189451218\n","step: 1840, loss: 0.11461251229047775\n","step: 1850, loss: 0.07564567774534225\n","step: 1860, loss: 0.09304115921258926\n","step: 1870, loss: 0.06315018981695175\n","step: 1880, loss: 0.1036301776766777\n","step: 1890, loss: 0.060603268444538116\n","step: 1900, loss: 0.20070190727710724\n","step: 1910, loss: 0.08998142927885056\n","step: 1920, loss: 0.04272888973355293\n","step: 1930, loss: 0.053101688623428345\n","step: 1940, loss: 0.09973093122243881\n","step: 1950, loss: 0.042990542948246\n","step: 1960, loss: 0.03873196989297867\n","step: 1970, loss: 0.12648069858551025\n","step: 1980, loss: 0.11617682129144669\n","step: 1990, loss: 0.08952818065881729\n","step: 2000, loss: 0.053982965648174286\n","step: 2010, loss: 0.09079147130250931\n","step: 2020, loss: 0.0968490019440651\n","step: 2030, loss: 0.11189096421003342\n","step: 2040, loss: 0.16788245737552643\n","step: 2050, loss: 0.09644705802202225\n","step: 2060, loss: 0.12220738083124161\n","step: 2070, loss: 0.10041793435811996\n","step: 2080, loss: 0.1584993600845337\n","step: 2090, loss: 0.09720455855131149\n","step: 2100, loss: 0.11002949625253677\n","step: 2110, loss: 0.1687563955783844\n","step: 2120, loss: 0.14676962792873383\n","step: 2130, loss: 0.08899674564599991\n","step: 2140, loss: 0.13881540298461914\n","step: 2150, loss: 0.21900200843811035\n","step: 2160, loss: 0.06313346326351166\n","step: 2170, loss: 0.10633645951747894\n","step: 2180, loss: 0.24166448414325714\n","step: 2190, loss: 0.16086091101169586\n","step: 2200, loss: 0.07113883644342422\n","step: 2210, loss: 0.09245029091835022\n","step: 2220, loss: 0.0629163384437561\n","step: 2230, loss: 0.08869434148073196\n","step: 2240, loss: 0.1140260100364685\n","step: 2250, loss: 0.03257571533322334\n","step: 2260, loss: 0.065702885389328\n","step: 2270, loss: 0.0774327963590622\n","step: 2280, loss: 0.08187997341156006\n","step: 2290, loss: 0.07436534017324448\n","step: 2300, loss: 0.16328103840351105\n","step: 2310, loss: 0.049858931452035904\n","step: 2320, loss: 0.11340753734111786\n","step: 2330, loss: 0.1158868744969368\n","step: 2340, loss: 0.07385533303022385\n","step: 2350, loss: 0.0924285501241684\n","step: 2360, loss: 0.06612129509449005\n","step: 2370, loss: 0.10233616083860397\n","step: 2380, loss: 0.06910927593708038\n","step: 2390, loss: 0.029869453981518745\n","step: 2400, loss: 0.03828856348991394\n","step: 2410, loss: 0.0559026300907135\n","step: 2420, loss: 0.05623781308531761\n","step: 2430, loss: 0.07393930852413177\n","step: 2440, loss: 0.1528523713350296\n","step: 2450, loss: 0.08385831862688065\n","step: 2460, loss: 0.19642409682273865\n","step: 2470, loss: 0.15452583134174347\n","step: 2480, loss: 0.12098684161901474\n","step: 2490, loss: 0.10171978920698166\n","step: 2500, loss: 0.05614403635263443\n","step: 2510, loss: 0.09910952299833298\n","step: 2520, loss: 0.03364986181259155\n","step: 2530, loss: 0.07100295275449753\n","step: 2540, loss: 0.04249820485711098\n","step: 2550, loss: 0.1355915665626526\n","step: 2560, loss: 0.06696950644254684\n","step: 2570, loss: 0.04177563264966011\n","step: 2580, loss: 0.19403626024723053\n","step: 2590, loss: 0.12186456471681595\n","step: 2600, loss: 0.05054821819067001\n","step: 2610, loss: 0.1842842400074005\n","step: 2620, loss: 0.08681970834732056\n","step: 2630, loss: 0.11064347624778748\n","step: 2640, loss: 0.058540888130664825\n","step: 2650, loss: 0.04273110255599022\n","step: 2660, loss: 0.08387655764818192\n","step: 2670, loss: 0.046051137149333954\n","step: 2680, loss: 0.09271261096000671\n","step: 2690, loss: 0.0736270472407341\n","step: 2700, loss: 0.13258428871631622\n","step: 2710, loss: 0.13911987841129303\n","step: 2720, loss: 0.19871269166469574\n","step: 2730, loss: 0.12144394218921661\n","step: 2740, loss: 0.05152404308319092\n","step: 2750, loss: 0.08305984735488892\n","step: 2760, loss: 0.0729881003499031\n","step: 2770, loss: 0.08259142935276031\n","step: 2780, loss: 0.06907548755407333\n","step: 2790, loss: 0.08396895229816437\n","step: 2800, loss: 0.13215896487236023\n","step: 2810, loss: 0.06901508569717407\n","step: 2820, loss: 0.06508327275514603\n","step: 2830, loss: 0.09905190765857697\n","step: 2840, loss: 0.1397540122270584\n","step: 2850, loss: 0.0490206778049469\n","step: 2860, loss: 0.09032444655895233\n","step: 2870, loss: 0.16037890315055847\n","step: 2880, loss: 0.13853128254413605\n","step: 2890, loss: 0.07578467577695847\n","step: 2900, loss: 0.1018504649400711\n","step: 2910, loss: 0.09445994347333908\n","step: 2920, loss: 0.07832887768745422\n","step: 2930, loss: 0.07026566565036774\n","step: 2940, loss: 0.16819560527801514\n","step: 2950, loss: 0.14721743762493134\n","step: 2960, loss: 0.08649321645498276\n","step: 2970, loss: 0.08641255646944046\n","step: 2980, loss: 0.08807798475027084\n","step: 2990, loss: 0.1435631513595581\n","step: 3000, loss: 0.03589902073144913\n","step: 3010, loss: 0.09510990977287292\n","step: 3020, loss: 0.08430358022451401\n","step: 3030, loss: 0.05239098146557808\n","step: 3040, loss: 0.07994396984577179\n","step: 3050, loss: 0.11105877161026001\n","step: 3060, loss: 0.06011386588215828\n","step: 3070, loss: 0.07557104527950287\n","step: 3080, loss: 0.20864538848400116\n","step: 3090, loss: 0.05485108494758606\n","step: 3100, loss: 0.03386778011918068\n","step: 3110, loss: 0.09928636997938156\n","step: 3120, loss: 0.1078408807516098\n","step: 3130, loss: 0.1560242921113968\n","step: 3140, loss: 0.06968867033720016\n","step: 3150, loss: 0.07070125639438629\n","step: 3160, loss: 0.05154122784733772\n","step: 3170, loss: 0.0839192271232605\n","step: 3180, loss: 0.06052204966545105\n","step: 3190, loss: 0.0723508819937706\n","step: 3200, loss: 0.06316482275724411\n","step: 3210, loss: 0.06355661898851395\n","step: 3220, loss: 0.017111847177147865\n","step: 3230, loss: 0.027996951714158058\n","step: 3240, loss: 0.1326056569814682\n","step: 3250, loss: 0.14835508167743683\n","step: 3260, loss: 0.1130037009716034\n","step: 3270, loss: 0.13898931443691254\n","step: 3280, loss: 0.07850252091884613\n","step: 3290, loss: 0.07894562184810638\n","step: 3300, loss: 0.10791785269975662\n","step: 3310, loss: 0.07460011541843414\n","step: 3320, loss: 0.04227488487958908\n","step: 3330, loss: 0.08281800150871277\n","step: 3340, loss: 0.14340373873710632\n","step: 3350, loss: 0.07727064192295074\n","step: 3360, loss: 0.046159639954566956\n","step: 3370, loss: 0.10437218099832535\n","step: 3380, loss: 0.08034821599721909\n","step: 3390, loss: 0.06247425451874733\n","step: 3400, loss: 0.03615570813417435\n","step: 3410, loss: 0.05930516868829727\n","step: 3420, loss: 0.0870058611035347\n","step: 3430, loss: 0.048438191413879395\n","step: 3440, loss: 0.042799513787031174\n","step: 3450, loss: 0.2355179339647293\n","step: 3460, loss: 0.09118885546922684\n","step: 3470, loss: 0.04150889068841934\n","step: 3480, loss: 0.10128934681415558\n","step: 3490, loss: 0.11122950911521912\n","step: 3500, loss: 0.1188569888472557\n","step: 3510, loss: 0.02128441445529461\n","step: 3520, loss: 0.07746313512325287\n","step: 3530, loss: 0.1763770580291748\n","step: 3540, loss: 0.1037055179476738\n","step: 3550, loss: 0.1138252317905426\n","step: 3560, loss: 0.03634996712207794\n","step: 3570, loss: 0.1239403635263443\n","step: 3580, loss: 0.05225631967186928\n","step: 3590, loss: 0.07805430889129639\n","step: 3600, loss: 0.11409242451190948\n","step: 3610, loss: 0.053107231855392456\n","step: 3620, loss: 0.14939677715301514\n","step: 3630, loss: 0.05854835361242294\n","step: 3640, loss: 0.041066404432058334\n","step: 3650, loss: 0.07260646671056747\n","step: 3660, loss: 0.12299761176109314\n","step: 3670, loss: 0.14091823995113373\n","step: 3680, loss: 0.057506758719682693\n","step: 3690, loss: 0.0781518816947937\n","step: 3700, loss: 0.10559660941362381\n","step: 3710, loss: 0.06210615113377571\n","step: 3720, loss: 0.06701244413852692\n","step: 3730, loss: 0.03609083220362663\n","step: 3740, loss: 0.15280790627002716\n","step: 3750, loss: 0.10842373967170715\n","step: 3760, loss: 0.061777908354997635\n","step: 3770, loss: 0.05032883211970329\n","step: 3780, loss: 0.07260897755622864\n","step: 3790, loss: 0.1478942483663559\n","step: 3800, loss: 0.132158100605011\n","step: 3810, loss: 0.057581134140491486\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.81      1.00      0.90        35\n","           2       0.68      0.17      0.27        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.88      0.84      0.86       291\n","           5       0.98      0.84      0.90       294\n","           6       0.97      0.99      0.98      1570\n","           7       0.53      0.94      0.67       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.94      0.98      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.22      0.92      0.36        13\n","          14       0.23      1.00      0.38        43\n","          15       0.95      0.99      0.97      2778\n","          16       0.84      0.85      0.84      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.72      0.97      0.83        32\n","          19       0.06      0.03      0.04        40\n","          20       1.00      1.00      1.00       584\n","          21       0.56      0.10      0.16        52\n","          22       0.93      0.73      0.82      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.33      0.73      0.46        44\n","          25       0.86      0.90      0.88       888\n","          26       0.70      0.78      0.74         9\n","          27       0.96      0.97      0.96        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.93      0.87      0.90      1136\n","          31       0.61      0.58      0.59        19\n","          32       1.00      0.38      0.55         8\n","          33       0.79      0.94      0.86        86\n","          34       0.06      0.09      0.07        32\n","          35       1.00      0.96      0.98       474\n","          36       0.84      0.18      0.29       182\n","          37       0.90      0.93      0.91      1592\n","          38       0.97      0.97      0.97       404\n","          39       0.95      0.95      0.95       485\n","          40       0.92      0.95      0.94       573\n","          41       0.95      0.93      0.94       841\n","          42       0.96      0.99      0.98       575\n","          43       0.94      0.78      0.85       152\n","          44       0.88      0.95      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.50      0.06      0.11        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.78      0.75     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 451\n","\n","Loop 6\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9690277576446533\n","step: 10, loss: 1.9730043411254883\n","step: 20, loss: 0.6982095837593079\n","step: 30, loss: 0.36827242374420166\n","step: 40, loss: 0.24303676187992096\n","step: 50, loss: 0.22193560004234314\n","step: 60, loss: 0.21804594993591309\n","step: 70, loss: 0.15588627755641937\n","step: 80, loss: 0.1567804217338562\n","step: 90, loss: 0.19538702070713043\n","step: 100, loss: 0.211819589138031\n","step: 110, loss: 0.20212848484516144\n","step: 120, loss: 0.30111923813819885\n","step: 130, loss: 0.15470995008945465\n","step: 140, loss: 0.10736053436994553\n","step: 150, loss: 0.08039255440235138\n","step: 160, loss: 0.20649497210979462\n","step: 170, loss: 0.16342946887016296\n","step: 180, loss: 0.26707860827445984\n","step: 190, loss: 0.13523732125759125\n","step: 200, loss: 0.14004255831241608\n","step: 210, loss: 0.15971338748931885\n","step: 220, loss: 0.09040651470422745\n","step: 230, loss: 0.19888286292552948\n","step: 240, loss: 0.09856810420751572\n","step: 250, loss: 0.19965335726737976\n","step: 260, loss: 0.12924858927726746\n","step: 270, loss: 0.09043928980827332\n","step: 280, loss: 0.05036813020706177\n","step: 290, loss: 0.208124041557312\n","step: 300, loss: 0.1204996332526207\n","step: 310, loss: 0.0938347578048706\n","step: 320, loss: 0.10263905674219131\n","step: 330, loss: 0.10745161026716232\n","step: 340, loss: 0.11312009394168854\n","step: 350, loss: 0.13948360085487366\n","step: 360, loss: 0.15408135950565338\n","step: 370, loss: 0.12446599453687668\n","step: 380, loss: 0.23202812671661377\n","step: 390, loss: 0.20244956016540527\n","step: 400, loss: 0.08861050754785538\n","step: 410, loss: 0.18828189373016357\n","step: 420, loss: 0.12311571091413498\n","step: 430, loss: 0.0463939793407917\n","step: 440, loss: 0.12121035903692245\n","step: 450, loss: 0.1545068472623825\n","step: 460, loss: 0.13185341656208038\n","step: 470, loss: 0.19428788125514984\n","step: 480, loss: 0.09770646691322327\n","step: 490, loss: 0.13146087527275085\n","step: 500, loss: 0.1085716113448143\n","step: 510, loss: 0.11182239651679993\n","step: 520, loss: 0.0817483589053154\n","step: 530, loss: 0.07829242199659348\n","step: 540, loss: 0.08652549982070923\n","step: 550, loss: 0.108865886926651\n","step: 560, loss: 0.039712969213724136\n","step: 570, loss: 0.15898016095161438\n","step: 580, loss: 0.10073705017566681\n","step: 590, loss: 0.07319721579551697\n","step: 600, loss: 0.0818672701716423\n","step: 610, loss: 0.1580057442188263\n","step: 620, loss: 0.07302682101726532\n","step: 630, loss: 0.09431853890419006\n","step: 640, loss: 0.11736350506544113\n","step: 650, loss: 0.14588992297649384\n","step: 660, loss: 0.23810552060604095\n","step: 670, loss: 0.13754819333553314\n","step: 680, loss: 0.1291949599981308\n","step: 690, loss: 0.1218818947672844\n","step: 700, loss: 0.1414051055908203\n","step: 710, loss: 0.13837984204292297\n","step: 720, loss: 0.09611401706933975\n","step: 730, loss: 0.03767913579940796\n","step: 740, loss: 0.17659825086593628\n","step: 750, loss: 0.08072980493307114\n","step: 760, loss: 0.1423877328634262\n","step: 770, loss: 0.07640418410301208\n","step: 780, loss: 0.14213056862354279\n","step: 790, loss: 0.18699002265930176\n","step: 800, loss: 0.17742078006267548\n","step: 810, loss: 0.1039370596408844\n","step: 820, loss: 0.09496009349822998\n","step: 830, loss: 0.22929838299751282\n","step: 840, loss: 0.11471531540155411\n","step: 850, loss: 0.253810852766037\n","step: 860, loss: 0.22216714918613434\n","step: 870, loss: 0.037139248102903366\n","step: 880, loss: 0.12339626252651215\n","step: 890, loss: 0.04861217364668846\n","step: 900, loss: 0.050720952451229095\n","step: 910, loss: 0.09845728427171707\n","step: 920, loss: 0.039818208664655685\n","step: 930, loss: 0.09972508251667023\n","step: 940, loss: 0.1873154491186142\n","step: 950, loss: 0.1248311698436737\n","step: 960, loss: 0.07196921110153198\n","step: 970, loss: 0.06857128441333771\n","step: 980, loss: 0.11202109605073929\n","step: 990, loss: 0.06131252646446228\n","step: 1000, loss: 0.11266763508319855\n","step: 1010, loss: 0.037580978125333786\n","step: 1020, loss: 0.08922439813613892\n","step: 1030, loss: 0.116104856133461\n","step: 1040, loss: 0.09264320135116577\n","step: 1050, loss: 0.09655998647212982\n","step: 1060, loss: 0.06472139805555344\n","step: 1070, loss: 0.0985117182135582\n","step: 1080, loss: 0.05019605532288551\n","step: 1090, loss: 0.10316075384616852\n","step: 1100, loss: 0.07147813588380814\n","step: 1110, loss: 0.06932857632637024\n","step: 1120, loss: 0.07321789115667343\n","step: 1130, loss: 0.12046633660793304\n","step: 1140, loss: 0.04538609832525253\n","step: 1150, loss: 0.07062650471925735\n","step: 1160, loss: 0.05599795654416084\n","step: 1170, loss: 0.19641219079494476\n","step: 1180, loss: 0.07636718451976776\n","step: 1190, loss: 0.08458492159843445\n","step: 1200, loss: 0.11872605979442596\n","step: 1210, loss: 0.269403338432312\n","step: 1220, loss: 0.07257082313299179\n","step: 1230, loss: 0.0757235512137413\n","step: 1240, loss: 0.07142461091279984\n","step: 1250, loss: 0.126730278134346\n","step: 1260, loss: 0.13767214119434357\n","step: 1270, loss: 0.06253694742918015\n","step: 1280, loss: 0.12788821756839752\n","step: 1290, loss: 0.05146770179271698\n","step: 1300, loss: 0.08152562379837036\n","step: 1310, loss: 0.21369022130966187\n","step: 1320, loss: 0.09904109686613083\n","step: 1330, loss: 0.0976383164525032\n","step: 1340, loss: 0.10345368832349777\n","step: 1350, loss: 0.07484884560108185\n","step: 1360, loss: 0.09869662672281265\n","step: 1370, loss: 0.12777402997016907\n","step: 1380, loss: 0.05295076593756676\n","step: 1390, loss: 0.10482459515333176\n","step: 1400, loss: 0.0595763698220253\n","step: 1410, loss: 0.09730533510446548\n","step: 1420, loss: 0.0732751339673996\n","step: 1430, loss: 0.09290700405836105\n","step: 1440, loss: 0.057431090623140335\n","step: 1450, loss: 0.19200405478477478\n","step: 1460, loss: 0.0700470432639122\n","step: 1470, loss: 0.04329230636358261\n","step: 1480, loss: 0.10244292765855789\n","step: 1490, loss: 0.08279585838317871\n","step: 1500, loss: 0.09512948244810104\n","step: 1510, loss: 0.03715525195002556\n","step: 1520, loss: 0.13787689805030823\n","step: 1530, loss: 0.07586736977100372\n","step: 1540, loss: 0.024664336815476418\n","step: 1550, loss: 0.0388617143034935\n","step: 1560, loss: 0.04946388676762581\n","step: 1570, loss: 0.12748494744300842\n","step: 1580, loss: 0.0691634863615036\n","step: 1590, loss: 0.050062697380781174\n","step: 1600, loss: 0.07511557638645172\n","step: 1610, loss: 0.11047233641147614\n","step: 1620, loss: 0.023717522621154785\n","step: 1630, loss: 0.08183896541595459\n","step: 1640, loss: 0.07925321906805038\n","step: 1650, loss: 0.04265674203634262\n","step: 1660, loss: 0.15875588357448578\n","step: 1670, loss: 0.07809653878211975\n","step: 1680, loss: 0.0669233649969101\n","step: 1690, loss: 0.13485923409461975\n","step: 1700, loss: 0.08580204844474792\n","step: 1710, loss: 0.08880136907100677\n","step: 1720, loss: 0.0646650567650795\n","step: 1730, loss: 0.1133633404970169\n","step: 1740, loss: 0.14154009521007538\n","step: 1750, loss: 0.10450008511543274\n","step: 1760, loss: 0.10534524917602539\n","step: 1770, loss: 0.09075982868671417\n","step: 1780, loss: 0.07901483029127121\n","step: 1790, loss: 0.05294831469655037\n","step: 1800, loss: 0.07452912628650665\n","step: 1810, loss: 0.14982013404369354\n","step: 1820, loss: 0.12462814897298813\n","step: 1830, loss: 0.12098485231399536\n","step: 1840, loss: 0.08075331896543503\n","step: 1850, loss: 0.21905569732189178\n","step: 1860, loss: 0.08900923281908035\n","step: 1870, loss: 0.08895659446716309\n","step: 1880, loss: 0.017173578962683678\n","step: 1890, loss: 0.27238377928733826\n","step: 1900, loss: 0.11796704679727554\n","step: 1910, loss: 0.04807467758655548\n","step: 1920, loss: 0.0804070457816124\n","step: 1930, loss: 0.1578129380941391\n","step: 1940, loss: 0.050428204238414764\n","step: 1950, loss: 0.14409621059894562\n","step: 1960, loss: 0.07209043949842453\n","step: 1970, loss: 0.094121053814888\n","step: 1980, loss: 0.1049676388502121\n","step: 1990, loss: 0.016109419986605644\n","step: 2000, loss: 0.10762914270162582\n","step: 2010, loss: 0.11231587082147598\n","step: 2020, loss: 0.0703853964805603\n","step: 2030, loss: 0.06509482115507126\n","step: 2040, loss: 0.06884800642728806\n","step: 2050, loss: 0.09343524277210236\n","step: 2060, loss: 0.0891367569565773\n","step: 2070, loss: 0.08492772281169891\n","step: 2080, loss: 0.18322890996932983\n","step: 2090, loss: 0.13933496177196503\n","step: 2100, loss: 0.1429404765367508\n","step: 2110, loss: 0.09173493832349777\n","step: 2120, loss: 0.09307686239480972\n","step: 2130, loss: 0.10705360770225525\n","step: 2140, loss: 0.09509441256523132\n","step: 2150, loss: 0.08678167313337326\n","step: 2160, loss: 0.1438618004322052\n","step: 2170, loss: 0.11350752413272858\n","step: 2180, loss: 0.12032622843980789\n","step: 2190, loss: 0.1323941946029663\n","step: 2200, loss: 0.09364232420921326\n","step: 2210, loss: 0.08800549805164337\n","step: 2220, loss: 0.0942130982875824\n","step: 2230, loss: 0.09389932453632355\n","step: 2240, loss: 0.07551910728216171\n","step: 2250, loss: 0.03804587200284004\n","step: 2260, loss: 0.06744898855686188\n","step: 2270, loss: 0.14665402472019196\n","step: 2280, loss: 0.10801941901445389\n","step: 2290, loss: 0.11189202964305878\n","step: 2300, loss: 0.1839020699262619\n","step: 2310, loss: 0.08508133143186569\n","step: 2320, loss: 0.058283910155296326\n","step: 2330, loss: 0.05994561314582825\n","step: 2340, loss: 0.09309454262256622\n","step: 2350, loss: 0.16146250069141388\n","step: 2360, loss: 0.07653945684432983\n","step: 2370, loss: 0.11766835302114487\n","step: 2380, loss: 0.07441093772649765\n","step: 2390, loss: 0.15819209814071655\n","step: 2400, loss: 0.04245350509881973\n","step: 2410, loss: 0.040060121566057205\n","step: 2420, loss: 0.016827259212732315\n","step: 2430, loss: 0.032999493181705475\n","step: 2440, loss: 0.256626158952713\n","step: 2450, loss: 0.13587988913059235\n","step: 2460, loss: 0.07571565359830856\n","step: 2470, loss: 0.01335245743393898\n","step: 2480, loss: 0.16778989136219025\n","step: 2490, loss: 0.05299212783575058\n","step: 2500, loss: 0.08029747009277344\n","step: 2510, loss: 0.1780221164226532\n","step: 2520, loss: 0.11756787449121475\n","step: 2530, loss: 0.1265174150466919\n","step: 2540, loss: 0.1466415673494339\n","step: 2550, loss: 0.05371052026748657\n","step: 2560, loss: 0.29098331928253174\n","step: 2570, loss: 0.13219833374023438\n","step: 2580, loss: 0.07403237372636795\n","step: 2590, loss: 0.1103573888540268\n","step: 2600, loss: 0.07267563790082932\n","step: 2610, loss: 0.1527426689863205\n","step: 2620, loss: 0.03426612913608551\n","step: 2630, loss: 0.03859182447195053\n","step: 2640, loss: 0.05157190188765526\n","step: 2650, loss: 0.0312965027987957\n","step: 2660, loss: 0.130489781498909\n","step: 2670, loss: 0.05484861135482788\n","step: 2680, loss: 0.08727383613586426\n","step: 2690, loss: 0.32434964179992676\n","step: 2700, loss: 0.05989987403154373\n","step: 2710, loss: 0.054297927767038345\n","step: 2720, loss: 0.09110333770513535\n","step: 2730, loss: 0.08833717554807663\n","step: 2740, loss: 0.08234509825706482\n","step: 2750, loss: 0.05773673951625824\n","step: 2760, loss: 0.0307120643556118\n","step: 2770, loss: 0.027149656787514687\n","step: 2780, loss: 0.061712898313999176\n","step: 2790, loss: 0.07494501769542694\n","step: 2800, loss: 0.02639300376176834\n","step: 2810, loss: 0.04897528886795044\n","step: 2820, loss: 0.06201822683215141\n","step: 2830, loss: 0.05981956049799919\n","step: 2840, loss: 0.13673214614391327\n","step: 2850, loss: 0.17472413182258606\n","step: 2860, loss: 0.07559553533792496\n","step: 2870, loss: 0.05701608583331108\n","step: 2880, loss: 0.14122872054576874\n","step: 2890, loss: 0.12828455865383148\n","step: 2900, loss: 0.05640798062086105\n","step: 2910, loss: 0.08732303231954575\n","step: 2920, loss: 0.09383764863014221\n","step: 2930, loss: 0.06002415344119072\n","step: 2940, loss: 0.06405546516180038\n","step: 2950, loss: 0.09486645460128784\n","step: 2960, loss: 0.07684869319200516\n","step: 2970, loss: 0.1261487752199173\n","step: 2980, loss: 0.1193799376487732\n","step: 2990, loss: 0.08208060264587402\n","step: 3000, loss: 0.10771769285202026\n","step: 3010, loss: 0.041614338755607605\n","step: 3020, loss: 0.03976694867014885\n","step: 3030, loss: 0.0919080376625061\n","step: 3040, loss: 0.04509197548031807\n","step: 3050, loss: 0.06534742563962936\n","step: 3060, loss: 0.040459178388118744\n","step: 3070, loss: 0.15388816595077515\n","step: 3080, loss: 0.10041596740484238\n","step: 3090, loss: 0.043534718453884125\n","step: 3100, loss: 0.12287362664937973\n","step: 3110, loss: 0.09685791283845901\n","step: 3120, loss: 0.13753549754619598\n","step: 3130, loss: 0.08544570207595825\n","step: 3140, loss: 0.04909177124500275\n","step: 3150, loss: 0.15698368847370148\n","step: 3160, loss: 0.08839603513479233\n","step: 3170, loss: 0.13207554817199707\n","step: 3180, loss: 0.09532072395086288\n","step: 3190, loss: 0.08935058861970901\n","step: 3200, loss: 0.11642419546842575\n","step: 3210, loss: 0.039397355169057846\n","step: 3220, loss: 0.16971257328987122\n","step: 3230, loss: 0.11884226649999619\n","step: 3240, loss: 0.1665666401386261\n","step: 3250, loss: 0.056147072464227676\n","step: 3260, loss: 0.25805914402008057\n","step: 3270, loss: 0.10140559077262878\n","step: 3280, loss: 0.06553099304437637\n","step: 3290, loss: 0.05716089904308319\n","step: 3300, loss: 0.037922680377960205\n","step: 3310, loss: 0.12285180389881134\n","step: 3320, loss: 0.09355334937572479\n","step: 3330, loss: 0.08930348604917526\n","step: 3340, loss: 0.15800853073596954\n","step: 3350, loss: 0.08830355107784271\n","step: 3360, loss: 0.05359683185815811\n","step: 3370, loss: 0.040013451129198074\n","step: 3380, loss: 0.0970669612288475\n","step: 3390, loss: 0.12177488952875137\n","step: 3400, loss: 0.09479997307062149\n","step: 3410, loss: 0.0530933178961277\n","step: 3420, loss: 0.09280142188072205\n","step: 3430, loss: 0.03916168957948685\n","step: 3440, loss: 0.13302892446517944\n","step: 3450, loss: 0.09039224684238434\n","step: 3460, loss: 0.0994451567530632\n","step: 3470, loss: 0.10046428442001343\n","step: 3480, loss: 0.054139185696840286\n","step: 3490, loss: 0.10975737869739532\n","step: 3500, loss: 0.033398982137441635\n","step: 3510, loss: 0.07159582525491714\n","step: 3520, loss: 0.07100340723991394\n","step: 3530, loss: 0.06974189728498459\n","step: 3540, loss: 0.058065157383680344\n","step: 3550, loss: 0.03558701276779175\n","step: 3560, loss: 0.05091175064444542\n","step: 3570, loss: 0.13269422948360443\n","step: 3580, loss: 0.043127842247486115\n","step: 3590, loss: 0.07874205708503723\n","step: 3600, loss: 0.09386475384235382\n","step: 3610, loss: 0.11862620711326599\n","step: 3620, loss: 0.12912815809249878\n","step: 3630, loss: 0.12875130772590637\n","step: 3640, loss: 0.20103634893894196\n","step: 3650, loss: 0.1228998601436615\n","step: 3660, loss: 0.043942540884017944\n","step: 3670, loss: 0.12665188312530518\n","step: 3680, loss: 0.05411870777606964\n","step: 3690, loss: 0.08378332108259201\n","step: 3700, loss: 0.018595978617668152\n","step: 3710, loss: 0.11415285617113113\n","step: 3720, loss: 0.03737936541438103\n","step: 3730, loss: 0.0662221610546112\n","step: 3740, loss: 0.03509140387177467\n","step: 3750, loss: 0.08527480810880661\n","step: 3760, loss: 0.0549057237803936\n","step: 3770, loss: 0.09127847105264664\n","step: 3780, loss: 0.06095951795578003\n","step: 3790, loss: 0.08067403733730316\n","step: 3800, loss: 0.1287352591753006\n","step: 3810, loss: 0.05436990037560463\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.76      1.00      0.86        35\n","           2       0.65      0.31      0.42        77\n","           3       0.97      0.80      0.88      1030\n","           4       0.91      0.85      0.88       291\n","           5       0.99      0.84      0.91       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.56      0.94      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.98      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.67      0.77      0.71        13\n","          14       0.24      1.00      0.39        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.76      0.88      0.82      1151\n","          17       0.93      0.95      0.94        41\n","          18       0.94      0.97      0.95        32\n","          19       0.45      0.72      0.56        40\n","          20       0.98      1.00      0.99       584\n","          21       0.09      0.06      0.07        52\n","          22       0.95      0.76      0.84      4175\n","          23       0.74      0.96      0.84      2253\n","          24       0.34      0.70      0.46        44\n","          25       0.86      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.93      1.00      0.97        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.89      0.86      0.88      1136\n","          31       0.58      0.74      0.65        19\n","          32       0.86      0.75      0.80         8\n","          33       0.59      0.99      0.74        86\n","          34       0.16      0.34      0.22        32\n","          35       0.99      0.99      0.99       474\n","          36       1.00      0.13      0.23       182\n","          37       0.88      0.97      0.93      1592\n","          38       0.95      0.99      0.97       404\n","          39       0.97      0.93      0.95       485\n","          40       0.94      0.85      0.89       573\n","          41       0.97      0.95      0.96       841\n","          42       0.99      0.99      0.99       575\n","          43       0.98      0.77      0.86       152\n","          44       0.86      0.96      0.91        75\n","          46       1.00      0.98      0.99        82\n","          48       1.00      0.06      0.12        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.82      0.78     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 466\n","\n","Loop 7\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.0825605392456055\n","step: 10, loss: 1.879938006401062\n","step: 20, loss: 0.8449520468711853\n","step: 30, loss: 0.5840408205986023\n","step: 40, loss: 0.27008071541786194\n","step: 50, loss: 0.20319201052188873\n","step: 60, loss: 0.17950443923473358\n","step: 70, loss: 0.230423241853714\n","step: 80, loss: 0.11885154992341995\n","step: 90, loss: 0.20110107958316803\n","step: 100, loss: 0.1553763747215271\n","step: 110, loss: 0.16068489849567413\n","step: 120, loss: 0.11905574798583984\n","step: 130, loss: 0.21470701694488525\n","step: 140, loss: 0.09895019978284836\n","step: 150, loss: 0.0785558819770813\n","step: 160, loss: 0.08523839712142944\n","step: 170, loss: 0.13092073798179626\n","step: 180, loss: 0.23065443336963654\n","step: 190, loss: 0.13586001098155975\n","step: 200, loss: 0.2619093656539917\n","step: 210, loss: 0.13073085248470306\n","step: 220, loss: 0.10901336371898651\n","step: 230, loss: 0.15178176760673523\n","step: 240, loss: 0.05098554119467735\n","step: 250, loss: 0.1300175040960312\n","step: 260, loss: 0.057034820318222046\n","step: 270, loss: 0.12746044993400574\n","step: 280, loss: 0.07480155676603317\n","step: 290, loss: 0.21189624071121216\n","step: 300, loss: 0.07134303450584412\n","step: 310, loss: 0.15039488673210144\n","step: 320, loss: 0.13423559069633484\n","step: 330, loss: 0.1704951971769333\n","step: 340, loss: 0.08011069893836975\n","step: 350, loss: 0.16777341067790985\n","step: 360, loss: 0.06686583906412125\n","step: 370, loss: 0.21777403354644775\n","step: 380, loss: 0.12193022668361664\n","step: 390, loss: 0.08749157935380936\n","step: 400, loss: 0.08666739612817764\n","step: 410, loss: 0.09768272936344147\n","step: 420, loss: 0.09594981372356415\n","step: 430, loss: 0.08866550773382187\n","step: 440, loss: 0.13537494838237762\n","step: 450, loss: 0.09244399517774582\n","step: 460, loss: 0.07936123013496399\n","step: 470, loss: 0.08712679147720337\n","step: 480, loss: 0.11409422010183334\n","step: 490, loss: 0.05759742856025696\n","step: 500, loss: 0.16829679906368256\n","step: 510, loss: 0.06914416700601578\n","step: 520, loss: 0.04762065038084984\n","step: 530, loss: 0.1086391732096672\n","step: 540, loss: 0.2054530531167984\n","step: 550, loss: 0.08480767905712128\n","step: 560, loss: 0.04288235306739807\n","step: 570, loss: 0.12698957324028015\n","step: 580, loss: 0.16363845765590668\n","step: 590, loss: 0.10987858474254608\n","step: 600, loss: 0.07323604077100754\n","step: 610, loss: 0.09912299364805222\n","step: 620, loss: 0.16490225493907928\n","step: 630, loss: 0.06321822106838226\n","step: 640, loss: 0.09070238471031189\n","step: 650, loss: 0.16579923033714294\n","step: 660, loss: 0.1005091443657875\n","step: 670, loss: 0.09731516987085342\n","step: 680, loss: 0.05127622187137604\n","step: 690, loss: 0.18578916788101196\n","step: 700, loss: 0.09499944001436234\n","step: 710, loss: 0.10562537610530853\n","step: 720, loss: 0.0688362717628479\n","step: 730, loss: 0.023244304582476616\n","step: 740, loss: 0.0805206298828125\n","step: 750, loss: 0.04876610264182091\n","step: 760, loss: 0.03889703005552292\n","step: 770, loss: 0.032886456698179245\n","step: 780, loss: 0.04096410796046257\n","step: 790, loss: 0.06476783752441406\n","step: 800, loss: 0.09866601228713989\n","step: 810, loss: 0.16763697564601898\n","step: 820, loss: 0.19320349395275116\n","step: 830, loss: 0.08903374522924423\n","step: 840, loss: 0.022174321115016937\n","step: 850, loss: 0.0369877927005291\n","step: 860, loss: 0.0852665826678276\n","step: 870, loss: 0.1949578821659088\n","step: 880, loss: 0.15681222081184387\n","step: 890, loss: 0.07008387893438339\n","step: 900, loss: 0.04214126244187355\n","step: 910, loss: 0.06444507092237473\n","step: 920, loss: 0.16525107622146606\n","step: 930, loss: 0.055513277649879456\n","step: 940, loss: 0.1181207075715065\n","step: 950, loss: 0.06666391342878342\n","step: 960, loss: 0.05123662203550339\n","step: 970, loss: 0.08504834026098251\n","step: 980, loss: 0.09969499707221985\n","step: 990, loss: 0.07308600097894669\n","step: 1000, loss: 0.06297428905963898\n","step: 1010, loss: 0.10297385603189468\n","step: 1020, loss: 0.10886319726705551\n","step: 1030, loss: 0.11274793744087219\n","step: 1040, loss: 0.07825860381126404\n","step: 1050, loss: 0.07448576390743256\n","step: 1060, loss: 0.11065603792667389\n","step: 1070, loss: 0.13725583255290985\n","step: 1080, loss: 0.05118997395038605\n","step: 1090, loss: 0.06594337522983551\n","step: 1100, loss: 0.06854245066642761\n","step: 1110, loss: 0.12654291093349457\n","step: 1120, loss: 0.16358621418476105\n","step: 1130, loss: 0.07232578843832016\n","step: 1140, loss: 0.05736275017261505\n","step: 1150, loss: 0.1713307946920395\n","step: 1160, loss: 0.12722148001194\n","step: 1170, loss: 0.17931507527828217\n","step: 1180, loss: 0.10456004738807678\n","step: 1190, loss: 0.07547777146100998\n","step: 1200, loss: 0.08215780556201935\n","step: 1210, loss: 0.07225120812654495\n","step: 1220, loss: 0.07791262120008469\n","step: 1230, loss: 0.07626249641180038\n","step: 1240, loss: 0.032203346490859985\n","step: 1250, loss: 0.22788706421852112\n","step: 1260, loss: 0.05988362431526184\n","step: 1270, loss: 0.0531114786863327\n","step: 1280, loss: 0.019100425764918327\n","step: 1290, loss: 0.04096854850649834\n","step: 1300, loss: 0.0535576269030571\n","step: 1310, loss: 0.0668981671333313\n","step: 1320, loss: 0.07037988305091858\n","step: 1330, loss: 0.17226946353912354\n","step: 1340, loss: 0.09861764311790466\n","step: 1350, loss: 0.07698940485715866\n","step: 1360, loss: 0.1142590120434761\n","step: 1370, loss: 0.04844781756401062\n","step: 1380, loss: 0.1809743493795395\n","step: 1390, loss: 0.08886181563138962\n","step: 1400, loss: 0.0979650616645813\n","step: 1410, loss: 0.09303104877471924\n","step: 1420, loss: 0.08713175356388092\n","step: 1430, loss: 0.09349995851516724\n","step: 1440, loss: 0.15321733057498932\n","step: 1450, loss: 0.1015438660979271\n","step: 1460, loss: 0.109598807990551\n","step: 1470, loss: 0.10405538976192474\n","step: 1480, loss: 0.09119395166635513\n","step: 1490, loss: 0.13305246829986572\n","step: 1500, loss: 0.016305530443787575\n","step: 1510, loss: 0.050399791449308395\n","step: 1520, loss: 0.06107095628976822\n","step: 1530, loss: 0.11245641112327576\n","step: 1540, loss: 0.1182863637804985\n","step: 1550, loss: 0.2216053158044815\n","step: 1560, loss: 0.09558365494012833\n","step: 1570, loss: 0.17918069660663605\n","step: 1580, loss: 0.11671634018421173\n","step: 1590, loss: 0.09311379492282867\n","step: 1600, loss: 0.11290990561246872\n","step: 1610, loss: 0.026268498972058296\n","step: 1620, loss: 0.24323366582393646\n","step: 1630, loss: 0.03648354113101959\n","step: 1640, loss: 0.06913875788450241\n","step: 1650, loss: 0.1852644681930542\n","step: 1660, loss: 0.0363818034529686\n","step: 1670, loss: 0.07122752070426941\n","step: 1680, loss: 0.089520663022995\n","step: 1690, loss: 0.15372715890407562\n","step: 1700, loss: 0.1710396111011505\n","step: 1710, loss: 0.0986669510602951\n","step: 1720, loss: 0.04133901000022888\n","step: 1730, loss: 0.10687220841646194\n","step: 1740, loss: 0.13958752155303955\n","step: 1750, loss: 0.08027391880750656\n","step: 1760, loss: 0.126492440700531\n","step: 1770, loss: 0.08236405998468399\n","step: 1780, loss: 0.023045426234602928\n","step: 1790, loss: 0.0889337807893753\n","step: 1800, loss: 0.11321988701820374\n","step: 1810, loss: 0.08992834389209747\n","step: 1820, loss: 0.1306479275226593\n","step: 1830, loss: 0.10138163715600967\n","step: 1840, loss: 0.23146824538707733\n","step: 1850, loss: 0.1744014173746109\n","step: 1860, loss: 0.14608009159564972\n","step: 1870, loss: 0.0680745542049408\n","step: 1880, loss: 0.1078590452671051\n","step: 1890, loss: 0.13873390853405\n","step: 1900, loss: 0.0797792449593544\n","step: 1910, loss: 0.09166968613862991\n","step: 1920, loss: 0.12877292931079865\n","step: 1930, loss: 0.018924269825220108\n","step: 1940, loss: 0.1715593785047531\n","step: 1950, loss: 0.11563299596309662\n","step: 1960, loss: 0.07224182039499283\n","step: 1970, loss: 0.06822265684604645\n","step: 1980, loss: 0.10378159582614899\n","step: 1990, loss: 0.08407778292894363\n","step: 2000, loss: 0.17926622927188873\n","step: 2010, loss: 0.06754247099161148\n","step: 2020, loss: 0.18611276149749756\n","step: 2030, loss: 0.04925372824072838\n","step: 2040, loss: 0.06945039331912994\n","step: 2050, loss: 0.1732308268547058\n","step: 2060, loss: 0.06013335660099983\n","step: 2070, loss: 0.0447460412979126\n","step: 2080, loss: 0.04262254759669304\n","step: 2090, loss: 0.019225401803851128\n","step: 2100, loss: 0.028344308957457542\n","step: 2110, loss: 0.1574096381664276\n","step: 2120, loss: 0.047015633434057236\n","step: 2130, loss: 0.07296120375394821\n","step: 2140, loss: 0.07071343064308167\n","step: 2150, loss: 0.1366492360830307\n","step: 2160, loss: 0.0358608216047287\n","step: 2170, loss: 0.07812361419200897\n","step: 2180, loss: 0.18519006669521332\n","step: 2190, loss: 0.15678934752941132\n","step: 2200, loss: 0.08736315369606018\n","step: 2210, loss: 0.2122258096933365\n","step: 2220, loss: 0.06877781450748444\n","step: 2230, loss: 0.08685708791017532\n","step: 2240, loss: 0.044708576053380966\n","step: 2250, loss: 0.1108730286359787\n","step: 2260, loss: 0.15241755545139313\n","step: 2270, loss: 0.0772758275270462\n","step: 2280, loss: 0.04659947752952576\n","step: 2290, loss: 0.12054193764925003\n","step: 2300, loss: 0.06349319964647293\n","step: 2310, loss: 0.07898914068937302\n","step: 2320, loss: 0.06487041711807251\n","step: 2330, loss: 0.2304258495569229\n","step: 2340, loss: 0.17286784946918488\n","step: 2350, loss: 0.08739862591028214\n","step: 2360, loss: 0.03125239908695221\n","step: 2370, loss: 0.10189519822597504\n","step: 2380, loss: 0.12779459357261658\n","step: 2390, loss: 0.14188168942928314\n","step: 2400, loss: 0.08502384275197983\n","step: 2410, loss: 0.07504241168498993\n","step: 2420, loss: 0.08956745266914368\n","step: 2430, loss: 0.049883145838975906\n","step: 2440, loss: 0.057850517332553864\n","step: 2450, loss: 0.1067330464720726\n","step: 2460, loss: 0.023295868188142776\n","step: 2470, loss: 0.11218705773353577\n","step: 2480, loss: 0.08483652025461197\n","step: 2490, loss: 0.070672906935215\n","step: 2500, loss: 0.08161726593971252\n","step: 2510, loss: 0.07411206513643265\n","step: 2520, loss: 0.06390879303216934\n","step: 2530, loss: 0.09685049206018448\n","step: 2540, loss: 0.1365879476070404\n","step: 2550, loss: 0.08401663601398468\n","step: 2560, loss: 0.13193584978580475\n","step: 2570, loss: 0.06072855368256569\n","step: 2580, loss: 0.05596928298473358\n","step: 2590, loss: 0.06378772854804993\n","step: 2600, loss: 0.09897138923406601\n","step: 2610, loss: 0.16367679834365845\n","step: 2620, loss: 0.10359743237495422\n","step: 2630, loss: 0.06699853390455246\n","step: 2640, loss: 0.07916104048490524\n","step: 2650, loss: 0.08013518899679184\n","step: 2660, loss: 0.061367157846689224\n","step: 2670, loss: 0.04414045438170433\n","step: 2680, loss: 0.073644258081913\n","step: 2690, loss: 0.1357729285955429\n","step: 2700, loss: 0.09215214848518372\n","step: 2710, loss: 0.0890595093369484\n","step: 2720, loss: 0.0939234122633934\n","step: 2730, loss: 0.12130186706781387\n","step: 2740, loss: 0.11045392602682114\n","step: 2750, loss: 0.13321226835250854\n","step: 2760, loss: 0.1323576718568802\n","step: 2770, loss: 0.11890922486782074\n","step: 2780, loss: 0.12706919014453888\n","step: 2790, loss: 0.06391970068216324\n","step: 2800, loss: 0.08972170948982239\n","step: 2810, loss: 0.042419128119945526\n","step: 2820, loss: 0.06452342122793198\n","step: 2830, loss: 0.02415635623037815\n","step: 2840, loss: 0.10081177949905396\n","step: 2850, loss: 0.06591476500034332\n","step: 2860, loss: 0.12353348731994629\n","step: 2870, loss: 0.14372631907463074\n","step: 2880, loss: 0.1583513766527176\n","step: 2890, loss: 0.042189035564661026\n","step: 2900, loss: 0.09896167367696762\n","step: 2910, loss: 0.13938748836517334\n","step: 2920, loss: 0.05934526398777962\n","step: 2930, loss: 0.20058681070804596\n","step: 2940, loss: 0.08449765294790268\n","step: 2950, loss: 0.08143831044435501\n","step: 2960, loss: 0.06641591340303421\n","step: 2970, loss: 0.07593771815299988\n","step: 2980, loss: 0.04076465964317322\n","step: 2990, loss: 0.05797341465950012\n","step: 3000, loss: 0.14398351311683655\n","step: 3010, loss: 0.10231316089630127\n","step: 3020, loss: 0.07834435999393463\n","step: 3030, loss: 0.08447376638650894\n","step: 3040, loss: 0.024844760075211525\n","step: 3050, loss: 0.06826873868703842\n","step: 3060, loss: 0.08499541133642197\n","step: 3070, loss: 0.04898825287818909\n","step: 3080, loss: 0.058878157287836075\n","step: 3090, loss: 0.05505523830652237\n","step: 3100, loss: 0.14799872040748596\n","step: 3110, loss: 0.1276562511920929\n","step: 3120, loss: 0.09743645042181015\n","step: 3130, loss: 0.038322217762470245\n","step: 3140, loss: 0.10895372182130814\n","step: 3150, loss: 0.15049727261066437\n","step: 3160, loss: 0.0367242731153965\n","step: 3170, loss: 0.11162354052066803\n","step: 3180, loss: 0.05669036880135536\n","step: 3190, loss: 0.07118533551692963\n","step: 3200, loss: 0.04738917574286461\n","step: 3210, loss: 0.09280524402856827\n","step: 3220, loss: 0.03057648055255413\n","step: 3230, loss: 0.07711026817560196\n","step: 3240, loss: 0.1487334668636322\n","step: 3250, loss: 0.11715345084667206\n","step: 3260, loss: 0.05646485090255737\n","step: 3270, loss: 0.07087922096252441\n","step: 3280, loss: 0.03755971044301987\n","step: 3290, loss: 0.0938713401556015\n","step: 3300, loss: 0.03898122161626816\n","step: 3310, loss: 0.043141622096300125\n","step: 3320, loss: 0.15562191605567932\n","step: 3330, loss: 0.08798098564147949\n","step: 3340, loss: 0.11790023744106293\n","step: 3350, loss: 0.10039316117763519\n","step: 3360, loss: 0.06517306715250015\n","step: 3370, loss: 0.05730624124407768\n","step: 3380, loss: 0.1158537045121193\n","step: 3390, loss: 0.04848355054855347\n","step: 3400, loss: 0.025879288092255592\n","step: 3410, loss: 0.06577135622501373\n","step: 3420, loss: 0.10336174070835114\n","step: 3430, loss: 0.10536795854568481\n","step: 3440, loss: 0.10159951448440552\n","step: 3450, loss: 0.05668797343969345\n","step: 3460, loss: 0.16008485853672028\n","step: 3470, loss: 0.08187404274940491\n","step: 3480, loss: 0.07507235556840897\n","step: 3490, loss: 0.14014028012752533\n","step: 3500, loss: 0.09799216687679291\n","step: 3510, loss: 0.04004575312137604\n","step: 3520, loss: 0.05372421815991402\n","step: 3530, loss: 0.15745942294597626\n","step: 3540, loss: 0.08115162700414658\n","step: 3550, loss: 0.080739326775074\n","step: 3560, loss: 0.07286243885755539\n","step: 3570, loss: 0.07929398119449615\n","step: 3580, loss: 0.10128908604383469\n","step: 3590, loss: 0.09523076564073563\n","step: 3600, loss: 0.08342965692281723\n","step: 3610, loss: 0.11564754694700241\n","step: 3620, loss: 0.08299850672483444\n","step: 3630, loss: 0.05102580785751343\n","step: 3640, loss: 0.063761405646801\n","step: 3650, loss: 0.07328560203313828\n","step: 3660, loss: 0.051335662603378296\n","step: 3670, loss: 0.04740985482931137\n","step: 3680, loss: 0.07785781472921371\n","step: 3690, loss: 0.08133787661790848\n","step: 3700, loss: 0.12734924256801605\n","step: 3710, loss: 0.04366353154182434\n","step: 3720, loss: 0.12911677360534668\n","step: 3730, loss: 0.0979141816496849\n","step: 3740, loss: 0.1496138721704483\n","step: 3750, loss: 0.09907733649015427\n","step: 3760, loss: 0.05319143831729889\n","step: 3770, loss: 0.02888265997171402\n","step: 3780, loss: 0.04939785972237587\n","step: 3790, loss: 0.09351971745491028\n","step: 3800, loss: 0.14716455340385437\n","step: 3810, loss: 0.07203487306833267\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.83      1.00      0.91        35\n","           2       0.66      0.53      0.59        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.83      0.87      0.85       291\n","           5       0.86      0.86      0.86       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.98      0.98       689\n","          10       0.95      0.98      0.96       901\n","          11       0.97      1.00      0.98      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.21      0.23      0.22        13\n","          14       0.47      1.00      0.64        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.89      0.82      0.85      1151\n","          17       0.87      0.95      0.91        41\n","          18       0.97      0.97      0.97        32\n","          19       0.35      0.45      0.39        40\n","          20       1.00      1.00      1.00       584\n","          21       0.12      0.04      0.06        52\n","          22       0.93      0.79      0.86      4175\n","          23       0.75      0.95      0.84      2253\n","          24       0.41      0.66      0.51        44\n","          25       0.86      0.92      0.88       888\n","          26       0.89      0.89      0.89         9\n","          27       0.97      0.99      0.98        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.91      0.85      0.88      1136\n","          31       0.45      0.74      0.56        19\n","          32       0.88      0.88      0.88         8\n","          33       0.64      0.99      0.78        86\n","          34       0.26      0.62      0.37        32\n","          35       0.98      0.99      0.98       474\n","          36       0.80      0.18      0.29       182\n","          37       0.90      0.95      0.92      1592\n","          38       0.95      0.97      0.96       404\n","          39       0.98      0.93      0.96       485\n","          40       0.87      0.96      0.91       573\n","          41       0.95      0.94      0.94       841\n","          42       0.98      0.98      0.98       575\n","          43       0.92      0.83      0.87       152\n","          44       0.87      0.92      0.90        75\n","          46       0.98      0.98      0.98        82\n","          48       1.00      0.30      0.47        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.79      0.82      0.79     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 453\n","\n","Loop 8\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.896695137023926\n","step: 10, loss: 1.9028788805007935\n","step: 20, loss: 0.6720321178436279\n","step: 30, loss: 0.3700006306171417\n","step: 40, loss: 0.2721523344516754\n","step: 50, loss: 0.236761674284935\n","step: 60, loss: 0.16902309656143188\n","step: 70, loss: 0.27194103598594666\n","step: 80, loss: 0.08656768500804901\n","step: 90, loss: 0.15788814425468445\n","step: 100, loss: 0.15663063526153564\n","step: 110, loss: 0.1809048056602478\n","step: 120, loss: 0.08330696821212769\n","step: 130, loss: 0.10459805279970169\n","step: 140, loss: 0.12266870588064194\n","step: 150, loss: 0.16048556566238403\n","step: 160, loss: 0.21818183362483978\n","step: 170, loss: 0.17591115832328796\n","step: 180, loss: 0.12851162254810333\n","step: 190, loss: 0.042650677263736725\n","step: 200, loss: 0.13496266305446625\n","step: 210, loss: 0.05805300548672676\n","step: 220, loss: 0.19912536442279816\n","step: 230, loss: 0.05298972502350807\n","step: 240, loss: 0.07252632081508636\n","step: 250, loss: 0.13393694162368774\n","step: 260, loss: 0.16904425621032715\n","step: 270, loss: 0.09814681857824326\n","step: 280, loss: 0.11644264310598373\n","step: 290, loss: 0.13884586095809937\n","step: 300, loss: 0.10298831760883331\n","step: 310, loss: 0.09620300680398941\n","step: 320, loss: 0.15669691562652588\n","step: 330, loss: 0.12296915799379349\n","step: 340, loss: 0.07205979526042938\n","step: 350, loss: 0.26667335629463196\n","step: 360, loss: 0.10887562483549118\n","step: 370, loss: 0.11512263864278793\n","step: 380, loss: 0.08855666220188141\n","step: 390, loss: 0.1315259337425232\n","step: 400, loss: 0.04462369531393051\n","step: 410, loss: 0.15780824422836304\n","step: 420, loss: 0.18696922063827515\n","step: 430, loss: 0.07112980633974075\n","step: 440, loss: 0.19360671937465668\n","step: 450, loss: 0.11012301594018936\n","step: 460, loss: 0.13488928973674774\n","step: 470, loss: 0.24675549566745758\n","step: 480, loss: 0.13052518665790558\n","step: 490, loss: 0.12858566641807556\n","step: 500, loss: 0.1782924085855484\n","step: 510, loss: 0.0686018317937851\n","step: 520, loss: 0.06719166785478592\n","step: 530, loss: 0.14134594798088074\n","step: 540, loss: 0.07503519207239151\n","step: 550, loss: 0.15852570533752441\n","step: 560, loss: 0.04740498587489128\n","step: 570, loss: 0.09968847781419754\n","step: 580, loss: 0.07937517762184143\n","step: 590, loss: 0.06169186159968376\n","step: 600, loss: 0.18792611360549927\n","step: 610, loss: 0.08156067132949829\n","step: 620, loss: 0.07676845043897629\n","step: 630, loss: 0.09560679644346237\n","step: 640, loss: 0.21069777011871338\n","step: 650, loss: 0.08265414834022522\n","step: 660, loss: 0.03164798021316528\n","step: 670, loss: 0.18055729568004608\n","step: 680, loss: 0.12324902415275574\n","step: 690, loss: 0.05325217917561531\n","step: 700, loss: 0.13407592475414276\n","step: 710, loss: 0.10841166973114014\n","step: 720, loss: 0.1741834133863449\n","step: 730, loss: 0.09328547865152359\n","step: 740, loss: 0.133959099650383\n","step: 750, loss: 0.20192615687847137\n","step: 760, loss: 0.2089596539735794\n","step: 770, loss: 0.1442767083644867\n","step: 780, loss: 0.0896526351571083\n","step: 790, loss: 0.09550629556179047\n","step: 800, loss: 0.15096671879291534\n","step: 810, loss: 0.05906281992793083\n","step: 820, loss: 0.08105441182851791\n","step: 830, loss: 0.17612214386463165\n","step: 840, loss: 0.20917904376983643\n","step: 850, loss: 0.10011652112007141\n","step: 860, loss: 0.17561307549476624\n","step: 870, loss: 0.10479054600000381\n","step: 880, loss: 0.12585607171058655\n","step: 890, loss: 0.14074508845806122\n","step: 900, loss: 0.06641066819429398\n","step: 910, loss: 0.0753190815448761\n","step: 920, loss: 0.15066596865653992\n","step: 930, loss: 0.09233497828245163\n","step: 940, loss: 0.09722935408353806\n","step: 950, loss: 0.1077994704246521\n","step: 960, loss: 0.04443340748548508\n","step: 970, loss: 0.12528900802135468\n","step: 980, loss: 0.12940742075443268\n","step: 990, loss: 0.05561472475528717\n","step: 1000, loss: 0.07849100232124329\n","step: 1010, loss: 0.22346527874469757\n","step: 1020, loss: 0.13925878703594208\n","step: 1030, loss: 0.14670641720294952\n","step: 1040, loss: 0.07595114409923553\n","step: 1050, loss: 0.09138433635234833\n","step: 1060, loss: 0.11014671623706818\n","step: 1070, loss: 0.08512014895677567\n","step: 1080, loss: 0.027781378477811813\n","step: 1090, loss: 0.1515497863292694\n","step: 1100, loss: 0.06385429203510284\n","step: 1110, loss: 0.07465554028749466\n","step: 1120, loss: 0.0940578505396843\n","step: 1130, loss: 0.10304319113492966\n","step: 1140, loss: 0.03766925260424614\n","step: 1150, loss: 0.12849637866020203\n","step: 1160, loss: 0.1345740407705307\n","step: 1170, loss: 0.11017417907714844\n","step: 1180, loss: 0.10484480857849121\n","step: 1190, loss: 0.04753716662526131\n","step: 1200, loss: 0.05663096532225609\n","step: 1210, loss: 0.0646364837884903\n","step: 1220, loss: 0.07909173518419266\n","step: 1230, loss: 0.09996772557497025\n","step: 1240, loss: 0.12013936787843704\n","step: 1250, loss: 0.1299041211605072\n","step: 1260, loss: 0.12401924282312393\n","step: 1270, loss: 0.1375715583562851\n","step: 1280, loss: 0.09338410198688507\n","step: 1290, loss: 0.059663623571395874\n","step: 1300, loss: 0.0393395870923996\n","step: 1310, loss: 0.15654833614826202\n","step: 1320, loss: 0.1905994564294815\n","step: 1330, loss: 0.060986604541540146\n","step: 1340, loss: 0.139614999294281\n","step: 1350, loss: 0.14897781610488892\n","step: 1360, loss: 0.06695817410945892\n","step: 1370, loss: 0.12900188565254211\n","step: 1380, loss: 0.06945028901100159\n","step: 1390, loss: 0.03904635086655617\n","step: 1400, loss: 0.13374143838882446\n","step: 1410, loss: 0.13965889811515808\n","step: 1420, loss: 0.051991723477840424\n","step: 1430, loss: 0.08597465604543686\n","step: 1440, loss: 0.07905668765306473\n","step: 1450, loss: 0.035840954631567\n","step: 1460, loss: 0.12388239800930023\n","step: 1470, loss: 0.1220395565032959\n","step: 1480, loss: 0.06503311544656754\n","step: 1490, loss: 0.05205608904361725\n","step: 1500, loss: 0.017393194139003754\n","step: 1510, loss: 0.12960603833198547\n","step: 1520, loss: 0.12565764784812927\n","step: 1530, loss: 0.10195543617010117\n","step: 1540, loss: 0.11212807148694992\n","step: 1550, loss: 0.04891543462872505\n","step: 1560, loss: 0.056727420538663864\n","step: 1570, loss: 0.16365142166614532\n","step: 1580, loss: 0.08708316087722778\n","step: 1590, loss: 0.06825059652328491\n","step: 1600, loss: 0.050624217838048935\n","step: 1610, loss: 0.041483841836452484\n","step: 1620, loss: 0.11828657984733582\n","step: 1630, loss: 0.11074911057949066\n","step: 1640, loss: 0.09697799384593964\n","step: 1650, loss: 0.10311058163642883\n","step: 1660, loss: 0.037983208894729614\n","step: 1670, loss: 0.0860191285610199\n","step: 1680, loss: 0.06939877569675446\n","step: 1690, loss: 0.09492192417383194\n","step: 1700, loss: 0.04193253070116043\n","step: 1710, loss: 0.09545230865478516\n","step: 1720, loss: 0.1053803414106369\n","step: 1730, loss: 0.08393197506666183\n","step: 1740, loss: 0.12449590116739273\n","step: 1750, loss: 0.10098955780267715\n","step: 1760, loss: 0.09514503180980682\n","step: 1770, loss: 0.10502994060516357\n","step: 1780, loss: 0.053690049797296524\n","step: 1790, loss: 0.08068288862705231\n","step: 1800, loss: 0.09838824719190598\n","step: 1810, loss: 0.050387606024742126\n","step: 1820, loss: 0.07520438730716705\n","step: 1830, loss: 0.09411323815584183\n","step: 1840, loss: 0.09012044966220856\n","step: 1850, loss: 0.13453614711761475\n","step: 1860, loss: 0.0898054763674736\n","step: 1870, loss: 0.08603078126907349\n","step: 1880, loss: 0.1467450112104416\n","step: 1890, loss: 0.08773455023765564\n","step: 1900, loss: 0.10509927570819855\n","step: 1910, loss: 0.028432324528694153\n","step: 1920, loss: 0.1267409324645996\n","step: 1930, loss: 0.048000700771808624\n","step: 1940, loss: 0.02950967103242874\n","step: 1950, loss: 0.12077029049396515\n","step: 1960, loss: 0.0834367573261261\n","step: 1970, loss: 0.110175222158432\n","step: 1980, loss: 0.0728469267487526\n","step: 1990, loss: 0.09714347124099731\n","step: 2000, loss: 0.1107759028673172\n","step: 2010, loss: 0.09235753864049911\n","step: 2020, loss: 0.0697551891207695\n","step: 2030, loss: 0.04672021418809891\n","step: 2040, loss: 0.14385345578193665\n","step: 2050, loss: 0.12705491483211517\n","step: 2060, loss: 0.08592147380113602\n","step: 2070, loss: 0.08174878358840942\n","step: 2080, loss: 0.03175785765051842\n","step: 2090, loss: 0.0646839588880539\n","step: 2100, loss: 0.12881413102149963\n","step: 2110, loss: 0.1374978870153427\n","step: 2120, loss: 0.04514824599027634\n","step: 2130, loss: 0.059136126190423965\n","step: 2140, loss: 0.07097925990819931\n","step: 2150, loss: 0.14426152408123016\n","step: 2160, loss: 0.13639436662197113\n","step: 2170, loss: 0.13328905403614044\n","step: 2180, loss: 0.07102639228105545\n","step: 2190, loss: 0.051642145961523056\n","step: 2200, loss: 0.05976079776883125\n","step: 2210, loss: 0.09489165246486664\n","step: 2220, loss: 0.04961889609694481\n","step: 2230, loss: 0.07111627608537674\n","step: 2240, loss: 0.09833040088415146\n","step: 2250, loss: 0.10857575386762619\n","step: 2260, loss: 0.08438896387815475\n","step: 2270, loss: 0.07210984081029892\n","step: 2280, loss: 0.04088018089532852\n","step: 2290, loss: 0.037108033895492554\n","step: 2300, loss: 0.05509001761674881\n","step: 2310, loss: 0.051821403205394745\n","step: 2320, loss: 0.09044144302606583\n","step: 2330, loss: 0.07122603803873062\n","step: 2340, loss: 0.08105465024709702\n","step: 2350, loss: 0.2671891748905182\n","step: 2360, loss: 0.037915781140327454\n","step: 2370, loss: 0.09188750386238098\n","step: 2380, loss: 0.11565172672271729\n","step: 2390, loss: 0.04892487823963165\n","step: 2400, loss: 0.10322573035955429\n","step: 2410, loss: 0.0759463831782341\n","step: 2420, loss: 0.06364256143569946\n","step: 2430, loss: 0.08563224226236343\n","step: 2440, loss: 0.0759168192744255\n","step: 2450, loss: 0.1362088918685913\n","step: 2460, loss: 0.07238566130399704\n","step: 2470, loss: 0.03240170329809189\n","step: 2480, loss: 0.08827238529920578\n","step: 2490, loss: 0.2501194179058075\n","step: 2500, loss: 0.05728130787611008\n","step: 2510, loss: 0.10934612900018692\n","step: 2520, loss: 0.08778469264507294\n","step: 2530, loss: 0.06746941059827805\n","step: 2540, loss: 0.07633206993341446\n","step: 2550, loss: 0.10583193600177765\n","step: 2560, loss: 0.11998095363378525\n","step: 2570, loss: 0.029559236019849777\n","step: 2580, loss: 0.04468570649623871\n","step: 2590, loss: 0.054994963109493256\n","step: 2600, loss: 0.027629753574728966\n","step: 2610, loss: 0.04900044947862625\n","step: 2620, loss: 0.0593649186193943\n","step: 2630, loss: 0.0678844153881073\n","step: 2640, loss: 0.06771816313266754\n","step: 2650, loss: 0.06657156348228455\n","step: 2660, loss: 0.07986592501401901\n","step: 2670, loss: 0.061211343854665756\n","step: 2680, loss: 0.025023534893989563\n","step: 2690, loss: 0.12257447093725204\n","step: 2700, loss: 0.10474839061498642\n","step: 2710, loss: 0.04436788707971573\n","step: 2720, loss: 0.07702099531888962\n","step: 2730, loss: 0.046147361397743225\n","step: 2740, loss: 0.16769160330295563\n","step: 2750, loss: 0.12715941667556763\n","step: 2760, loss: 0.02104315347969532\n","step: 2770, loss: 0.13427229225635529\n","step: 2780, loss: 0.023653250187635422\n","step: 2790, loss: 0.07368746399879456\n","step: 2800, loss: 0.1806081235408783\n","step: 2810, loss: 0.16581706702709198\n","step: 2820, loss: 0.11530575156211853\n","step: 2830, loss: 0.0546177439391613\n","step: 2840, loss: 0.12223009765148163\n","step: 2850, loss: 0.06343719363212585\n","step: 2860, loss: 0.06858783960342407\n","step: 2870, loss: 0.10637390613555908\n","step: 2880, loss: 0.09292108565568924\n","step: 2890, loss: 0.05349882319569588\n","step: 2900, loss: 0.08839022368192673\n","step: 2910, loss: 0.15537962317466736\n","step: 2920, loss: 0.19033512473106384\n","step: 2930, loss: 0.07511331886053085\n","step: 2940, loss: 0.11582942306995392\n","step: 2950, loss: 0.07494359463453293\n","step: 2960, loss: 0.07698261737823486\n","step: 2970, loss: 0.1460949331521988\n","step: 2980, loss: 0.04050520434975624\n","step: 2990, loss: 0.10261957347393036\n","step: 3000, loss: 0.06605521589517593\n","step: 3010, loss: 0.044814758002758026\n","step: 3020, loss: 0.08822475373744965\n","step: 3030, loss: 0.05886712297797203\n","step: 3040, loss: 0.0354042574763298\n","step: 3050, loss: 0.09949424117803574\n","step: 3060, loss: 0.10770673304796219\n","step: 3070, loss: 0.1636863648891449\n","step: 3080, loss: 0.04823162406682968\n","step: 3090, loss: 0.06672275066375732\n","step: 3100, loss: 0.081829734146595\n","step: 3110, loss: 0.14065638184547424\n","step: 3120, loss: 0.11587440967559814\n","step: 3130, loss: 0.17125996947288513\n","step: 3140, loss: 0.06030502915382385\n","step: 3150, loss: 0.0714370608329773\n","step: 3160, loss: 0.1378890573978424\n","step: 3170, loss: 0.11989107728004456\n","step: 3180, loss: 0.07511061429977417\n","step: 3190, loss: 0.0482979379594326\n","step: 3200, loss: 0.10534045845270157\n","step: 3210, loss: 0.11830495297908783\n","step: 3220, loss: 0.08668914437294006\n","step: 3230, loss: 0.10564807802438736\n","step: 3240, loss: 0.09766824543476105\n","step: 3250, loss: 0.1440221220254898\n","step: 3260, loss: 0.06123620271682739\n","step: 3270, loss: 0.0741613432765007\n","step: 3280, loss: 0.12232662737369537\n","step: 3290, loss: 0.09049087017774582\n","step: 3300, loss: 0.07367506623268127\n","step: 3310, loss: 0.06224124878644943\n","step: 3320, loss: 0.08220281451940536\n","step: 3330, loss: 0.04656919091939926\n","step: 3340, loss: 0.0694221705198288\n","step: 3350, loss: 0.20124532282352448\n","step: 3360, loss: 0.0644046887755394\n","step: 3370, loss: 0.11910112202167511\n","step: 3380, loss: 0.02746504545211792\n","step: 3390, loss: 0.041586220264434814\n","step: 3400, loss: 0.05174877122044563\n","step: 3410, loss: 0.19426998496055603\n","step: 3420, loss: 0.07012491673231125\n","step: 3430, loss: 0.06694599241018295\n","step: 3440, loss: 0.11145199835300446\n","step: 3450, loss: 0.12223115563392639\n","step: 3460, loss: 0.10119765996932983\n","step: 3470, loss: 0.13895343244075775\n","step: 3480, loss: 0.10639587044715881\n","step: 3490, loss: 0.06340154260396957\n","step: 3500, loss: 0.1266682744026184\n","step: 3510, loss: 0.05349118635058403\n","step: 3520, loss: 0.17934168875217438\n","step: 3530, loss: 0.0959082767367363\n","step: 3540, loss: 0.015084270387887955\n","step: 3550, loss: 0.09543582051992416\n","step: 3560, loss: 0.07190178334712982\n","step: 3570, loss: 0.16570137441158295\n","step: 3580, loss: 0.09599236398935318\n","step: 3590, loss: 0.04914022982120514\n","step: 3600, loss: 0.07483433932065964\n","step: 3610, loss: 0.06446778029203415\n","step: 3620, loss: 0.06539133191108704\n","step: 3630, loss: 0.10802551358938217\n","step: 3640, loss: 0.10584521293640137\n","step: 3650, loss: 0.02903866209089756\n","step: 3660, loss: 0.1756087690591812\n","step: 3670, loss: 0.05654007941484451\n","step: 3680, loss: 0.04956389218568802\n","step: 3690, loss: 0.02799118682742119\n","step: 3700, loss: 0.06184132397174835\n","step: 3710, loss: 0.03255534917116165\n","step: 3720, loss: 0.04647408425807953\n","step: 3730, loss: 0.13151399791240692\n","step: 3740, loss: 0.0967181995511055\n","step: 3750, loss: 0.13780787587165833\n","step: 3760, loss: 0.07957769930362701\n","step: 3770, loss: 0.06079996004700661\n","step: 3780, loss: 0.13565285503864288\n","step: 3790, loss: 0.04467509314417839\n","step: 3800, loss: 0.10251190513372421\n","step: 3810, loss: 0.06513462215662003\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        35\n","           2       0.83      0.31      0.45        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.94      0.84      0.89       291\n","           5       0.84      0.84      0.84       294\n","           6       0.99      0.97      0.98      1570\n","           7       0.49      0.96      0.65       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.99       689\n","          10       0.95      0.98      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.79      0.85      0.81        13\n","          14       0.57      1.00      0.72        43\n","          15       0.93      0.98      0.96      2778\n","          16       0.92      0.83      0.87      1151\n","          17       0.93      0.95      0.94        41\n","          18       0.91      0.97      0.94        32\n","          19       0.89      0.62      0.74        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.73      0.82      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.32      0.57      0.41        44\n","          25       0.85      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.91      0.86      0.88      1136\n","          31       0.54      0.68      0.60        19\n","          32       0.80      0.50      0.62         8\n","          33       0.62      0.94      0.75        86\n","          34       0.08      0.06      0.07        32\n","          35       0.99      0.99      0.99       474\n","          36       1.00      0.04      0.08       182\n","          37       0.86      0.97      0.91      1592\n","          38       0.91      0.99      0.95       404\n","          39       0.93      0.98      0.96       485\n","          40       0.91      0.94      0.93       573\n","          41       0.98      0.92      0.95       841\n","          42       0.98      0.99      0.98       575\n","          43       0.94      0.71      0.81       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.95      0.97        82\n","          48       0.83      0.06      0.12        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.82      0.79      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 463\n","\n","Loop 9\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9275403022766113\n","step: 10, loss: 1.9157668352127075\n","step: 20, loss: 0.7409091591835022\n","step: 30, loss: 0.4379444420337677\n","step: 40, loss: 0.3263144791126251\n","step: 50, loss: 0.07470570504665375\n","step: 60, loss: 0.2887437045574188\n","step: 70, loss: 0.14281487464904785\n","step: 80, loss: 0.2260415107011795\n","step: 90, loss: 0.13477903604507446\n","step: 100, loss: 0.10014750063419342\n","step: 110, loss: 0.13216157257556915\n","step: 120, loss: 0.13885223865509033\n","step: 130, loss: 0.1790265142917633\n","step: 140, loss: 0.10138984769582748\n","step: 150, loss: 0.09131795912981033\n","step: 160, loss: 0.14452825486660004\n","step: 170, loss: 0.1532616913318634\n","step: 180, loss: 0.11955077201128006\n","step: 190, loss: 0.17294184863567352\n","step: 200, loss: 0.1437111645936966\n","step: 210, loss: 0.07649702578783035\n","step: 220, loss: 0.09204573929309845\n","step: 230, loss: 0.09060344845056534\n","step: 240, loss: 0.14645566046237946\n","step: 250, loss: 0.09022996574640274\n","step: 260, loss: 0.09629148244857788\n","step: 270, loss: 0.1061977967619896\n","step: 280, loss: 0.07274097949266434\n","step: 290, loss: 0.1342426836490631\n","step: 300, loss: 0.13556891679763794\n","step: 310, loss: 0.1930937021970749\n","step: 320, loss: 0.08338756859302521\n","step: 330, loss: 0.23945099115371704\n","step: 340, loss: 0.11920943111181259\n","step: 350, loss: 0.132900670170784\n","step: 360, loss: 0.10439857095479965\n","step: 370, loss: 0.11860081553459167\n","step: 380, loss: 0.048463210463523865\n","step: 390, loss: 0.08064451813697815\n","step: 400, loss: 0.051430314779281616\n","step: 410, loss: 0.04591960087418556\n","step: 420, loss: 0.06032160297036171\n","step: 430, loss: 0.14089061319828033\n","step: 440, loss: 0.21625348925590515\n","step: 450, loss: 0.04817301407456398\n","step: 460, loss: 0.04788249731063843\n","step: 470, loss: 0.04945914447307587\n","step: 480, loss: 0.2464301437139511\n","step: 490, loss: 0.11707442998886108\n","step: 500, loss: 0.11370628327131271\n","step: 510, loss: 0.12199557572603226\n","step: 520, loss: 0.062160883098840714\n","step: 530, loss: 0.09480325877666473\n","step: 540, loss: 0.11601073294878006\n","step: 550, loss: 0.14307133853435516\n","step: 560, loss: 0.05480004847049713\n","step: 570, loss: 0.05386436730623245\n","step: 580, loss: 0.05868526175618172\n","step: 590, loss: 0.18065042793750763\n","step: 600, loss: 0.13679338991641998\n","step: 610, loss: 0.13057179749011993\n","step: 620, loss: 0.16112001240253448\n","step: 630, loss: 0.062093429267406464\n","step: 640, loss: 0.08218535780906677\n","step: 650, loss: 0.228787362575531\n","step: 660, loss: 0.1492520421743393\n","step: 670, loss: 0.11610943078994751\n","step: 680, loss: 0.1461823284626007\n","step: 690, loss: 0.20130555331707\n","step: 700, loss: 0.10882050544023514\n","step: 710, loss: 0.11095971614122391\n","step: 720, loss: 0.15944114327430725\n","step: 730, loss: 0.08729536086320877\n","step: 740, loss: 0.07478874176740646\n","step: 750, loss: 0.046291276812553406\n","step: 760, loss: 0.1371275931596756\n","step: 770, loss: 0.08735289424657822\n","step: 780, loss: 0.06425187736749649\n","step: 790, loss: 0.04295070469379425\n","step: 800, loss: 0.20107847452163696\n","step: 810, loss: 0.08998511731624603\n","step: 820, loss: 0.13692675530910492\n","step: 830, loss: 0.031029457226395607\n","step: 840, loss: 0.19127604365348816\n","step: 850, loss: 0.04710165038704872\n","step: 860, loss: 0.11070127040147781\n","step: 870, loss: 0.09263730049133301\n","step: 880, loss: 0.18460993468761444\n","step: 890, loss: 0.09141261875629425\n","step: 900, loss: 0.035143930464982986\n","step: 910, loss: 0.035129841417074203\n","step: 920, loss: 0.07473498582839966\n","step: 930, loss: 0.16038621962070465\n","step: 940, loss: 0.08021482825279236\n","step: 950, loss: 0.07525886595249176\n","step: 960, loss: 0.20688393712043762\n","step: 970, loss: 0.10126978904008865\n","step: 980, loss: 0.06480336934328079\n","step: 990, loss: 0.13232716917991638\n","step: 1000, loss: 0.1654716581106186\n","step: 1010, loss: 0.17705029249191284\n","step: 1020, loss: 0.04650624841451645\n","step: 1030, loss: 0.05222482606768608\n","step: 1040, loss: 0.14589481055736542\n","step: 1050, loss: 0.07174865156412125\n","step: 1060, loss: 0.0898805633187294\n","step: 1070, loss: 0.0872262641787529\n","step: 1080, loss: 0.22894999384880066\n","step: 1090, loss: 0.044064000248909\n","step: 1100, loss: 0.05368349701166153\n","step: 1110, loss: 0.06586851179599762\n","step: 1120, loss: 0.06265835464000702\n","step: 1130, loss: 0.1116614043712616\n","step: 1140, loss: 0.01839369162917137\n","step: 1150, loss: 0.13224086165428162\n","step: 1160, loss: 0.09837094694375992\n","step: 1170, loss: 0.10175731778144836\n","step: 1180, loss: 0.11578734964132309\n","step: 1190, loss: 0.09108376502990723\n","step: 1200, loss: 0.11856662482023239\n","step: 1210, loss: 0.042514216154813766\n","step: 1220, loss: 0.2194761037826538\n","step: 1230, loss: 0.16615210473537445\n","step: 1240, loss: 0.060397908091545105\n","step: 1250, loss: 0.03149392828345299\n","step: 1260, loss: 0.08895856142044067\n","step: 1270, loss: 0.0916990116238594\n","step: 1280, loss: 0.22349432110786438\n","step: 1290, loss: 0.08915089070796967\n","step: 1300, loss: 0.13447174429893494\n","step: 1310, loss: 0.05158458277583122\n","step: 1320, loss: 0.09024730324745178\n","step: 1330, loss: 0.08494865894317627\n","step: 1340, loss: 0.13934136927127838\n","step: 1350, loss: 0.1111174002289772\n","step: 1360, loss: 0.0939561128616333\n","step: 1370, loss: 0.21728341281414032\n","step: 1380, loss: 0.1318187713623047\n","step: 1390, loss: 0.0863608717918396\n","step: 1400, loss: 0.08964525163173676\n","step: 1410, loss: 0.11862271279096603\n","step: 1420, loss: 0.07467135787010193\n","step: 1430, loss: 0.04705937206745148\n","step: 1440, loss: 0.14114965498447418\n","step: 1450, loss: 0.06854314357042313\n","step: 1460, loss: 0.06947890669107437\n","step: 1470, loss: 0.09495969861745834\n","step: 1480, loss: 0.07988613098859787\n","step: 1490, loss: 0.08751457184553146\n","step: 1500, loss: 0.09445924311876297\n","step: 1510, loss: 0.22100169956684113\n","step: 1520, loss: 0.19022661447525024\n","step: 1530, loss: 0.08551904559135437\n","step: 1540, loss: 0.09274143725633621\n","step: 1550, loss: 0.1078731119632721\n","step: 1560, loss: 0.21611490845680237\n","step: 1570, loss: 0.061235424131155014\n","step: 1580, loss: 0.07017472386360168\n","step: 1590, loss: 0.02882014960050583\n","step: 1600, loss: 0.04703674465417862\n","step: 1610, loss: 0.1154579445719719\n","step: 1620, loss: 0.053190574049949646\n","step: 1630, loss: 0.128301739692688\n","step: 1640, loss: 0.17701293528079987\n","step: 1650, loss: 0.08751262724399567\n","step: 1660, loss: 0.21414369344711304\n","step: 1670, loss: 0.07446519285440445\n","step: 1680, loss: 0.07451938092708588\n","step: 1690, loss: 0.11672764271497726\n","step: 1700, loss: 0.14442628622055054\n","step: 1710, loss: 0.07070544362068176\n","step: 1720, loss: 0.04850790277123451\n","step: 1730, loss: 0.12524206936359406\n","step: 1740, loss: 0.098592109978199\n","step: 1750, loss: 0.10744262486696243\n","step: 1760, loss: 0.047314755618572235\n","step: 1770, loss: 0.057336047291755676\n","step: 1780, loss: 0.08132394403219223\n","step: 1790, loss: 0.13002796471118927\n","step: 1800, loss: 0.08668040484189987\n","step: 1810, loss: 0.08855576068162918\n","step: 1820, loss: 0.1396934688091278\n","step: 1830, loss: 0.08685694634914398\n","step: 1840, loss: 0.13178889453411102\n","step: 1850, loss: 0.0757490023970604\n","step: 1860, loss: 0.08496861904859543\n","step: 1870, loss: 0.052056245505809784\n","step: 1880, loss: 0.18707793951034546\n","step: 1890, loss: 0.10973114520311356\n","step: 1900, loss: 0.10116148740053177\n","step: 1910, loss: 0.05195310711860657\n","step: 1920, loss: 0.18191054463386536\n","step: 1930, loss: 0.09641262888908386\n","step: 1940, loss: 0.08502607047557831\n","step: 1950, loss: 0.05014878883957863\n","step: 1960, loss: 0.07096754014492035\n","step: 1970, loss: 0.1333613246679306\n","step: 1980, loss: 0.07133909314870834\n","step: 1990, loss: 0.03996080160140991\n","step: 2000, loss: 0.04254119470715523\n","step: 2010, loss: 0.09361884742975235\n","step: 2020, loss: 0.08651918917894363\n","step: 2030, loss: 0.07291340082883835\n","step: 2040, loss: 0.06587188690900803\n","step: 2050, loss: 0.07890043407678604\n","step: 2060, loss: 0.028264325112104416\n","step: 2070, loss: 0.15321658551692963\n","step: 2080, loss: 0.03219926357269287\n","step: 2090, loss: 0.03205692768096924\n","step: 2100, loss: 0.06898567080497742\n","step: 2110, loss: 0.18484477698802948\n","step: 2120, loss: 0.12742049992084503\n","step: 2130, loss: 0.06303641945123672\n","step: 2140, loss: 0.14912177622318268\n","step: 2150, loss: 0.05418001115322113\n","step: 2160, loss: 0.07554662227630615\n","step: 2170, loss: 0.07803922891616821\n","step: 2180, loss: 0.08346470445394516\n","step: 2190, loss: 0.08567371964454651\n","step: 2200, loss: 0.08106283098459244\n","step: 2210, loss: 0.1519092172384262\n","step: 2220, loss: 0.07150814682245255\n","step: 2230, loss: 0.12336032092571259\n","step: 2240, loss: 0.1684942990541458\n","step: 2250, loss: 0.1189982071518898\n","step: 2260, loss: 0.04938066378235817\n","step: 2270, loss: 0.12378812581300735\n","step: 2280, loss: 0.05768963694572449\n","step: 2290, loss: 0.07244587689638138\n","step: 2300, loss: 0.05073259025812149\n","step: 2310, loss: 0.02825809270143509\n","step: 2320, loss: 0.0902988389134407\n","step: 2330, loss: 0.13866817951202393\n","step: 2340, loss: 0.23040324449539185\n","step: 2350, loss: 0.08310603350400925\n","step: 2360, loss: 0.06674675643444061\n","step: 2370, loss: 0.09116411209106445\n","step: 2380, loss: 0.1620572805404663\n","step: 2390, loss: 0.14030149579048157\n","step: 2400, loss: 0.09386511147022247\n","step: 2410, loss: 0.07011587917804718\n","step: 2420, loss: 0.028360137715935707\n","step: 2430, loss: 0.18659956753253937\n","step: 2440, loss: 0.05793069675564766\n","step: 2450, loss: 0.06857655942440033\n","step: 2460, loss: 0.047847405076026917\n","step: 2470, loss: 0.04873937740921974\n","step: 2480, loss: 0.040641896426677704\n","step: 2490, loss: 0.1558668166399002\n","step: 2500, loss: 0.07970903813838959\n","step: 2510, loss: 0.10718072205781937\n","step: 2520, loss: 0.07928748428821564\n","step: 2530, loss: 0.0623333603143692\n","step: 2540, loss: 0.09485585987567902\n","step: 2550, loss: 0.07341436296701431\n","step: 2560, loss: 0.05986257642507553\n","step: 2570, loss: 0.04045308381319046\n","step: 2580, loss: 0.032442931085824966\n","step: 2590, loss: 0.06945085525512695\n","step: 2600, loss: 0.08957923948764801\n","step: 2610, loss: 0.0888235941529274\n","step: 2620, loss: 0.14647194743156433\n","step: 2630, loss: 0.10478641837835312\n","step: 2640, loss: 0.06294672191143036\n","step: 2650, loss: 0.0675903707742691\n","step: 2660, loss: 0.09898395091295242\n","step: 2670, loss: 0.1358182579278946\n","step: 2680, loss: 0.07458291202783585\n","step: 2690, loss: 0.07120246440172195\n","step: 2700, loss: 0.03227156400680542\n","step: 2710, loss: 0.04713565856218338\n","step: 2720, loss: 0.0548156313598156\n","step: 2730, loss: 0.040069881826639175\n","step: 2740, loss: 0.04834349825978279\n","step: 2750, loss: 0.08796873688697815\n","step: 2760, loss: 0.09202753007411957\n","step: 2770, loss: 0.13785071671009064\n","step: 2780, loss: 0.09688450396060944\n","step: 2790, loss: 0.05696050450205803\n","step: 2800, loss: 0.05552605912089348\n","step: 2810, loss: 0.09063144028186798\n","step: 2820, loss: 0.05955490469932556\n","step: 2830, loss: 0.09457504749298096\n","step: 2840, loss: 0.06380005180835724\n","step: 2850, loss: 0.10495219379663467\n","step: 2860, loss: 0.08399297297000885\n","step: 2870, loss: 0.1883222907781601\n","step: 2880, loss: 0.05213906988501549\n","step: 2890, loss: 0.056934211403131485\n","step: 2900, loss: 0.092132069170475\n","step: 2910, loss: 0.057639628648757935\n","step: 2920, loss: 0.054208509624004364\n","step: 2930, loss: 0.11224246025085449\n","step: 2940, loss: 0.10451843589544296\n","step: 2950, loss: 0.04409141466021538\n","step: 2960, loss: 0.052299000322818756\n","step: 2970, loss: 0.06509844958782196\n","step: 2980, loss: 0.07831176370382309\n","step: 2990, loss: 0.12392500042915344\n","step: 3000, loss: 0.05797041952610016\n","step: 3010, loss: 0.03295927867293358\n","step: 3020, loss: 0.05910859629511833\n","step: 3030, loss: 0.05900750309228897\n","step: 3040, loss: 0.08766555786132812\n","step: 3050, loss: 0.15340888500213623\n","step: 3060, loss: 0.10025078803300858\n","step: 3070, loss: 0.06631574779748917\n","step: 3080, loss: 0.15458065271377563\n","step: 3090, loss: 0.09684787690639496\n","step: 3100, loss: 0.06781128793954849\n","step: 3110, loss: 0.17527702450752258\n","step: 3120, loss: 0.24255524575710297\n","step: 3130, loss: 0.04460969939827919\n","step: 3140, loss: 0.08774378150701523\n","step: 3150, loss: 0.09736115485429764\n","step: 3160, loss: 0.16963565349578857\n","step: 3170, loss: 0.0711381658911705\n","step: 3180, loss: 0.08969458192586899\n","step: 3190, loss: 0.07913260906934738\n","step: 3200, loss: 0.10505851358175278\n","step: 3210, loss: 0.038495518267154694\n","step: 3220, loss: 0.06336493790149689\n","step: 3230, loss: 0.07124800980091095\n","step: 3240, loss: 0.05731995031237602\n","step: 3250, loss: 0.1040334478020668\n","step: 3260, loss: 0.022564148530364037\n","step: 3270, loss: 0.08914748579263687\n","step: 3280, loss: 0.13828124105930328\n","step: 3290, loss: 0.13088347017765045\n","step: 3300, loss: 0.06206183135509491\n","step: 3310, loss: 0.03883514180779457\n","step: 3320, loss: 0.11157584935426712\n","step: 3330, loss: 0.095777727663517\n","step: 3340, loss: 0.11523725837469101\n","step: 3350, loss: 0.04532081261277199\n","step: 3360, loss: 0.05673051252961159\n","step: 3370, loss: 0.03444186970591545\n","step: 3380, loss: 0.06917211413383484\n","step: 3390, loss: 0.06065606325864792\n","step: 3400, loss: 0.12711206078529358\n","step: 3410, loss: 0.06504140049219131\n","step: 3420, loss: 0.04490000009536743\n","step: 3430, loss: 0.12067893147468567\n","step: 3440, loss: 0.13737592101097107\n","step: 3450, loss: 0.024941062554717064\n","step: 3460, loss: 0.0980740562081337\n","step: 3470, loss: 0.057456668466329575\n","step: 3480, loss: 0.10074705630540848\n","step: 3490, loss: 0.04629509150981903\n","step: 3500, loss: 0.051830634474754333\n","step: 3510, loss: 0.07494399696588516\n","step: 3520, loss: 0.05412215739488602\n","step: 3530, loss: 0.045365042984485626\n","step: 3540, loss: 0.10720429569482803\n","step: 3550, loss: 0.035138651728630066\n","step: 3560, loss: 0.06607475131750107\n","step: 3570, loss: 0.17259502410888672\n","step: 3580, loss: 0.07869665324687958\n","step: 3590, loss: 0.03837452456355095\n","step: 3600, loss: 0.026400813832879066\n","step: 3610, loss: 0.16702595353126526\n","step: 3620, loss: 0.06770772486925125\n","step: 3630, loss: 0.06269890815019608\n","step: 3640, loss: 0.06774818152189255\n","step: 3650, loss: 0.09358798712491989\n","step: 3660, loss: 0.07834527641534805\n","step: 3670, loss: 0.06452511250972748\n","step: 3680, loss: 0.05835501849651337\n","step: 3690, loss: 0.05539150908589363\n","step: 3700, loss: 0.06338506191968918\n","step: 3710, loss: 0.09803919494152069\n","step: 3720, loss: 0.13258418440818787\n","step: 3730, loss: 0.061611466109752655\n","step: 3740, loss: 0.04575126990675926\n","step: 3750, loss: 0.13511903584003448\n","step: 3760, loss: 0.10215482860803604\n","step: 3770, loss: 0.10760786384344101\n","step: 3780, loss: 0.059567082673311234\n","step: 3790, loss: 0.05880389362573624\n","step: 3800, loss: 0.14288170635700226\n","step: 3810, loss: 0.059495747089385986\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.76      1.00      0.86        35\n","           2       0.78      0.27      0.40        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.90      0.84      0.87       291\n","           5       0.97      0.79      0.87       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.41      0.96      0.58       186\n","           8       0.00      0.00      0.00        11\n","           9       0.97      0.99      0.98       689\n","          10       0.92      0.98      0.95       901\n","          11       0.97      1.00      0.98      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.71      0.92      0.80        13\n","          14       0.40      1.00      0.57        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.86      0.83      0.84      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.94      0.97      0.95        32\n","          19       0.54      0.47      0.51        40\n","          20       0.99      1.00      1.00       584\n","          21       0.31      0.08      0.12        52\n","          22       0.96      0.72      0.83      4175\n","          23       0.69      0.97      0.81      2253\n","          24       0.34      0.75      0.47        44\n","          25       0.86      0.90      0.88       888\n","          26       0.60      1.00      0.75         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      0.99      1.00      1864\n","          29       0.98      0.99      0.98       344\n","          30       0.93      0.84      0.88      1136\n","          31       0.61      0.58      0.59        19\n","          32       0.86      0.75      0.80         8\n","          33       0.73      0.86      0.79        86\n","          34       0.10      0.19      0.13        32\n","          35       0.99      0.97      0.98       474\n","          36       1.00      0.01      0.01       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.98      0.97      0.97       404\n","          39       0.97      0.97      0.97       485\n","          40       0.90      0.96      0.93       573\n","          41       0.93      0.94      0.93       841\n","          42       0.99      0.98      0.98       575\n","          43       0.94      0.88      0.91       152\n","          44       0.86      0.93      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.85      0.57      0.68        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 477\n","\n","Loop 10\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.0573201179504395\n","step: 10, loss: 1.9797073602676392\n","step: 20, loss: 0.6292812824249268\n","step: 30, loss: 0.523412823677063\n","step: 40, loss: 0.2269495725631714\n","step: 50, loss: 0.21297359466552734\n","step: 60, loss: 0.12505146861076355\n","step: 70, loss: 0.09353400766849518\n","step: 80, loss: 0.15048782527446747\n","step: 90, loss: 0.1534477025270462\n","step: 100, loss: 0.09721474349498749\n","step: 110, loss: 0.16122518479824066\n","step: 120, loss: 0.19338031113147736\n","step: 130, loss: 0.1490844041109085\n","step: 140, loss: 0.15460069477558136\n","step: 150, loss: 0.16041000187397003\n","step: 160, loss: 0.15262767672538757\n","step: 170, loss: 0.17866796255111694\n","step: 180, loss: 0.16105547547340393\n","step: 190, loss: 0.13469646871089935\n","step: 200, loss: 0.21410807967185974\n","step: 210, loss: 0.08034147322177887\n","step: 220, loss: 0.08281338214874268\n","step: 230, loss: 0.31083741784095764\n","step: 240, loss: 0.14387376606464386\n","step: 250, loss: 0.18849314749240875\n","step: 260, loss: 0.2488463819026947\n","step: 270, loss: 0.08885829895734787\n","step: 280, loss: 0.08662223070859909\n","step: 290, loss: 0.11443718522787094\n","step: 300, loss: 0.1127290427684784\n","step: 310, loss: 0.06955096125602722\n","step: 320, loss: 0.07850901782512665\n","step: 330, loss: 0.13569848239421844\n","step: 340, loss: 0.11242744326591492\n","step: 350, loss: 0.11970166862010956\n","step: 360, loss: 0.09679353982210159\n","step: 370, loss: 0.07491213828325272\n","step: 380, loss: 0.09442733973264694\n","step: 390, loss: 0.11909916251897812\n","step: 400, loss: 0.25592291355133057\n","step: 410, loss: 0.06831589341163635\n","step: 420, loss: 0.06380289793014526\n","step: 430, loss: 0.15083162486553192\n","step: 440, loss: 0.1395438015460968\n","step: 450, loss: 0.12846827507019043\n","step: 460, loss: 0.0563073568046093\n","step: 470, loss: 0.0945994183421135\n","step: 480, loss: 0.10999179631471634\n","step: 490, loss: 0.12873560190200806\n","step: 500, loss: 0.16425171494483948\n","step: 510, loss: 0.11700471490621567\n","step: 520, loss: 0.09505663067102432\n","step: 530, loss: 0.02312419004738331\n","step: 540, loss: 0.12965776026248932\n","step: 550, loss: 0.14164584875106812\n","step: 560, loss: 0.1579926609992981\n","step: 570, loss: 0.0929926410317421\n","step: 580, loss: 0.10529959201812744\n","step: 590, loss: 0.06498024612665176\n","step: 600, loss: 0.10855251550674438\n","step: 610, loss: 0.10669685155153275\n","step: 620, loss: 0.017569230869412422\n","step: 630, loss: 0.15747079253196716\n","step: 640, loss: 0.19133427739143372\n","step: 650, loss: 0.08457668125629425\n","step: 660, loss: 0.15318624675273895\n","step: 670, loss: 0.10228820890188217\n","step: 680, loss: 0.09572244435548782\n","step: 690, loss: 0.10998357832431793\n","step: 700, loss: 0.09936175495386124\n","step: 710, loss: 0.16717186570167542\n","step: 720, loss: 0.08360723406076431\n","step: 730, loss: 0.1649283617734909\n","step: 740, loss: 0.03996705263853073\n","step: 750, loss: 0.09846632927656174\n","step: 760, loss: 0.13454103469848633\n","step: 770, loss: 0.16800174117088318\n","step: 780, loss: 0.05626305192708969\n","step: 790, loss: 0.17497482895851135\n","step: 800, loss: 0.10628470033407211\n","step: 810, loss: 0.09193170070648193\n","step: 820, loss: 0.08635643869638443\n","step: 830, loss: 0.2113790363073349\n","step: 840, loss: 0.18060508370399475\n","step: 850, loss: 0.10267163813114166\n","step: 860, loss: 0.15593521296977997\n","step: 870, loss: 0.08387242257595062\n","step: 880, loss: 0.06322629749774933\n","step: 890, loss: 0.04087129980325699\n","step: 900, loss: 0.15251176059246063\n","step: 910, loss: 0.07598777860403061\n","step: 920, loss: 0.17793108522891998\n","step: 930, loss: 0.08615649491548538\n","step: 940, loss: 0.08502597361803055\n","step: 950, loss: 0.0653509646654129\n","step: 960, loss: 0.06747099757194519\n","step: 970, loss: 0.08267448842525482\n","step: 980, loss: 0.039476923644542694\n","step: 990, loss: 0.13222315907478333\n","step: 1000, loss: 0.17923051118850708\n","step: 1010, loss: 0.01735779270529747\n","step: 1020, loss: 0.10500788688659668\n","step: 1030, loss: 0.07076670229434967\n","step: 1040, loss: 0.07569193840026855\n","step: 1050, loss: 0.17776091396808624\n","step: 1060, loss: 0.14517684280872345\n","step: 1070, loss: 0.1629781872034073\n","step: 1080, loss: 0.13612642884254456\n","step: 1090, loss: 0.1279585361480713\n","step: 1100, loss: 0.03087332658469677\n","step: 1110, loss: 0.06912694126367569\n","step: 1120, loss: 0.10893671959638596\n","step: 1130, loss: 0.1267043501138687\n","step: 1140, loss: 0.07028571516275406\n","step: 1150, loss: 0.06784448772668839\n","step: 1160, loss: 0.20146703720092773\n","step: 1170, loss: 0.06945453584194183\n","step: 1180, loss: 0.06562063097953796\n","step: 1190, loss: 0.042605239897966385\n","step: 1200, loss: 0.15056568384170532\n","step: 1210, loss: 0.1570836901664734\n","step: 1220, loss: 0.039090994745492935\n","step: 1230, loss: 0.07021740823984146\n","step: 1240, loss: 0.09589254856109619\n","step: 1250, loss: 0.1495552510023117\n","step: 1260, loss: 0.11247552186250687\n","step: 1270, loss: 0.11452629417181015\n","step: 1280, loss: 0.07136867195367813\n","step: 1290, loss: 0.040833067148923874\n","step: 1300, loss: 0.028885826468467712\n","step: 1310, loss: 0.1552123874425888\n","step: 1320, loss: 0.07830008119344711\n","step: 1330, loss: 0.08716370910406113\n","step: 1340, loss: 0.07434507459402084\n","step: 1350, loss: 0.07488716393709183\n","step: 1360, loss: 0.05752067267894745\n","step: 1370, loss: 0.13314253091812134\n","step: 1380, loss: 0.0632234588265419\n","step: 1390, loss: 0.09349681437015533\n","step: 1400, loss: 0.11666077375411987\n","step: 1410, loss: 0.15071561932563782\n","step: 1420, loss: 0.05551772192120552\n","step: 1430, loss: 0.11081299930810928\n","step: 1440, loss: 0.12553998827934265\n","step: 1450, loss: 0.16199079155921936\n","step: 1460, loss: 0.08858326077461243\n","step: 1470, loss: 0.12316855788230896\n","step: 1480, loss: 0.09974402189254761\n","step: 1490, loss: 0.1140238344669342\n","step: 1500, loss: 0.10347781330347061\n","step: 1510, loss: 0.060635585337877274\n","step: 1520, loss: 0.06203661859035492\n","step: 1530, loss: 0.1626928597688675\n","step: 1540, loss: 0.15147313475608826\n","step: 1550, loss: 0.11437156051397324\n","step: 1560, loss: 0.11239802092313766\n","step: 1570, loss: 0.10288599133491516\n","step: 1580, loss: 0.03871770575642586\n","step: 1590, loss: 0.10654675960540771\n","step: 1600, loss: 0.05930114910006523\n","step: 1610, loss: 0.16967998445034027\n","step: 1620, loss: 0.06947741657495499\n","step: 1630, loss: 0.1166994497179985\n","step: 1640, loss: 0.06801820546388626\n","step: 1650, loss: 0.13898834586143494\n","step: 1660, loss: 0.06125686690211296\n","step: 1670, loss: 0.02890678681433201\n","step: 1680, loss: 0.15728947520256042\n","step: 1690, loss: 0.12079724669456482\n","step: 1700, loss: 0.04789486527442932\n","step: 1710, loss: 0.13229762017726898\n","step: 1720, loss: 0.024797797203063965\n","step: 1730, loss: 0.1728544682264328\n","step: 1740, loss: 0.12960706651210785\n","step: 1750, loss: 0.14817127585411072\n","step: 1760, loss: 0.05838505178689957\n","step: 1770, loss: 0.12255403399467468\n","step: 1780, loss: 0.05029195919632912\n","step: 1790, loss: 0.11003709584474564\n","step: 1800, loss: 0.21232850849628448\n","step: 1810, loss: 0.0997827798128128\n","step: 1820, loss: 0.23433122038841248\n","step: 1830, loss: 0.12662921845912933\n","step: 1840, loss: 0.1383787840604782\n","step: 1850, loss: 0.14755979180335999\n","step: 1860, loss: 0.020089540630578995\n","step: 1870, loss: 0.1794484704732895\n","step: 1880, loss: 0.1442217379808426\n","step: 1890, loss: 0.10434893518686295\n","step: 1900, loss: 0.08143435418605804\n","step: 1910, loss: 0.04421563446521759\n","step: 1920, loss: 0.08604218065738678\n","step: 1930, loss: 0.06280042231082916\n","step: 1940, loss: 0.07254805415868759\n","step: 1950, loss: 0.059481024742126465\n","step: 1960, loss: 0.0843418762087822\n","step: 1970, loss: 0.1920131891965866\n","step: 1980, loss: 0.1500045210123062\n","step: 1990, loss: 0.12186802923679352\n","step: 2000, loss: 0.06361755728721619\n","step: 2010, loss: 0.15248766541481018\n","step: 2020, loss: 0.09058181941509247\n","step: 2030, loss: 0.07911928743124008\n","step: 2040, loss: 0.21559982001781464\n","step: 2050, loss: 0.1281605064868927\n","step: 2060, loss: 0.10764399915933609\n","step: 2070, loss: 0.13248060643672943\n","step: 2080, loss: 0.04327273368835449\n","step: 2090, loss: 0.03153929486870766\n","step: 2100, loss: 0.054073501378297806\n","step: 2110, loss: 0.14855323731899261\n","step: 2120, loss: 0.03713478520512581\n","step: 2130, loss: 0.102461077272892\n","step: 2140, loss: 0.05010077357292175\n","step: 2150, loss: 0.14775846898555756\n","step: 2160, loss: 0.08480251580476761\n","step: 2170, loss: 0.04480690509080887\n","step: 2180, loss: 0.22486945986747742\n","step: 2190, loss: 0.08074425160884857\n","step: 2200, loss: 0.1367572844028473\n","step: 2210, loss: 0.10603450983762741\n","step: 2220, loss: 0.21764995157718658\n","step: 2230, loss: 0.08991040289402008\n","step: 2240, loss: 0.06387437134981155\n","step: 2250, loss: 0.06635410338640213\n","step: 2260, loss: 0.0649985820055008\n","step: 2270, loss: 0.140359565615654\n","step: 2280, loss: 0.157645121216774\n","step: 2290, loss: 0.08266467601060867\n","step: 2300, loss: 0.04969782382249832\n","step: 2310, loss: 0.2591562271118164\n","step: 2320, loss: 0.1015959158539772\n","step: 2330, loss: 0.12118560075759888\n","step: 2340, loss: 0.022907886654138565\n","step: 2350, loss: 0.09298110753297806\n","step: 2360, loss: 0.1382366418838501\n","step: 2370, loss: 0.09688332676887512\n","step: 2380, loss: 0.07250916212797165\n","step: 2390, loss: 0.06549300253391266\n","step: 2400, loss: 0.06684935837984085\n","step: 2410, loss: 0.051807064563035965\n","step: 2420, loss: 0.08923695981502533\n","step: 2430, loss: 0.1827925592660904\n","step: 2440, loss: 0.13094249367713928\n","step: 2450, loss: 0.053903162479400635\n","step: 2460, loss: 0.10978281497955322\n","step: 2470, loss: 0.10025004297494888\n","step: 2480, loss: 0.14449891448020935\n","step: 2490, loss: 0.057234443724155426\n","step: 2500, loss: 0.08213558048009872\n","step: 2510, loss: 0.11725280433893204\n","step: 2520, loss: 0.040671560913324356\n","step: 2530, loss: 0.0868561714887619\n","step: 2540, loss: 0.1252775639295578\n","step: 2550, loss: 0.07665383070707321\n","step: 2560, loss: 0.11763181537389755\n","step: 2570, loss: 0.025631200522184372\n","step: 2580, loss: 0.13973113894462585\n","step: 2590, loss: 0.08001525700092316\n","step: 2600, loss: 0.053653061389923096\n","step: 2610, loss: 0.11959385871887207\n","step: 2620, loss: 0.03558451309800148\n","step: 2630, loss: 0.043214522302150726\n","step: 2640, loss: 0.07194127887487411\n","step: 2650, loss: 0.054765477776527405\n","step: 2660, loss: 0.16425873339176178\n","step: 2670, loss: 0.12164029479026794\n","step: 2680, loss: 0.14987678825855255\n","step: 2690, loss: 0.07935582101345062\n","step: 2700, loss: 0.15994533896446228\n","step: 2710, loss: 0.21595795452594757\n","step: 2720, loss: 0.11330044269561768\n","step: 2730, loss: 0.03493506833910942\n","step: 2740, loss: 0.03829650953412056\n","step: 2750, loss: 0.031653378158807755\n","step: 2760, loss: 0.13936156034469604\n","step: 2770, loss: 0.08494974672794342\n","step: 2780, loss: 0.05419108271598816\n","step: 2790, loss: 0.07634935528039932\n","step: 2800, loss: 0.04913691431283951\n","step: 2810, loss: 0.12964913249015808\n","step: 2820, loss: 0.07095380872488022\n","step: 2830, loss: 0.1330062747001648\n","step: 2840, loss: 0.05258648097515106\n","step: 2850, loss: 0.0448761060833931\n","step: 2860, loss: 0.06860869377851486\n","step: 2870, loss: 0.11583899706602097\n","step: 2880, loss: 0.06332936882972717\n","step: 2890, loss: 0.0992375984787941\n","step: 2900, loss: 0.048587147146463394\n","step: 2910, loss: 0.061319511383771896\n","step: 2920, loss: 0.1180744543671608\n","step: 2930, loss: 0.10399941354990005\n","step: 2940, loss: 0.020011242479085922\n","step: 2950, loss: 0.12793321907520294\n","step: 2960, loss: 0.054785873740911484\n","step: 2970, loss: 0.05422953888773918\n","step: 2980, loss: 0.09150068461894989\n","step: 2990, loss: 0.05383257195353508\n","step: 3000, loss: 0.03594661504030228\n","step: 3010, loss: 0.042660970240831375\n","step: 3020, loss: 0.08092290163040161\n","step: 3030, loss: 0.03666087985038757\n","step: 3040, loss: 0.15237374603748322\n","step: 3050, loss: 0.030687838792800903\n","step: 3060, loss: 0.10983932763338089\n","step: 3070, loss: 0.1082034483551979\n","step: 3080, loss: 0.08398272097110748\n","step: 3090, loss: 0.03336199000477791\n","step: 3100, loss: 0.1634168177843094\n","step: 3110, loss: 0.07988095283508301\n","step: 3120, loss: 0.09674832969903946\n","step: 3130, loss: 0.04882466420531273\n","step: 3140, loss: 0.09091676771640778\n","step: 3150, loss: 0.051415033638477325\n","step: 3160, loss: 0.07304239273071289\n","step: 3170, loss: 0.10088478028774261\n","step: 3180, loss: 0.11784762144088745\n","step: 3190, loss: 0.1288141906261444\n","step: 3200, loss: 0.11654656380414963\n","step: 3210, loss: 0.026290884241461754\n","step: 3220, loss: 0.10664989799261093\n","step: 3230, loss: 0.06982529163360596\n","step: 3240, loss: 0.0578000470995903\n","step: 3250, loss: 0.11368317157030106\n","step: 3260, loss: 0.13094864785671234\n","step: 3270, loss: 0.04156911373138428\n","step: 3280, loss: 0.10335645079612732\n","step: 3290, loss: 0.11426866799592972\n","step: 3300, loss: 0.1474149227142334\n","step: 3310, loss: 0.10309196263551712\n","step: 3320, loss: 0.05958181992173195\n","step: 3330, loss: 0.04935736954212189\n","step: 3340, loss: 0.0790100023150444\n","step: 3350, loss: 0.1872023195028305\n","step: 3360, loss: 0.03636224567890167\n","step: 3370, loss: 0.04892536625266075\n","step: 3380, loss: 0.1323230266571045\n","step: 3390, loss: 0.02114938572049141\n","step: 3400, loss: 0.28575268387794495\n","step: 3410, loss: 0.11311931908130646\n","step: 3420, loss: 0.05253005027770996\n","step: 3430, loss: 0.1053357943892479\n","step: 3440, loss: 0.05447918921709061\n","step: 3450, loss: 0.165498286485672\n","step: 3460, loss: 0.10785812884569168\n","step: 3470, loss: 0.038895267993211746\n","step: 3480, loss: 0.07064899802207947\n","step: 3490, loss: 0.17050190269947052\n","step: 3500, loss: 0.09507595002651215\n","step: 3510, loss: 0.023317469283938408\n","step: 3520, loss: 0.030105726793408394\n","step: 3530, loss: 0.05495769903063774\n","step: 3540, loss: 0.05026473104953766\n","step: 3550, loss: 0.1103697270154953\n","step: 3560, loss: 0.05205477401614189\n","step: 3570, loss: 0.0605391189455986\n","step: 3580, loss: 0.05158168077468872\n","step: 3590, loss: 0.12252093106508255\n","step: 3600, loss: 0.11607321351766586\n","step: 3610, loss: 0.03170255199074745\n","step: 3620, loss: 0.10467322915792465\n","step: 3630, loss: 0.05728304013609886\n","step: 3640, loss: 0.0662897527217865\n","step: 3650, loss: 0.10843461006879807\n","step: 3660, loss: 0.051513925194740295\n","step: 3670, loss: 0.08362188935279846\n","step: 3680, loss: 0.05902403965592384\n","step: 3690, loss: 0.08183643966913223\n","step: 3700, loss: 0.06984120607376099\n","step: 3710, loss: 0.06959442794322968\n","step: 3720, loss: 0.1573566049337387\n","step: 3730, loss: 0.05371054634451866\n","step: 3740, loss: 0.056751132011413574\n","step: 3750, loss: 0.10521440953016281\n","step: 3760, loss: 0.16629624366760254\n","step: 3770, loss: 0.1525261253118515\n","step: 3780, loss: 0.03845256194472313\n","step: 3790, loss: 0.12253004312515259\n","step: 3800, loss: 0.08572821319103241\n","step: 3810, loss: 0.0720260888338089\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.56      1.00      0.72        35\n","           2       0.93      0.53      0.68        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.91      0.84      0.88       291\n","           5       0.98      0.84      0.90       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.56      0.93      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.09      1.00      0.17        13\n","          14       0.27      1.00      0.42        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.88      0.83      0.86      1151\n","          17       0.89      0.95      0.92        41\n","          18       0.86      1.00      0.93        32\n","          19       0.67      0.65      0.66        40\n","          20       0.99      1.00      1.00       584\n","          21       0.13      0.17      0.15        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.70      0.95      0.81      2253\n","          24       0.34      0.61      0.44        44\n","          25       0.87      0.92      0.89       888\n","          26       1.00      0.89      0.94         9\n","          27       0.97      0.99      0.98        69\n","          28       0.97      0.99      0.98      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.83      0.88      1136\n","          31       0.52      0.58      0.55        19\n","          32       1.00      0.38      0.55         8\n","          33       0.59      0.99      0.74        86\n","          34       0.17      0.44      0.25        32\n","          35       0.99      0.98      0.98       474\n","          36       1.00      0.14      0.25       182\n","          37       0.90      0.94      0.92      1592\n","          38       0.97      0.97      0.97       404\n","          39       0.96      0.96      0.96       485\n","          40       0.91      0.94      0.93       573\n","          41       0.94      0.95      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.87      0.96      0.92       152\n","          44       0.92      0.91      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.06      0.12        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 463\n","\n","Loop 11\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.991445302963257\n","step: 10, loss: 2.0112874507904053\n","step: 20, loss: 0.8278156518936157\n","step: 30, loss: 0.5379595756530762\n","step: 40, loss: 0.49792927503585815\n","step: 50, loss: 0.24866564571857452\n","step: 60, loss: 0.2313840240240097\n","step: 70, loss: 0.2047153115272522\n","step: 80, loss: 0.15343692898750305\n","step: 90, loss: 0.13402478396892548\n","step: 100, loss: 0.17040768265724182\n","step: 110, loss: 0.11502620577812195\n","step: 120, loss: 0.10390444844961166\n","step: 130, loss: 0.11828173696994781\n","step: 140, loss: 0.08847404271364212\n","step: 150, loss: 0.13598564267158508\n","step: 160, loss: 0.09748796373605728\n","step: 170, loss: 0.08929076045751572\n","step: 180, loss: 0.1378687024116516\n","step: 190, loss: 0.09372030198574066\n","step: 200, loss: 0.1299930065870285\n","step: 210, loss: 0.08296355605125427\n","step: 220, loss: 0.11877577751874924\n","step: 230, loss: 0.16172464191913605\n","step: 240, loss: 0.08114086091518402\n","step: 250, loss: 0.17174866795539856\n","step: 260, loss: 0.1185283213853836\n","step: 270, loss: 0.1053929552435875\n","step: 280, loss: 0.2669233977794647\n","step: 290, loss: 0.1261952519416809\n","step: 300, loss: 0.28826484084129333\n","step: 310, loss: 0.07218295335769653\n","step: 320, loss: 0.1363450586795807\n","step: 330, loss: 0.09009242057800293\n","step: 340, loss: 0.1813448816537857\n","step: 350, loss: 0.09070577472448349\n","step: 360, loss: 0.15491452813148499\n","step: 370, loss: 0.1088642030954361\n","step: 380, loss: 0.09570295363664627\n","step: 390, loss: 0.07567281275987625\n","step: 400, loss: 0.10478882491588593\n","step: 410, loss: 0.11405017226934433\n","step: 420, loss: 0.08635076135396957\n","step: 430, loss: 0.0452323853969574\n","step: 440, loss: 0.13547684252262115\n","step: 450, loss: 0.08364038914442062\n","step: 460, loss: 0.06405093520879745\n","step: 470, loss: 0.05219467356801033\n","step: 480, loss: 0.08699436485767365\n","step: 490, loss: 0.10984592884778976\n","step: 500, loss: 0.12715856730937958\n","step: 510, loss: 0.13019788265228271\n","step: 520, loss: 0.09165731072425842\n","step: 530, loss: 0.060583434998989105\n","step: 540, loss: 0.13404235243797302\n","step: 550, loss: 0.1135694831609726\n","step: 560, loss: 0.1553889811038971\n","step: 570, loss: 0.14455154538154602\n","step: 580, loss: 0.05065084248781204\n","step: 590, loss: 0.09695576131343842\n","step: 600, loss: 0.048724520951509476\n","step: 610, loss: 0.057144489139318466\n","step: 620, loss: 0.0713936984539032\n","step: 630, loss: 0.14981865882873535\n","step: 640, loss: 0.09977762401103973\n","step: 650, loss: 0.055994343012571335\n","step: 660, loss: 0.03410313278436661\n","step: 670, loss: 0.1589144617319107\n","step: 680, loss: 0.05779153108596802\n","step: 690, loss: 0.10435181111097336\n","step: 700, loss: 0.08229303359985352\n","step: 710, loss: 0.0948338508605957\n","step: 720, loss: 0.2498425394296646\n","step: 730, loss: 0.09773976355791092\n","step: 740, loss: 0.14600713551044464\n","step: 750, loss: 0.029864320531487465\n","step: 760, loss: 0.0695275068283081\n","step: 770, loss: 0.053174614906311035\n","step: 780, loss: 0.15099823474884033\n","step: 790, loss: 0.1547556072473526\n","step: 800, loss: 0.11702024936676025\n","step: 810, loss: 0.04435635358095169\n","step: 820, loss: 0.09534242749214172\n","step: 830, loss: 0.163244366645813\n","step: 840, loss: 0.044666752219200134\n","step: 850, loss: 0.05891289934515953\n","step: 860, loss: 0.05041494220495224\n","step: 870, loss: 0.06572500616312027\n","step: 880, loss: 0.18697065114974976\n","step: 890, loss: 0.043804291635751724\n","step: 900, loss: 0.030032319948077202\n","step: 910, loss: 0.1184883564710617\n","step: 920, loss: 0.15162870287895203\n","step: 930, loss: 0.14765797555446625\n","step: 940, loss: 0.14083315432071686\n","step: 950, loss: 0.02532963640987873\n","step: 960, loss: 0.09516066312789917\n","step: 970, loss: 0.14751344919204712\n","step: 980, loss: 0.15750814974308014\n","step: 990, loss: 0.09523830562829971\n","step: 1000, loss: 0.09077177941799164\n","step: 1010, loss: 0.0854247435927391\n","step: 1020, loss: 0.15162064135074615\n","step: 1030, loss: 0.15431061387062073\n","step: 1040, loss: 0.06991346925497055\n","step: 1050, loss: 0.10382096469402313\n","step: 1060, loss: 0.12519225478172302\n","step: 1070, loss: 0.06464988738298416\n","step: 1080, loss: 0.1384376436471939\n","step: 1090, loss: 0.03933720663189888\n","step: 1100, loss: 0.11997423321008682\n","step: 1110, loss: 0.11442288011312485\n","step: 1120, loss: 0.02978082001209259\n","step: 1130, loss: 0.07643906027078629\n","step: 1140, loss: 0.22696904838085175\n","step: 1150, loss: 0.10987652838230133\n","step: 1160, loss: 0.11660337448120117\n","step: 1170, loss: 0.07171586900949478\n","step: 1180, loss: 0.10975527763366699\n","step: 1190, loss: 0.2040521651506424\n","step: 1200, loss: 0.0906430035829544\n","step: 1210, loss: 0.2091219127178192\n","step: 1220, loss: 0.08324921131134033\n","step: 1230, loss: 0.10617595911026001\n","step: 1240, loss: 0.06586294621229172\n","step: 1250, loss: 0.20793205499649048\n","step: 1260, loss: 0.11099955439567566\n","step: 1270, loss: 0.1700548380613327\n","step: 1280, loss: 0.04223897308111191\n","step: 1290, loss: 0.1456580013036728\n","step: 1300, loss: 0.09786470979452133\n","step: 1310, loss: 0.0405033640563488\n","step: 1320, loss: 0.14814633131027222\n","step: 1330, loss: 0.11360065639019012\n","step: 1340, loss: 0.10466919839382172\n","step: 1350, loss: 0.11939650028944016\n","step: 1360, loss: 0.17341621220111847\n","step: 1370, loss: 0.0709587037563324\n","step: 1380, loss: 0.07674410194158554\n","step: 1390, loss: 0.20831261575222015\n","step: 1400, loss: 0.13064806163311005\n","step: 1410, loss: 0.06102757155895233\n","step: 1420, loss: 0.04794318974018097\n","step: 1430, loss: 0.1802392452955246\n","step: 1440, loss: 0.04718998819589615\n","step: 1450, loss: 0.05095013231039047\n","step: 1460, loss: 0.05431440845131874\n","step: 1470, loss: 0.07515961676836014\n","step: 1480, loss: 0.09324364364147186\n","step: 1490, loss: 0.041569024324417114\n","step: 1500, loss: 0.07764670252799988\n","step: 1510, loss: 0.07021147757768631\n","step: 1520, loss: 0.11191478371620178\n","step: 1530, loss: 0.04552506282925606\n","step: 1540, loss: 0.09438738971948624\n","step: 1550, loss: 0.12247201055288315\n","step: 1560, loss: 0.14477503299713135\n","step: 1570, loss: 0.07336613535881042\n","step: 1580, loss: 0.10169491171836853\n","step: 1590, loss: 0.02499655820429325\n","step: 1600, loss: 0.08824598044157028\n","step: 1610, loss: 0.0770135149359703\n","step: 1620, loss: 0.1057339757680893\n","step: 1630, loss: 0.18861150741577148\n","step: 1640, loss: 0.08864983171224594\n","step: 1650, loss: 0.050422023981809616\n","step: 1660, loss: 0.06217591464519501\n","step: 1670, loss: 0.034007634967565536\n","step: 1680, loss: 0.03265966847538948\n","step: 1690, loss: 0.054923322051763535\n","step: 1700, loss: 0.1226528137922287\n","step: 1710, loss: 0.05476703122258186\n","step: 1720, loss: 0.049807947129011154\n","step: 1730, loss: 0.07319888472557068\n","step: 1740, loss: 0.07865636795759201\n","step: 1750, loss: 0.07204379886388779\n","step: 1760, loss: 0.07596802711486816\n","step: 1770, loss: 0.1365659534931183\n","step: 1780, loss: 0.01938420906662941\n","step: 1790, loss: 0.1008906289935112\n","step: 1800, loss: 0.048249442130327225\n","step: 1810, loss: 0.0935652032494545\n","step: 1820, loss: 0.06247461959719658\n","step: 1830, loss: 0.09881994128227234\n","step: 1840, loss: 0.15977728366851807\n","step: 1850, loss: 0.12093964219093323\n","step: 1860, loss: 0.13908354938030243\n","step: 1870, loss: 0.01842617616057396\n","step: 1880, loss: 0.18394936621189117\n","step: 1890, loss: 0.033435557037591934\n","step: 1900, loss: 0.12282632291316986\n","step: 1910, loss: 0.05541258677840233\n","step: 1920, loss: 0.04856794700026512\n","step: 1930, loss: 0.22047662734985352\n","step: 1940, loss: 0.05994332209229469\n","step: 1950, loss: 0.05400610342621803\n","step: 1960, loss: 0.16883406043052673\n","step: 1970, loss: 0.11668405681848526\n","step: 1980, loss: 0.12159615010023117\n","step: 1990, loss: 0.1304219365119934\n","step: 2000, loss: 0.03726763278245926\n","step: 2010, loss: 0.11690770089626312\n","step: 2020, loss: 0.10645297169685364\n","step: 2030, loss: 0.030465800315141678\n","step: 2040, loss: 0.1356111615896225\n","step: 2050, loss: 0.08250511437654495\n","step: 2060, loss: 0.11671166121959686\n","step: 2070, loss: 0.16517792642116547\n","step: 2080, loss: 0.06523579359054565\n","step: 2090, loss: 0.14195752143859863\n","step: 2100, loss: 0.14685627818107605\n","step: 2110, loss: 0.11190237104892731\n","step: 2120, loss: 0.10290966182947159\n","step: 2130, loss: 0.08405478298664093\n","step: 2140, loss: 0.09496436268091202\n","step: 2150, loss: 0.066132552921772\n","step: 2160, loss: 0.06097029894590378\n","step: 2170, loss: 0.07326041907072067\n","step: 2180, loss: 0.050445716828107834\n","step: 2190, loss: 0.11143022775650024\n","step: 2200, loss: 0.10395534336566925\n","step: 2210, loss: 0.04891590774059296\n","step: 2220, loss: 0.20488756895065308\n","step: 2230, loss: 0.15734444558620453\n","step: 2240, loss: 0.07375803589820862\n","step: 2250, loss: 0.14324922859668732\n","step: 2260, loss: 0.047763675451278687\n","step: 2270, loss: 0.156093567609787\n","step: 2280, loss: 0.0674285739660263\n","step: 2290, loss: 0.09922093152999878\n","step: 2300, loss: 0.11088885366916656\n","step: 2310, loss: 0.06281843036413193\n","step: 2320, loss: 0.09557946026325226\n","step: 2330, loss: 0.030940495431423187\n","step: 2340, loss: 0.07611604034900665\n","step: 2350, loss: 0.04442673176527023\n","step: 2360, loss: 0.13767161965370178\n","step: 2370, loss: 0.06670479476451874\n","step: 2380, loss: 0.0976298525929451\n","step: 2390, loss: 0.0797087624669075\n","step: 2400, loss: 0.07897518575191498\n","step: 2410, loss: 0.057045675814151764\n","step: 2420, loss: 0.05381467193365097\n","step: 2430, loss: 0.0669187679886818\n","step: 2440, loss: 0.09553873538970947\n","step: 2450, loss: 0.09466803073883057\n","step: 2460, loss: 0.1116008535027504\n","step: 2470, loss: 0.1639660745859146\n","step: 2480, loss: 0.07622012495994568\n","step: 2490, loss: 0.09717568755149841\n","step: 2500, loss: 0.06105654314160347\n","step: 2510, loss: 0.06236287206411362\n","step: 2520, loss: 0.06589820235967636\n","step: 2530, loss: 0.10308844596147537\n","step: 2540, loss: 0.08195855468511581\n","step: 2550, loss: 0.06193728744983673\n","step: 2560, loss: 0.14860814809799194\n","step: 2570, loss: 0.08438355475664139\n","step: 2580, loss: 0.08382152020931244\n","step: 2590, loss: 0.0883578509092331\n","step: 2600, loss: 0.12227831780910492\n","step: 2610, loss: 0.06914771348237991\n","step: 2620, loss: 0.04499942064285278\n","step: 2630, loss: 0.07470744103193283\n","step: 2640, loss: 0.06053715571761131\n","step: 2650, loss: 0.07652046531438828\n","step: 2660, loss: 0.054121021181344986\n","step: 2670, loss: 0.058283042162656784\n","step: 2680, loss: 0.054954204708337784\n","step: 2690, loss: 0.09938279539346695\n","step: 2700, loss: 0.11291273683309555\n","step: 2710, loss: 0.11654745787382126\n","step: 2720, loss: 0.17426171898841858\n","step: 2730, loss: 0.026476701721549034\n","step: 2740, loss: 0.04698536917567253\n","step: 2750, loss: 0.030571304261684418\n","step: 2760, loss: 0.07360865920782089\n","step: 2770, loss: 0.04903184995055199\n","step: 2780, loss: 0.10438932478427887\n","step: 2790, loss: 0.03434174135327339\n","step: 2800, loss: 0.17664960026741028\n","step: 2810, loss: 0.09890410304069519\n","step: 2820, loss: 0.04130737856030464\n","step: 2830, loss: 0.1118873804807663\n","step: 2840, loss: 0.08466462790966034\n","step: 2850, loss: 0.10320355743169785\n","step: 2860, loss: 0.0625947117805481\n","step: 2870, loss: 0.13416345417499542\n","step: 2880, loss: 0.04234417900443077\n","step: 2890, loss: 0.10949184000492096\n","step: 2900, loss: 0.09152864664793015\n","step: 2910, loss: 0.12873022258281708\n","step: 2920, loss: 0.1357160061597824\n","step: 2930, loss: 0.06386128067970276\n","step: 2940, loss: 0.04476921260356903\n","step: 2950, loss: 0.06010959669947624\n","step: 2960, loss: 0.07211480289697647\n","step: 2970, loss: 0.09782510250806808\n","step: 2980, loss: 0.0785311609506607\n","step: 2990, loss: 0.051818929612636566\n","step: 3000, loss: 0.0741882175207138\n","step: 3010, loss: 0.09808067977428436\n","step: 3020, loss: 0.047246064990758896\n","step: 3030, loss: 0.043236445635557175\n","step: 3040, loss: 0.1298885941505432\n","step: 3050, loss: 0.11924239993095398\n","step: 3060, loss: 0.16488789021968842\n","step: 3070, loss: 0.06961315870285034\n","step: 3080, loss: 0.07151710987091064\n","step: 3090, loss: 0.04363814368844032\n","step: 3100, loss: 0.0848529264330864\n","step: 3110, loss: 0.05854158103466034\n","step: 3120, loss: 0.16342057287693024\n","step: 3130, loss: 0.06733287870883942\n","step: 3140, loss: 0.23539191484451294\n","step: 3150, loss: 0.07669136673212051\n","step: 3160, loss: 0.05630626529455185\n","step: 3170, loss: 0.024698298424482346\n","step: 3180, loss: 0.050145070999860764\n","step: 3190, loss: 0.07245699316263199\n","step: 3200, loss: 0.12255580723285675\n","step: 3210, loss: 0.13244758546352386\n","step: 3220, loss: 0.07674869894981384\n","step: 3230, loss: 0.060711588710546494\n","step: 3240, loss: 0.02091207541525364\n","step: 3250, loss: 0.08347900956869125\n","step: 3260, loss: 0.09865693002939224\n","step: 3270, loss: 0.10397015511989594\n","step: 3280, loss: 0.11219461262226105\n","step: 3290, loss: 0.020430931821465492\n","step: 3300, loss: 0.03012058697640896\n","step: 3310, loss: 0.0442253053188324\n","step: 3320, loss: 0.13798512518405914\n","step: 3330, loss: 0.08073540031909943\n","step: 3340, loss: 0.06361451745033264\n","step: 3350, loss: 0.06038996949791908\n","step: 3360, loss: 0.11484117060899734\n","step: 3370, loss: 0.11717621982097626\n","step: 3380, loss: 0.13972784578800201\n","step: 3390, loss: 0.04959697276353836\n","step: 3400, loss: 0.09141398221254349\n","step: 3410, loss: 0.16423164308071136\n","step: 3420, loss: 0.10790146142244339\n","step: 3430, loss: 0.0428488627076149\n","step: 3440, loss: 0.10486651957035065\n","step: 3450, loss: 0.07867354899644852\n","step: 3460, loss: 0.1688305139541626\n","step: 3470, loss: 0.10315531492233276\n","step: 3480, loss: 0.05121661350131035\n","step: 3490, loss: 0.025987165048718452\n","step: 3500, loss: 0.05895092338323593\n","step: 3510, loss: 0.052872102707624435\n","step: 3520, loss: 0.10929659754037857\n","step: 3530, loss: 0.05446993187069893\n","step: 3540, loss: 0.13211321830749512\n","step: 3550, loss: 0.07144533842802048\n","step: 3560, loss: 0.1477615386247635\n","step: 3570, loss: 0.05776276811957359\n","step: 3580, loss: 0.10562453418970108\n","step: 3590, loss: 0.17444147169589996\n","step: 3600, loss: 0.15021419525146484\n","step: 3610, loss: 0.11480000615119934\n","step: 3620, loss: 0.08615849912166595\n","step: 3630, loss: 0.07802122831344604\n","step: 3640, loss: 0.11531766504049301\n","step: 3650, loss: 0.0948759987950325\n","step: 3660, loss: 0.02818099781870842\n","step: 3670, loss: 0.03122824616730213\n","step: 3680, loss: 0.08545545488595963\n","step: 3690, loss: 0.0687287449836731\n","step: 3700, loss: 0.11962731927633286\n","step: 3710, loss: 0.13832496106624603\n","step: 3720, loss: 0.06233784928917885\n","step: 3730, loss: 0.1650201678276062\n","step: 3740, loss: 0.10570400953292847\n","step: 3750, loss: 0.12250304967164993\n","step: 3760, loss: 0.029806602746248245\n","step: 3770, loss: 0.040106698870658875\n","step: 3780, loss: 0.05653667449951172\n","step: 3790, loss: 0.09077554196119308\n","step: 3800, loss: 0.07459959387779236\n","step: 3810, loss: 0.1165895015001297\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      1.00      0.92        35\n","           2       0.68      0.36      0.47        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.99      0.80      0.88       291\n","           5       0.89      0.84      0.87       294\n","           6       1.00      0.98      0.99      1570\n","           7       0.50      0.94      0.65       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.95      0.98      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.23      0.85      0.36        13\n","          14       0.38      1.00      0.55        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.86      0.84      0.85      1151\n","          17       0.93      0.95      0.94        41\n","          18       0.91      0.97      0.94        32\n","          19       0.94      0.42      0.59        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.90      0.79      0.84      4175\n","          23       0.68      0.94      0.79      2253\n","          24       0.24      0.36      0.29        44\n","          25       0.86      0.91      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       0.97      0.97      0.97        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.94      0.85      0.89      1136\n","          31       0.67      0.63      0.65        19\n","          32       0.80      0.50      0.62         8\n","          33       0.61      0.95      0.74        86\n","          34       0.24      0.56      0.33        32\n","          35       0.98      0.99      0.99       474\n","          36       1.00      0.09      0.17       182\n","          37       0.90      0.94      0.92      1592\n","          38       0.97      0.98      0.98       404\n","          39       0.98      0.94      0.96       485\n","          40       0.92      0.93      0.93       573\n","          41       0.98      0.93      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.97      0.78      0.87       152\n","          44       0.87      0.96      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.80      0.77     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 446\n","\n","Loop 12\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9403436183929443\n","step: 10, loss: 2.0531342029571533\n","step: 20, loss: 0.7216070890426636\n","step: 30, loss: 0.3804892599582672\n","step: 40, loss: 0.3335835635662079\n","step: 50, loss: 0.21826422214508057\n","step: 60, loss: 0.2583637237548828\n","step: 70, loss: 0.12352269887924194\n","step: 80, loss: 0.09352466464042664\n","step: 90, loss: 0.35929957032203674\n","step: 100, loss: 0.1931179016828537\n","step: 110, loss: 0.07361634075641632\n","step: 120, loss: 0.18202240765094757\n","step: 130, loss: 0.14493204653263092\n","step: 140, loss: 0.09601446241140366\n","step: 150, loss: 0.14333181083202362\n","step: 160, loss: 0.1480947732925415\n","step: 170, loss: 0.1672770082950592\n","step: 180, loss: 0.1937815099954605\n","step: 190, loss: 0.13820065557956696\n","step: 200, loss: 0.09963555634021759\n","step: 210, loss: 0.10692352801561356\n","step: 220, loss: 0.06334523856639862\n","step: 230, loss: 0.2422124147415161\n","step: 240, loss: 0.09418055415153503\n","step: 250, loss: 0.15311411023139954\n","step: 260, loss: 0.079678975045681\n","step: 270, loss: 0.1293543428182602\n","step: 280, loss: 0.09363918751478195\n","step: 290, loss: 0.19361068308353424\n","step: 300, loss: 0.0248542670160532\n","step: 310, loss: 0.08891486376523972\n","step: 320, loss: 0.04588155448436737\n","step: 330, loss: 0.08400864154100418\n","step: 340, loss: 0.10313963890075684\n","step: 350, loss: 0.18734918534755707\n","step: 360, loss: 0.10710570961236954\n","step: 370, loss: 0.12878352403640747\n","step: 380, loss: 0.1538166105747223\n","step: 390, loss: 0.044145360589027405\n","step: 400, loss: 0.11187773942947388\n","step: 410, loss: 0.16149282455444336\n","step: 420, loss: 0.11207706481218338\n","step: 430, loss: 0.15734927356243134\n","step: 440, loss: 0.15557263791561127\n","step: 450, loss: 0.03676128759980202\n","step: 460, loss: 0.048547618091106415\n","step: 470, loss: 0.1644747257232666\n","step: 480, loss: 0.15935416519641876\n","step: 490, loss: 0.10598013550043106\n","step: 500, loss: 0.14206869900226593\n","step: 510, loss: 0.05045876279473305\n","step: 520, loss: 0.0747159793972969\n","step: 530, loss: 0.11909401416778564\n","step: 540, loss: 0.1890011578798294\n","step: 550, loss: 0.05092586576938629\n","step: 560, loss: 0.11250941455364227\n","step: 570, loss: 0.08622376620769501\n","step: 580, loss: 0.2027856707572937\n","step: 590, loss: 0.060413919389247894\n","step: 600, loss: 0.09121551364660263\n","step: 610, loss: 0.09540016949176788\n","step: 620, loss: 0.08111947029829025\n","step: 630, loss: 0.1824130266904831\n","step: 640, loss: 0.09821292012929916\n","step: 650, loss: 0.06520279496908188\n","step: 660, loss: 0.07559293508529663\n","step: 670, loss: 0.10376784205436707\n","step: 680, loss: 0.15986813604831696\n","step: 690, loss: 0.06285367161035538\n","step: 700, loss: 0.06065896153450012\n","step: 710, loss: 0.10564520210027695\n","step: 720, loss: 0.12839315831661224\n","step: 730, loss: 0.047966741025447845\n","step: 740, loss: 0.11095217615365982\n","step: 750, loss: 0.1439231038093567\n","step: 760, loss: 0.06691157072782516\n","step: 770, loss: 0.17926451563835144\n","step: 780, loss: 0.08391186594963074\n","step: 790, loss: 0.10068540275096893\n","step: 800, loss: 0.12453895062208176\n","step: 810, loss: 0.07634638249874115\n","step: 820, loss: 0.059336137026548386\n","step: 830, loss: 0.07232456654310226\n","step: 840, loss: 0.06966309994459152\n","step: 850, loss: 0.1366332322359085\n","step: 860, loss: 0.04099755361676216\n","step: 870, loss: 0.06017867848277092\n","step: 880, loss: 0.05955980718135834\n","step: 890, loss: 0.03717592731118202\n","step: 900, loss: 0.024593979120254517\n","step: 910, loss: 0.13571900129318237\n","step: 920, loss: 0.18894097208976746\n","step: 930, loss: 0.08616876602172852\n","step: 940, loss: 0.2184397429227829\n","step: 950, loss: 0.08319469541311264\n","step: 960, loss: 0.23690420389175415\n","step: 970, loss: 0.12193751335144043\n","step: 980, loss: 0.18249166011810303\n","step: 990, loss: 0.05559900403022766\n","step: 1000, loss: 0.08657153695821762\n","step: 1010, loss: 0.11412140727043152\n","step: 1020, loss: 0.06376832723617554\n","step: 1030, loss: 0.16366629302501678\n","step: 1040, loss: 0.11153851449489594\n","step: 1050, loss: 0.10023058950901031\n","step: 1060, loss: 0.04858868941664696\n","step: 1070, loss: 0.05499750375747681\n","step: 1080, loss: 0.11924072355031967\n","step: 1090, loss: 0.18326838314533234\n","step: 1100, loss: 0.12451642006635666\n","step: 1110, loss: 0.06289684027433395\n","step: 1120, loss: 0.08426430076360703\n","step: 1130, loss: 0.10362152010202408\n","step: 1140, loss: 0.13968729972839355\n","step: 1150, loss: 0.09295263886451721\n","step: 1160, loss: 0.09568268060684204\n","step: 1170, loss: 0.07567018270492554\n","step: 1180, loss: 0.17208413779735565\n","step: 1190, loss: 0.08321115374565125\n","step: 1200, loss: 0.13330937922000885\n","step: 1210, loss: 0.03275587409734726\n","step: 1220, loss: 0.09626786410808563\n","step: 1230, loss: 0.04460000991821289\n","step: 1240, loss: 0.07310349494218826\n","step: 1250, loss: 0.04449458420276642\n","step: 1260, loss: 0.15639373660087585\n","step: 1270, loss: 0.17447642982006073\n","step: 1280, loss: 0.09221287071704865\n","step: 1290, loss: 0.1253069043159485\n","step: 1300, loss: 0.12455493211746216\n","step: 1310, loss: 0.15563754737377167\n","step: 1320, loss: 0.1446380913257599\n","step: 1330, loss: 0.17748625576496124\n","step: 1340, loss: 0.07461640238761902\n","step: 1350, loss: 0.13618789613246918\n","step: 1360, loss: 0.0950840637087822\n","step: 1370, loss: 0.05981822684407234\n","step: 1380, loss: 0.054414600133895874\n","step: 1390, loss: 0.09382729232311249\n","step: 1400, loss: 0.08866499364376068\n","step: 1410, loss: 0.12731783092021942\n","step: 1420, loss: 0.027753004804253578\n","step: 1430, loss: 0.0600082091987133\n","step: 1440, loss: 0.12617598474025726\n","step: 1450, loss: 0.12196289747953415\n","step: 1460, loss: 0.10720720887184143\n","step: 1470, loss: 0.055885180830955505\n","step: 1480, loss: 0.11311017721891403\n","step: 1490, loss: 0.10654476284980774\n","step: 1500, loss: 0.055209048092365265\n","step: 1510, loss: 0.06713906675577164\n","step: 1520, loss: 0.14314769208431244\n","step: 1530, loss: 0.1697792261838913\n","step: 1540, loss: 0.04978788271546364\n","step: 1550, loss: 0.0586518831551075\n","step: 1560, loss: 0.08472906053066254\n","step: 1570, loss: 0.0623638741672039\n","step: 1580, loss: 0.0537918321788311\n","step: 1590, loss: 0.11116728186607361\n","step: 1600, loss: 0.1188153550028801\n","step: 1610, loss: 0.12096864730119705\n","step: 1620, loss: 0.15746933221817017\n","step: 1630, loss: 0.08363509923219681\n","step: 1640, loss: 0.06580692529678345\n","step: 1650, loss: 0.06746038794517517\n","step: 1660, loss: 0.0808638334274292\n","step: 1670, loss: 0.07619684189558029\n","step: 1680, loss: 0.13366694748401642\n","step: 1690, loss: 0.1421494334936142\n","step: 1700, loss: 0.25819897651672363\n","step: 1710, loss: 0.21005333960056305\n","step: 1720, loss: 0.0577964261174202\n","step: 1730, loss: 0.06742580235004425\n","step: 1740, loss: 0.08549390733242035\n","step: 1750, loss: 0.10217048972845078\n","step: 1760, loss: 0.11209035664796829\n","step: 1770, loss: 0.032010097056627274\n","step: 1780, loss: 0.03666789084672928\n","step: 1790, loss: 0.09546425193548203\n","step: 1800, loss: 0.06255600601434708\n","step: 1810, loss: 0.08256068825721741\n","step: 1820, loss: 0.14069081842899323\n","step: 1830, loss: 0.09801796823740005\n","step: 1840, loss: 0.048480063676834106\n","step: 1850, loss: 0.08384805172681808\n","step: 1860, loss: 0.07697638869285583\n","step: 1870, loss: 0.06283511966466904\n","step: 1880, loss: 0.054672759026288986\n","step: 1890, loss: 0.03443228453397751\n","step: 1900, loss: 0.11339609324932098\n","step: 1910, loss: 0.053343553096055984\n","step: 1920, loss: 0.1271204948425293\n","step: 1930, loss: 0.12754376232624054\n","step: 1940, loss: 0.08725015074014664\n","step: 1950, loss: 0.04273495823144913\n","step: 1960, loss: 0.03958762809634209\n","step: 1970, loss: 0.08092904835939407\n","step: 1980, loss: 0.12120912224054337\n","step: 1990, loss: 0.05015343427658081\n","step: 2000, loss: 0.07655099779367447\n","step: 2010, loss: 0.12255308032035828\n","step: 2020, loss: 0.15618322789669037\n","step: 2030, loss: 0.10350655764341354\n","step: 2040, loss: 0.1158115416765213\n","step: 2050, loss: 0.10297539830207825\n","step: 2060, loss: 0.09024852514266968\n","step: 2070, loss: 0.06892930716276169\n","step: 2080, loss: 0.03338591009378433\n","step: 2090, loss: 0.06975092738866806\n","step: 2100, loss: 0.10899913311004639\n","step: 2110, loss: 0.05016785115003586\n","step: 2120, loss: 0.08028833568096161\n","step: 2130, loss: 0.0430806428194046\n","step: 2140, loss: 0.19984909892082214\n","step: 2150, loss: 0.14610187709331512\n","step: 2160, loss: 0.08572252839803696\n","step: 2170, loss: 0.02635040692985058\n","step: 2180, loss: 0.13412925601005554\n","step: 2190, loss: 0.07637986540794373\n","step: 2200, loss: 0.09013587236404419\n","step: 2210, loss: 0.11285331100225449\n","step: 2220, loss: 0.06451425701379776\n","step: 2230, loss: 0.05610206723213196\n","step: 2240, loss: 0.11623242497444153\n","step: 2250, loss: 0.09459412842988968\n","step: 2260, loss: 0.06043493375182152\n","step: 2270, loss: 0.15072809159755707\n","step: 2280, loss: 0.0932425856590271\n","step: 2290, loss: 0.07448288798332214\n","step: 2300, loss: 0.10917437076568604\n","step: 2310, loss: 0.028029197826981544\n","step: 2320, loss: 0.18991748988628387\n","step: 2330, loss: 0.13802246749401093\n","step: 2340, loss: 0.06674923747777939\n","step: 2350, loss: 0.08542125672101974\n","step: 2360, loss: 0.08689369261264801\n","step: 2370, loss: 0.12146182358264923\n","step: 2380, loss: 0.05547503009438515\n","step: 2390, loss: 0.14692462980747223\n","step: 2400, loss: 0.03752054274082184\n","step: 2410, loss: 0.1669512838125229\n","step: 2420, loss: 0.11062759160995483\n","step: 2430, loss: 0.04795082286000252\n","step: 2440, loss: 0.07096300274133682\n","step: 2450, loss: 0.12129618972539902\n","step: 2460, loss: 0.1275346875190735\n","step: 2470, loss: 0.08825308829545975\n","step: 2480, loss: 0.09005825221538544\n","step: 2490, loss: 0.16940338909626007\n","step: 2500, loss: 0.17484673857688904\n","step: 2510, loss: 0.054654087871313095\n","step: 2520, loss: 0.06992755830287933\n","step: 2530, loss: 0.04977397620677948\n","step: 2540, loss: 0.09099013358354568\n","step: 2550, loss: 0.05122853443026543\n","step: 2560, loss: 0.05983870103955269\n","step: 2570, loss: 0.07356327027082443\n","step: 2580, loss: 0.055320579558610916\n","step: 2590, loss: 0.08330973237752914\n","step: 2600, loss: 0.10862070322036743\n","step: 2610, loss: 0.06453406810760498\n","step: 2620, loss: 0.16063840687274933\n","step: 2630, loss: 0.1441410332918167\n","step: 2640, loss: 0.07199536263942719\n","step: 2650, loss: 0.0967768058180809\n","step: 2660, loss: 0.13365688920021057\n","step: 2670, loss: 0.04822758212685585\n","step: 2680, loss: 0.1517738401889801\n","step: 2690, loss: 0.07754955440759659\n","step: 2700, loss: 0.09109312295913696\n","step: 2710, loss: 0.1704077273607254\n","step: 2720, loss: 0.04093029722571373\n","step: 2730, loss: 0.06066960468888283\n","step: 2740, loss: 0.07449352741241455\n","step: 2750, loss: 0.07261309027671814\n","step: 2760, loss: 0.1603289097547531\n","step: 2770, loss: 0.08435359597206116\n","step: 2780, loss: 0.05619766190648079\n","step: 2790, loss: 0.07986316829919815\n","step: 2800, loss: 0.09436435997486115\n","step: 2810, loss: 0.04248794540762901\n","step: 2820, loss: 0.09812963753938675\n","step: 2830, loss: 0.16367603838443756\n","step: 2840, loss: 0.15784050524234772\n","step: 2850, loss: 0.04106085002422333\n","step: 2860, loss: 0.10235187411308289\n","step: 2870, loss: 0.13102810084819794\n","step: 2880, loss: 0.10751745849847794\n","step: 2890, loss: 0.043356169015169144\n","step: 2900, loss: 0.08937953412532806\n","step: 2910, loss: 0.08265107125043869\n","step: 2920, loss: 0.163751021027565\n","step: 2930, loss: 0.045895710587501526\n","step: 2940, loss: 0.17802788317203522\n","step: 2950, loss: 0.12304513901472092\n","step: 2960, loss: 0.1050543263554573\n","step: 2970, loss: 0.10269703716039658\n","step: 2980, loss: 0.10791834443807602\n","step: 2990, loss: 0.053663481026887894\n","step: 3000, loss: 0.05963243544101715\n","step: 3010, loss: 0.05419675633311272\n","step: 3020, loss: 0.058983057737350464\n","step: 3030, loss: 0.04391864687204361\n","step: 3040, loss: 0.09796032309532166\n","step: 3050, loss: 0.09380396455526352\n","step: 3060, loss: 0.18046511709690094\n","step: 3070, loss: 0.11906544119119644\n","step: 3080, loss: 0.10089142620563507\n","step: 3090, loss: 0.07404099404811859\n","step: 3100, loss: 0.059044137597084045\n","step: 3110, loss: 0.04878908023238182\n","step: 3120, loss: 0.06075444445014\n","step: 3130, loss: 0.09094692021608353\n","step: 3140, loss: 0.06977899372577667\n","step: 3150, loss: 0.10669080913066864\n","step: 3160, loss: 0.07751140743494034\n","step: 3170, loss: 0.1588882952928543\n","step: 3180, loss: 0.17073692381381989\n","step: 3190, loss: 0.048907045274972916\n","step: 3200, loss: 0.048820048570632935\n","step: 3210, loss: 0.10253214091062546\n","step: 3220, loss: 0.08818487823009491\n","step: 3230, loss: 0.05162724852561951\n","step: 3240, loss: 0.07858479022979736\n","step: 3250, loss: 0.059837426990270615\n","step: 3260, loss: 0.07528173178434372\n","step: 3270, loss: 0.05450471118092537\n","step: 3280, loss: 0.13501231372356415\n","step: 3290, loss: 0.027764100581407547\n","step: 3300, loss: 0.06784167885780334\n","step: 3310, loss: 0.06434082239866257\n","step: 3320, loss: 0.17054903507232666\n","step: 3330, loss: 0.13481710851192474\n","step: 3340, loss: 0.12925778329372406\n","step: 3350, loss: 0.062174953520298004\n","step: 3360, loss: 0.08505986630916595\n","step: 3370, loss: 0.09451008588075638\n","step: 3380, loss: 0.041912149637937546\n","step: 3390, loss: 0.09273602068424225\n","step: 3400, loss: 0.1218571811914444\n","step: 3410, loss: 0.14322268962860107\n","step: 3420, loss: 0.07456432282924652\n","step: 3430, loss: 0.038593392819166183\n","step: 3440, loss: 0.11025359481573105\n","step: 3450, loss: 0.04165549576282501\n","step: 3460, loss: 0.09775117039680481\n","step: 3470, loss: 0.1137317642569542\n","step: 3480, loss: 0.05071801692247391\n","step: 3490, loss: 0.10488099604845047\n","step: 3500, loss: 0.06459242850542068\n","step: 3510, loss: 0.1103416383266449\n","step: 3520, loss: 0.09670793265104294\n","step: 3530, loss: 0.04543669521808624\n","step: 3540, loss: 0.023155231028795242\n","step: 3550, loss: 0.06593189388513565\n","step: 3560, loss: 0.10564354062080383\n","step: 3570, loss: 0.14251968264579773\n","step: 3580, loss: 0.05698907747864723\n","step: 3590, loss: 0.03713257610797882\n","step: 3600, loss: 0.09477021545171738\n","step: 3610, loss: 0.1497889757156372\n","step: 3620, loss: 0.08097029477357864\n","step: 3630, loss: 0.11260328441858292\n","step: 3640, loss: 0.06670376658439636\n","step: 3650, loss: 0.09855317324399948\n","step: 3660, loss: 0.08101893216371536\n","step: 3670, loss: 0.14530493319034576\n","step: 3680, loss: 0.06093359366059303\n","step: 3690, loss: 0.02942628785967827\n","step: 3700, loss: 0.05266665294766426\n","step: 3710, loss: 0.05462827906012535\n","step: 3720, loss: 0.16495396196842194\n","step: 3730, loss: 0.03837909549474716\n","step: 3740, loss: 0.09990309178829193\n","step: 3750, loss: 0.08616480231285095\n","step: 3760, loss: 0.1463298797607422\n","step: 3770, loss: 0.07231631129980087\n","step: 3780, loss: 0.024879680946469307\n","step: 3790, loss: 0.04547817260026932\n","step: 3800, loss: 0.18689413368701935\n","step: 3810, loss: 0.027041852474212646\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.37      0.13      0.19        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.88      0.84      0.86       291\n","           5       0.96      0.85      0.90       294\n","           6       1.00      0.98      0.99      1570\n","           7       0.44      0.95      0.60       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.91      0.95        47\n","          13       0.71      0.92      0.80        13\n","          14       0.41      1.00      0.58        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.85      0.84      0.85      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.91      1.00      0.96        32\n","          19       0.75      0.38      0.50        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.96      0.71      0.81      4175\n","          23       0.64      0.97      0.77      2253\n","          24       0.30      0.64      0.41        44\n","          25       0.86      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.99      1.00      0.99        69\n","          28       0.99      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.92      0.88      0.90      1136\n","          31       0.63      0.63      0.63        19\n","          32       1.00      0.62      0.77         8\n","          33       0.80      0.90      0.85        86\n","          34       0.22      0.50      0.31        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.09      0.17       182\n","          37       0.87      0.97      0.91      1592\n","          38       0.98      0.97      0.98       404\n","          39       0.97      0.95      0.96       485\n","          40       0.92      0.97      0.94       573\n","          41       0.97      0.92      0.94       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.97      0.96       152\n","          44       0.96      0.92      0.94        75\n","          46       1.00      0.98      0.99        82\n","          48       1.00      0.05      0.10        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.82      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 443\n","\n","Loop 13\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9069712162017822\n","step: 10, loss: 1.8787645101547241\n","step: 20, loss: 0.6748934984207153\n","step: 30, loss: 0.3437769412994385\n","step: 40, loss: 0.3890940546989441\n","step: 50, loss: 0.27232038974761963\n","step: 60, loss: 0.4394935369491577\n","step: 70, loss: 0.23620577156543732\n","step: 80, loss: 0.2323307991027832\n","step: 90, loss: 0.10862003266811371\n","step: 100, loss: 0.09980751574039459\n","step: 110, loss: 0.21189072728157043\n","step: 120, loss: 0.1781594604253769\n","step: 130, loss: 0.2842238247394562\n","step: 140, loss: 0.10637518018484116\n","step: 150, loss: 0.16100066900253296\n","step: 160, loss: 0.1406358927488327\n","step: 170, loss: 0.13335967063903809\n","step: 180, loss: 0.12216637283563614\n","step: 190, loss: 0.09625202417373657\n","step: 200, loss: 0.13009871542453766\n","step: 210, loss: 0.08908061683177948\n","step: 220, loss: 0.12696292996406555\n","step: 230, loss: 0.22058886289596558\n","step: 240, loss: 0.06854107230901718\n","step: 250, loss: 0.11107953637838364\n","step: 260, loss: 0.11895472556352615\n","step: 270, loss: 0.19177310168743134\n","step: 280, loss: 0.18844304978847504\n","step: 290, loss: 0.2165907323360443\n","step: 300, loss: 0.0863318145275116\n","step: 310, loss: 0.1197924092411995\n","step: 320, loss: 0.09817865490913391\n","step: 330, loss: 0.18548019230365753\n","step: 340, loss: 0.15280836820602417\n","step: 350, loss: 0.15159910917282104\n","step: 360, loss: 0.13380667567253113\n","step: 370, loss: 0.19572238624095917\n","step: 380, loss: 0.1207248866558075\n","step: 390, loss: 0.152350515127182\n","step: 400, loss: 0.24056574702262878\n","step: 410, loss: 0.11099860817193985\n","step: 420, loss: 0.1193360984325409\n","step: 430, loss: 0.09724055230617523\n","step: 440, loss: 0.14785444736480713\n","step: 450, loss: 0.172117218375206\n","step: 460, loss: 0.08007554709911346\n","step: 470, loss: 0.08625967800617218\n","step: 480, loss: 0.17493148148059845\n","step: 490, loss: 0.07321109622716904\n","step: 500, loss: 0.09674357622861862\n","step: 510, loss: 0.12596090137958527\n","step: 520, loss: 0.0966162383556366\n","step: 530, loss: 0.11540445685386658\n","step: 540, loss: 0.13011611998081207\n","step: 550, loss: 0.08037443459033966\n","step: 560, loss: 0.09009509533643723\n","step: 570, loss: 0.02655297890305519\n","step: 580, loss: 0.1514272540807724\n","step: 590, loss: 0.08285106718540192\n","step: 600, loss: 0.148622065782547\n","step: 610, loss: 0.17456328868865967\n","step: 620, loss: 0.0802675187587738\n","step: 630, loss: 0.14202672243118286\n","step: 640, loss: 0.1096014752984047\n","step: 650, loss: 0.1419801115989685\n","step: 660, loss: 0.1071372851729393\n","step: 670, loss: 0.16516944766044617\n","step: 680, loss: 0.08879822492599487\n","step: 690, loss: 0.13210709393024445\n","step: 700, loss: 0.09644989669322968\n","step: 710, loss: 0.03521164506673813\n","step: 720, loss: 0.03360266238451004\n","step: 730, loss: 0.06995625793933868\n","step: 740, loss: 0.06947765499353409\n","step: 750, loss: 0.13305437564849854\n","step: 760, loss: 0.057815294712781906\n","step: 770, loss: 0.20956416428089142\n","step: 780, loss: 0.08514910191297531\n","step: 790, loss: 0.04049690440297127\n","step: 800, loss: 0.07397600263357162\n","step: 810, loss: 0.16372181475162506\n","step: 820, loss: 0.13051609694957733\n","step: 830, loss: 0.10568378120660782\n","step: 840, loss: 0.12555521726608276\n","step: 850, loss: 0.1446579396724701\n","step: 860, loss: 0.05495806410908699\n","step: 870, loss: 0.05938127264380455\n","step: 880, loss: 0.04981077462434769\n","step: 890, loss: 0.057763855904340744\n","step: 900, loss: 0.08617439866065979\n","step: 910, loss: 0.10389876365661621\n","step: 920, loss: 0.1210729256272316\n","step: 930, loss: 0.06679334491491318\n","step: 940, loss: 0.15523408353328705\n","step: 950, loss: 0.10791116952896118\n","step: 960, loss: 0.13056735694408417\n","step: 970, loss: 0.08570396900177002\n","step: 980, loss: 0.053023796528577805\n","step: 990, loss: 0.06643174588680267\n","step: 1000, loss: 0.0871652215719223\n","step: 1010, loss: 0.07369766384363174\n","step: 1020, loss: 0.07018039375543594\n","step: 1030, loss: 0.13735301792621613\n","step: 1040, loss: 0.13928896188735962\n","step: 1050, loss: 0.06984537094831467\n","step: 1060, loss: 0.24173268675804138\n","step: 1070, loss: 0.08017656952142715\n","step: 1080, loss: 0.07179872691631317\n","step: 1090, loss: 0.12455672770738602\n","step: 1100, loss: 0.09219256788492203\n","step: 1110, loss: 0.2417178601026535\n","step: 1120, loss: 0.038815222680568695\n","step: 1130, loss: 0.0670597180724144\n","step: 1140, loss: 0.047564517706632614\n","step: 1150, loss: 0.09169430285692215\n","step: 1160, loss: 0.17002983391284943\n","step: 1170, loss: 0.10512852668762207\n","step: 1180, loss: 0.12247957289218903\n","step: 1190, loss: 0.13943792879581451\n","step: 1200, loss: 0.06515888124704361\n","step: 1210, loss: 0.04457825794816017\n","step: 1220, loss: 0.10994324088096619\n","step: 1230, loss: 0.10880688577890396\n","step: 1240, loss: 0.1188078448176384\n","step: 1250, loss: 0.23032641410827637\n","step: 1260, loss: 0.06161581724882126\n","step: 1270, loss: 0.07544337958097458\n","step: 1280, loss: 0.09863776713609695\n","step: 1290, loss: 0.11661675572395325\n","step: 1300, loss: 0.09046393632888794\n","step: 1310, loss: 0.09485647827386856\n","step: 1320, loss: 0.05804897099733353\n","step: 1330, loss: 0.06462010741233826\n","step: 1340, loss: 0.027704142034053802\n","step: 1350, loss: 0.04033748432993889\n","step: 1360, loss: 0.1531197875738144\n","step: 1370, loss: 0.06103252246975899\n","step: 1380, loss: 0.07564954459667206\n","step: 1390, loss: 0.027981014922261238\n","step: 1400, loss: 0.04564116895198822\n","step: 1410, loss: 0.028217365965247154\n","step: 1420, loss: 0.0994432121515274\n","step: 1430, loss: 0.06315968930721283\n","step: 1440, loss: 0.08420488983392715\n","step: 1450, loss: 0.09415720403194427\n","step: 1460, loss: 0.15549181401729584\n","step: 1470, loss: 0.08638693392276764\n","step: 1480, loss: 0.05871481075882912\n","step: 1490, loss: 0.0849744901061058\n","step: 1500, loss: 0.12262081354856491\n","step: 1510, loss: 0.07166559994220734\n","step: 1520, loss: 0.08773015439510345\n","step: 1530, loss: 0.057342641055583954\n","step: 1540, loss: 0.07461372017860413\n","step: 1550, loss: 0.14650222659111023\n","step: 1560, loss: 0.05561334639787674\n","step: 1570, loss: 0.05090716481208801\n","step: 1580, loss: 0.16282904148101807\n","step: 1590, loss: 0.057562824338674545\n","step: 1600, loss: 0.10559085756540298\n","step: 1610, loss: 0.1312553733587265\n","step: 1620, loss: 0.17130011320114136\n","step: 1630, loss: 0.08288154006004333\n","step: 1640, loss: 0.06311750411987305\n","step: 1650, loss: 0.08554165065288544\n","step: 1660, loss: 0.06273852288722992\n","step: 1670, loss: 0.11784941703081131\n","step: 1680, loss: 0.08936546742916107\n","step: 1690, loss: 0.08070416748523712\n","step: 1700, loss: 0.09178826212882996\n","step: 1710, loss: 0.030884914100170135\n","step: 1720, loss: 0.14330926537513733\n","step: 1730, loss: 0.054905399680137634\n","step: 1740, loss: 0.016943970695137978\n","step: 1750, loss: 0.06468023359775543\n","step: 1760, loss: 0.07535529881715775\n","step: 1770, loss: 0.11467782407999039\n","step: 1780, loss: 0.126751109957695\n","step: 1790, loss: 0.05858530104160309\n","step: 1800, loss: 0.08319278061389923\n","step: 1810, loss: 0.09338246285915375\n","step: 1820, loss: 0.08013065904378891\n","step: 1830, loss: 0.1583430916070938\n","step: 1840, loss: 0.17511782050132751\n","step: 1850, loss: 0.05944755673408508\n","step: 1860, loss: 0.07752467691898346\n","step: 1870, loss: 0.08803689479827881\n","step: 1880, loss: 0.11245715618133545\n","step: 1890, loss: 0.09729941934347153\n","step: 1900, loss: 0.1767062097787857\n","step: 1910, loss: 0.1177835687994957\n","step: 1920, loss: 0.10987308621406555\n","step: 1930, loss: 0.10836641490459442\n","step: 1940, loss: 0.2006499320268631\n","step: 1950, loss: 0.036806777119636536\n","step: 1960, loss: 0.18606682121753693\n","step: 1970, loss: 0.10199090838432312\n","step: 1980, loss: 0.12081114202737808\n","step: 1990, loss: 0.1457965075969696\n","step: 2000, loss: 0.06007207930088043\n","step: 2010, loss: 0.0885060727596283\n","step: 2020, loss: 0.06450177729129791\n","step: 2030, loss: 0.06132664531469345\n","step: 2040, loss: 0.185249924659729\n","step: 2050, loss: 0.052024636417627335\n","step: 2060, loss: 0.05645996704697609\n","step: 2070, loss: 0.16870126128196716\n","step: 2080, loss: 0.13045237958431244\n","step: 2090, loss: 0.06225084513425827\n","step: 2100, loss: 0.14591439068317413\n","step: 2110, loss: 0.09780361503362656\n","step: 2120, loss: 0.10864368826150894\n","step: 2130, loss: 0.02330758050084114\n","step: 2140, loss: 0.03380994498729706\n","step: 2150, loss: 0.034682679921388626\n","step: 2160, loss: 0.06040051206946373\n","step: 2170, loss: 0.03570486232638359\n","step: 2180, loss: 0.04382132738828659\n","step: 2190, loss: 0.09151219576597214\n","step: 2200, loss: 0.12933562695980072\n","step: 2210, loss: 0.18660561740398407\n","step: 2220, loss: 0.07324304431676865\n","step: 2230, loss: 0.07622218877077103\n","step: 2240, loss: 0.08925315737724304\n","step: 2250, loss: 0.06501636654138565\n","step: 2260, loss: 0.2525103688240051\n","step: 2270, loss: 0.09922592341899872\n","step: 2280, loss: 0.04424135014414787\n","step: 2290, loss: 0.04354940727353096\n","step: 2300, loss: 0.10162605345249176\n","step: 2310, loss: 0.1269676685333252\n","step: 2320, loss: 0.1248667910695076\n","step: 2330, loss: 0.10309586673974991\n","step: 2340, loss: 0.08296281099319458\n","step: 2350, loss: 0.10944198817014694\n","step: 2360, loss: 0.08163058757781982\n","step: 2370, loss: 0.12081655114889145\n","step: 2380, loss: 0.10564596951007843\n","step: 2390, loss: 0.03275030106306076\n","step: 2400, loss: 0.06608428806066513\n","step: 2410, loss: 0.04928203299641609\n","step: 2420, loss: 0.0915665328502655\n","step: 2430, loss: 0.12216179817914963\n","step: 2440, loss: 0.09042514115571976\n","step: 2450, loss: 0.07989612966775894\n","step: 2460, loss: 0.12996818125247955\n","step: 2470, loss: 0.1127236858010292\n","step: 2480, loss: 0.04750535637140274\n","step: 2490, loss: 0.060370173305273056\n","step: 2500, loss: 0.07215854525566101\n","step: 2510, loss: 0.20079417526721954\n","step: 2520, loss: 0.09587883204221725\n","step: 2530, loss: 0.1453460305929184\n","step: 2540, loss: 0.11162761598825455\n","step: 2550, loss: 0.12565597891807556\n","step: 2560, loss: 0.07311589270830154\n","step: 2570, loss: 0.06257767230272293\n","step: 2580, loss: 0.15216191112995148\n","step: 2590, loss: 0.09997032582759857\n","step: 2600, loss: 0.044078998267650604\n","step: 2610, loss: 0.14612814784049988\n","step: 2620, loss: 0.042203038930892944\n","step: 2630, loss: 0.0691174790263176\n","step: 2640, loss: 0.12115099281072617\n","step: 2650, loss: 0.20949597656726837\n","step: 2660, loss: 0.07433062791824341\n","step: 2670, loss: 0.11092618107795715\n","step: 2680, loss: 0.2111383080482483\n","step: 2690, loss: 0.09561024606227875\n","step: 2700, loss: 0.040155697613954544\n","step: 2710, loss: 0.05906778201460838\n","step: 2720, loss: 0.014731001108884811\n","step: 2730, loss: 0.08751078695058823\n","step: 2740, loss: 0.1062643751502037\n","step: 2750, loss: 0.05930236354470253\n","step: 2760, loss: 0.12199043482542038\n","step: 2770, loss: 0.06820058822631836\n","step: 2780, loss: 0.08060479164123535\n","step: 2790, loss: 0.1424212008714676\n","step: 2800, loss: 0.0937044695019722\n","step: 2810, loss: 0.054002389311790466\n","step: 2820, loss: 0.1715894341468811\n","step: 2830, loss: 0.08438818901777267\n","step: 2840, loss: 0.027204325422644615\n","step: 2850, loss: 0.099986232817173\n","step: 2860, loss: 0.10113954544067383\n","step: 2870, loss: 0.09187847375869751\n","step: 2880, loss: 0.033799733966588974\n","step: 2890, loss: 0.10143814980983734\n","step: 2900, loss: 0.06301186978816986\n","step: 2910, loss: 0.02474115416407585\n","step: 2920, loss: 0.11774972081184387\n","step: 2930, loss: 0.09001708775758743\n","step: 2940, loss: 0.06986849755048752\n","step: 2950, loss: 0.06151336431503296\n","step: 2960, loss: 0.08312131464481354\n","step: 2970, loss: 0.06499362736940384\n","step: 2980, loss: 0.14705508947372437\n","step: 2990, loss: 0.12343524396419525\n","step: 3000, loss: 0.11563309282064438\n","step: 3010, loss: 0.03647874668240547\n","step: 3020, loss: 0.04950368031859398\n","step: 3030, loss: 0.10396790504455566\n","step: 3040, loss: 0.06471386551856995\n","step: 3050, loss: 0.07998717576265335\n","step: 3060, loss: 0.07542416453361511\n","step: 3070, loss: 0.07193223387002945\n","step: 3080, loss: 0.07490907609462738\n","step: 3090, loss: 0.062059905380010605\n","step: 3100, loss: 0.08561301976442337\n","step: 3110, loss: 0.07976680994033813\n","step: 3120, loss: 0.028236640617251396\n","step: 3130, loss: 0.09338697046041489\n","step: 3140, loss: 0.06838826090097427\n","step: 3150, loss: 0.08373801410198212\n","step: 3160, loss: 0.06825901567935944\n","step: 3170, loss: 0.201887309551239\n","step: 3180, loss: 0.03532760962843895\n","step: 3190, loss: 0.05075198784470558\n","step: 3200, loss: 0.055659156292676926\n","step: 3210, loss: 0.034067168831825256\n","step: 3220, loss: 0.060654789209365845\n","step: 3230, loss: 0.10619962960481644\n","step: 3240, loss: 0.038738686591386795\n","step: 3250, loss: 0.04241305962204933\n","step: 3260, loss: 0.09711723029613495\n","step: 3270, loss: 0.14038488268852234\n","step: 3280, loss: 0.14442838728427887\n","step: 3290, loss: 0.12080120295286179\n","step: 3300, loss: 0.10987893491983414\n","step: 3310, loss: 0.026735398918390274\n","step: 3320, loss: 0.05463239923119545\n","step: 3330, loss: 0.09118304401636124\n","step: 3340, loss: 0.10182830691337585\n","step: 3350, loss: 0.0829693004488945\n","step: 3360, loss: 0.07288675010204315\n","step: 3370, loss: 0.11676713824272156\n","step: 3380, loss: 0.09541255980730057\n","step: 3390, loss: 0.0966775044798851\n","step: 3400, loss: 0.10467306524515152\n","step: 3410, loss: 0.12434826046228409\n","step: 3420, loss: 0.12667815387248993\n","step: 3430, loss: 0.10683080554008484\n","step: 3440, loss: 0.10475672036409378\n","step: 3450, loss: 0.11740527302026749\n","step: 3460, loss: 0.08251369744539261\n","step: 3470, loss: 0.054571691900491714\n","step: 3480, loss: 0.11084393411874771\n","step: 3490, loss: 0.04464060440659523\n","step: 3500, loss: 0.0785718560218811\n","step: 3510, loss: 0.13766278326511383\n","step: 3520, loss: 0.11184599250555038\n","step: 3530, loss: 0.12821820378303528\n","step: 3540, loss: 0.1367531716823578\n","step: 3550, loss: 0.040937311947345734\n","step: 3560, loss: 0.07397866994142532\n","step: 3570, loss: 0.16579291224479675\n","step: 3580, loss: 0.0731905996799469\n","step: 3590, loss: 0.05448348447680473\n","step: 3600, loss: 0.08905293047428131\n","step: 3610, loss: 0.0787285640835762\n","step: 3620, loss: 0.12168797105550766\n","step: 3630, loss: 0.07351241260766983\n","step: 3640, loss: 0.08762948960065842\n","step: 3650, loss: 0.06422875821590424\n","step: 3660, loss: 0.14766155183315277\n","step: 3670, loss: 0.05009720101952553\n","step: 3680, loss: 0.1513647884130478\n","step: 3690, loss: 0.0992615669965744\n","step: 3700, loss: 0.08045998960733414\n","step: 3710, loss: 0.1010640412569046\n","step: 3720, loss: 0.02371840924024582\n","step: 3730, loss: 0.026131823658943176\n","step: 3740, loss: 0.12375213205814362\n","step: 3750, loss: 0.09001338481903076\n","step: 3760, loss: 0.04929021745920181\n","step: 3770, loss: 0.036381494253873825\n","step: 3780, loss: 0.0634002685546875\n","step: 3790, loss: 0.0757594183087349\n","step: 3800, loss: 0.19806510210037231\n","step: 3810, loss: 0.042913008481264114\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.92      0.97      0.94        35\n","           2       0.69      0.71      0.70        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.94      0.85      0.89       291\n","           5       0.97      0.84      0.90       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.92      0.97      0.94       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.02      0.15      0.03        13\n","          14       0.29      1.00      0.45        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.93      0.81      0.87      1151\n","          17       0.95      0.98      0.96        41\n","          18       0.94      1.00      0.97        32\n","          19       0.74      0.65      0.69        40\n","          20       0.92      1.00      0.96       584\n","          21       0.19      0.12      0.14        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.23      0.18      0.20        44\n","          25       0.82      0.93      0.87       888\n","          26       0.80      0.89      0.84         9\n","          27       0.94      0.97      0.96        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.86      0.90      1136\n","          31       0.56      0.74      0.64        19\n","          32       1.00      0.75      0.86         8\n","          33       0.67      0.95      0.79        86\n","          34       0.23      0.84      0.36        32\n","          35       0.96      0.99      0.98       474\n","          36       0.79      0.19      0.30       182\n","          37       0.88      0.95      0.92      1592\n","          38       0.94      0.99      0.96       404\n","          39       0.96      0.94      0.95       485\n","          40       0.91      0.95      0.93       573\n","          41       0.94      0.90      0.92       841\n","          42       0.99      0.98      0.99       575\n","          43       0.96      0.80      0.87       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.25      0.01      0.02        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 452\n","\n","Loop 14\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8613836765289307\n","step: 10, loss: 1.8091992139816284\n","step: 20, loss: 0.6875391006469727\n","step: 30, loss: 0.48938101530075073\n","step: 40, loss: 0.3098689615726471\n","step: 50, loss: 0.2807363271713257\n","step: 60, loss: 0.16606588661670685\n","step: 70, loss: 0.16780385375022888\n","step: 80, loss: 0.2265755534172058\n","step: 90, loss: 0.17395718395709991\n","step: 100, loss: 0.17145000398159027\n","step: 110, loss: 0.2528264820575714\n","step: 120, loss: 0.13788264989852905\n","step: 130, loss: 0.14141373336315155\n","step: 140, loss: 0.18186712265014648\n","step: 150, loss: 0.09173239022493362\n","step: 160, loss: 0.10886622220277786\n","step: 170, loss: 0.12275420129299164\n","step: 180, loss: 0.09554234147071838\n","step: 190, loss: 0.10596947371959686\n","step: 200, loss: 0.2373856157064438\n","step: 210, loss: 0.07222053408622742\n","step: 220, loss: 0.18356241285800934\n","step: 230, loss: 0.18776607513427734\n","step: 240, loss: 0.24624884128570557\n","step: 250, loss: 0.08674322813749313\n","step: 260, loss: 0.1396520584821701\n","step: 270, loss: 0.09800227731466293\n","step: 280, loss: 0.1365329623222351\n","step: 290, loss: 0.12153751403093338\n","step: 300, loss: 0.09772986173629761\n","step: 310, loss: 0.13990212976932526\n","step: 320, loss: 0.07356547564268112\n","step: 330, loss: 0.13212724030017853\n","step: 340, loss: 0.17999351024627686\n","step: 350, loss: 0.12925896048545837\n","step: 360, loss: 0.1412975937128067\n","step: 370, loss: 0.07432688772678375\n","step: 380, loss: 0.1780453324317932\n","step: 390, loss: 0.07344547659158707\n","step: 400, loss: 0.10486644506454468\n","step: 410, loss: 0.10099968314170837\n","step: 420, loss: 0.08753646165132523\n","step: 430, loss: 0.10659290105104446\n","step: 440, loss: 0.09253911674022675\n","step: 450, loss: 0.1265646368265152\n","step: 460, loss: 0.14047487080097198\n","step: 470, loss: 0.07844214886426926\n","step: 480, loss: 0.04474279657006264\n","step: 490, loss: 0.24599620699882507\n","step: 500, loss: 0.13437703251838684\n","step: 510, loss: 0.11773111671209335\n","step: 520, loss: 0.0926097109913826\n","step: 530, loss: 0.06725476682186127\n","step: 540, loss: 0.13714268803596497\n","step: 550, loss: 0.1932634860277176\n","step: 560, loss: 0.19648955762386322\n","step: 570, loss: 0.11966858804225922\n","step: 580, loss: 0.1238011047244072\n","step: 590, loss: 0.12481750547885895\n","step: 600, loss: 0.15415649116039276\n","step: 610, loss: 0.0744018703699112\n","step: 620, loss: 0.1325708031654358\n","step: 630, loss: 0.07640811055898666\n","step: 640, loss: 0.13486288487911224\n","step: 650, loss: 0.06975752860307693\n","step: 660, loss: 0.046339839696884155\n","step: 670, loss: 0.05907579883933067\n","step: 680, loss: 0.03802061080932617\n","step: 690, loss: 0.11569929122924805\n","step: 700, loss: 0.20928986370563507\n","step: 710, loss: 0.13507592678070068\n","step: 720, loss: 0.08634989708662033\n","step: 730, loss: 0.03608524426817894\n","step: 740, loss: 0.05483255907893181\n","step: 750, loss: 0.06786219775676727\n","step: 760, loss: 0.13801977038383484\n","step: 770, loss: 0.17152252793312073\n","step: 780, loss: 0.09147801995277405\n","step: 790, loss: 0.16723062098026276\n","step: 800, loss: 0.08816612511873245\n","step: 810, loss: 0.2779219150543213\n","step: 820, loss: 0.12730024755001068\n","step: 830, loss: 0.051926419138908386\n","step: 840, loss: 0.16296541690826416\n","step: 850, loss: 0.11647846549749374\n","step: 860, loss: 0.0678992047905922\n","step: 870, loss: 0.07121752202510834\n","step: 880, loss: 0.05648978799581528\n","step: 890, loss: 0.12023138254880905\n","step: 900, loss: 0.07179462164640427\n","step: 910, loss: 0.060662075877189636\n","step: 920, loss: 0.03634386137127876\n","step: 930, loss: 0.20208223164081573\n","step: 940, loss: 0.06549771130084991\n","step: 950, loss: 0.025035185739398003\n","step: 960, loss: 0.1351325660943985\n","step: 970, loss: 0.056896816939115524\n","step: 980, loss: 0.08332206308841705\n","step: 990, loss: 0.1413581371307373\n","step: 1000, loss: 0.12792916595935822\n","step: 1010, loss: 0.052050501108169556\n","step: 1020, loss: 0.0549309067428112\n","step: 1030, loss: 0.09905139356851578\n","step: 1040, loss: 0.07455189526081085\n","step: 1050, loss: 0.14283448457717896\n","step: 1060, loss: 0.0973065197467804\n","step: 1070, loss: 0.15843266248703003\n","step: 1080, loss: 0.08042621612548828\n","step: 1090, loss: 0.08691231906414032\n","step: 1100, loss: 0.07502001523971558\n","step: 1110, loss: 0.07915312796831131\n","step: 1120, loss: 0.23367351293563843\n","step: 1130, loss: 0.14997585117816925\n","step: 1140, loss: 0.06684388220310211\n","step: 1150, loss: 0.1437443494796753\n","step: 1160, loss: 0.09862948954105377\n","step: 1170, loss: 0.08916157484054565\n","step: 1180, loss: 0.13052812218666077\n","step: 1190, loss: 0.08553177863359451\n","step: 1200, loss: 0.17750045657157898\n","step: 1210, loss: 0.015024426393210888\n","step: 1220, loss: 0.12394355982542038\n","step: 1230, loss: 0.0650196298956871\n","step: 1240, loss: 0.052047450095415115\n","step: 1250, loss: 0.1605638563632965\n","step: 1260, loss: 0.06836380809545517\n","step: 1270, loss: 0.07162269949913025\n","step: 1280, loss: 0.06061553955078125\n","step: 1290, loss: 0.06470899283885956\n","step: 1300, loss: 0.26566168665885925\n","step: 1310, loss: 0.10362953692674637\n","step: 1320, loss: 0.11860890686511993\n","step: 1330, loss: 0.06123312562704086\n","step: 1340, loss: 0.14298014342784882\n","step: 1350, loss: 0.062468212097883224\n","step: 1360, loss: 0.03517637774348259\n","step: 1370, loss: 0.08605244010686874\n","step: 1380, loss: 0.16374310851097107\n","step: 1390, loss: 0.09366306662559509\n","step: 1400, loss: 0.10310333967208862\n","step: 1410, loss: 0.14845605194568634\n","step: 1420, loss: 0.07788984477519989\n","step: 1430, loss: 0.1821717768907547\n","step: 1440, loss: 0.05777031183242798\n","step: 1450, loss: 0.0906030461192131\n","step: 1460, loss: 0.0685834139585495\n","step: 1470, loss: 0.06512366980314255\n","step: 1480, loss: 0.21042203903198242\n","step: 1490, loss: 0.11206357181072235\n","step: 1500, loss: 0.13675256073474884\n","step: 1510, loss: 0.06986641883850098\n","step: 1520, loss: 0.11878059804439545\n","step: 1530, loss: 0.23319777846336365\n","step: 1540, loss: 0.1075437068939209\n","step: 1550, loss: 0.055569399148225784\n","step: 1560, loss: 0.08881819248199463\n","step: 1570, loss: 0.06603872030973434\n","step: 1580, loss: 0.02441076561808586\n","step: 1590, loss: 0.0368611216545105\n","step: 1600, loss: 0.06951869279146194\n","step: 1610, loss: 0.279879093170166\n","step: 1620, loss: 0.08204033225774765\n","step: 1630, loss: 0.09418066591024399\n","step: 1640, loss: 0.11137516051530838\n","step: 1650, loss: 0.046264827251434326\n","step: 1660, loss: 0.09699433296918869\n","step: 1670, loss: 0.028127821162343025\n","step: 1680, loss: 0.06052850931882858\n","step: 1690, loss: 0.09175921231508255\n","step: 1700, loss: 0.055556342005729675\n","step: 1710, loss: 0.09221842139959335\n","step: 1720, loss: 0.06064577400684357\n","step: 1730, loss: 0.18229499459266663\n","step: 1740, loss: 0.0635845810174942\n","step: 1750, loss: 0.05309886485338211\n","step: 1760, loss: 0.10607047379016876\n","step: 1770, loss: 0.07722502201795578\n","step: 1780, loss: 0.1367824375629425\n","step: 1790, loss: 0.04368522763252258\n","step: 1800, loss: 0.10750699043273926\n","step: 1810, loss: 0.16069479286670685\n","step: 1820, loss: 0.07234079390764236\n","step: 1830, loss: 0.07436804473400116\n","step: 1840, loss: 0.038040973246097565\n","step: 1850, loss: 0.09901835024356842\n","step: 1860, loss: 0.05000460147857666\n","step: 1870, loss: 0.08719367533922195\n","step: 1880, loss: 0.04180401563644409\n","step: 1890, loss: 0.20743925869464874\n","step: 1900, loss: 0.057887088507413864\n","step: 1910, loss: 0.050765521824359894\n","step: 1920, loss: 0.14654266834259033\n","step: 1930, loss: 0.06406890600919724\n","step: 1940, loss: 0.03791312128305435\n","step: 1950, loss: 0.042874570935964584\n","step: 1960, loss: 0.08781218528747559\n","step: 1970, loss: 0.041951946914196014\n","step: 1980, loss: 0.07061921805143356\n","step: 1990, loss: 0.16691459715366364\n","step: 2000, loss: 0.1657520979642868\n","step: 2010, loss: 0.06335161626338959\n","step: 2020, loss: 0.06446641683578491\n","step: 2030, loss: 0.053317081183195114\n","step: 2040, loss: 0.07367371022701263\n","step: 2050, loss: 0.04530876874923706\n","step: 2060, loss: 0.1286344677209854\n","step: 2070, loss: 0.0960196927189827\n","step: 2080, loss: 0.09191923588514328\n","step: 2090, loss: 0.09081856906414032\n","step: 2100, loss: 0.03997066989541054\n","step: 2110, loss: 0.09202741831541061\n","step: 2120, loss: 0.08495792001485825\n","step: 2130, loss: 0.051325295120477676\n","step: 2140, loss: 0.14309148490428925\n","step: 2150, loss: 0.15073221921920776\n","step: 2160, loss: 0.15644527971744537\n","step: 2170, loss: 0.08102510869503021\n","step: 2180, loss: 0.040995679795742035\n","step: 2190, loss: 0.031116332858800888\n","step: 2200, loss: 0.15476864576339722\n","step: 2210, loss: 0.22532135248184204\n","step: 2220, loss: 0.1253008395433426\n","step: 2230, loss: 0.06867299973964691\n","step: 2240, loss: 0.05542341247200966\n","step: 2250, loss: 0.08683609217405319\n","step: 2260, loss: 0.1485902965068817\n","step: 2270, loss: 0.11533316969871521\n","step: 2280, loss: 0.16168181598186493\n","step: 2290, loss: 0.06932264566421509\n","step: 2300, loss: 0.12253835797309875\n","step: 2310, loss: 0.1402519941329956\n","step: 2320, loss: 0.09304273128509521\n","step: 2330, loss: 0.06017515808343887\n","step: 2340, loss: 0.08036059141159058\n","step: 2350, loss: 0.06422816216945648\n","step: 2360, loss: 0.10997626930475235\n","step: 2370, loss: 0.09092935174703598\n","step: 2380, loss: 0.0831688791513443\n","step: 2390, loss: 0.053878847509622574\n","step: 2400, loss: 0.15883547067642212\n","step: 2410, loss: 0.12316407263278961\n","step: 2420, loss: 0.1337556391954422\n","step: 2430, loss: 0.08283411711454391\n","step: 2440, loss: 0.11597925424575806\n","step: 2450, loss: 0.0844973772764206\n","step: 2460, loss: 0.11075612902641296\n","step: 2470, loss: 0.13697633147239685\n","step: 2480, loss: 0.15331636369228363\n","step: 2490, loss: 0.09171874821186066\n","step: 2500, loss: 0.08345998823642731\n","step: 2510, loss: 0.07815409451723099\n","step: 2520, loss: 0.11061108857393265\n","step: 2530, loss: 0.05888531357049942\n","step: 2540, loss: 0.08322933316230774\n","step: 2550, loss: 0.11771710962057114\n","step: 2560, loss: 0.106315977871418\n","step: 2570, loss: 0.1771559864282608\n","step: 2580, loss: 0.06859801709651947\n","step: 2590, loss: 0.0734061524271965\n","step: 2600, loss: 0.07783787697553635\n","step: 2610, loss: 0.039907556027173996\n","step: 2620, loss: 0.15635743737220764\n","step: 2630, loss: 0.07579369843006134\n","step: 2640, loss: 0.04089925065636635\n","step: 2650, loss: 0.060894694179296494\n","step: 2660, loss: 0.048672813922166824\n","step: 2670, loss: 0.105646513402462\n","step: 2680, loss: 0.1345992088317871\n","step: 2690, loss: 0.06083862483501434\n","step: 2700, loss: 0.16455015540122986\n","step: 2710, loss: 0.11018684506416321\n","step: 2720, loss: 0.04863451421260834\n","step: 2730, loss: 0.1527550369501114\n","step: 2740, loss: 0.08587708324193954\n","step: 2750, loss: 0.04833662882447243\n","step: 2760, loss: 0.01788935996592045\n","step: 2770, loss: 0.05145367607474327\n","step: 2780, loss: 0.08880410343408585\n","step: 2790, loss: 0.05943332612514496\n","step: 2800, loss: 0.0946226641535759\n","step: 2810, loss: 0.058173853904008865\n","step: 2820, loss: 0.059023089706897736\n","step: 2830, loss: 0.09572884440422058\n","step: 2840, loss: 0.1284317672252655\n","step: 2850, loss: 0.04989226534962654\n","step: 2860, loss: 0.08596403896808624\n","step: 2870, loss: 0.08368703722953796\n","step: 2880, loss: 0.15917110443115234\n","step: 2890, loss: 0.08371240645647049\n","step: 2900, loss: 0.049338970333337784\n","step: 2910, loss: 0.036935579031705856\n","step: 2920, loss: 0.0718262791633606\n","step: 2930, loss: 0.2117837369441986\n","step: 2940, loss: 0.07491406053304672\n","step: 2950, loss: 0.0703672245144844\n","step: 2960, loss: 0.10462886840105057\n","step: 2970, loss: 0.08614341169595718\n","step: 2980, loss: 0.11222375184297562\n","step: 2990, loss: 0.09673391282558441\n","step: 3000, loss: 0.08997035026550293\n","step: 3010, loss: 0.13022221624851227\n","step: 3020, loss: 0.09138447046279907\n","step: 3030, loss: 0.08138420432806015\n","step: 3040, loss: 0.11085401475429535\n","step: 3050, loss: 0.12646478414535522\n","step: 3060, loss: 0.04696878418326378\n","step: 3070, loss: 0.1235257014632225\n","step: 3080, loss: 0.13345248997211456\n","step: 3090, loss: 0.03907958045601845\n","step: 3100, loss: 0.18003802001476288\n","step: 3110, loss: 0.0889933779835701\n","step: 3120, loss: 0.11654753983020782\n","step: 3130, loss: 0.1437574028968811\n","step: 3140, loss: 0.05509302765130997\n","step: 3150, loss: 0.029440725222229958\n","step: 3160, loss: 0.0936463251709938\n","step: 3170, loss: 0.09543494135141373\n","step: 3180, loss: 0.040169067680835724\n","step: 3190, loss: 0.051364753395318985\n","step: 3200, loss: 0.1315920650959015\n","step: 3210, loss: 0.1337491273880005\n","step: 3220, loss: 0.16712860763072968\n","step: 3230, loss: 0.12976683676242828\n","step: 3240, loss: 0.1605374664068222\n","step: 3250, loss: 0.049985986202955246\n","step: 3260, loss: 0.09931264817714691\n","step: 3270, loss: 0.06654451787471771\n","step: 3280, loss: 0.042851805686950684\n","step: 3290, loss: 0.03215775266289711\n","step: 3300, loss: 0.05501341074705124\n","step: 3310, loss: 0.050284940749406815\n","step: 3320, loss: 0.0842568650841713\n","step: 3330, loss: 0.039207931607961655\n","step: 3340, loss: 0.07132315635681152\n","step: 3350, loss: 0.08259527385234833\n","step: 3360, loss: 0.13051022589206696\n","step: 3370, loss: 0.08894558250904083\n","step: 3380, loss: 0.09561473876237869\n","step: 3390, loss: 0.13626597821712494\n","step: 3400, loss: 0.10856863856315613\n","step: 3410, loss: 0.08258431404829025\n","step: 3420, loss: 0.04710228368639946\n","step: 3430, loss: 0.06843708455562592\n","step: 3440, loss: 0.03075861558318138\n","step: 3450, loss: 0.10694589465856552\n","step: 3460, loss: 0.06952575594186783\n","step: 3470, loss: 0.05370931699872017\n","step: 3480, loss: 0.20942358672618866\n","step: 3490, loss: 0.058718182146549225\n","step: 3500, loss: 0.06020677089691162\n","step: 3510, loss: 0.0324079766869545\n","step: 3520, loss: 0.02659076824784279\n","step: 3530, loss: 0.0692092552781105\n","step: 3540, loss: 0.16048221290111542\n","step: 3550, loss: 0.15270431339740753\n","step: 3560, loss: 0.07183852046728134\n","step: 3570, loss: 0.016521865501999855\n","step: 3580, loss: 0.060049548745155334\n","step: 3590, loss: 0.06471788138151169\n","step: 3600, loss: 0.04817042127251625\n","step: 3610, loss: 0.14658720791339874\n","step: 3620, loss: 0.12120575457811356\n","step: 3630, loss: 0.04984017834067345\n","step: 3640, loss: 0.045693445950746536\n","step: 3650, loss: 0.12012609094381332\n","step: 3660, loss: 0.10293233394622803\n","step: 3670, loss: 0.11986368894577026\n","step: 3680, loss: 0.07806607335805893\n","step: 3690, loss: 0.07357016950845718\n","step: 3700, loss: 0.04049665480852127\n","step: 3710, loss: 0.09542131423950195\n","step: 3720, loss: 0.09002930670976639\n","step: 3730, loss: 0.08551070839166641\n","step: 3740, loss: 0.10026653856039047\n","step: 3750, loss: 0.09329788386821747\n","step: 3760, loss: 0.09069989621639252\n","step: 3770, loss: 0.1071605384349823\n","step: 3780, loss: 0.04563033580780029\n","step: 3790, loss: 0.13760684430599213\n","step: 3800, loss: 0.2139672189950943\n","step: 3810, loss: 0.04337410628795624\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      0.97      0.91        35\n","           2       0.83      0.65      0.73        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.90      0.85      0.88       291\n","           5       0.97      0.76      0.85       294\n","           6       0.99      0.97      0.98      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.85      0.98      0.91       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.90      0.69      0.78        13\n","          14       0.29      1.00      0.45        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.92      0.81      0.86      1151\n","          17       0.85      0.98      0.91        41\n","          18       0.91      1.00      0.96        32\n","          19       0.00      0.00      0.00        40\n","          20       1.00      1.00      1.00       584\n","          21       0.29      0.12      0.16        52\n","          22       0.92      0.75      0.83      4175\n","          23       0.70      0.96      0.81      2253\n","          24       0.36      0.73      0.48        44\n","          25       0.87      0.89      0.88       888\n","          26       0.88      0.78      0.82         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.86      0.87      0.86      1136\n","          31       0.59      0.53      0.56        19\n","          32       1.00      0.75      0.86         8\n","          33       0.74      0.91      0.82        86\n","          34       0.25      0.69      0.37        32\n","          35       0.96      0.99      0.97       474\n","          36       0.90      0.24      0.38       182\n","          37       0.90      0.97      0.94      1592\n","          38       0.93      0.97      0.95       404\n","          39       0.98      0.92      0.95       485\n","          40       0.91      0.95      0.93       573\n","          41       0.95      0.93      0.94       841\n","          42       0.90      0.99      0.95       575\n","          43       0.96      0.72      0.82       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.14      0.24        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 451\n","\n","Loop 15\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8841285705566406\n","step: 10, loss: 1.6446744203567505\n","step: 20, loss: 0.638746440410614\n","step: 30, loss: 0.4540846347808838\n","step: 40, loss: 0.19706076383590698\n","step: 50, loss: 0.20209261775016785\n","step: 60, loss: 0.3552052080631256\n","step: 70, loss: 0.18566270172595978\n","step: 80, loss: 0.24620017409324646\n","step: 90, loss: 0.18699319660663605\n","step: 100, loss: 0.09805489331483841\n","step: 110, loss: 0.1670546531677246\n","step: 120, loss: 0.16490438580513\n","step: 130, loss: 0.13000652194023132\n","step: 140, loss: 0.11909761279821396\n","step: 150, loss: 0.4146212935447693\n","step: 160, loss: 0.18943218886852264\n","step: 170, loss: 0.11611038446426392\n","step: 180, loss: 0.1841069757938385\n","step: 190, loss: 0.08119755983352661\n","step: 200, loss: 0.08386681973934174\n","step: 210, loss: 0.16603967547416687\n","step: 220, loss: 0.14355915784835815\n","step: 230, loss: 0.15428023040294647\n","step: 240, loss: 0.12113872170448303\n","step: 250, loss: 0.12563304603099823\n","step: 260, loss: 0.1800914853811264\n","step: 270, loss: 0.11110839247703552\n","step: 280, loss: 0.0937746912240982\n","step: 290, loss: 0.0997328907251358\n","step: 300, loss: 0.06129898503422737\n","step: 310, loss: 0.14557021856307983\n","step: 320, loss: 0.02526848018169403\n","step: 330, loss: 0.18801817297935486\n","step: 340, loss: 0.11536036431789398\n","step: 350, loss: 0.18259160220623016\n","step: 360, loss: 0.16613148152828217\n","step: 370, loss: 0.10500528663396835\n","step: 380, loss: 0.10166148096323013\n","step: 390, loss: 0.08018466085195541\n","step: 400, loss: 0.08291017264127731\n","step: 410, loss: 0.07381072640419006\n","step: 420, loss: 0.21523389220237732\n","step: 430, loss: 0.1574738770723343\n","step: 440, loss: 0.15025170147418976\n","step: 450, loss: 0.13053427636623383\n","step: 460, loss: 0.15481966733932495\n","step: 470, loss: 0.08506615459918976\n","step: 480, loss: 0.049547791481018066\n","step: 490, loss: 0.0955522209405899\n","step: 500, loss: 0.12800008058547974\n","step: 510, loss: 0.1394052654504776\n","step: 520, loss: 0.122209832072258\n","step: 530, loss: 0.15279033780097961\n","step: 540, loss: 0.14864619076251984\n","step: 550, loss: 0.19846606254577637\n","step: 560, loss: 0.1970873773097992\n","step: 570, loss: 0.1542285978794098\n","step: 580, loss: 0.07531323283910751\n","step: 590, loss: 0.11983698606491089\n","step: 600, loss: 0.04408854991197586\n","step: 610, loss: 0.100755974650383\n","step: 620, loss: 0.15209202468395233\n","step: 630, loss: 0.09108155220746994\n","step: 640, loss: 0.11685095727443695\n","step: 650, loss: 0.12497762590646744\n","step: 660, loss: 0.20995189249515533\n","step: 670, loss: 0.1156163364648819\n","step: 680, loss: 0.13886167109012604\n","step: 690, loss: 0.03284786641597748\n","step: 700, loss: 0.037116196006536484\n","step: 710, loss: 0.08358701318502426\n","step: 720, loss: 0.11861618608236313\n","step: 730, loss: 0.07602789998054504\n","step: 740, loss: 0.0768243670463562\n","step: 750, loss: 0.1648353487253189\n","step: 760, loss: 0.07727394998073578\n","step: 770, loss: 0.1895686239004135\n","step: 780, loss: 0.14354689419269562\n","step: 790, loss: 0.036919966340065\n","step: 800, loss: 0.0935855582356453\n","step: 810, loss: 0.06852412968873978\n","step: 820, loss: 0.08173103630542755\n","step: 830, loss: 0.07364143431186676\n","step: 840, loss: 0.08791737258434296\n","step: 850, loss: 0.09960200637578964\n","step: 860, loss: 0.15217122435569763\n","step: 870, loss: 0.14831411838531494\n","step: 880, loss: 0.13823309540748596\n","step: 890, loss: 0.042401935905218124\n","step: 900, loss: 0.08283114433288574\n","step: 910, loss: 0.17993247509002686\n","step: 920, loss: 0.12933124601840973\n","step: 930, loss: 0.09443924576044083\n","step: 940, loss: 0.1203305795788765\n","step: 950, loss: 0.08751796185970306\n","step: 960, loss: 0.052053600549697876\n","step: 970, loss: 0.06749790161848068\n","step: 980, loss: 0.08240687847137451\n","step: 990, loss: 0.12960992753505707\n","step: 1000, loss: 0.13774870336055756\n","step: 1010, loss: 0.10776330530643463\n","step: 1020, loss: 0.059925924986600876\n","step: 1030, loss: 0.1184808611869812\n","step: 1040, loss: 0.026931168511509895\n","step: 1050, loss: 0.0941438302397728\n","step: 1060, loss: 0.06381270289421082\n","step: 1070, loss: 0.17382735013961792\n","step: 1080, loss: 0.19825443625450134\n","step: 1090, loss: 0.05827483534812927\n","step: 1100, loss: 0.09119077026844025\n","step: 1110, loss: 0.14462564885616302\n","step: 1120, loss: 0.09787726402282715\n","step: 1130, loss: 0.11665122956037521\n","step: 1140, loss: 0.03114774264395237\n","step: 1150, loss: 0.07916639745235443\n","step: 1160, loss: 0.1673346310853958\n","step: 1170, loss: 0.09735674411058426\n","step: 1180, loss: 0.08291616290807724\n","step: 1190, loss: 0.06958147883415222\n","step: 1200, loss: 0.1413596123456955\n","step: 1210, loss: 0.03338335454463959\n","step: 1220, loss: 0.1385478526353836\n","step: 1230, loss: 0.061268582940101624\n","step: 1240, loss: 0.06313367933034897\n","step: 1250, loss: 0.08606176823377609\n","step: 1260, loss: 0.11644072830677032\n","step: 1270, loss: 0.16939562559127808\n","step: 1280, loss: 0.20250146090984344\n","step: 1290, loss: 0.045124124735593796\n","step: 1300, loss: 0.08553033322095871\n","step: 1310, loss: 0.12834036350250244\n","step: 1320, loss: 0.13808201253414154\n","step: 1330, loss: 0.07890205085277557\n","step: 1340, loss: 0.18124477565288544\n","step: 1350, loss: 0.061670370399951935\n","step: 1360, loss: 0.06795088201761246\n","step: 1370, loss: 0.02466028556227684\n","step: 1380, loss: 0.06384626775979996\n","step: 1390, loss: 0.17040817439556122\n","step: 1400, loss: 0.07928750663995743\n","step: 1410, loss: 0.020134814083576202\n","step: 1420, loss: 0.04623539745807648\n","step: 1430, loss: 0.21187014877796173\n","step: 1440, loss: 0.11488253623247147\n","step: 1450, loss: 0.07174358516931534\n","step: 1460, loss: 0.13957831263542175\n","step: 1470, loss: 0.12257740646600723\n","step: 1480, loss: 0.05662967264652252\n","step: 1490, loss: 0.1544022560119629\n","step: 1500, loss: 0.02402675524353981\n","step: 1510, loss: 0.1624334752559662\n","step: 1520, loss: 0.13625580072402954\n","step: 1530, loss: 0.1182045117020607\n","step: 1540, loss: 0.07499144226312637\n","step: 1550, loss: 0.11260640621185303\n","step: 1560, loss: 0.12851576507091522\n","step: 1570, loss: 0.10363881289958954\n","step: 1580, loss: 0.1008797213435173\n","step: 1590, loss: 0.08595185726881027\n","step: 1600, loss: 0.16374552249908447\n","step: 1610, loss: 0.1867014467716217\n","step: 1620, loss: 0.09222964942455292\n","step: 1630, loss: 0.0850440263748169\n","step: 1640, loss: 0.07943494617938995\n","step: 1650, loss: 0.13976125419139862\n","step: 1660, loss: 0.0397048182785511\n","step: 1670, loss: 0.07651698589324951\n","step: 1680, loss: 0.0811675488948822\n","step: 1690, loss: 0.06141194328665733\n","step: 1700, loss: 0.06790438294410706\n","step: 1710, loss: 0.059715840965509415\n","step: 1720, loss: 0.06809968501329422\n","step: 1730, loss: 0.16988833248615265\n","step: 1740, loss: 0.14957544207572937\n","step: 1750, loss: 0.03857899457216263\n","step: 1760, loss: 0.07189628481864929\n","step: 1770, loss: 0.16494256258010864\n","step: 1780, loss: 0.13082006573677063\n","step: 1790, loss: 0.08456301689147949\n","step: 1800, loss: 0.08178295940160751\n","step: 1810, loss: 0.049468744546175\n","step: 1820, loss: 0.06517483294010162\n","step: 1830, loss: 0.05922224745154381\n","step: 1840, loss: 0.05557021498680115\n","step: 1850, loss: 0.11879204958677292\n","step: 1860, loss: 0.08970922231674194\n","step: 1870, loss: 0.2112581580877304\n","step: 1880, loss: 0.14850254356861115\n","step: 1890, loss: 0.14494024217128754\n","step: 1900, loss: 0.03747901692986488\n","step: 1910, loss: 0.11944613605737686\n","step: 1920, loss: 0.06227290630340576\n","step: 1930, loss: 0.10095314681529999\n","step: 1940, loss: 0.03052894026041031\n","step: 1950, loss: 0.14455817639827728\n","step: 1960, loss: 0.11728139221668243\n","step: 1970, loss: 0.15219096839427948\n","step: 1980, loss: 0.07212626188993454\n","step: 1990, loss: 0.115835040807724\n","step: 2000, loss: 0.09357491880655289\n","step: 2010, loss: 0.10276547819375992\n","step: 2020, loss: 0.03292832151055336\n","step: 2030, loss: 0.10353906452655792\n","step: 2040, loss: 0.13095569610595703\n","step: 2050, loss: 0.0711219534277916\n","step: 2060, loss: 0.08196894079446793\n","step: 2070, loss: 0.06440330296754837\n","step: 2080, loss: 0.07837178558111191\n","step: 2090, loss: 0.07166576385498047\n","step: 2100, loss: 0.05488677695393562\n","step: 2110, loss: 0.04124043881893158\n","step: 2120, loss: 0.05638045445084572\n","step: 2130, loss: 0.11450284719467163\n","step: 2140, loss: 0.09537241607904434\n","step: 2150, loss: 0.1458912342786789\n","step: 2160, loss: 0.14660224318504333\n","step: 2170, loss: 0.05046612024307251\n","step: 2180, loss: 0.05847615748643875\n","step: 2190, loss: 0.03943401202559471\n","step: 2200, loss: 0.08621503412723541\n","step: 2210, loss: 0.09979856759309769\n","step: 2220, loss: 0.06343802809715271\n","step: 2230, loss: 0.04339823126792908\n","step: 2240, loss: 0.10910969227552414\n","step: 2250, loss: 0.16498206555843353\n","step: 2260, loss: 0.07753907889127731\n","step: 2270, loss: 0.14834889769554138\n","step: 2280, loss: 0.059828225523233414\n","step: 2290, loss: 0.14288021624088287\n","step: 2300, loss: 0.16460493206977844\n","step: 2310, loss: 0.08598248660564423\n","step: 2320, loss: 0.060936339199543\n","step: 2330, loss: 0.1001238226890564\n","step: 2340, loss: 0.13160547614097595\n","step: 2350, loss: 0.0861649364233017\n","step: 2360, loss: 0.10348256677389145\n","step: 2370, loss: 0.10753419250249863\n","step: 2380, loss: 0.047451186925172806\n","step: 2390, loss: 0.10925602167844772\n","step: 2400, loss: 0.1598822921514511\n","step: 2410, loss: 0.15013985335826874\n","step: 2420, loss: 0.1367005556821823\n","step: 2430, loss: 0.054763663560152054\n","step: 2440, loss: 0.13980716466903687\n","step: 2450, loss: 0.09766513854265213\n","step: 2460, loss: 0.07618054002523422\n","step: 2470, loss: 0.07242109626531601\n","step: 2480, loss: 0.14542323350906372\n","step: 2490, loss: 0.11958613991737366\n","step: 2500, loss: 0.060452576726675034\n","step: 2510, loss: 0.0701042115688324\n","step: 2520, loss: 0.12609422206878662\n","step: 2530, loss: 0.13926179707050323\n","step: 2540, loss: 0.13838805258274078\n","step: 2550, loss: 0.06768542528152466\n","step: 2560, loss: 0.18702580034732819\n","step: 2570, loss: 0.08079536259174347\n","step: 2580, loss: 0.028813501819968224\n","step: 2590, loss: 0.12099224328994751\n","step: 2600, loss: 0.14785294234752655\n","step: 2610, loss: 0.1793845295906067\n","step: 2620, loss: 0.09410511702299118\n","step: 2630, loss: 0.09147249162197113\n","step: 2640, loss: 0.08734197169542313\n","step: 2650, loss: 0.13950440287590027\n","step: 2660, loss: 0.08545802533626556\n","step: 2670, loss: 0.11068953573703766\n","step: 2680, loss: 0.032218024134635925\n","step: 2690, loss: 0.15609529614448547\n","step: 2700, loss: 0.11390352994203568\n","step: 2710, loss: 0.10130054503679276\n","step: 2720, loss: 0.18767541646957397\n","step: 2730, loss: 0.037110377103090286\n","step: 2740, loss: 0.024560073390603065\n","step: 2750, loss: 0.03745197877287865\n","step: 2760, loss: 0.054024070501327515\n","step: 2770, loss: 0.15065832436084747\n","step: 2780, loss: 0.10429795831441879\n","step: 2790, loss: 0.2471168339252472\n","step: 2800, loss: 0.09647449105978012\n","step: 2810, loss: 0.08031322807073593\n","step: 2820, loss: 0.0762544572353363\n","step: 2830, loss: 0.10106787830591202\n","step: 2840, loss: 0.14378906786441803\n","step: 2850, loss: 0.07309772819280624\n","step: 2860, loss: 0.14199179410934448\n","step: 2870, loss: 0.06896320730447769\n","step: 2880, loss: 0.07710079848766327\n","step: 2890, loss: 0.07781436294317245\n","step: 2900, loss: 0.10521988570690155\n","step: 2910, loss: 0.05456669256091118\n","step: 2920, loss: 0.049930620938539505\n","step: 2930, loss: 0.06705379486083984\n","step: 2940, loss: 0.08958443254232407\n","step: 2950, loss: 0.057799093425273895\n","step: 2960, loss: 0.06890466809272766\n","step: 2970, loss: 0.05440899729728699\n","step: 2980, loss: 0.09961950778961182\n","step: 2990, loss: 0.10251850634813309\n","step: 3000, loss: 0.061881545931100845\n","step: 3010, loss: 0.0682593286037445\n","step: 3020, loss: 0.0874054804444313\n","step: 3030, loss: 0.22384297847747803\n","step: 3040, loss: 0.1029694676399231\n","step: 3050, loss: 0.0475531630218029\n","step: 3060, loss: 0.04917576536536217\n","step: 3070, loss: 0.05791044235229492\n","step: 3080, loss: 0.022049982100725174\n","step: 3090, loss: 0.1481628268957138\n","step: 3100, loss: 0.03101041354238987\n","step: 3110, loss: 0.06596899777650833\n","step: 3120, loss: 0.05454753339290619\n","step: 3130, loss: 0.08346493542194366\n","step: 3140, loss: 0.0814683809876442\n","step: 3150, loss: 0.1025463417172432\n","step: 3160, loss: 0.08863761276006699\n","step: 3170, loss: 0.045631177723407745\n","step: 3180, loss: 0.09246636182069778\n","step: 3190, loss: 0.08768639713525772\n","step: 3200, loss: 0.08936132490634918\n","step: 3210, loss: 0.1020912379026413\n","step: 3220, loss: 0.018182961270213127\n","step: 3230, loss: 0.04804899916052818\n","step: 3240, loss: 0.05621642991900444\n","step: 3250, loss: 0.051825981587171555\n","step: 3260, loss: 0.14997048676013947\n","step: 3270, loss: 0.06328348070383072\n","step: 3280, loss: 0.12331251800060272\n","step: 3290, loss: 0.08882591128349304\n","step: 3300, loss: 0.07030200213193893\n","step: 3310, loss: 0.167299285531044\n","step: 3320, loss: 0.09306158870458603\n","step: 3330, loss: 0.14395664632320404\n","step: 3340, loss: 0.07234258204698563\n","step: 3350, loss: 0.23097287118434906\n","step: 3360, loss: 0.04428865388035774\n","step: 3370, loss: 0.04379582405090332\n","step: 3380, loss: 0.06838953495025635\n","step: 3390, loss: 0.09150027483701706\n","step: 3400, loss: 0.10565667599439621\n","step: 3410, loss: 0.062152352184057236\n","step: 3420, loss: 0.04732486605644226\n","step: 3430, loss: 0.05601152032613754\n","step: 3440, loss: 0.11592811346054077\n","step: 3450, loss: 0.07907576113939285\n","step: 3460, loss: 0.13465189933776855\n","step: 3470, loss: 0.06839154660701752\n","step: 3480, loss: 0.20375874638557434\n","step: 3490, loss: 0.0913257822394371\n","step: 3500, loss: 0.10648049414157867\n","step: 3510, loss: 0.026408910751342773\n","step: 3520, loss: 0.08506976813077927\n","step: 3530, loss: 0.1544860303401947\n","step: 3540, loss: 0.051463499665260315\n","step: 3550, loss: 0.07130894809961319\n","step: 3560, loss: 0.16410017013549805\n","step: 3570, loss: 0.11053358018398285\n","step: 3580, loss: 0.08400368690490723\n","step: 3590, loss: 0.11785809695720673\n","step: 3600, loss: 0.15547317266464233\n","step: 3610, loss: 0.20591816306114197\n","step: 3620, loss: 0.05558519810438156\n","step: 3630, loss: 0.06733815371990204\n","step: 3640, loss: 0.07880273461341858\n","step: 3650, loss: 0.14180715382099152\n","step: 3660, loss: 0.11776573210954666\n","step: 3670, loss: 0.059794750064611435\n","step: 3680, loss: 0.044704001396894455\n","step: 3690, loss: 0.14770986139774323\n","step: 3700, loss: 0.03899354487657547\n","step: 3710, loss: 0.06042652204632759\n","step: 3720, loss: 0.12409467995166779\n","step: 3730, loss: 0.09741263091564178\n","step: 3740, loss: 0.11452709883451462\n","step: 3750, loss: 0.02997995726764202\n","step: 3760, loss: 0.11014842987060547\n","step: 3770, loss: 0.037032514810562134\n","step: 3780, loss: 0.06660495698451996\n","step: 3790, loss: 0.04679698869585991\n","step: 3800, loss: 0.03220634534955025\n","step: 3810, loss: 0.08485659211874008\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.67      1.00      0.80        35\n","           2       0.79      0.53      0.64        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.95      0.84      0.89       291\n","           5       0.97      0.84      0.90       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.96      0.97      0.96       901\n","          11       0.98      0.99      0.98      2111\n","          12       0.98      0.96      0.97        47\n","          13       0.11      0.23      0.15        13\n","          14       0.26      1.00      0.41        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.95      0.98      0.96        41\n","          18       0.70      0.81      0.75        32\n","          19       0.62      0.25      0.36        40\n","          20       1.00      1.00      1.00       584\n","          21       0.20      0.21      0.21        52\n","          22       0.95      0.72      0.82      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.37      0.25      0.30        44\n","          25       0.86      0.93      0.89       888\n","          26       0.80      0.89      0.84         9\n","          27       0.99      1.00      0.99        69\n","          28       0.97      1.00      0.98      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.84      0.88      1136\n","          31       0.58      0.79      0.67        19\n","          32       0.46      0.75      0.57         8\n","          33       0.66      0.98      0.79        86\n","          34       0.20      0.53      0.29        32\n","          35       0.97      0.99      0.98       474\n","          36       0.92      0.18      0.30       182\n","          37       0.90      0.96      0.93      1592\n","          38       0.94      0.98      0.96       404\n","          39       0.97      0.95      0.96       485\n","          40       0.90      0.96      0.93       573\n","          41       0.93      0.92      0.92       841\n","          42       0.99      0.94      0.96       575\n","          43       0.91      0.80      0.85       152\n","          44       0.87      0.92      0.90        75\n","          46       0.99      0.99      0.99        82\n","          48       0.50      0.04      0.07        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.79      0.76     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 463\n","\n","Loop 16\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9269959926605225\n","step: 10, loss: 1.9160631895065308\n","step: 20, loss: 0.6904247999191284\n","step: 30, loss: 0.4166235625743866\n","step: 40, loss: 0.3457145094871521\n","step: 50, loss: 0.21977710723876953\n","step: 60, loss: 0.2856850326061249\n","step: 70, loss: 0.40003734827041626\n","step: 80, loss: 0.31122416257858276\n","step: 90, loss: 0.11160939186811447\n","step: 100, loss: 0.13138988614082336\n","step: 110, loss: 0.11083505302667618\n","step: 120, loss: 0.11122409999370575\n","step: 130, loss: 0.10638990998268127\n","step: 140, loss: 0.1433199644088745\n","step: 150, loss: 0.1286270022392273\n","step: 160, loss: 0.16854669153690338\n","step: 170, loss: 0.12092898786067963\n","step: 180, loss: 0.1022808775305748\n","step: 190, loss: 0.193094402551651\n","step: 200, loss: 0.09089161455631256\n","step: 210, loss: 0.11051743477582932\n","step: 220, loss: 0.1206386536359787\n","step: 230, loss: 0.1658034771680832\n","step: 240, loss: 0.11235080659389496\n","step: 250, loss: 0.12678919732570648\n","step: 260, loss: 0.04766169190406799\n","step: 270, loss: 0.06369686871767044\n","step: 280, loss: 0.1989324688911438\n","step: 290, loss: 0.10259325802326202\n","step: 300, loss: 0.1437646895647049\n","step: 310, loss: 0.06562463939189911\n","step: 320, loss: 0.08437225222587585\n","step: 330, loss: 0.2560790479183197\n","step: 340, loss: 0.12257194519042969\n","step: 350, loss: 0.2090129256248474\n","step: 360, loss: 0.14185136556625366\n","step: 370, loss: 0.14147908985614777\n","step: 380, loss: 0.12067639827728271\n","step: 390, loss: 0.15038783848285675\n","step: 400, loss: 0.1360040009021759\n","step: 410, loss: 0.19320248067378998\n","step: 420, loss: 0.16690631210803986\n","step: 430, loss: 0.14684848487377167\n","step: 440, loss: 0.10275933146476746\n","step: 450, loss: 0.151405930519104\n","step: 460, loss: 0.08329948782920837\n","step: 470, loss: 0.12060774862766266\n","step: 480, loss: 0.09068600833415985\n","step: 490, loss: 0.1173224076628685\n","step: 500, loss: 0.15863379836082458\n","step: 510, loss: 0.09854166209697723\n","step: 520, loss: 0.17395973205566406\n","step: 530, loss: 0.0836198702454567\n","step: 540, loss: 0.08434922248125076\n","step: 550, loss: 0.05167701095342636\n","step: 560, loss: 0.04728061705827713\n","step: 570, loss: 0.16707982122898102\n","step: 580, loss: 0.16252729296684265\n","step: 590, loss: 0.10808056592941284\n","step: 600, loss: 0.09806950390338898\n","step: 610, loss: 0.04279731959104538\n","step: 620, loss: 0.0456717349588871\n","step: 630, loss: 0.07462341338396072\n","step: 640, loss: 0.11182587593793869\n","step: 650, loss: 0.055667463690042496\n","step: 660, loss: 0.10417826473712921\n","step: 670, loss: 0.18831056356430054\n","step: 680, loss: 0.0512002557516098\n","step: 690, loss: 0.06588073819875717\n","step: 700, loss: 0.13541796803474426\n","step: 710, loss: 0.02587714046239853\n","step: 720, loss: 0.04698465019464493\n","step: 730, loss: 0.07695650309324265\n","step: 740, loss: 0.09155108034610748\n","step: 750, loss: 0.08323640376329422\n","step: 760, loss: 0.18619190156459808\n","step: 770, loss: 0.021032515913248062\n","step: 780, loss: 0.06685976684093475\n","step: 790, loss: 0.0903620719909668\n","step: 800, loss: 0.1428787112236023\n","step: 810, loss: 0.06120414286851883\n","step: 820, loss: 0.05050400644540787\n","step: 830, loss: 0.05259592458605766\n","step: 840, loss: 0.06399794667959213\n","step: 850, loss: 0.06495963037014008\n","step: 860, loss: 0.1096566691994667\n","step: 870, loss: 0.2440384179353714\n","step: 880, loss: 0.10918772965669632\n","step: 890, loss: 0.0737261176109314\n","step: 900, loss: 0.08851034194231033\n","step: 910, loss: 0.14734244346618652\n","step: 920, loss: 0.11869195103645325\n","step: 930, loss: 0.05495478957891464\n","step: 940, loss: 0.07528690248727798\n","step: 950, loss: 0.05260969325900078\n","step: 960, loss: 0.053401801735162735\n","step: 970, loss: 0.07273170351982117\n","step: 980, loss: 0.11112002283334732\n","step: 990, loss: 0.12007128447294235\n","step: 1000, loss: 0.1604195088148117\n","step: 1010, loss: 0.06752908229827881\n","step: 1020, loss: 0.12579497694969177\n","step: 1030, loss: 0.0864577665925026\n","step: 1040, loss: 0.08936052024364471\n","step: 1050, loss: 0.06980093568563461\n","step: 1060, loss: 0.12236081808805466\n","step: 1070, loss: 0.04698009788990021\n","step: 1080, loss: 0.1875799000263214\n","step: 1090, loss: 0.06284763664007187\n","step: 1100, loss: 0.08485708385705948\n","step: 1110, loss: 0.06363619118928909\n","step: 1120, loss: 0.07361537963151932\n","step: 1130, loss: 0.0507093109190464\n","step: 1140, loss: 0.1700151264667511\n","step: 1150, loss: 0.060039322823286057\n","step: 1160, loss: 0.012219068594276905\n","step: 1170, loss: 0.04736878722906113\n","step: 1180, loss: 0.13802027702331543\n","step: 1190, loss: 0.06890720129013062\n","step: 1200, loss: 0.09726221859455109\n","step: 1210, loss: 0.13121511042118073\n","step: 1220, loss: 0.11770304292440414\n","step: 1230, loss: 0.2357603758573532\n","step: 1240, loss: 0.043083325028419495\n","step: 1250, loss: 0.08556818217039108\n","step: 1260, loss: 0.07058633863925934\n","step: 1270, loss: 0.15976600348949432\n","step: 1280, loss: 0.08173563331365585\n","step: 1290, loss: 0.15726670622825623\n","step: 1300, loss: 0.0638195052742958\n","step: 1310, loss: 0.07009260356426239\n","step: 1320, loss: 0.09282491356134415\n","step: 1330, loss: 0.05038827657699585\n","step: 1340, loss: 0.054313406348228455\n","step: 1350, loss: 0.10614845901727676\n","step: 1360, loss: 0.11993403732776642\n","step: 1370, loss: 0.2009982466697693\n","step: 1380, loss: 0.1488071233034134\n","step: 1390, loss: 0.07160815596580505\n","step: 1400, loss: 0.12482470273971558\n","step: 1410, loss: 0.08815415948629379\n","step: 1420, loss: 0.07473330944776535\n","step: 1430, loss: 0.03969813138246536\n","step: 1440, loss: 0.062029656022787094\n","step: 1450, loss: 0.15723979473114014\n","step: 1460, loss: 0.0909891352057457\n","step: 1470, loss: 0.057954054325819016\n","step: 1480, loss: 0.1382002830505371\n","step: 1490, loss: 0.07804372161626816\n","step: 1500, loss: 0.11130546778440475\n","step: 1510, loss: 0.09653448313474655\n","step: 1520, loss: 0.04856886342167854\n","step: 1530, loss: 0.07708357274532318\n","step: 1540, loss: 0.13902336359024048\n","step: 1550, loss: 0.08149013668298721\n","step: 1560, loss: 0.027976347133517265\n","step: 1570, loss: 0.05248251184821129\n","step: 1580, loss: 0.10324797034263611\n","step: 1590, loss: 0.15969274938106537\n","step: 1600, loss: 0.10361769050359726\n","step: 1610, loss: 0.09048079699277878\n","step: 1620, loss: 0.055529408156871796\n","step: 1630, loss: 0.18963798880577087\n","step: 1640, loss: 0.08175838738679886\n","step: 1650, loss: 0.12127769738435745\n","step: 1660, loss: 0.08681277930736542\n","step: 1670, loss: 0.13392764329910278\n","step: 1680, loss: 0.0937528982758522\n","step: 1690, loss: 0.036416638642549515\n","step: 1700, loss: 0.05943659693002701\n","step: 1710, loss: 0.06762857735157013\n","step: 1720, loss: 0.10477623343467712\n","step: 1730, loss: 0.1381123661994934\n","step: 1740, loss: 0.06715681403875351\n","step: 1750, loss: 0.04808373749256134\n","step: 1760, loss: 0.059058211743831635\n","step: 1770, loss: 0.06577841192483902\n","step: 1780, loss: 0.034928664565086365\n","step: 1790, loss: 0.10027289390563965\n","step: 1800, loss: 0.0663503035902977\n","step: 1810, loss: 0.10464966297149658\n","step: 1820, loss: 0.13430555164813995\n","step: 1830, loss: 0.13541269302368164\n","step: 1840, loss: 0.04466250166296959\n","step: 1850, loss: 0.11563140153884888\n","step: 1860, loss: 0.0851779505610466\n","step: 1870, loss: 0.10275973379611969\n","step: 1880, loss: 0.12231332808732986\n","step: 1890, loss: 0.04653153195977211\n","step: 1900, loss: 0.11278270930051804\n","step: 1910, loss: 0.05865596607327461\n","step: 1920, loss: 0.05101410299539566\n","step: 1930, loss: 0.07752290368080139\n","step: 1940, loss: 0.039602722972631454\n","step: 1950, loss: 0.04674689471721649\n","step: 1960, loss: 0.07379452139139175\n","step: 1970, loss: 0.04403004050254822\n","step: 1980, loss: 0.1312495470046997\n","step: 1990, loss: 0.1527010202407837\n","step: 2000, loss: 0.08327880501747131\n","step: 2010, loss: 0.07559332251548767\n","step: 2020, loss: 0.07587212324142456\n","step: 2030, loss: 0.06237122789025307\n","step: 2040, loss: 0.1143224686384201\n","step: 2050, loss: 0.04405229166150093\n","step: 2060, loss: 0.12774771451950073\n","step: 2070, loss: 0.09077651798725128\n","step: 2080, loss: 0.17316533625125885\n","step: 2090, loss: 0.03387528285384178\n","step: 2100, loss: 0.07286960631608963\n","step: 2110, loss: 0.15570276975631714\n","step: 2120, loss: 0.04250664263963699\n","step: 2130, loss: 0.03953446075320244\n","step: 2140, loss: 0.1480979472398758\n","step: 2150, loss: 0.07203124463558197\n","step: 2160, loss: 0.08233387768268585\n","step: 2170, loss: 0.08159704506397247\n","step: 2180, loss: 0.11264564841985703\n","step: 2190, loss: 0.04542656987905502\n","step: 2200, loss: 0.16717904806137085\n","step: 2210, loss: 0.0739240050315857\n","step: 2220, loss: 0.1032167598605156\n","step: 2230, loss: 0.07898786664009094\n","step: 2240, loss: 0.09622489660978317\n","step: 2250, loss: 0.045433416962623596\n","step: 2260, loss: 0.05397354066371918\n","step: 2270, loss: 0.1168924868106842\n","step: 2280, loss: 0.12864355742931366\n","step: 2290, loss: 0.11413151770830154\n","step: 2300, loss: 0.12785664200782776\n","step: 2310, loss: 0.06131277233362198\n","step: 2320, loss: 0.07778137177228928\n","step: 2330, loss: 0.1713619977235794\n","step: 2340, loss: 0.08647686243057251\n","step: 2350, loss: 0.032923296093940735\n","step: 2360, loss: 0.038859348744153976\n","step: 2370, loss: 0.08698084950447083\n","step: 2380, loss: 0.1869966834783554\n","step: 2390, loss: 0.06610213220119476\n","step: 2400, loss: 0.011238287203013897\n","step: 2410, loss: 0.09802399575710297\n","step: 2420, loss: 0.03274528309702873\n","step: 2430, loss: 0.14622557163238525\n","step: 2440, loss: 0.10520759969949722\n","step: 2450, loss: 0.07689400762319565\n","step: 2460, loss: 0.04853394255042076\n","step: 2470, loss: 0.09911161661148071\n","step: 2480, loss: 0.1247071772813797\n","step: 2490, loss: 0.02672220766544342\n","step: 2500, loss: 0.14656372368335724\n","step: 2510, loss: 0.06951861828565598\n","step: 2520, loss: 0.07323213666677475\n","step: 2530, loss: 0.0544852577149868\n","step: 2540, loss: 0.04096054285764694\n","step: 2550, loss: 0.10823915153741837\n","step: 2560, loss: 0.15081468224525452\n","step: 2570, loss: 0.07302092760801315\n","step: 2580, loss: 0.19515666365623474\n","step: 2590, loss: 0.05211380869150162\n","step: 2600, loss: 0.17670036852359772\n","step: 2610, loss: 0.07519721239805222\n","step: 2620, loss: 0.08007555454969406\n","step: 2630, loss: 0.03829202055931091\n","step: 2640, loss: 0.18902117013931274\n","step: 2650, loss: 0.06469946354627609\n","step: 2660, loss: 0.07886247336864471\n","step: 2670, loss: 0.20871137082576752\n","step: 2680, loss: 0.16385288536548615\n","step: 2690, loss: 0.11029603332281113\n","step: 2700, loss: 0.1421433538198471\n","step: 2710, loss: 0.06679137051105499\n","step: 2720, loss: 0.03758484125137329\n","step: 2730, loss: 0.06895259767770767\n","step: 2740, loss: 0.16577987372875214\n","step: 2750, loss: 0.04322582110762596\n","step: 2760, loss: 0.29734373092651367\n","step: 2770, loss: 0.08175279200077057\n","step: 2780, loss: 0.10705184191465378\n","step: 2790, loss: 0.10462311655282974\n","step: 2800, loss: 0.03979285806417465\n","step: 2810, loss: 0.07690045982599258\n","step: 2820, loss: 0.21975813806056976\n","step: 2830, loss: 0.05122855305671692\n","step: 2840, loss: 0.08426686376333237\n","step: 2850, loss: 0.12623725831508636\n","step: 2860, loss: 0.10946456342935562\n","step: 2870, loss: 0.11954114586114883\n","step: 2880, loss: 0.16999757289886475\n","step: 2890, loss: 0.07255730032920837\n","step: 2900, loss: 0.08593357354402542\n","step: 2910, loss: 0.08768831193447113\n","step: 2920, loss: 0.1007666140794754\n","step: 2930, loss: 0.10966494679450989\n","step: 2940, loss: 0.15027277171611786\n","step: 2950, loss: 0.14233477413654327\n","step: 2960, loss: 0.1375313103199005\n","step: 2970, loss: 0.050208110362291336\n","step: 2980, loss: 0.10850296914577484\n","step: 2990, loss: 0.08494984358549118\n","step: 3000, loss: 0.10261234641075134\n","step: 3010, loss: 0.0687301829457283\n","step: 3020, loss: 0.08263283967971802\n","step: 3030, loss: 0.1262454241514206\n","step: 3040, loss: 0.08364591747522354\n","step: 3050, loss: 0.031128695234656334\n","step: 3060, loss: 0.015132250264286995\n","step: 3070, loss: 0.14504370093345642\n","step: 3080, loss: 0.15683966875076294\n","step: 3090, loss: 0.10449355840682983\n","step: 3100, loss: 0.0503668449819088\n","step: 3110, loss: 0.11507454514503479\n","step: 3120, loss: 0.13684098422527313\n","step: 3130, loss: 0.06439606845378876\n","step: 3140, loss: 0.05995717644691467\n","step: 3150, loss: 0.06262581795454025\n","step: 3160, loss: 0.063479945063591\n","step: 3170, loss: 0.09119364619255066\n","step: 3180, loss: 0.08516401797533035\n","step: 3190, loss: 0.06354043632745743\n","step: 3200, loss: 0.09264902770519257\n","step: 3210, loss: 0.02651335299015045\n","step: 3220, loss: 0.03985031321644783\n","step: 3230, loss: 0.03272794187068939\n","step: 3240, loss: 0.07621641457080841\n","step: 3250, loss: 0.06071123108267784\n","step: 3260, loss: 0.0413518100976944\n","step: 3270, loss: 0.12077069282531738\n","step: 3280, loss: 0.12934233248233795\n","step: 3290, loss: 0.08969087898731232\n","step: 3300, loss: 0.034490928053855896\n","step: 3310, loss: 0.1382647454738617\n","step: 3320, loss: 0.07038969546556473\n","step: 3330, loss: 0.05102045089006424\n","step: 3340, loss: 0.06955917924642563\n","step: 3350, loss: 0.1439642757177353\n","step: 3360, loss: 0.12251479178667068\n","step: 3370, loss: 0.07536393404006958\n","step: 3380, loss: 0.07559318095445633\n","step: 3390, loss: 0.06220771744847298\n","step: 3400, loss: 0.0987304225564003\n","step: 3410, loss: 0.043504390865564346\n","step: 3420, loss: 0.0562671534717083\n","step: 3430, loss: 0.03399909287691116\n","step: 3440, loss: 0.029798036441206932\n","step: 3450, loss: 0.091007761657238\n","step: 3460, loss: 0.14474843442440033\n","step: 3470, loss: 0.10746514052152634\n","step: 3480, loss: 0.0949825644493103\n","step: 3490, loss: 0.1132124736905098\n","step: 3500, loss: 0.07865849882364273\n","step: 3510, loss: 0.03129422292113304\n","step: 3520, loss: 0.11015744507312775\n","step: 3530, loss: 0.062149081379175186\n","step: 3540, loss: 0.0696035698056221\n","step: 3550, loss: 0.039190635085105896\n","step: 3560, loss: 0.11130484193563461\n","step: 3570, loss: 0.0701122060418129\n","step: 3580, loss: 0.10077430307865143\n","step: 3590, loss: 0.03402384743094444\n","step: 3600, loss: 0.07100123167037964\n","step: 3610, loss: 0.03411268815398216\n","step: 3620, loss: 0.14800173044204712\n","step: 3630, loss: 0.05570749193429947\n","step: 3640, loss: 0.14163640141487122\n","step: 3650, loss: 0.06298831850290298\n","step: 3660, loss: 0.04784097522497177\n","step: 3670, loss: 0.06915124505758286\n","step: 3680, loss: 0.051214929670095444\n","step: 3690, loss: 0.11262998729944229\n","step: 3700, loss: 0.08370814472436905\n","step: 3710, loss: 0.06285323202610016\n","step: 3720, loss: 0.09681107848882675\n","step: 3730, loss: 0.12217546254396439\n","step: 3740, loss: 0.02327541448175907\n","step: 3750, loss: 0.14322181046009064\n","step: 3760, loss: 0.195753276348114\n","step: 3770, loss: 0.03706497326493263\n","step: 3780, loss: 0.07374823093414307\n","step: 3790, loss: 0.064475879073143\n","step: 3800, loss: 0.12978127598762512\n","step: 3810, loss: 0.06274104863405228\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      1.00      0.92        35\n","           2       0.56      0.12      0.19        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.94      0.84      0.88       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.51      0.94      0.66       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.97      0.97      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.33      0.08      0.12        13\n","          14       0.39      1.00      0.56        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.91      0.84      0.87      1151\n","          17       0.97      0.88      0.92        41\n","          18       0.94      1.00      0.97        32\n","          19       0.72      0.72      0.73        40\n","          20       0.99      1.00      1.00       584\n","          21       0.21      0.12      0.15        52\n","          22       0.92      0.75      0.82      4175\n","          23       0.66      0.95      0.78      2253\n","          24       0.43      0.14      0.21        44\n","          25       0.87      0.89      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.99      0.97      0.98        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.84      0.89      1136\n","          31       0.50      0.89      0.64        19\n","          32       1.00      0.62      0.77         8\n","          33       0.62      0.95      0.75        86\n","          34       0.28      0.72      0.41        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.13      0.22       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.96      0.98      0.97       404\n","          39       0.98      0.92      0.95       485\n","          40       0.90      0.97      0.93       573\n","          41       0.94      0.94      0.94       841\n","          42       0.98      0.99      0.98       575\n","          43       0.95      0.74      0.83       152\n","          44       0.87      0.92      0.90        75\n","          46       0.99      0.98      0.98        82\n","          48       0.48      0.14      0.22        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.79      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 466\n","\n","Loop 17\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.944784164428711\n","step: 10, loss: 1.6161683797836304\n","step: 20, loss: 0.8584020137786865\n","step: 30, loss: 0.39894142746925354\n","step: 40, loss: 0.39631372690200806\n","step: 50, loss: 0.3257835805416107\n","step: 60, loss: 0.2395678162574768\n","step: 70, loss: 0.15072502195835114\n","step: 80, loss: 0.21674053370952606\n","step: 90, loss: 0.10730284452438354\n","step: 100, loss: 0.16021861135959625\n","step: 110, loss: 0.3832399547100067\n","step: 120, loss: 0.20167988538742065\n","step: 130, loss: 0.32324615120887756\n","step: 140, loss: 0.13781270384788513\n","step: 150, loss: 0.18074500560760498\n","step: 160, loss: 0.20000584423542023\n","step: 170, loss: 0.12392185628414154\n","step: 180, loss: 0.2626858055591583\n","step: 190, loss: 0.13482442498207092\n","step: 200, loss: 0.1908084601163864\n","step: 210, loss: 0.09369484335184097\n","step: 220, loss: 0.17346583306789398\n","step: 230, loss: 0.11788983643054962\n","step: 240, loss: 0.041077326983213425\n","step: 250, loss: 0.2081400752067566\n","step: 260, loss: 0.08691635727882385\n","step: 270, loss: 0.08775041997432709\n","step: 280, loss: 0.10959084331989288\n","step: 290, loss: 0.14001260697841644\n","step: 300, loss: 0.08324658870697021\n","step: 310, loss: 0.16648562252521515\n","step: 320, loss: 0.16059322655200958\n","step: 330, loss: 0.06840407103300095\n","step: 340, loss: 0.16100509464740753\n","step: 350, loss: 0.07146776467561722\n","step: 360, loss: 0.17049840092658997\n","step: 370, loss: 0.08444658666849136\n","step: 380, loss: 0.159016415476799\n","step: 390, loss: 0.11629219353199005\n","step: 400, loss: 0.17054541409015656\n","step: 410, loss: 0.057201679795980453\n","step: 420, loss: 0.09748289734125137\n","step: 430, loss: 0.07898222655057907\n","step: 440, loss: 0.10982748121023178\n","step: 450, loss: 0.11216919869184494\n","step: 460, loss: 0.15336856245994568\n","step: 470, loss: 0.18628834187984467\n","step: 480, loss: 0.12520897388458252\n","step: 490, loss: 0.1622001975774765\n","step: 500, loss: 0.13208529353141785\n","step: 510, loss: 0.1093468889594078\n","step: 520, loss: 0.05912504345178604\n","step: 530, loss: 0.09216482937335968\n","step: 540, loss: 0.1366090178489685\n","step: 550, loss: 0.03175670653581619\n","step: 560, loss: 0.08987516909837723\n","step: 570, loss: 0.07831824570894241\n","step: 580, loss: 0.14154930412769318\n","step: 590, loss: 0.1459992527961731\n","step: 600, loss: 0.11479667574167252\n","step: 610, loss: 0.044770270586013794\n","step: 620, loss: 0.11468181014060974\n","step: 630, loss: 0.09728303551673889\n","step: 640, loss: 0.0498281866312027\n","step: 650, loss: 0.12615619599819183\n","step: 660, loss: 0.11803500354290009\n","step: 670, loss: 0.0833091288805008\n","step: 680, loss: 0.07643675804138184\n","step: 690, loss: 0.05602420121431351\n","step: 700, loss: 0.07951665669679642\n","step: 710, loss: 0.14373676478862762\n","step: 720, loss: 0.07154593616724014\n","step: 730, loss: 0.06126489117741585\n","step: 740, loss: 0.051944222301244736\n","step: 750, loss: 0.1492699533700943\n","step: 760, loss: 0.07617359608411789\n","step: 770, loss: 0.12817369401454926\n","step: 780, loss: 0.08228222280740738\n","step: 790, loss: 0.034707143902778625\n","step: 800, loss: 0.10225171595811844\n","step: 810, loss: 0.09787193685770035\n","step: 820, loss: 0.09093105792999268\n","step: 830, loss: 0.06013638153672218\n","step: 840, loss: 0.11207308620214462\n","step: 850, loss: 0.07794780284166336\n","step: 860, loss: 0.22996515035629272\n","step: 870, loss: 0.09538821876049042\n","step: 880, loss: 0.13227231800556183\n","step: 890, loss: 0.04414181783795357\n","step: 900, loss: 0.15442568063735962\n","step: 910, loss: 0.02368868887424469\n","step: 920, loss: 0.07413100451231003\n","step: 930, loss: 0.04298632964491844\n","step: 940, loss: 0.15985918045043945\n","step: 950, loss: 0.0885758325457573\n","step: 960, loss: 0.08310253173112869\n","step: 970, loss: 0.0811009556055069\n","step: 980, loss: 0.0900256559252739\n","step: 990, loss: 0.13332107663154602\n","step: 1000, loss: 0.10765019059181213\n","step: 1010, loss: 0.054597433656454086\n","step: 1020, loss: 0.08873942494392395\n","step: 1030, loss: 0.055189698934555054\n","step: 1040, loss: 0.08858766406774521\n","step: 1050, loss: 0.06641969084739685\n","step: 1060, loss: 0.051107216626405716\n","step: 1070, loss: 0.07313086837530136\n","step: 1080, loss: 0.07596980780363083\n","step: 1090, loss: 0.09589868783950806\n","step: 1100, loss: 0.0544155091047287\n","step: 1110, loss: 0.1571500152349472\n","step: 1120, loss: 0.057073839008808136\n","step: 1130, loss: 0.12982265651226044\n","step: 1140, loss: 0.1646643877029419\n","step: 1150, loss: 0.13135142624378204\n","step: 1160, loss: 0.1017371416091919\n","step: 1170, loss: 0.13585889339447021\n","step: 1180, loss: 0.12297439575195312\n","step: 1190, loss: 0.09922408312559128\n","step: 1200, loss: 0.012347043491899967\n","step: 1210, loss: 0.25523585081100464\n","step: 1220, loss: 0.05395609512925148\n","step: 1230, loss: 0.10938888043165207\n","step: 1240, loss: 0.10392793267965317\n","step: 1250, loss: 0.1473003476858139\n","step: 1260, loss: 0.054959118366241455\n","step: 1270, loss: 0.03241198509931564\n","step: 1280, loss: 0.09760720282793045\n","step: 1290, loss: 0.08789394050836563\n","step: 1300, loss: 0.16593582928180695\n","step: 1310, loss: 0.02798101305961609\n","step: 1320, loss: 0.10578667372465134\n","step: 1330, loss: 0.09856819361448288\n","step: 1340, loss: 0.16694311797618866\n","step: 1350, loss: 0.062197260558605194\n","step: 1360, loss: 0.0579647570848465\n","step: 1370, loss: 0.09465169161558151\n","step: 1380, loss: 0.16573253273963928\n","step: 1390, loss: 0.04398353025317192\n","step: 1400, loss: 0.1984691470861435\n","step: 1410, loss: 0.04059295356273651\n","step: 1420, loss: 0.10292334854602814\n","step: 1430, loss: 0.08001862466335297\n","step: 1440, loss: 0.09727771580219269\n","step: 1450, loss: 0.08104745298624039\n","step: 1460, loss: 0.21101543307304382\n","step: 1470, loss: 0.15950924158096313\n","step: 1480, loss: 0.06184942275285721\n","step: 1490, loss: 0.13140209019184113\n","step: 1500, loss: 0.14911891520023346\n","step: 1510, loss: 0.052457477897405624\n","step: 1520, loss: 0.08926760405302048\n","step: 1530, loss: 0.06437370181083679\n","step: 1540, loss: 0.15541264414787292\n","step: 1550, loss: 0.214904323220253\n","step: 1560, loss: 0.05711524188518524\n","step: 1570, loss: 0.0566398911178112\n","step: 1580, loss: 0.10827314853668213\n","step: 1590, loss: 0.13851121068000793\n","step: 1600, loss: 0.09445873647928238\n","step: 1610, loss: 0.10373992472887039\n","step: 1620, loss: 0.1451241672039032\n","step: 1630, loss: 0.044908810406923294\n","step: 1640, loss: 0.06782800704240799\n","step: 1650, loss: 0.09731023013591766\n","step: 1660, loss: 0.1261567771434784\n","step: 1670, loss: 0.13932392001152039\n","step: 1680, loss: 0.07051560282707214\n","step: 1690, loss: 0.03763212263584137\n","step: 1700, loss: 0.06790953129529953\n","step: 1710, loss: 0.10076848417520523\n","step: 1720, loss: 0.10527555644512177\n","step: 1730, loss: 0.06344752758741379\n","step: 1740, loss: 0.04589242860674858\n","step: 1750, loss: 0.07251172512769699\n","step: 1760, loss: 0.15153299272060394\n","step: 1770, loss: 0.1202673390507698\n","step: 1780, loss: 0.0620909109711647\n","step: 1790, loss: 0.10220947116613388\n","step: 1800, loss: 0.12262574583292007\n","step: 1810, loss: 0.09020884335041046\n","step: 1820, loss: 0.10033740103244781\n","step: 1830, loss: 0.07207638770341873\n","step: 1840, loss: 0.06036466360092163\n","step: 1850, loss: 0.11777752637863159\n","step: 1860, loss: 0.15376627445220947\n","step: 1870, loss: 0.12278560549020767\n","step: 1880, loss: 0.16481943428516388\n","step: 1890, loss: 0.03239455819129944\n","step: 1900, loss: 0.0655553787946701\n","step: 1910, loss: 0.1463114619255066\n","step: 1920, loss: 0.06267818808555603\n","step: 1930, loss: 0.06547044962644577\n","step: 1940, loss: 0.03147757425904274\n","step: 1950, loss: 0.1693291813135147\n","step: 1960, loss: 0.09053319692611694\n","step: 1970, loss: 0.13211362063884735\n","step: 1980, loss: 0.0831792801618576\n","step: 1990, loss: 0.16885754466056824\n","step: 2000, loss: 0.09993831068277359\n","step: 2010, loss: 0.07636024802923203\n","step: 2020, loss: 0.041433271020650864\n","step: 2030, loss: 0.08085895329713821\n","step: 2040, loss: 0.06524565070867538\n","step: 2050, loss: 0.021895306184887886\n","step: 2060, loss: 0.10619497299194336\n","step: 2070, loss: 0.07520741969347\n","step: 2080, loss: 0.06984054297208786\n","step: 2090, loss: 0.11947081983089447\n","step: 2100, loss: 0.13438227772712708\n","step: 2110, loss: 0.05533413589000702\n","step: 2120, loss: 0.1793447732925415\n","step: 2130, loss: 0.03965861350297928\n","step: 2140, loss: 0.03133426979184151\n","step: 2150, loss: 0.06423240154981613\n","step: 2160, loss: 0.05024131387472153\n","step: 2170, loss: 0.0767325758934021\n","step: 2180, loss: 0.04372720792889595\n","step: 2190, loss: 0.1319306641817093\n","step: 2200, loss: 0.061415042728185654\n","step: 2210, loss: 0.06396989524364471\n","step: 2220, loss: 0.05573863163590431\n","step: 2230, loss: 0.1554514318704605\n","step: 2240, loss: 0.13770738244056702\n","step: 2250, loss: 0.11259184032678604\n","step: 2260, loss: 0.07781554758548737\n","step: 2270, loss: 0.07921023666858673\n","step: 2280, loss: 0.13961437344551086\n","step: 2290, loss: 0.1208803802728653\n","step: 2300, loss: 0.12209498882293701\n","step: 2310, loss: 0.16872534155845642\n","step: 2320, loss: 0.1624562293291092\n","step: 2330, loss: 0.06992227584123611\n","step: 2340, loss: 0.18122930824756622\n","step: 2350, loss: 0.048480525612831116\n","step: 2360, loss: 0.03939719498157501\n","step: 2370, loss: 0.02197338081896305\n","step: 2380, loss: 0.050404999405145645\n","step: 2390, loss: 0.09532426297664642\n","step: 2400, loss: 0.08253153413534164\n","step: 2410, loss: 0.18487253785133362\n","step: 2420, loss: 0.15763059258460999\n","step: 2430, loss: 0.11899296939373016\n","step: 2440, loss: 0.07082782685756683\n","step: 2450, loss: 0.10943018645048141\n","step: 2460, loss: 0.09407991170883179\n","step: 2470, loss: 0.05951468273997307\n","step: 2480, loss: 0.07261793315410614\n","step: 2490, loss: 0.06579627841711044\n","step: 2500, loss: 0.06800439953804016\n","step: 2510, loss: 0.03522640839219093\n","step: 2520, loss: 0.09324437379837036\n","step: 2530, loss: 0.06301803886890411\n","step: 2540, loss: 0.056515034288167953\n","step: 2550, loss: 0.07784523069858551\n","step: 2560, loss: 0.08922441303730011\n","step: 2570, loss: 0.041516005992889404\n","step: 2580, loss: 0.06635753810405731\n","step: 2590, loss: 0.09025228023529053\n","step: 2600, loss: 0.05678175389766693\n","step: 2610, loss: 0.04181130602955818\n","step: 2620, loss: 0.12007074058055878\n","step: 2630, loss: 0.10265207290649414\n","step: 2640, loss: 0.0893603041768074\n","step: 2650, loss: 0.07513195276260376\n","step: 2660, loss: 0.1236109733581543\n","step: 2670, loss: 0.0461917482316494\n","step: 2680, loss: 0.035068392753601074\n","step: 2690, loss: 0.14903970062732697\n","step: 2700, loss: 0.0924077108502388\n","step: 2710, loss: 0.06957146525382996\n","step: 2720, loss: 0.142793670296669\n","step: 2730, loss: 0.03718129172921181\n","step: 2740, loss: 0.06806162744760513\n","step: 2750, loss: 0.038722824305295944\n","step: 2760, loss: 0.060326315462589264\n","step: 2770, loss: 0.07902096956968307\n","step: 2780, loss: 0.06902972608804703\n","step: 2790, loss: 0.11670690029859543\n","step: 2800, loss: 0.06527019292116165\n","step: 2810, loss: 0.05129045993089676\n","step: 2820, loss: 0.11251287907361984\n","step: 2830, loss: 0.08738448470830917\n","step: 2840, loss: 0.09702176600694656\n","step: 2850, loss: 0.10206444561481476\n","step: 2860, loss: 0.04494108632206917\n","step: 2870, loss: 0.1812114268541336\n","step: 2880, loss: 0.20116467773914337\n","step: 2890, loss: 0.10864640027284622\n","step: 2900, loss: 0.11838041990995407\n","step: 2910, loss: 0.09073419123888016\n","step: 2920, loss: 0.03088448941707611\n","step: 2930, loss: 0.0689878761768341\n","step: 2940, loss: 0.07053577899932861\n","step: 2950, loss: 0.036860715597867966\n","step: 2960, loss: 0.09801777452230453\n","step: 2970, loss: 0.12323697656393051\n","step: 2980, loss: 0.15625876188278198\n","step: 2990, loss: 0.08469117432832718\n","step: 3000, loss: 0.12419623881578445\n","step: 3010, loss: 0.05433627590537071\n","step: 3020, loss: 0.12756836414337158\n","step: 3030, loss: 0.035609710961580276\n","step: 3040, loss: 0.12873803079128265\n","step: 3050, loss: 0.09822800010442734\n","step: 3060, loss: 0.09475354105234146\n","step: 3070, loss: 0.044938668608665466\n","step: 3080, loss: 0.05654071643948555\n","step: 3090, loss: 0.042769480496644974\n","step: 3100, loss: 0.057590603828430176\n","step: 3110, loss: 0.025330806151032448\n","step: 3120, loss: 0.18600128591060638\n","step: 3130, loss: 0.08406492322683334\n","step: 3140, loss: 0.144118070602417\n","step: 3150, loss: 0.08537077903747559\n","step: 3160, loss: 0.048817750066518784\n","step: 3170, loss: 0.08688685297966003\n","step: 3180, loss: 0.12095368653535843\n","step: 3190, loss: 0.06340748071670532\n","step: 3200, loss: 0.06781887263059616\n","step: 3210, loss: 0.1866782307624817\n","step: 3220, loss: 0.056928906589746475\n","step: 3230, loss: 0.09102537482976913\n","step: 3240, loss: 0.12531965970993042\n","step: 3250, loss: 0.1563071608543396\n","step: 3260, loss: 0.05578143894672394\n","step: 3270, loss: 0.02553834207355976\n","step: 3280, loss: 0.0229464303702116\n","step: 3290, loss: 0.14195477962493896\n","step: 3300, loss: 0.21306359767913818\n","step: 3310, loss: 0.1773100197315216\n","step: 3320, loss: 0.09960903227329254\n","step: 3330, loss: 0.13337881863117218\n","step: 3340, loss: 0.043984223157167435\n","step: 3350, loss: 0.08399453014135361\n","step: 3360, loss: 0.01598929800093174\n","step: 3370, loss: 0.03644679859280586\n","step: 3380, loss: 0.05772516503930092\n","step: 3390, loss: 0.13495375216007233\n","step: 3400, loss: 0.055646006017923355\n","step: 3410, loss: 0.11963404715061188\n","step: 3420, loss: 0.10660456866025925\n","step: 3430, loss: 0.08196096122264862\n","step: 3440, loss: 0.027494126930832863\n","step: 3450, loss: 0.10811655223369598\n","step: 3460, loss: 0.11798352003097534\n","step: 3470, loss: 0.15076716244220734\n","step: 3480, loss: 0.08710508048534393\n","step: 3490, loss: 0.031212812289595604\n","step: 3500, loss: 0.09485135227441788\n","step: 3510, loss: 0.10048509389162064\n","step: 3520, loss: 0.07406904548406601\n","step: 3530, loss: 0.11400719732046127\n","step: 3540, loss: 0.175498366355896\n","step: 3550, loss: 0.04019180312752724\n","step: 3560, loss: 0.05581009387969971\n","step: 3570, loss: 0.04834333062171936\n","step: 3580, loss: 0.12433432787656784\n","step: 3590, loss: 0.04870588704943657\n","step: 3600, loss: 0.17173804342746735\n","step: 3610, loss: 0.1418701559305191\n","step: 3620, loss: 0.12827543914318085\n","step: 3630, loss: 0.09191091358661652\n","step: 3640, loss: 0.07312589883804321\n","step: 3650, loss: 0.08727750927209854\n","step: 3660, loss: 0.10129125416278839\n","step: 3670, loss: 0.14782096445560455\n","step: 3680, loss: 0.09263808280229568\n","step: 3690, loss: 0.05829356238245964\n","step: 3700, loss: 0.19243723154067993\n","step: 3710, loss: 0.08278393745422363\n","step: 3720, loss: 0.07737985998392105\n","step: 3730, loss: 0.053713273257017136\n","step: 3740, loss: 0.02342016063630581\n","step: 3750, loss: 0.03155520185828209\n","step: 3760, loss: 0.13046550750732422\n","step: 3770, loss: 0.03680208697915077\n","step: 3780, loss: 0.03526924178004265\n","step: 3790, loss: 0.056920163333415985\n","step: 3800, loss: 0.1264016032218933\n","step: 3810, loss: 0.0810326486825943\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.76      1.00      0.86        35\n","           2       0.55      0.69      0.61        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.95      0.83      0.89       291\n","           5       0.90      0.84      0.87       294\n","           6       0.99      0.97      0.98      1570\n","           7       0.47      0.95      0.63       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.98      0.96      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.79      0.98      0.88        47\n","          13       0.12      0.69      0.20        13\n","          14       0.52      1.00      0.68        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.93      0.80      0.86      1151\n","          17       0.89      0.95      0.92        41\n","          18       0.91      1.00      0.96        32\n","          19       0.50      0.68      0.57        40\n","          20       0.99      1.00      1.00       584\n","          21       0.27      0.17      0.21        52\n","          22       0.95      0.74      0.83      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.26      0.45      0.33        44\n","          25       0.86      0.90      0.88       888\n","          26       0.89      0.89      0.89         9\n","          27       0.96      0.99      0.97        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.85      0.90      1136\n","          31       0.57      0.63      0.60        19\n","          32       1.00      0.50      0.67         8\n","          33       0.73      0.93      0.82        86\n","          34       0.22      0.84      0.35        32\n","          35       0.99      0.99      0.99       474\n","          36       0.78      0.20      0.32       182\n","          37       0.87      0.96      0.91      1592\n","          38       0.96      0.97      0.97       404\n","          39       0.96      0.96      0.96       485\n","          40       0.88      0.97      0.92       573\n","          41       0.93      0.94      0.94       841\n","          42       0.97      0.99      0.98       575\n","          43       0.94      0.89      0.92       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.98      0.99        82\n","          48       1.00      0.08      0.14        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.82      0.78     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 468\n","\n","Loop 18\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9956462383270264\n","step: 10, loss: 1.7541296482086182\n","step: 20, loss: 0.9237406849861145\n","step: 30, loss: 0.5112792253494263\n","step: 40, loss: 0.3055051863193512\n","step: 50, loss: 0.24705947935581207\n","step: 60, loss: 0.2244643121957779\n","step: 70, loss: 0.37748947739601135\n","step: 80, loss: 0.1376863270998001\n","step: 90, loss: 0.4831281006336212\n","step: 100, loss: 0.15899161994457245\n","step: 110, loss: 0.1795313060283661\n","step: 120, loss: 0.22283141314983368\n","step: 130, loss: 0.1845567524433136\n","step: 140, loss: 0.13910026848316193\n","step: 150, loss: 0.15875005722045898\n","step: 160, loss: 0.18439945578575134\n","step: 170, loss: 0.23945793509483337\n","step: 180, loss: 0.1437680572271347\n","step: 190, loss: 0.2532637119293213\n","step: 200, loss: 0.12273586541414261\n","step: 210, loss: 0.12561224400997162\n","step: 220, loss: 0.04909883439540863\n","step: 230, loss: 0.16837531328201294\n","step: 240, loss: 0.099160335958004\n","step: 250, loss: 0.0563281886279583\n","step: 260, loss: 0.18867377936840057\n","step: 270, loss: 0.11929689347743988\n","step: 280, loss: 0.12853823602199554\n","step: 290, loss: 0.08408629894256592\n","step: 300, loss: 0.229932501912117\n","step: 310, loss: 0.18119946122169495\n","step: 320, loss: 0.14559271931648254\n","step: 330, loss: 0.03573034703731537\n","step: 340, loss: 0.08909939974546432\n","step: 350, loss: 0.18073423206806183\n","step: 360, loss: 0.2036331295967102\n","step: 370, loss: 0.061941180378198624\n","step: 380, loss: 0.1033419668674469\n","step: 390, loss: 0.2459089457988739\n","step: 400, loss: 0.0613848976790905\n","step: 410, loss: 0.1256255805492401\n","step: 420, loss: 0.1277286261320114\n","step: 430, loss: 0.031111162155866623\n","step: 440, loss: 0.06717806309461594\n","step: 450, loss: 0.1358649581670761\n","step: 460, loss: 0.04648689553141594\n","step: 470, loss: 0.09891495853662491\n","step: 480, loss: 0.06200464814901352\n","step: 490, loss: 0.0846564769744873\n","step: 500, loss: 0.10855548083782196\n","step: 510, loss: 0.07349568605422974\n","step: 520, loss: 0.17533819377422333\n","step: 530, loss: 0.0675884261727333\n","step: 540, loss: 0.06803043186664581\n","step: 550, loss: 0.07256808131933212\n","step: 560, loss: 0.10519146919250488\n","step: 570, loss: 0.14796914160251617\n","step: 580, loss: 0.07354599982500076\n","step: 590, loss: 0.08297951519489288\n","step: 600, loss: 0.09371955692768097\n","step: 610, loss: 0.08557504415512085\n","step: 620, loss: 0.14677579700946808\n","step: 630, loss: 0.05665772408246994\n","step: 640, loss: 0.2138442099094391\n","step: 650, loss: 0.08743017166852951\n","step: 660, loss: 0.09652964025735855\n","step: 670, loss: 0.053636759519577026\n","step: 680, loss: 0.0474710613489151\n","step: 690, loss: 0.16880467534065247\n","step: 700, loss: 0.061475563794374466\n","step: 710, loss: 0.06423082947731018\n","step: 720, loss: 0.1520378291606903\n","step: 730, loss: 0.15727749466896057\n","step: 740, loss: 0.12439073622226715\n","step: 750, loss: 0.09109377861022949\n","step: 760, loss: 0.08383641391992569\n","step: 770, loss: 0.08946231007575989\n","step: 780, loss: 0.21585579216480255\n","step: 790, loss: 0.07975982874631882\n","step: 800, loss: 0.06278039515018463\n","step: 810, loss: 0.10920534282922745\n","step: 820, loss: 0.15341190993785858\n","step: 830, loss: 0.09212251752614975\n","step: 840, loss: 0.03428104147315025\n","step: 850, loss: 0.09074518829584122\n","step: 860, loss: 0.08219727873802185\n","step: 870, loss: 0.04660862684249878\n","step: 880, loss: 0.080938920378685\n","step: 890, loss: 0.10190686583518982\n","step: 900, loss: 0.17675386369228363\n","step: 910, loss: 0.0650445818901062\n","step: 920, loss: 0.09993322938680649\n","step: 930, loss: 0.042882490903139114\n","step: 940, loss: 0.04330074414610863\n","step: 950, loss: 0.08520466834306717\n","step: 960, loss: 0.06609839200973511\n","step: 970, loss: 0.07595279067754745\n","step: 980, loss: 0.18354591727256775\n","step: 990, loss: 0.045889563858509064\n","step: 1000, loss: 0.09237170964479446\n","step: 1010, loss: 0.0479908213019371\n","step: 1020, loss: 0.11031990498304367\n","step: 1030, loss: 0.1960630714893341\n","step: 1040, loss: 0.07652927190065384\n","step: 1050, loss: 0.09556301683187485\n","step: 1060, loss: 0.1436872035264969\n","step: 1070, loss: 0.11226294189691544\n","step: 1080, loss: 0.08438587188720703\n","step: 1090, loss: 0.035759810358285904\n","step: 1100, loss: 0.07064301520586014\n","step: 1110, loss: 0.05705543980002403\n","step: 1120, loss: 0.11119547486305237\n","step: 1130, loss: 0.07276441156864166\n","step: 1140, loss: 0.04769505187869072\n","step: 1150, loss: 0.1059914231300354\n","step: 1160, loss: 0.05978615954518318\n","step: 1170, loss: 0.025367476046085358\n","step: 1180, loss: 0.09060129523277283\n","step: 1190, loss: 0.05224278196692467\n","step: 1200, loss: 0.10845779627561569\n","step: 1210, loss: 0.025226227939128876\n","step: 1220, loss: 0.04217543080449104\n","step: 1230, loss: 0.05361935496330261\n","step: 1240, loss: 0.07919546961784363\n","step: 1250, loss: 0.07972930371761322\n","step: 1260, loss: 0.04369378834962845\n","step: 1270, loss: 0.01751399040222168\n","step: 1280, loss: 0.030770616605877876\n","step: 1290, loss: 0.15965570509433746\n","step: 1300, loss: 0.10502710193395615\n","step: 1310, loss: 0.08443333208560944\n","step: 1320, loss: 0.09423016011714935\n","step: 1330, loss: 0.030715126544237137\n","step: 1340, loss: 0.19091679155826569\n","step: 1350, loss: 0.09668246656656265\n","step: 1360, loss: 0.12034458667039871\n","step: 1370, loss: 0.08994881808757782\n","step: 1380, loss: 0.23901821672916412\n","step: 1390, loss: 0.10893518477678299\n","step: 1400, loss: 0.06034640222787857\n","step: 1410, loss: 0.0746329203248024\n","step: 1420, loss: 0.20568260550498962\n","step: 1430, loss: 0.08578095585107803\n","step: 1440, loss: 0.04968816041946411\n","step: 1450, loss: 0.10823121666908264\n","step: 1460, loss: 0.3204037845134735\n","step: 1470, loss: 0.10268204659223557\n","step: 1480, loss: 0.09296521544456482\n","step: 1490, loss: 0.054656144231557846\n","step: 1500, loss: 0.06967215240001678\n","step: 1510, loss: 0.12464535236358643\n","step: 1520, loss: 0.07591880112886429\n","step: 1530, loss: 0.12734153866767883\n","step: 1540, loss: 0.0948459580540657\n","step: 1550, loss: 0.07559756934642792\n","step: 1560, loss: 0.06989508867263794\n","step: 1570, loss: 0.03157500550150871\n","step: 1580, loss: 0.06765633821487427\n","step: 1590, loss: 0.1498595029115677\n","step: 1600, loss: 0.18535752594470978\n","step: 1610, loss: 0.0645284578204155\n","step: 1620, loss: 0.06525634229183197\n","step: 1630, loss: 0.09933731704950333\n","step: 1640, loss: 0.08546220511198044\n","step: 1650, loss: 0.0581640899181366\n","step: 1660, loss: 0.1588694453239441\n","step: 1670, loss: 0.07568288594484329\n","step: 1680, loss: 0.05768899992108345\n","step: 1690, loss: 0.044979386031627655\n","step: 1700, loss: 0.2470693439245224\n","step: 1710, loss: 0.029182784259319305\n","step: 1720, loss: 0.0735480785369873\n","step: 1730, loss: 0.12702131271362305\n","step: 1740, loss: 0.12925109267234802\n","step: 1750, loss: 0.04342084378004074\n","step: 1760, loss: 0.18131285905838013\n","step: 1770, loss: 0.12434931844472885\n","step: 1780, loss: 0.08164609968662262\n","step: 1790, loss: 0.10354170203208923\n","step: 1800, loss: 0.09043821692466736\n","step: 1810, loss: 0.08952019363641739\n","step: 1820, loss: 0.13048943877220154\n","step: 1830, loss: 0.117439404129982\n","step: 1840, loss: 0.18188413977622986\n","step: 1850, loss: 0.075215645134449\n","step: 1860, loss: 0.0994848683476448\n","step: 1870, loss: 0.14200851321220398\n","step: 1880, loss: 0.045684028416872025\n","step: 1890, loss: 0.22820846736431122\n","step: 1900, loss: 0.1330779641866684\n","step: 1910, loss: 0.15610705316066742\n","step: 1920, loss: 0.18929289281368256\n","step: 1930, loss: 0.0819273591041565\n","step: 1940, loss: 0.07238160818815231\n","step: 1950, loss: 0.11462849378585815\n","step: 1960, loss: 0.19961874186992645\n","step: 1970, loss: 0.033696554601192474\n","step: 1980, loss: 0.07649697363376617\n","step: 1990, loss: 0.04961981251835823\n","step: 2000, loss: 0.04922410100698471\n","step: 2010, loss: 0.06492055952548981\n","step: 2020, loss: 0.12949036061763763\n","step: 2030, loss: 0.12063087522983551\n","step: 2040, loss: 0.2336166352033615\n","step: 2050, loss: 0.059128548949956894\n","step: 2060, loss: 0.1306697428226471\n","step: 2070, loss: 0.06434392184019089\n","step: 2080, loss: 0.05928254500031471\n","step: 2090, loss: 0.10228210687637329\n","step: 2100, loss: 0.10421368479728699\n","step: 2110, loss: 0.08825892955064774\n","step: 2120, loss: 0.043846264481544495\n","step: 2130, loss: 0.10696981102228165\n","step: 2140, loss: 0.04459747299551964\n","step: 2150, loss: 0.11938677728176117\n","step: 2160, loss: 0.05866442993283272\n","step: 2170, loss: 0.11572888493537903\n","step: 2180, loss: 0.03232850879430771\n","step: 2190, loss: 0.08921681344509125\n","step: 2200, loss: 0.1919558197259903\n","step: 2210, loss: 0.08131831139326096\n","step: 2220, loss: 0.1746806800365448\n","step: 2230, loss: 0.04830475151538849\n","step: 2240, loss: 0.030210990458726883\n","step: 2250, loss: 0.05912458151578903\n","step: 2260, loss: 0.025806548073887825\n","step: 2270, loss: 0.0735938549041748\n","step: 2280, loss: 0.08125295490026474\n","step: 2290, loss: 0.045696504414081573\n","step: 2300, loss: 0.04577603191137314\n","step: 2310, loss: 0.09677540510892868\n","step: 2320, loss: 0.11066172271966934\n","step: 2330, loss: 0.04997218772768974\n","step: 2340, loss: 0.1355067789554596\n","step: 2350, loss: 0.13060477375984192\n","step: 2360, loss: 0.11585371196269989\n","step: 2370, loss: 0.02499930001795292\n","step: 2380, loss: 0.08529578894376755\n","step: 2390, loss: 0.08140408247709274\n","step: 2400, loss: 0.06023835390806198\n","step: 2410, loss: 0.07364993542432785\n","step: 2420, loss: 0.09148460626602173\n","step: 2430, loss: 0.04207736253738403\n","step: 2440, loss: 0.10645914077758789\n","step: 2450, loss: 0.11528897285461426\n","step: 2460, loss: 0.05807461589574814\n","step: 2470, loss: 0.09691561013460159\n","step: 2480, loss: 0.047972869127988815\n","step: 2490, loss: 0.06850254535675049\n","step: 2500, loss: 0.15043413639068604\n","step: 2510, loss: 0.07035915553569794\n","step: 2520, loss: 0.11094135046005249\n","step: 2530, loss: 0.05856652930378914\n","step: 2540, loss: 0.05106991529464722\n","step: 2550, loss: 0.06306692957878113\n","step: 2560, loss: 0.17164283990859985\n","step: 2570, loss: 0.11590877175331116\n","step: 2580, loss: 0.11344491690397263\n","step: 2590, loss: 0.16150063276290894\n","step: 2600, loss: 0.06996329128742218\n","step: 2610, loss: 0.20384164154529572\n","step: 2620, loss: 0.12946070730686188\n","step: 2630, loss: 0.051622696220874786\n","step: 2640, loss: 0.120449498295784\n","step: 2650, loss: 0.06231173872947693\n","step: 2660, loss: 0.04479420930147171\n","step: 2670, loss: 0.06703769415616989\n","step: 2680, loss: 0.07732639461755753\n","step: 2690, loss: 0.10507965832948685\n","step: 2700, loss: 0.07451502233743668\n","step: 2710, loss: 0.06145264953374863\n","step: 2720, loss: 0.06874746829271317\n","step: 2730, loss: 0.06564433872699738\n","step: 2740, loss: 0.10314033180475235\n","step: 2750, loss: 0.10508517920970917\n","step: 2760, loss: 0.10583128780126572\n","step: 2770, loss: 0.12222679704427719\n","step: 2780, loss: 0.07404865324497223\n","step: 2790, loss: 0.07845858484506607\n","step: 2800, loss: 0.08009295165538788\n","step: 2810, loss: 0.08556262403726578\n","step: 2820, loss: 0.09427352249622345\n","step: 2830, loss: 0.0695805773139\n","step: 2840, loss: 0.04628656804561615\n","step: 2850, loss: 0.07519127428531647\n","step: 2860, loss: 0.05215183272957802\n","step: 2870, loss: 0.045125242322683334\n","step: 2880, loss: 0.1604354828596115\n","step: 2890, loss: 0.06277463585138321\n","step: 2900, loss: 0.150437131524086\n","step: 2910, loss: 0.039836570620536804\n","step: 2920, loss: 0.03692777454853058\n","step: 2930, loss: 0.17492704093456268\n","step: 2940, loss: 0.13735179603099823\n","step: 2950, loss: 0.11623918265104294\n","step: 2960, loss: 0.06968439370393753\n","step: 2970, loss: 0.09987882524728775\n","step: 2980, loss: 0.08841124922037125\n","step: 2990, loss: 0.0959063172340393\n","step: 3000, loss: 0.020767465233802795\n","step: 3010, loss: 0.0895003080368042\n","step: 3020, loss: 0.040196653455495834\n","step: 3030, loss: 0.09726481884717941\n","step: 3040, loss: 0.10778658837080002\n","step: 3050, loss: 0.07646436244249344\n","step: 3060, loss: 0.12144893407821655\n","step: 3070, loss: 0.1382545381784439\n","step: 3080, loss: 0.09589993953704834\n","step: 3090, loss: 0.023650242015719414\n","step: 3100, loss: 0.10008582472801208\n","step: 3110, loss: 0.030527055263519287\n","step: 3120, loss: 0.031087562441825867\n","step: 3130, loss: 0.05641255900263786\n","step: 3140, loss: 0.09271840006113052\n","step: 3150, loss: 0.16423878073692322\n","step: 3160, loss: 0.1478240191936493\n","step: 3170, loss: 0.06299743056297302\n","step: 3180, loss: 0.08699650317430496\n","step: 3190, loss: 0.07338367402553558\n","step: 3200, loss: 0.08204895257949829\n","step: 3210, loss: 0.09362594783306122\n","step: 3220, loss: 0.03458239883184433\n","step: 3230, loss: 0.11285044252872467\n","step: 3240, loss: 0.15566259622573853\n","step: 3250, loss: 0.13115200400352478\n","step: 3260, loss: 0.1488904058933258\n","step: 3270, loss: 0.05543074756860733\n","step: 3280, loss: 0.028180401772260666\n","step: 3290, loss: 0.12339528650045395\n","step: 3300, loss: 0.06876199692487717\n","step: 3310, loss: 0.10442224144935608\n","step: 3320, loss: 0.1205001175403595\n","step: 3330, loss: 0.08722232282161713\n","step: 3340, loss: 0.1316947340965271\n","step: 3350, loss: 0.07523312419652939\n","step: 3360, loss: 0.0935433954000473\n","step: 3370, loss: 0.030978549271821976\n","step: 3380, loss: 0.07816210389137268\n","step: 3390, loss: 0.040439195930957794\n","step: 3400, loss: 0.04261784628033638\n","step: 3410, loss: 0.05312327668070793\n","step: 3420, loss: 0.16317740082740784\n","step: 3430, loss: 0.11120802164077759\n","step: 3440, loss: 0.033457037061452866\n","step: 3450, loss: 0.11359211057424545\n","step: 3460, loss: 0.10460936278104782\n","step: 3470, loss: 0.05396281182765961\n","step: 3480, loss: 0.07863765209913254\n","step: 3490, loss: 0.08700866252183914\n","step: 3500, loss: 0.0698220282793045\n","step: 3510, loss: 0.0294505562633276\n","step: 3520, loss: 0.04933156445622444\n","step: 3530, loss: 0.12386695295572281\n","step: 3540, loss: 0.12075597792863846\n","step: 3550, loss: 0.039048194885253906\n","step: 3560, loss: 0.15900707244873047\n","step: 3570, loss: 0.09054934978485107\n","step: 3580, loss: 0.1525251716375351\n","step: 3590, loss: 0.14502032101154327\n","step: 3600, loss: 0.10463816672563553\n","step: 3610, loss: 0.18806780874729156\n","step: 3620, loss: 0.10569063574075699\n","step: 3630, loss: 0.07700750231742859\n","step: 3640, loss: 0.12699678540229797\n","step: 3650, loss: 0.13414162397384644\n","step: 3660, loss: 0.055924613028764725\n","step: 3670, loss: 0.04495100677013397\n","step: 3680, loss: 0.10824865847826004\n","step: 3690, loss: 0.06989238411188126\n","step: 3700, loss: 0.08491578698158264\n","step: 3710, loss: 0.14723645150661469\n","step: 3720, loss: 0.06643599271774292\n","step: 3730, loss: 0.06327811628580093\n","step: 3740, loss: 0.0812319964170456\n","step: 3750, loss: 0.07246238738298416\n","step: 3760, loss: 0.07143384218215942\n","step: 3770, loss: 0.0913105458021164\n","step: 3780, loss: 0.08971554785966873\n","step: 3790, loss: 0.06265082955360413\n","step: 3800, loss: 0.07918836921453476\n","step: 3810, loss: 0.06529846042394638\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        35\n","           2       0.61      0.44      0.51        77\n","           3       1.00      0.80      0.89      1030\n","           4       0.99      0.77      0.87       291\n","           5       0.89      0.84      0.87       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.53      0.94      0.68       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.98      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.10      1.00      0.18        13\n","          14       0.49      1.00      0.66        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.83      0.87      0.85      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.86      1.00      0.93        32\n","          19       0.56      0.62      0.59        40\n","          20       0.97      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.97      0.71      0.82      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.19      0.30      0.23        44\n","          25       0.81      0.92      0.86       888\n","          26       0.89      0.89      0.89         9\n","          27       0.97      0.96      0.96        69\n","          28       0.97      0.99      0.98      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.96      0.84      0.90      1136\n","          31       0.57      0.68      0.62        19\n","          32       1.00      0.50      0.67         8\n","          33       0.64      0.95      0.76        86\n","          34       0.23      0.72      0.34        32\n","          35       0.97      0.99      0.98       474\n","          36       0.91      0.45      0.60       182\n","          37       0.92      0.95      0.93      1592\n","          38       0.92      0.97      0.95       404\n","          39       0.93      0.98      0.95       485\n","          40       0.93      0.86      0.90       573\n","          41       0.94      0.86      0.90       841\n","          42       0.97      0.99      0.98       575\n","          43       0.96      0.70      0.81       152\n","          44       0.88      0.92      0.90        75\n","          46       1.00      0.99      0.99        82\n","          48       0.31      0.44      0.37        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.82      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 448\n","\n","Loop 19\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.022330284118652\n","step: 10, loss: 1.700323224067688\n","step: 20, loss: 0.5659964084625244\n","step: 30, loss: 0.6056168079376221\n","step: 40, loss: 0.27035611867904663\n","step: 50, loss: 0.23254869878292084\n","step: 60, loss: 0.39092814922332764\n","step: 70, loss: 0.2337976098060608\n","step: 80, loss: 0.11550624668598175\n","step: 90, loss: 0.1174728199839592\n","step: 100, loss: 0.19289706647396088\n","step: 110, loss: 0.17073389887809753\n","step: 120, loss: 0.13881461322307587\n","step: 130, loss: 0.12919196486473083\n","step: 140, loss: 0.23297466337680817\n","step: 150, loss: 0.15386012196540833\n","step: 160, loss: 0.3050698935985565\n","step: 170, loss: 0.07579963654279709\n","step: 180, loss: 0.1581028699874878\n","step: 190, loss: 0.07795506715774536\n","step: 200, loss: 0.06225283816456795\n","step: 210, loss: 0.13297785818576813\n","step: 220, loss: 0.22495107352733612\n","step: 230, loss: 0.15030020475387573\n","step: 240, loss: 0.2153753936290741\n","step: 250, loss: 0.16764818131923676\n","step: 260, loss: 0.06617183238267899\n","step: 270, loss: 0.17298609018325806\n","step: 280, loss: 0.10875222086906433\n","step: 290, loss: 0.04136756435036659\n","step: 300, loss: 0.13613437116146088\n","step: 310, loss: 0.07432068139314651\n","step: 320, loss: 0.06377965211868286\n","step: 330, loss: 0.06898138672113419\n","step: 340, loss: 0.17232266068458557\n","step: 350, loss: 0.25949788093566895\n","step: 360, loss: 0.057162776589393616\n","step: 370, loss: 0.08839882165193558\n","step: 380, loss: 0.07747401297092438\n","step: 390, loss: 0.042815353721380234\n","step: 400, loss: 0.1199730709195137\n","step: 410, loss: 0.08783579617738724\n","step: 420, loss: 0.1536661684513092\n","step: 430, loss: 0.11059524118900299\n","step: 440, loss: 0.13047072291374207\n","step: 450, loss: 0.1231173649430275\n","step: 460, loss: 0.12868988513946533\n","step: 470, loss: 0.10835684835910797\n","step: 480, loss: 0.10994930565357208\n","step: 490, loss: 0.06969846040010452\n","step: 500, loss: 0.06644372642040253\n","step: 510, loss: 0.2122308760881424\n","step: 520, loss: 0.129679337143898\n","step: 530, loss: 0.18231576681137085\n","step: 540, loss: 0.13656297326087952\n","step: 550, loss: 0.10926634073257446\n","step: 560, loss: 0.09591991454362869\n","step: 570, loss: 0.07258181273937225\n","step: 580, loss: 0.08139166980981827\n","step: 590, loss: 0.11086450517177582\n","step: 600, loss: 0.13974477350711823\n","step: 610, loss: 0.13684271275997162\n","step: 620, loss: 0.06559068709611893\n","step: 630, loss: 0.04008812829852104\n","step: 640, loss: 0.08283806592226028\n","step: 650, loss: 0.13163599371910095\n","step: 660, loss: 0.1568998247385025\n","step: 670, loss: 0.058233097195625305\n","step: 680, loss: 0.10495393723249435\n","step: 690, loss: 0.034666821360588074\n","step: 700, loss: 0.06239905208349228\n","step: 710, loss: 0.1058344766497612\n","step: 720, loss: 0.2039867639541626\n","step: 730, loss: 0.12454526871442795\n","step: 740, loss: 0.016711855307221413\n","step: 750, loss: 0.13117815554141998\n","step: 760, loss: 0.09481344372034073\n","step: 770, loss: 0.17347781360149384\n","step: 780, loss: 0.15098918974399567\n","step: 790, loss: 0.06981173157691956\n","step: 800, loss: 0.05465249717235565\n","step: 810, loss: 0.11950724571943283\n","step: 820, loss: 0.08644524961709976\n","step: 830, loss: 0.09928371757268906\n","step: 840, loss: 0.09651926904916763\n","step: 850, loss: 0.14236249029636383\n","step: 860, loss: 0.2128400355577469\n","step: 870, loss: 0.11196145415306091\n","step: 880, loss: 0.10327988862991333\n","step: 890, loss: 0.030784031376242638\n","step: 900, loss: 0.11600138992071152\n","step: 910, loss: 0.08220210671424866\n","step: 920, loss: 0.1910373419523239\n","step: 930, loss: 0.14963077008724213\n","step: 940, loss: 0.0825105533003807\n","step: 950, loss: 0.1288386583328247\n","step: 960, loss: 0.2604110538959503\n","step: 970, loss: 0.054979871958494186\n","step: 980, loss: 0.07951901108026505\n","step: 990, loss: 0.041323479264974594\n","step: 1000, loss: 0.07707104086875916\n","step: 1010, loss: 0.1926395446062088\n","step: 1020, loss: 0.058902472257614136\n","step: 1030, loss: 0.10699973255395889\n","step: 1040, loss: 0.11340492963790894\n","step: 1050, loss: 0.0807737484574318\n","step: 1060, loss: 0.14500023424625397\n","step: 1070, loss: 0.12141990661621094\n","step: 1080, loss: 0.13761967420578003\n","step: 1090, loss: 0.13321641087532043\n","step: 1100, loss: 0.04284035041928291\n","step: 1110, loss: 0.09448467940092087\n","step: 1120, loss: 0.032870322465896606\n","step: 1130, loss: 0.04005097225308418\n","step: 1140, loss: 0.08605260401964188\n","step: 1150, loss: 0.1117081493139267\n","step: 1160, loss: 0.08556147664785385\n","step: 1170, loss: 0.06311572343111038\n","step: 1180, loss: 0.0975869745016098\n","step: 1190, loss: 0.1017119511961937\n","step: 1200, loss: 0.18420939147472382\n","step: 1210, loss: 0.1311088353395462\n","step: 1220, loss: 0.1871490478515625\n","step: 1230, loss: 0.11099188029766083\n","step: 1240, loss: 0.12468533962965012\n","step: 1250, loss: 0.11841907352209091\n","step: 1260, loss: 0.0631696879863739\n","step: 1270, loss: 0.05786091834306717\n","step: 1280, loss: 0.06508568674325943\n","step: 1290, loss: 0.10242020338773727\n","step: 1300, loss: 0.1621481329202652\n","step: 1310, loss: 0.06225137040019035\n","step: 1320, loss: 0.13587212562561035\n","step: 1330, loss: 0.09166550636291504\n","step: 1340, loss: 0.1097545251250267\n","step: 1350, loss: 0.06234540417790413\n","step: 1360, loss: 0.09960483014583588\n","step: 1370, loss: 0.058365609496831894\n","step: 1380, loss: 0.10425121337175369\n","step: 1390, loss: 0.06931557506322861\n","step: 1400, loss: 0.1213841587305069\n","step: 1410, loss: 0.12964873015880585\n","step: 1420, loss: 0.09282505512237549\n","step: 1430, loss: 0.12244267016649246\n","step: 1440, loss: 0.060965124517679214\n","step: 1450, loss: 0.08569859713315964\n","step: 1460, loss: 0.07328441739082336\n","step: 1470, loss: 0.09191042929887772\n","step: 1480, loss: 0.1391981840133667\n","step: 1490, loss: 0.09851153939962387\n","step: 1500, loss: 0.05496394634246826\n","step: 1510, loss: 0.026376066729426384\n","step: 1520, loss: 0.20025932788848877\n","step: 1530, loss: 0.06875289231538773\n","step: 1540, loss: 0.18633441627025604\n","step: 1550, loss: 0.014274902641773224\n","step: 1560, loss: 0.08157829195261002\n","step: 1570, loss: 0.12785914540290833\n","step: 1580, loss: 0.10591310262680054\n","step: 1590, loss: 0.09350350499153137\n","step: 1600, loss: 0.11679301410913467\n","step: 1610, loss: 0.1173260360956192\n","step: 1620, loss: 0.06277409940958023\n","step: 1630, loss: 0.07345034927129745\n","step: 1640, loss: 0.10158655047416687\n","step: 1650, loss: 0.08397547900676727\n","step: 1660, loss: 0.0764518603682518\n","step: 1670, loss: 0.0627228170633316\n","step: 1680, loss: 0.06654994189739227\n","step: 1690, loss: 0.08934084326028824\n","step: 1700, loss: 0.05863501504063606\n","step: 1710, loss: 0.09703272581100464\n","step: 1720, loss: 0.13953515887260437\n","step: 1730, loss: 0.09953119605779648\n","step: 1740, loss: 0.11301803588867188\n","step: 1750, loss: 0.10156778991222382\n","step: 1760, loss: 0.0699925646185875\n","step: 1770, loss: 0.12607938051223755\n","step: 1780, loss: 0.08306241780519485\n","step: 1790, loss: 0.07900740206241608\n","step: 1800, loss: 0.07253523170948029\n","step: 1810, loss: 0.042413122951984406\n","step: 1820, loss: 0.11747750639915466\n","step: 1830, loss: 0.0888897106051445\n","step: 1840, loss: 0.04406772181391716\n","step: 1850, loss: 0.07578033208847046\n","step: 1860, loss: 0.045019809156656265\n","step: 1870, loss: 0.0752187967300415\n","step: 1880, loss: 0.1082121953368187\n","step: 1890, loss: 0.05758554860949516\n","step: 1900, loss: 0.18299151957035065\n","step: 1910, loss: 0.038969509303569794\n","step: 1920, loss: 0.052359096705913544\n","step: 1930, loss: 0.07542011141777039\n","step: 1940, loss: 0.024827973917126656\n","step: 1950, loss: 0.06376483291387558\n","step: 1960, loss: 0.04943615198135376\n","step: 1970, loss: 0.09256255626678467\n","step: 1980, loss: 0.10273049026727676\n","step: 1990, loss: 0.06058104336261749\n","step: 2000, loss: 0.06766688823699951\n","step: 2010, loss: 0.17367598414421082\n","step: 2020, loss: 0.11607196182012558\n","step: 2030, loss: 0.153376042842865\n","step: 2040, loss: 0.09584435075521469\n","step: 2050, loss: 0.045261628925800323\n","step: 2060, loss: 0.060642775148153305\n","step: 2070, loss: 0.09759104251861572\n","step: 2080, loss: 0.048900723457336426\n","step: 2090, loss: 0.12175438553094864\n","step: 2100, loss: 0.08805476129055023\n","step: 2110, loss: 0.04339982941746712\n","step: 2120, loss: 0.05635412037372589\n","step: 2130, loss: 0.12469948828220367\n","step: 2140, loss: 0.1273474544286728\n","step: 2150, loss: 0.1425236016511917\n","step: 2160, loss: 0.06565678119659424\n","step: 2170, loss: 0.0910302922129631\n","step: 2180, loss: 0.12992900609970093\n","step: 2190, loss: 0.06793113052845001\n","step: 2200, loss: 0.16268672049045563\n","step: 2210, loss: 0.04602959379553795\n","step: 2220, loss: 0.12446919083595276\n","step: 2230, loss: 0.019918030127882957\n","step: 2240, loss: 0.10466817766427994\n","step: 2250, loss: 0.04869493842124939\n","step: 2260, loss: 0.08382197469472885\n","step: 2270, loss: 0.11044418066740036\n","step: 2280, loss: 0.08262873440980911\n","step: 2290, loss: 0.13301678001880646\n","step: 2300, loss: 0.038026854395866394\n","step: 2310, loss: 0.09869030117988586\n","step: 2320, loss: 0.11307350546121597\n","step: 2330, loss: 0.10049115866422653\n","step: 2340, loss: 0.07503443956375122\n","step: 2350, loss: 0.04511028155684471\n","step: 2360, loss: 0.06274090707302094\n","step: 2370, loss: 0.13062337040901184\n","step: 2380, loss: 0.11735569685697556\n","step: 2390, loss: 0.05482976883649826\n","step: 2400, loss: 0.035833004862070084\n","step: 2410, loss: 0.04168727993965149\n","step: 2420, loss: 0.06487702578306198\n","step: 2430, loss: 0.08080050349235535\n","step: 2440, loss: 0.06705628335475922\n","step: 2450, loss: 0.07526712119579315\n","step: 2460, loss: 0.11915472149848938\n","step: 2470, loss: 0.11876611411571503\n","step: 2480, loss: 0.15818080306053162\n","step: 2490, loss: 0.18023180961608887\n","step: 2500, loss: 0.1455504298210144\n","step: 2510, loss: 0.24057017266750336\n","step: 2520, loss: 0.09967663139104843\n","step: 2530, loss: 0.12281173467636108\n","step: 2540, loss: 0.02863389626145363\n","step: 2550, loss: 0.19088809192180634\n","step: 2560, loss: 0.10045051574707031\n","step: 2570, loss: 0.1340205818414688\n","step: 2580, loss: 0.10868745297193527\n","step: 2590, loss: 0.0407954566180706\n","step: 2600, loss: 0.12192680686712265\n","step: 2610, loss: 0.11179693788290024\n","step: 2620, loss: 0.06919007003307343\n","step: 2630, loss: 0.08458492159843445\n","step: 2640, loss: 0.03907306119799614\n","step: 2650, loss: 0.10173166543245316\n","step: 2660, loss: 0.12262089550495148\n","step: 2670, loss: 0.12912170588970184\n","step: 2680, loss: 0.04932091757655144\n","step: 2690, loss: 0.025061991065740585\n","step: 2700, loss: 0.14547531306743622\n","step: 2710, loss: 0.1429559588432312\n","step: 2720, loss: 0.06149141490459442\n","step: 2730, loss: 0.07253295183181763\n","step: 2740, loss: 0.09413174539804459\n","step: 2750, loss: 0.1956074982881546\n","step: 2760, loss: 0.09132381528615952\n","step: 2770, loss: 0.10042332112789154\n","step: 2780, loss: 0.12111496180295944\n","step: 2790, loss: 0.128067746758461\n","step: 2800, loss: 0.08153879642486572\n","step: 2810, loss: 0.0870445966720581\n","step: 2820, loss: 0.17638972401618958\n","step: 2830, loss: 0.06745104491710663\n","step: 2840, loss: 0.060750387609004974\n","step: 2850, loss: 0.08146738260984421\n","step: 2860, loss: 0.06130613014101982\n","step: 2870, loss: 0.03854570910334587\n","step: 2880, loss: 0.06012400984764099\n","step: 2890, loss: 0.10122658312320709\n","step: 2900, loss: 0.13287384808063507\n","step: 2910, loss: 0.10367503017187119\n","step: 2920, loss: 0.07940320670604706\n","step: 2930, loss: 0.06610441207885742\n","step: 2940, loss: 0.037720225751399994\n","step: 2950, loss: 0.14447875320911407\n","step: 2960, loss: 0.11059682816267014\n","step: 2970, loss: 0.09860631823539734\n","step: 2980, loss: 0.04497325420379639\n","step: 2990, loss: 0.051538873463869095\n","step: 3000, loss: 0.12531127035617828\n","step: 3010, loss: 0.08290335536003113\n","step: 3020, loss: 0.04682216793298721\n","step: 3030, loss: 0.09328796714544296\n","step: 3040, loss: 0.10570498555898666\n","step: 3050, loss: 0.060736339539289474\n","step: 3060, loss: 0.05409453064203262\n","step: 3070, loss: 0.09920714795589447\n","step: 3080, loss: 0.19465892016887665\n","step: 3090, loss: 0.07757921516895294\n","step: 3100, loss: 0.07514352351427078\n","step: 3110, loss: 0.16061018407344818\n","step: 3120, loss: 0.04793689772486687\n","step: 3130, loss: 0.07376904040575027\n","step: 3140, loss: 0.06710190325975418\n","step: 3150, loss: 0.15734845399856567\n","step: 3160, loss: 0.1355394423007965\n","step: 3170, loss: 0.06896248459815979\n","step: 3180, loss: 0.07309798151254654\n","step: 3190, loss: 0.1527860164642334\n","step: 3200, loss: 0.12054621428251266\n","step: 3210, loss: 0.0420965813100338\n","step: 3220, loss: 0.046126145869493484\n","step: 3230, loss: 0.05397002771496773\n","step: 3240, loss: 0.03993809223175049\n","step: 3250, loss: 0.1010955348610878\n","step: 3260, loss: 0.13554880023002625\n","step: 3270, loss: 0.16111542284488678\n","step: 3280, loss: 0.028622301295399666\n","step: 3290, loss: 0.11781817674636841\n","step: 3300, loss: 0.15089792013168335\n","step: 3310, loss: 0.14502206444740295\n","step: 3320, loss: 0.12498144805431366\n","step: 3330, loss: 0.09734444320201874\n","step: 3340, loss: 0.044560596346855164\n","step: 3350, loss: 0.020095568150281906\n","step: 3360, loss: 0.06944411993026733\n","step: 3370, loss: 0.11422819644212723\n","step: 3380, loss: 0.12429209053516388\n","step: 3390, loss: 0.09905406832695007\n","step: 3400, loss: 0.05929622799158096\n","step: 3410, loss: 0.07983558624982834\n","step: 3420, loss: 0.08459039032459259\n","step: 3430, loss: 0.06924914568662643\n","step: 3440, loss: 0.10251187533140182\n","step: 3450, loss: 0.09208069741725922\n","step: 3460, loss: 0.07693760842084885\n","step: 3470, loss: 0.040185000747442245\n","step: 3480, loss: 0.019497673958539963\n","step: 3490, loss: 0.11600198596715927\n","step: 3500, loss: 0.08676264435052872\n","step: 3510, loss: 0.12858198583126068\n","step: 3520, loss: 0.0950838029384613\n","step: 3530, loss: 0.11766917258501053\n","step: 3540, loss: 0.09682270139455795\n","step: 3550, loss: 0.23849022388458252\n","step: 3560, loss: 0.15919682383537292\n","step: 3570, loss: 0.12989722192287445\n","step: 3580, loss: 0.27288681268692017\n","step: 3590, loss: 0.08088365197181702\n","step: 3600, loss: 0.10634209960699081\n","step: 3610, loss: 0.13960132002830505\n","step: 3620, loss: 0.052661970257759094\n","step: 3630, loss: 0.06470151990652084\n","step: 3640, loss: 0.12203653901815414\n","step: 3650, loss: 0.07536154985427856\n","step: 3660, loss: 0.04550530016422272\n","step: 3670, loss: 0.04807441309094429\n","step: 3680, loss: 0.052002739161252975\n","step: 3690, loss: 0.08087242394685745\n","step: 3700, loss: 0.05518681928515434\n","step: 3710, loss: 0.057321082800626755\n","step: 3720, loss: 0.1697053611278534\n","step: 3730, loss: 0.09448911994695663\n","step: 3740, loss: 0.051128339022397995\n","step: 3750, loss: 0.12159695476293564\n","step: 3760, loss: 0.05617334693670273\n","step: 3770, loss: 0.027727384120225906\n","step: 3780, loss: 0.09866338968276978\n","step: 3790, loss: 0.07876323163509369\n","step: 3800, loss: 0.08583636581897736\n","step: 3810, loss: 0.06819739937782288\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.73      1.00      0.84        35\n","           2       0.38      0.13      0.19        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.96      0.84      0.89       291\n","           5       0.89      0.84      0.86       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.44      0.94      0.60       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.95      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.42      0.62      0.50        13\n","          14       0.47      1.00      0.64        43\n","          15       0.93      0.99      0.96      2778\n","          16       0.78      0.88      0.83      1151\n","          17       0.93      0.90      0.91        41\n","          18       0.94      0.97      0.95        32\n","          19       0.78      0.78      0.78        40\n","          20       0.97      1.00      0.98       584\n","          21       0.13      0.23      0.17        52\n","          22       0.96      0.71      0.82      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.27      0.77      0.40        44\n","          25       0.85      0.90      0.88       888\n","          26       0.90      1.00      0.95         9\n","          27       0.97      0.97      0.97        69\n","          28       0.99      1.00      0.99      1864\n","          29       0.99      0.98      0.99       344\n","          30       0.97      0.79      0.87      1136\n","          31       0.54      0.74      0.62        19\n","          32       1.00      0.75      0.86         8\n","          33       0.60      0.97      0.74        86\n","          34       0.24      0.56      0.34        32\n","          35       0.99      0.98      0.98       474\n","          36       0.86      0.10      0.19       182\n","          37       0.90      0.95      0.92      1592\n","          38       0.97      0.98      0.98       404\n","          39       0.96      0.94      0.95       485\n","          40       0.94      0.84      0.89       573\n","          41       0.95      0.94      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.95      0.74      0.83       152\n","          44       0.88      0.92      0.90        75\n","          46       1.00      0.99      0.99        82\n","          48       0.67      0.20      0.31        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.82      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 460\n","\n","Loop 20\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9132847785949707\n","step: 10, loss: 1.807345986366272\n","step: 20, loss: 0.7086188793182373\n","step: 30, loss: 0.42111682891845703\n","step: 40, loss: 0.23415933549404144\n","step: 50, loss: 0.20486943423748016\n","step: 60, loss: 0.2645816504955292\n","step: 70, loss: 0.1461145430803299\n","step: 80, loss: 0.13646213710308075\n","step: 90, loss: 0.12115296721458435\n","step: 100, loss: 0.28781768679618835\n","step: 110, loss: 0.18952211737632751\n","step: 120, loss: 0.19143354892730713\n","step: 130, loss: 0.16268666088581085\n","step: 140, loss: 0.16852818429470062\n","step: 150, loss: 0.1475127935409546\n","step: 160, loss: 0.16313420236110687\n","step: 170, loss: 0.12855976819992065\n","step: 180, loss: 0.24178668856620789\n","step: 190, loss: 0.19800151884555817\n","step: 200, loss: 0.09162814915180206\n","step: 210, loss: 0.02297055721282959\n","step: 220, loss: 0.08016464114189148\n","step: 230, loss: 0.09890475869178772\n","step: 240, loss: 0.09005188941955566\n","step: 250, loss: 0.13580770790576935\n","step: 260, loss: 0.03598422929644585\n","step: 270, loss: 0.11704138666391373\n","step: 280, loss: 0.14832860231399536\n","step: 290, loss: 0.0819164365530014\n","step: 300, loss: 0.15163733065128326\n","step: 310, loss: 0.13095161318778992\n","step: 320, loss: 0.20482957363128662\n","step: 330, loss: 0.32761648297309875\n","step: 340, loss: 0.08667457848787308\n","step: 350, loss: 0.1470944732427597\n","step: 360, loss: 0.04130418971180916\n","step: 370, loss: 0.09487781673669815\n","step: 380, loss: 0.1253572702407837\n","step: 390, loss: 0.044336289167404175\n","step: 400, loss: 0.1243683323264122\n","step: 410, loss: 0.061126306653022766\n","step: 420, loss: 0.06516499072313309\n","step: 430, loss: 0.042238540947437286\n","step: 440, loss: 0.06528685986995697\n","step: 450, loss: 0.1096729263663292\n","step: 460, loss: 0.08215519040822983\n","step: 470, loss: 0.04643404111266136\n","step: 480, loss: 0.09063948690891266\n","step: 490, loss: 0.07267273962497711\n","step: 500, loss: 0.10124725848436356\n","step: 510, loss: 0.05527033656835556\n","step: 520, loss: 0.10390234738588333\n","step: 530, loss: 0.08231370151042938\n","step: 540, loss: 0.12367245554924011\n","step: 550, loss: 0.12625591456890106\n","step: 560, loss: 0.12584471702575684\n","step: 570, loss: 0.1206759586930275\n","step: 580, loss: 0.1982170194387436\n","step: 590, loss: 0.08945098519325256\n","step: 600, loss: 0.1487303078174591\n","step: 610, loss: 0.1109144538640976\n","step: 620, loss: 0.09619505703449249\n","step: 630, loss: 0.08165907859802246\n","step: 640, loss: 0.12368449568748474\n","step: 650, loss: 0.10471440851688385\n","step: 660, loss: 0.07736733555793762\n","step: 670, loss: 0.07627732306718826\n","step: 680, loss: 0.08228582888841629\n","step: 690, loss: 0.15282979607582092\n","step: 700, loss: 0.09751398861408234\n","step: 710, loss: 0.08320623636245728\n","step: 720, loss: 0.16389864683151245\n","step: 730, loss: 0.12230260670185089\n","step: 740, loss: 0.2711198329925537\n","step: 750, loss: 0.12922683358192444\n","step: 760, loss: 0.08117489516735077\n","step: 770, loss: 0.08770277351140976\n","step: 780, loss: 0.10512618720531464\n","step: 790, loss: 0.12822236120700836\n","step: 800, loss: 0.07720886915922165\n","step: 810, loss: 0.0655493214726448\n","step: 820, loss: 0.12166538089513779\n","step: 830, loss: 0.07028720527887344\n","step: 840, loss: 0.23093253374099731\n","step: 850, loss: 0.06117277219891548\n","step: 860, loss: 0.053287915885448456\n","step: 870, loss: 0.0893503949046135\n","step: 880, loss: 0.06702364236116409\n","step: 890, loss: 0.060655683279037476\n","step: 900, loss: 0.09104594588279724\n","step: 910, loss: 0.07169033586978912\n","step: 920, loss: 0.020314278081059456\n","step: 930, loss: 0.09517009556293488\n","step: 940, loss: 0.12978360056877136\n","step: 950, loss: 0.0686689019203186\n","step: 960, loss: 0.03029775619506836\n","step: 970, loss: 0.029093075543642044\n","step: 980, loss: 0.14467203617095947\n","step: 990, loss: 0.1547314077615738\n","step: 1000, loss: 0.11653214693069458\n","step: 1010, loss: 0.11479596048593521\n","step: 1020, loss: 0.028428614139556885\n","step: 1030, loss: 0.0797228217124939\n","step: 1040, loss: 0.09547899663448334\n","step: 1050, loss: 0.21104827523231506\n","step: 1060, loss: 0.08750813454389572\n","step: 1070, loss: 0.09919372200965881\n","step: 1080, loss: 0.11859466135501862\n","step: 1090, loss: 0.03758735582232475\n","step: 1100, loss: 0.10826990753412247\n","step: 1110, loss: 0.13492509722709656\n","step: 1120, loss: 0.10372533649206161\n","step: 1130, loss: 0.03872630000114441\n","step: 1140, loss: 0.043098676949739456\n","step: 1150, loss: 0.06354588270187378\n","step: 1160, loss: 0.04476569965481758\n","step: 1170, loss: 0.11970595270395279\n","step: 1180, loss: 0.1813725382089615\n","step: 1190, loss: 0.1078229695558548\n","step: 1200, loss: 0.13520140945911407\n","step: 1210, loss: 0.13932007551193237\n","step: 1220, loss: 0.0651949867606163\n","step: 1230, loss: 0.14860913157463074\n","step: 1240, loss: 0.11024879664182663\n","step: 1250, loss: 0.1347951740026474\n","step: 1260, loss: 0.253871351480484\n","step: 1270, loss: 0.03729136288166046\n","step: 1280, loss: 0.1318841576576233\n","step: 1290, loss: 0.16837258636951447\n","step: 1300, loss: 0.08994659036397934\n","step: 1310, loss: 0.08089740574359894\n","step: 1320, loss: 0.16571572422981262\n","step: 1330, loss: 0.13769932091236115\n","step: 1340, loss: 0.10968974977731705\n","step: 1350, loss: 0.0879475548863411\n","step: 1360, loss: 0.15106070041656494\n","step: 1370, loss: 0.07631386071443558\n","step: 1380, loss: 0.13187678158283234\n","step: 1390, loss: 0.02767890691757202\n","step: 1400, loss: 0.18980129063129425\n","step: 1410, loss: 0.1247795820236206\n","step: 1420, loss: 0.17212648689746857\n","step: 1430, loss: 0.10331365466117859\n","step: 1440, loss: 0.11441531032323837\n","step: 1450, loss: 0.06475566327571869\n","step: 1460, loss: 0.10520923137664795\n","step: 1470, loss: 0.15430793166160583\n","step: 1480, loss: 0.12033937871456146\n","step: 1490, loss: 0.07501830160617828\n","step: 1500, loss: 0.09323705732822418\n","step: 1510, loss: 0.056625280529260635\n","step: 1520, loss: 0.10769125074148178\n","step: 1530, loss: 0.10311879962682724\n","step: 1540, loss: 0.05253371223807335\n","step: 1550, loss: 0.06907644867897034\n","step: 1560, loss: 0.05457831174135208\n","step: 1570, loss: 0.04668841511011124\n","step: 1580, loss: 0.07914099097251892\n","step: 1590, loss: 0.05996185541152954\n","step: 1600, loss: 0.08590711653232574\n","step: 1610, loss: 0.01832656003534794\n","step: 1620, loss: 0.07584882527589798\n","step: 1630, loss: 0.02369404211640358\n","step: 1640, loss: 0.1264697015285492\n","step: 1650, loss: 0.06927759200334549\n","step: 1660, loss: 0.08096208423376083\n","step: 1670, loss: 0.14529457688331604\n","step: 1680, loss: 0.05670204758644104\n","step: 1690, loss: 0.10480162501335144\n","step: 1700, loss: 0.07867510616779327\n","step: 1710, loss: 0.11379951983690262\n","step: 1720, loss: 0.08033867925405502\n","step: 1730, loss: 0.06660467386245728\n","step: 1740, loss: 0.11094929277896881\n","step: 1750, loss: 0.08500274270772934\n","step: 1760, loss: 0.07629694789648056\n","step: 1770, loss: 0.05673521012067795\n","step: 1780, loss: 0.08872319012880325\n","step: 1790, loss: 0.08737963438034058\n","step: 1800, loss: 0.11307435482740402\n","step: 1810, loss: 0.06163698807358742\n","step: 1820, loss: 0.0723300576210022\n","step: 1830, loss: 0.08656005561351776\n","step: 1840, loss: 0.21951109170913696\n","step: 1850, loss: 0.06513375043869019\n","step: 1860, loss: 0.12288772314786911\n","step: 1870, loss: 0.12739676237106323\n","step: 1880, loss: 0.2625144124031067\n","step: 1890, loss: 0.0641205832362175\n","step: 1900, loss: 0.08559706062078476\n","step: 1910, loss: 0.07466894388198853\n","step: 1920, loss: 0.041954636573791504\n","step: 1930, loss: 0.15837028622627258\n","step: 1940, loss: 0.07607978582382202\n","step: 1950, loss: 0.10222545266151428\n","step: 1960, loss: 0.04165775701403618\n","step: 1970, loss: 0.038534730672836304\n","step: 1980, loss: 0.06161534786224365\n","step: 1990, loss: 0.15697365999221802\n","step: 2000, loss: 0.09219551831483841\n","step: 2010, loss: 0.047518711537122726\n","step: 2020, loss: 0.06081337854266167\n","step: 2030, loss: 0.06104649230837822\n","step: 2040, loss: 0.09316743165254593\n","step: 2050, loss: 0.07231810688972473\n","step: 2060, loss: 0.09680213034152985\n","step: 2070, loss: 0.07045488059520721\n","step: 2080, loss: 0.045631565153598785\n","step: 2090, loss: 0.10112043470144272\n","step: 2100, loss: 0.09541607648134232\n","step: 2110, loss: 0.17159125208854675\n","step: 2120, loss: 0.080128513276577\n","step: 2130, loss: 0.13272814452648163\n","step: 2140, loss: 0.17256762087345123\n","step: 2150, loss: 0.08851318061351776\n","step: 2160, loss: 0.1396518051624298\n","step: 2170, loss: 0.06914187967777252\n","step: 2180, loss: 0.08766733109951019\n","step: 2190, loss: 0.09762727469205856\n","step: 2200, loss: 0.1230015829205513\n","step: 2210, loss: 0.12140128016471863\n","step: 2220, loss: 0.07595641911029816\n","step: 2230, loss: 0.11016062647104263\n","step: 2240, loss: 0.04984694719314575\n","step: 2250, loss: 0.08640134334564209\n","step: 2260, loss: 0.09425327926874161\n","step: 2270, loss: 0.1322559118270874\n","step: 2280, loss: 0.09587080776691437\n","step: 2290, loss: 0.08683446049690247\n","step: 2300, loss: 0.06805088371038437\n","step: 2310, loss: 0.0680808424949646\n","step: 2320, loss: 0.12676800787448883\n","step: 2330, loss: 0.1641140878200531\n","step: 2340, loss: 0.08016068488359451\n","step: 2350, loss: 0.05069225654006004\n","step: 2360, loss: 0.05683986097574234\n","step: 2370, loss: 0.13209760189056396\n","step: 2380, loss: 0.08080977946519852\n","step: 2390, loss: 0.07784007489681244\n","step: 2400, loss: 0.058687563985586166\n","step: 2410, loss: 0.032427381724119186\n","step: 2420, loss: 0.04711939021945\n","step: 2430, loss: 0.025398924946784973\n","step: 2440, loss: 0.0258653461933136\n","step: 2450, loss: 0.08007139712572098\n","step: 2460, loss: 0.07245587557554245\n","step: 2470, loss: 0.06939569115638733\n","step: 2480, loss: 0.02842829003930092\n","step: 2490, loss: 0.08355612307786942\n","step: 2500, loss: 0.08258981257677078\n","step: 2510, loss: 0.0943828895688057\n","step: 2520, loss: 0.05184280872344971\n","step: 2530, loss: 0.07207832485437393\n","step: 2540, loss: 0.0701172947883606\n","step: 2550, loss: 0.06537343561649323\n","step: 2560, loss: 0.01574755273759365\n","step: 2570, loss: 0.12546071410179138\n","step: 2580, loss: 0.030113721266388893\n","step: 2590, loss: 0.031967341899871826\n","step: 2600, loss: 0.1543440818786621\n","step: 2610, loss: 0.09978195279836655\n","step: 2620, loss: 0.07469980418682098\n","step: 2630, loss: 0.14397834241390228\n","step: 2640, loss: 0.07478111982345581\n","step: 2650, loss: 0.03785993158817291\n","step: 2660, loss: 0.1599733978509903\n","step: 2670, loss: 0.06802330166101456\n","step: 2680, loss: 0.05178948864340782\n","step: 2690, loss: 0.08083148300647736\n","step: 2700, loss: 0.09863230586051941\n","step: 2710, loss: 0.05608101561665535\n","step: 2720, loss: 0.03957083448767662\n","step: 2730, loss: 0.05329792946577072\n","step: 2740, loss: 0.058960579335689545\n","step: 2750, loss: 0.12670224905014038\n","step: 2760, loss: 0.058035124093294144\n","step: 2770, loss: 0.06190889701247215\n","step: 2780, loss: 0.07647781819105148\n","step: 2790, loss: 0.03658539056777954\n","step: 2800, loss: 0.11254560202360153\n","step: 2810, loss: 0.05680067092180252\n","step: 2820, loss: 0.1024589091539383\n","step: 2830, loss: 0.0650053471326828\n","step: 2840, loss: 0.019747955724596977\n","step: 2850, loss: 0.06806183606386185\n","step: 2860, loss: 0.11541783064603806\n","step: 2870, loss: 0.08857642114162445\n","step: 2880, loss: 0.0735931396484375\n","step: 2890, loss: 0.061578646302223206\n","step: 2900, loss: 0.14861732721328735\n","step: 2910, loss: 0.04635819047689438\n","step: 2920, loss: 0.0935475155711174\n","step: 2930, loss: 0.22013352811336517\n","step: 2940, loss: 0.12245535850524902\n","step: 2950, loss: 0.024393979460000992\n","step: 2960, loss: 0.10287632048130035\n","step: 2970, loss: 0.12116088718175888\n","step: 2980, loss: 0.055378302931785583\n","step: 2990, loss: 0.09384512901306152\n","step: 3000, loss: 0.15224286913871765\n","step: 3010, loss: 0.1058361604809761\n","step: 3020, loss: 0.14117993414402008\n","step: 3030, loss: 0.1023608073592186\n","step: 3040, loss: 0.056589867919683456\n","step: 3050, loss: 0.08085227012634277\n","step: 3060, loss: 0.14439977705478668\n","step: 3070, loss: 0.08584382385015488\n","step: 3080, loss: 0.2452978640794754\n","step: 3090, loss: 0.021085966378450394\n","step: 3100, loss: 0.06742222607135773\n","step: 3110, loss: 0.03397022560238838\n","step: 3120, loss: 0.16867488622665405\n","step: 3130, loss: 0.023705117404460907\n","step: 3140, loss: 0.07392227649688721\n","step: 3150, loss: 0.046487290412187576\n","step: 3160, loss: 0.050613824278116226\n","step: 3170, loss: 0.1286875158548355\n","step: 3180, loss: 0.10392516106367111\n","step: 3190, loss: 0.08127199113368988\n","step: 3200, loss: 0.09333294630050659\n","step: 3210, loss: 0.06975037604570389\n","step: 3220, loss: 0.14018109440803528\n","step: 3230, loss: 0.12474529445171356\n","step: 3240, loss: 0.13702212274074554\n","step: 3250, loss: 0.1055159792304039\n","step: 3260, loss: 0.03485512733459473\n","step: 3270, loss: 0.09852392226457596\n","step: 3280, loss: 0.09142446517944336\n","step: 3290, loss: 0.1077459380030632\n","step: 3300, loss: 0.05569710582494736\n","step: 3310, loss: 0.03525019437074661\n","step: 3320, loss: 0.05902852118015289\n","step: 3330, loss: 0.07853332161903381\n","step: 3340, loss: 0.13241787254810333\n","step: 3350, loss: 0.03258292004466057\n","step: 3360, loss: 0.08516697585582733\n","step: 3370, loss: 0.05892728269100189\n","step: 3380, loss: 0.09744750708341599\n","step: 3390, loss: 0.09400186687707901\n","step: 3400, loss: 0.03691209480166435\n","step: 3410, loss: 0.03647270053625107\n","step: 3420, loss: 0.09885223954916\n","step: 3430, loss: 0.13689646124839783\n","step: 3440, loss: 0.06794878095388412\n","step: 3450, loss: 0.104354627430439\n","step: 3460, loss: 0.09203954041004181\n","step: 3470, loss: 0.14683008193969727\n","step: 3480, loss: 0.03512588143348694\n","step: 3490, loss: 0.07178209722042084\n","step: 3500, loss: 0.13111737370491028\n","step: 3510, loss: 0.09038018435239792\n","step: 3520, loss: 0.03500669077038765\n","step: 3530, loss: 0.07940506935119629\n","step: 3540, loss: 0.0717528909444809\n","step: 3550, loss: 0.09545381367206573\n","step: 3560, loss: 0.14243920147418976\n","step: 3570, loss: 0.039241623133420944\n","step: 3580, loss: 0.2099112719297409\n","step: 3590, loss: 0.11125194281339645\n","step: 3600, loss: 0.08058737963438034\n","step: 3610, loss: 0.07728885859251022\n","step: 3620, loss: 0.11974826455116272\n","step: 3630, loss: 0.0832962617278099\n","step: 3640, loss: 0.09120968729257584\n","step: 3650, loss: 0.09782376885414124\n","step: 3660, loss: 0.053147900849580765\n","step: 3670, loss: 0.049452878534793854\n","step: 3680, loss: 0.27628064155578613\n","step: 3690, loss: 0.13495133817195892\n","step: 3700, loss: 0.14616668224334717\n","step: 3710, loss: 0.05243365466594696\n","step: 3720, loss: 0.1315101534128189\n","step: 3730, loss: 0.16179285943508148\n","step: 3740, loss: 0.060008455067873\n","step: 3750, loss: 0.15576422214508057\n","step: 3760, loss: 0.07241888344287872\n","step: 3770, loss: 0.15395748615264893\n","step: 3780, loss: 0.05499895662069321\n","step: 3790, loss: 0.08681926876306534\n","step: 3800, loss: 0.05119124427437782\n","step: 3810, loss: 0.19959664344787598\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.88      1.00      0.93        35\n","           2       0.82      0.91      0.86        77\n","           3       0.99      0.80      0.88      1030\n","           4       0.89      0.84      0.86       291\n","           5       0.97      0.84      0.90       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.58      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.93      0.97      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.64      0.69      0.67        13\n","          14       0.26      1.00      0.41        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.84      0.83      0.83      1151\n","          17       0.91      0.95      0.93        41\n","          18       0.94      0.97      0.95        32\n","          19       0.77      0.57      0.66        40\n","          20       1.00      0.99      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.76      0.84      4175\n","          23       0.72      0.95      0.82      2253\n","          24       0.31      0.55      0.39        44\n","          25       0.80      0.93      0.86       888\n","          26       1.00      1.00      1.00         9\n","          27       0.91      0.97      0.94        69\n","          28       0.98      0.98      0.98      1864\n","          29       0.94      0.99      0.96       344\n","          30       0.91      0.90      0.91      1136\n","          31       0.60      0.63      0.62        19\n","          32       0.86      0.75      0.80         8\n","          33       0.82      0.87      0.85        86\n","          34       0.07      0.12      0.09        32\n","          35       0.98      0.99      0.99       474\n","          36       0.78      0.21      0.33       182\n","          37       0.90      0.96      0.93      1592\n","          38       0.94      0.97      0.96       404\n","          39       0.98      0.89      0.94       485\n","          40       0.90      0.93      0.91       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.98      0.99       575\n","          43       0.96      0.86      0.91       152\n","          44       0.88      0.93      0.90        75\n","          46       1.00      0.98      0.99        82\n","          48       0.86      0.30      0.45        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.82      0.80     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 450\n","\n","Loop 21\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9896697998046875\n","step: 10, loss: 1.8846784830093384\n","step: 20, loss: 0.8624448776245117\n","step: 30, loss: 0.5587204098701477\n","step: 40, loss: 0.389956533908844\n","step: 50, loss: 0.2493620067834854\n","step: 60, loss: 0.18079142272472382\n","step: 70, loss: 0.1528192162513733\n","step: 80, loss: 0.12847131490707397\n","step: 90, loss: 0.12568405270576477\n","step: 100, loss: 0.15704526007175446\n","step: 110, loss: 0.10345528274774551\n","step: 120, loss: 0.2872602343559265\n","step: 130, loss: 0.16904321312904358\n","step: 140, loss: 0.0967603325843811\n","step: 150, loss: 0.2188660204410553\n","step: 160, loss: 0.20297937095165253\n","step: 170, loss: 0.21141228079795837\n","step: 180, loss: 0.20063988864421844\n","step: 190, loss: 0.18996533751487732\n","step: 200, loss: 0.13780808448791504\n","step: 210, loss: 0.09000132232904434\n","step: 220, loss: 0.08703868836164474\n","step: 230, loss: 0.12351173907518387\n","step: 240, loss: 0.14363998174667358\n","step: 250, loss: 0.0882001519203186\n","step: 260, loss: 0.04256143048405647\n","step: 270, loss: 0.08341079205274582\n","step: 280, loss: 0.12136541306972504\n","step: 290, loss: 0.051389362663030624\n","step: 300, loss: 0.09827322512865067\n","step: 310, loss: 0.13484178483486176\n","step: 320, loss: 0.07149182260036469\n","step: 330, loss: 0.08961031585931778\n","step: 340, loss: 0.12400950491428375\n","step: 350, loss: 0.16735109686851501\n","step: 360, loss: 0.07564849406480789\n","step: 370, loss: 0.127949520945549\n","step: 380, loss: 0.06182902306318283\n","step: 390, loss: 0.12027774751186371\n","step: 400, loss: 0.07913234829902649\n","step: 410, loss: 0.07097257673740387\n","step: 420, loss: 0.1482936590909958\n","step: 430, loss: 0.13222938776016235\n","step: 440, loss: 0.10317857563495636\n","step: 450, loss: 0.24878855049610138\n","step: 460, loss: 0.07521916180849075\n","step: 470, loss: 0.14876797795295715\n","step: 480, loss: 0.04426049813628197\n","step: 490, loss: 0.14271320402622223\n","step: 500, loss: 0.3113410472869873\n","step: 510, loss: 0.17803046107292175\n","step: 520, loss: 0.09065554291009903\n","step: 530, loss: 0.027587400749325752\n","step: 540, loss: 0.04890213906764984\n","step: 550, loss: 0.09395090490579605\n","step: 560, loss: 0.08253182470798492\n","step: 570, loss: 0.0667375847697258\n","step: 580, loss: 0.03940928727388382\n","step: 590, loss: 0.04600832983851433\n","step: 600, loss: 0.15379144251346588\n","step: 610, loss: 0.04524673521518707\n","step: 620, loss: 0.08069836348295212\n","step: 630, loss: 0.16506071388721466\n","step: 640, loss: 0.2512361705303192\n","step: 650, loss: 0.08055952191352844\n","step: 660, loss: 0.05167875438928604\n","step: 670, loss: 0.2876918911933899\n","step: 680, loss: 0.04737536981701851\n","step: 690, loss: 0.1574523001909256\n","step: 700, loss: 0.1010843962430954\n","step: 710, loss: 0.029078399762511253\n","step: 720, loss: 0.08945348858833313\n","step: 730, loss: 0.0460197739303112\n","step: 740, loss: 0.07458022236824036\n","step: 750, loss: 0.05713175609707832\n","step: 760, loss: 0.08136095851659775\n","step: 770, loss: 0.06599332392215729\n","step: 780, loss: 0.08857168257236481\n","step: 790, loss: 0.06900422275066376\n","step: 800, loss: 0.09925317019224167\n","step: 810, loss: 0.029322031885385513\n","step: 820, loss: 0.05658363178372383\n","step: 830, loss: 0.13455519080162048\n","step: 840, loss: 0.10259085148572922\n","step: 850, loss: 0.11050964146852493\n","step: 860, loss: 0.0764169916510582\n","step: 870, loss: 0.13053067028522491\n","step: 880, loss: 0.08269347250461578\n","step: 890, loss: 0.09843581169843674\n","step: 900, loss: 0.06665092706680298\n","step: 910, loss: 0.13324764370918274\n","step: 920, loss: 0.2834477126598358\n","step: 930, loss: 0.1640772968530655\n","step: 940, loss: 0.07958223670721054\n","step: 950, loss: 0.19447290897369385\n","step: 960, loss: 0.07647311687469482\n","step: 970, loss: 0.06490432471036911\n","step: 980, loss: 0.19283203780651093\n","step: 990, loss: 0.06411479413509369\n","step: 1000, loss: 0.03814615309238434\n","step: 1010, loss: 0.05826917663216591\n","step: 1020, loss: 0.1384515017271042\n","step: 1030, loss: 0.07808061689138412\n","step: 1040, loss: 0.04674998298287392\n","step: 1050, loss: 0.16220566630363464\n","step: 1060, loss: 0.07701543718576431\n","step: 1070, loss: 0.24321985244750977\n","step: 1080, loss: 0.12525001168251038\n","step: 1090, loss: 0.05133726820349693\n","step: 1100, loss: 0.1329856514930725\n","step: 1110, loss: 0.09124302864074707\n","step: 1120, loss: 0.0311488788574934\n","step: 1130, loss: 0.10544322431087494\n","step: 1140, loss: 0.11475439369678497\n","step: 1150, loss: 0.07745082676410675\n","step: 1160, loss: 0.10413841158151627\n","step: 1170, loss: 0.10258659720420837\n","step: 1180, loss: 0.11078597605228424\n","step: 1190, loss: 0.14699086546897888\n","step: 1200, loss: 0.07538924366235733\n","step: 1210, loss: 0.12141048908233643\n","step: 1220, loss: 0.08752521872520447\n","step: 1230, loss: 0.053015414625406265\n","step: 1240, loss: 0.12395402044057846\n","step: 1250, loss: 0.09325885772705078\n","step: 1260, loss: 0.05741333216428757\n","step: 1270, loss: 0.08484867960214615\n","step: 1280, loss: 0.08131042867898941\n","step: 1290, loss: 0.031293705105781555\n","step: 1300, loss: 0.046923112124204636\n","step: 1310, loss: 0.04865625873208046\n","step: 1320, loss: 0.060448046773672104\n","step: 1330, loss: 0.045580554753541946\n","step: 1340, loss: 0.11747533082962036\n","step: 1350, loss: 0.07944516837596893\n","step: 1360, loss: 0.0773136168718338\n","step: 1370, loss: 0.09153798222541809\n","step: 1380, loss: 0.04585792124271393\n","step: 1390, loss: 0.05166041851043701\n","step: 1400, loss: 0.07377831637859344\n","step: 1410, loss: 0.07129072397947311\n","step: 1420, loss: 0.2078413963317871\n","step: 1430, loss: 0.10263921320438385\n","step: 1440, loss: 0.06037232279777527\n","step: 1450, loss: 0.06781268864870071\n","step: 1460, loss: 0.09213398396968842\n","step: 1470, loss: 0.12197902053594589\n","step: 1480, loss: 0.14869128167629242\n","step: 1490, loss: 0.14929838478565216\n","step: 1500, loss: 0.1525324434041977\n","step: 1510, loss: 0.14586859941482544\n","step: 1520, loss: 0.1727629452943802\n","step: 1530, loss: 0.058450207114219666\n","step: 1540, loss: 0.053034503012895584\n","step: 1550, loss: 0.061949845403432846\n","step: 1560, loss: 0.19574059545993805\n","step: 1570, loss: 0.07890880852937698\n","step: 1580, loss: 0.08690348267555237\n","step: 1590, loss: 0.04640457034111023\n","step: 1600, loss: 0.1278657764196396\n","step: 1610, loss: 0.05340629070997238\n","step: 1620, loss: 0.07115648686885834\n","step: 1630, loss: 0.08989506959915161\n","step: 1640, loss: 0.14753369987010956\n","step: 1650, loss: 0.04497572407126427\n","step: 1660, loss: 0.07602138817310333\n","step: 1670, loss: 0.12378648668527603\n","step: 1680, loss: 0.19332441687583923\n","step: 1690, loss: 0.0991419330239296\n","step: 1700, loss: 0.13099533319473267\n","step: 1710, loss: 0.08798924833536148\n","step: 1720, loss: 0.08954841643571854\n","step: 1730, loss: 0.04230436310172081\n","step: 1740, loss: 0.09722486883401871\n","step: 1750, loss: 0.08918996900320053\n","step: 1760, loss: 0.11425883322954178\n","step: 1770, loss: 0.10935791581869125\n","step: 1780, loss: 0.06210599094629288\n","step: 1790, loss: 0.13091342151165009\n","step: 1800, loss: 0.0797780305147171\n","step: 1810, loss: 0.07678459584712982\n","step: 1820, loss: 0.1621268093585968\n","step: 1830, loss: 0.1028757318854332\n","step: 1840, loss: 0.17614826560020447\n","step: 1850, loss: 0.19255468249320984\n","step: 1860, loss: 0.09203391522169113\n","step: 1870, loss: 0.09643788635730743\n","step: 1880, loss: 0.05503806099295616\n","step: 1890, loss: 0.09546835720539093\n","step: 1900, loss: 0.1731576919555664\n","step: 1910, loss: 0.05600569397211075\n","step: 1920, loss: 0.0688818022608757\n","step: 1930, loss: 0.049959488213062286\n","step: 1940, loss: 0.15990157425403595\n","step: 1950, loss: 0.03409944474697113\n","step: 1960, loss: 0.06236330792307854\n","step: 1970, loss: 0.18261227011680603\n","step: 1980, loss: 0.1917259395122528\n","step: 1990, loss: 0.05833045020699501\n","step: 2000, loss: 0.04121250659227371\n","step: 2010, loss: 0.03943168371915817\n","step: 2020, loss: 0.18332508206367493\n","step: 2030, loss: 0.0335230678319931\n","step: 2040, loss: 0.12689636647701263\n","step: 2050, loss: 0.07557981461286545\n","step: 2060, loss: 0.0843881294131279\n","step: 2070, loss: 0.0939793586730957\n","step: 2080, loss: 0.10438255965709686\n","step: 2090, loss: 0.04760463535785675\n","step: 2100, loss: 0.030724916607141495\n","step: 2110, loss: 0.10541892796754837\n","step: 2120, loss: 0.09513198584318161\n","step: 2130, loss: 0.05472523346543312\n","step: 2140, loss: 0.1405927836894989\n","step: 2150, loss: 0.06460610032081604\n","step: 2160, loss: 0.11468560993671417\n","step: 2170, loss: 0.08821772783994675\n","step: 2180, loss: 0.1237177699804306\n","step: 2190, loss: 0.07252714037895203\n","step: 2200, loss: 0.18187154829502106\n","step: 2210, loss: 0.16546332836151123\n","step: 2220, loss: 0.09184318035840988\n","step: 2230, loss: 0.055434051901102066\n","step: 2240, loss: 0.05714167281985283\n","step: 2250, loss: 0.1062731072306633\n","step: 2260, loss: 0.04109863564372063\n","step: 2270, loss: 0.11401034891605377\n","step: 2280, loss: 0.10244850814342499\n","step: 2290, loss: 0.04315243661403656\n","step: 2300, loss: 0.07974030077457428\n","step: 2310, loss: 0.024806851521134377\n","step: 2320, loss: 0.09358766674995422\n","step: 2330, loss: 0.060258157551288605\n","step: 2340, loss: 0.07136283069849014\n","step: 2350, loss: 0.08457095175981522\n","step: 2360, loss: 0.1411592811346054\n","step: 2370, loss: 0.07750298827886581\n","step: 2380, loss: 0.08116979897022247\n","step: 2390, loss: 0.17271031439304352\n","step: 2400, loss: 0.11950517445802689\n","step: 2410, loss: 0.0906708613038063\n","step: 2420, loss: 0.07525327801704407\n","step: 2430, loss: 0.08754290640354156\n","step: 2440, loss: 0.06606210768222809\n","step: 2450, loss: 0.03935281187295914\n","step: 2460, loss: 0.12235523015260696\n","step: 2470, loss: 0.0675729364156723\n","step: 2480, loss: 0.04829029738903046\n","step: 2490, loss: 0.07390659302473068\n","step: 2500, loss: 0.11009243130683899\n","step: 2510, loss: 0.05582240968942642\n","step: 2520, loss: 0.0989672914147377\n","step: 2530, loss: 0.04925151914358139\n","step: 2540, loss: 0.10886093974113464\n","step: 2550, loss: 0.06802895665168762\n","step: 2560, loss: 0.07977253198623657\n","step: 2570, loss: 0.07227135449647903\n","step: 2580, loss: 0.10961344093084335\n","step: 2590, loss: 0.04481631889939308\n","step: 2600, loss: 0.030887773260474205\n","step: 2610, loss: 0.08672550320625305\n","step: 2620, loss: 0.0673484057188034\n","step: 2630, loss: 0.05082615464925766\n","step: 2640, loss: 0.10678073763847351\n","step: 2650, loss: 0.10115891695022583\n","step: 2660, loss: 0.042939115315675735\n","step: 2670, loss: 0.12865355610847473\n","step: 2680, loss: 0.09078215807676315\n","step: 2690, loss: 0.11583297699689865\n","step: 2700, loss: 0.02856902778148651\n","step: 2710, loss: 0.049793921411037445\n","step: 2720, loss: 0.05808081850409508\n","step: 2730, loss: 0.16902290284633636\n","step: 2740, loss: 0.03104614093899727\n","step: 2750, loss: 0.11452607810497284\n","step: 2760, loss: 0.12002693116664886\n","step: 2770, loss: 0.07692393660545349\n","step: 2780, loss: 0.20788873732089996\n","step: 2790, loss: 0.10863019526004791\n","step: 2800, loss: 0.10785038769245148\n","step: 2810, loss: 0.17752300202846527\n","step: 2820, loss: 0.05955537408590317\n","step: 2830, loss: 0.14338290691375732\n","step: 2840, loss: 0.07358406484127045\n","step: 2850, loss: 0.07121532410383224\n","step: 2860, loss: 0.06890188902616501\n","step: 2870, loss: 0.06467088311910629\n","step: 2880, loss: 0.07065322250127792\n","step: 2890, loss: 0.04928393289446831\n","step: 2900, loss: 0.06391545385122299\n","step: 2910, loss: 0.08301087468862534\n","step: 2920, loss: 0.11667131632566452\n","step: 2930, loss: 0.054427728056907654\n","step: 2940, loss: 0.06946808099746704\n","step: 2950, loss: 0.06650803983211517\n","step: 2960, loss: 0.0887964740395546\n","step: 2970, loss: 0.13126873970031738\n","step: 2980, loss: 0.1886146068572998\n","step: 2990, loss: 0.09510283917188644\n","step: 3000, loss: 0.13096359372138977\n","step: 3010, loss: 0.03623458743095398\n","step: 3020, loss: 0.04777897149324417\n","step: 3030, loss: 0.07214841991662979\n","step: 3040, loss: 0.19368699193000793\n","step: 3050, loss: 0.14858470857143402\n","step: 3060, loss: 0.019416669383645058\n","step: 3070, loss: 0.03274289891123772\n","step: 3080, loss: 0.08211514353752136\n","step: 3090, loss: 0.02049015276134014\n","step: 3100, loss: 0.05100516602396965\n","step: 3110, loss: 0.08936291188001633\n","step: 3120, loss: 0.08254649490118027\n","step: 3130, loss: 0.056112244725227356\n","step: 3140, loss: 0.240574911236763\n","step: 3150, loss: 0.05120920389890671\n","step: 3160, loss: 0.06899908930063248\n","step: 3170, loss: 0.030751002952456474\n","step: 3180, loss: 0.12245272099971771\n","step: 3190, loss: 0.1464368999004364\n","step: 3200, loss: 0.10977312922477722\n","step: 3210, loss: 0.11176135390996933\n","step: 3220, loss: 0.07185827195644379\n","step: 3230, loss: 0.11484932154417038\n","step: 3240, loss: 0.15260492265224457\n","step: 3250, loss: 0.04614322632551193\n","step: 3260, loss: 0.04483038932085037\n","step: 3270, loss: 0.03626290708780289\n","step: 3280, loss: 0.11887885630130768\n","step: 3290, loss: 0.052463650703430176\n","step: 3300, loss: 0.07412122189998627\n","step: 3310, loss: 0.1086883619427681\n","step: 3320, loss: 0.12516650557518005\n","step: 3330, loss: 0.08020911365747452\n","step: 3340, loss: 0.07323943823575974\n","step: 3350, loss: 0.06639279425144196\n","step: 3360, loss: 0.11568297445774078\n","step: 3370, loss: 0.17359237372875214\n","step: 3380, loss: 0.03337547555565834\n","step: 3390, loss: 0.13246403634548187\n","step: 3400, loss: 0.08721613138914108\n","step: 3410, loss: 0.07889137417078018\n","step: 3420, loss: 0.07408661395311356\n","step: 3430, loss: 0.04762784019112587\n","step: 3440, loss: 0.0715102031826973\n","step: 3450, loss: 0.04932600259780884\n","step: 3460, loss: 0.06724295765161514\n","step: 3470, loss: 0.04843994975090027\n","step: 3480, loss: 0.08700148016214371\n","step: 3490, loss: 0.07440928369760513\n","step: 3500, loss: 0.1388654261827469\n","step: 3510, loss: 0.16241803765296936\n","step: 3520, loss: 0.12134875357151031\n","step: 3530, loss: 0.0804070234298706\n","step: 3540, loss: 0.03438766673207283\n","step: 3550, loss: 0.05915422737598419\n","step: 3560, loss: 0.04843549057841301\n","step: 3570, loss: 0.02124498412013054\n","step: 3580, loss: 0.04885953664779663\n","step: 3590, loss: 0.04345250502228737\n","step: 3600, loss: 0.05515559762716293\n","step: 3610, loss: 0.07174473255872726\n","step: 3620, loss: 0.016629762947559357\n","step: 3630, loss: 0.08040159195661545\n","step: 3640, loss: 0.12260393798351288\n","step: 3650, loss: 0.09297273308038712\n","step: 3660, loss: 0.09246063232421875\n","step: 3670, loss: 0.13252325356006622\n","step: 3680, loss: 0.1478050798177719\n","step: 3690, loss: 0.10156263411045074\n","step: 3700, loss: 0.07645115256309509\n","step: 3710, loss: 0.10999967902898788\n","step: 3720, loss: 0.05694291740655899\n","step: 3730, loss: 0.08145898580551147\n","step: 3740, loss: 0.09646843373775482\n","step: 3750, loss: 0.06531905382871628\n","step: 3760, loss: 0.025558877736330032\n","step: 3770, loss: 0.09343060106039047\n","step: 3780, loss: 0.11082115024328232\n","step: 3790, loss: 0.06495830416679382\n","step: 3800, loss: 0.07229325920343399\n","step: 3810, loss: 0.0889669731259346\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.77      0.97      0.86        35\n","           2       0.68      0.70      0.69        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.81      0.84      0.82       291\n","           5       0.86      0.84      0.85       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.53      0.94      0.68       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.85      0.91        47\n","          13       0.83      0.77      0.80        13\n","          14       0.34      1.00      0.50        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.86      0.84      0.85      1151\n","          17       0.93      0.93      0.93        41\n","          18       0.94      0.97      0.95        32\n","          19       0.29      0.25      0.27        40\n","          20       1.00      1.00      1.00       584\n","          21       0.03      0.02      0.02        52\n","          22       0.93      0.77      0.84      4175\n","          23       0.76      0.88      0.82      2253\n","          24       0.34      0.64      0.44        44\n","          25       0.84      0.91      0.87       888\n","          26       0.90      1.00      0.95         9\n","          27       0.99      0.96      0.97        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.81      0.87      0.84      1136\n","          31       0.52      0.63      0.57        19\n","          32       0.83      0.62      0.71         8\n","          33       0.65      0.98      0.78        86\n","          34       0.25      0.59      0.35        32\n","          35       0.96      1.00      0.98       474\n","          36       0.71      0.05      0.10       182\n","          37       0.85      0.97      0.91      1592\n","          38       0.95      0.98      0.97       404\n","          39       0.97      0.96      0.97       485\n","          40       0.90      0.96      0.93       573\n","          41       0.91      0.94      0.92       841\n","          42       0.97      0.99      0.98       575\n","          43       0.97      0.74      0.84       152\n","          44       0.88      0.95      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.10      0.18        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.78     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 443\n","\n","Loop 22\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9324679374694824\n","step: 10, loss: 1.994004249572754\n","step: 20, loss: 0.9637191295623779\n","step: 30, loss: 0.3446963131427765\n","step: 40, loss: 0.2951321601867676\n","step: 50, loss: 0.3754969835281372\n","step: 60, loss: 0.2238864153623581\n","step: 70, loss: 0.34374305605888367\n","step: 80, loss: 0.2303379476070404\n","step: 90, loss: 0.24483147263526917\n","step: 100, loss: 0.18632151186466217\n","step: 110, loss: 0.19645732641220093\n","step: 120, loss: 0.24959418177604675\n","step: 130, loss: 0.20999877154827118\n","step: 140, loss: 0.15741701424121857\n","step: 150, loss: 0.11659684777259827\n","step: 160, loss: 0.07076996564865112\n","step: 170, loss: 0.10800254344940186\n","step: 180, loss: 0.17792987823486328\n","step: 190, loss: 0.12680844962596893\n","step: 200, loss: 0.09899895638227463\n","step: 210, loss: 0.06645120680332184\n","step: 220, loss: 0.10914675146341324\n","step: 230, loss: 0.09380616247653961\n","step: 240, loss: 0.07786660641431808\n","step: 250, loss: 0.11673562973737717\n","step: 260, loss: 0.16421835124492645\n","step: 270, loss: 0.10127055644989014\n","step: 280, loss: 0.06502897292375565\n","step: 290, loss: 0.18155615031719208\n","step: 300, loss: 0.10009706765413284\n","step: 310, loss: 0.07921109348535538\n","step: 320, loss: 0.25749626755714417\n","step: 330, loss: 0.1748053878545761\n","step: 340, loss: 0.07338698953390121\n","step: 350, loss: 0.13036705553531647\n","step: 360, loss: 0.08748181164264679\n","step: 370, loss: 0.1066991314291954\n","step: 380, loss: 0.15138429403305054\n","step: 390, loss: 0.20359225571155548\n","step: 400, loss: 0.08279837667942047\n","step: 410, loss: 0.23569044470787048\n","step: 420, loss: 0.17446820437908173\n","step: 430, loss: 0.07107666879892349\n","step: 440, loss: 0.10305964201688766\n","step: 450, loss: 0.06897237151861191\n","step: 460, loss: 0.12048926949501038\n","step: 470, loss: 0.12137920409440994\n","step: 480, loss: 0.045409366488456726\n","step: 490, loss: 0.06597863882780075\n","step: 500, loss: 0.1783926635980606\n","step: 510, loss: 0.12790609896183014\n","step: 520, loss: 0.06598273664712906\n","step: 530, loss: 0.13405901193618774\n","step: 540, loss: 0.0750778466463089\n","step: 550, loss: 0.16405713558197021\n","step: 560, loss: 0.11311782151460648\n","step: 570, loss: 0.0994752049446106\n","step: 580, loss: 0.11192608624696732\n","step: 590, loss: 0.16246502101421356\n","step: 600, loss: 0.19066399335861206\n","step: 610, loss: 0.1013002097606659\n","step: 620, loss: 0.03269701451063156\n","step: 630, loss: 0.10524725914001465\n","step: 640, loss: 0.08169642090797424\n","step: 650, loss: 0.14819759130477905\n","step: 660, loss: 0.05754602700471878\n","step: 670, loss: 0.05515407770872116\n","step: 680, loss: 0.09983982145786285\n","step: 690, loss: 0.024481046944856644\n","step: 700, loss: 0.07282501459121704\n","step: 710, loss: 0.04730072617530823\n","step: 720, loss: 0.05023878440260887\n","step: 730, loss: 0.0945572704076767\n","step: 740, loss: 0.09627662599086761\n","step: 750, loss: 0.07730256766080856\n","step: 760, loss: 0.025108011439442635\n","step: 770, loss: 0.055652718991041183\n","step: 780, loss: 0.19864842295646667\n","step: 790, loss: 0.10930535197257996\n","step: 800, loss: 0.13276369869709015\n","step: 810, loss: 0.10261543840169907\n","step: 820, loss: 0.08428478986024857\n","step: 830, loss: 0.13296182453632355\n","step: 840, loss: 0.1272195279598236\n","step: 850, loss: 0.16918917000293732\n","step: 860, loss: 0.034198421984910965\n","step: 870, loss: 0.06393879652023315\n","step: 880, loss: 0.12378384172916412\n","step: 890, loss: 0.08320588618516922\n","step: 900, loss: 0.10847575962543488\n","step: 910, loss: 0.06359585374593735\n","step: 920, loss: 0.0946323499083519\n","step: 930, loss: 0.0852501317858696\n","step: 940, loss: 0.10214675962924957\n","step: 950, loss: 0.19499072432518005\n","step: 960, loss: 0.14665529131889343\n","step: 970, loss: 0.06705930829048157\n","step: 980, loss: 0.10046359896659851\n","step: 990, loss: 0.09489595144987106\n","step: 1000, loss: 0.07734977453947067\n","step: 1010, loss: 0.07037337124347687\n","step: 1020, loss: 0.07578525692224503\n","step: 1030, loss: 0.08058946579694748\n","step: 1040, loss: 0.1156943291425705\n","step: 1050, loss: 0.2781504988670349\n","step: 1060, loss: 0.1905420571565628\n","step: 1070, loss: 0.0743565708398819\n","step: 1080, loss: 0.012116685509681702\n","step: 1090, loss: 0.11233681440353394\n","step: 1100, loss: 0.09115926921367645\n","step: 1110, loss: 0.13436976075172424\n","step: 1120, loss: 0.1873212307691574\n","step: 1130, loss: 0.07222408801317215\n","step: 1140, loss: 0.0731712132692337\n","step: 1150, loss: 0.1991214156150818\n","step: 1160, loss: 0.07655259966850281\n","step: 1170, loss: 0.1817600280046463\n","step: 1180, loss: 0.0544520802795887\n","step: 1190, loss: 0.11871421337127686\n","step: 1200, loss: 0.06533005833625793\n","step: 1210, loss: 0.08929061889648438\n","step: 1220, loss: 0.13348008692264557\n","step: 1230, loss: 0.07856051623821259\n","step: 1240, loss: 0.25005507469177246\n","step: 1250, loss: 0.10086689889431\n","step: 1260, loss: 0.045983798801898956\n","step: 1270, loss: 0.07738418132066727\n","step: 1280, loss: 0.09387332946062088\n","step: 1290, loss: 0.1285591572523117\n","step: 1300, loss: 0.14056828618049622\n","step: 1310, loss: 0.04420223832130432\n","step: 1320, loss: 0.11390691250562668\n","step: 1330, loss: 0.08685131371021271\n","step: 1340, loss: 0.13886022567749023\n","step: 1350, loss: 0.06092669814825058\n","step: 1360, loss: 0.10859023779630661\n","step: 1370, loss: 0.12022966146469116\n","step: 1380, loss: 0.055756475776433945\n","step: 1390, loss: 0.0657922774553299\n","step: 1400, loss: 0.11301959306001663\n","step: 1410, loss: 0.02990049123764038\n","step: 1420, loss: 0.08123517781496048\n","step: 1430, loss: 0.07099580764770508\n","step: 1440, loss: 0.11079263687133789\n","step: 1450, loss: 0.12549585103988647\n","step: 1460, loss: 0.1378689557313919\n","step: 1470, loss: 0.1730784773826599\n","step: 1480, loss: 0.02476005256175995\n","step: 1490, loss: 0.11408606171607971\n","step: 1500, loss: 0.09875356405973434\n","step: 1510, loss: 0.041119106113910675\n","step: 1520, loss: 0.09409943968057632\n","step: 1530, loss: 0.06343565881252289\n","step: 1540, loss: 0.18065328896045685\n","step: 1550, loss: 0.037769582122564316\n","step: 1560, loss: 0.04766476899385452\n","step: 1570, loss: 0.05383436754345894\n","step: 1580, loss: 0.08408620953559875\n","step: 1590, loss: 0.05975162982940674\n","step: 1600, loss: 0.14996229112148285\n","step: 1610, loss: 0.047119464725255966\n","step: 1620, loss: 0.0697549507021904\n","step: 1630, loss: 0.09342579543590546\n","step: 1640, loss: 0.07740268111228943\n","step: 1650, loss: 0.05721404775977135\n","step: 1660, loss: 0.06224741041660309\n","step: 1670, loss: 0.1503845453262329\n","step: 1680, loss: 0.08914347738027573\n","step: 1690, loss: 0.0802927166223526\n","step: 1700, loss: 0.07467935979366302\n","step: 1710, loss: 0.08759051561355591\n","step: 1720, loss: 0.12168114632368088\n","step: 1730, loss: 0.07962141931056976\n","step: 1740, loss: 0.019790075719356537\n","step: 1750, loss: 0.03558835759758949\n","step: 1760, loss: 0.07744903862476349\n","step: 1770, loss: 0.08137615025043488\n","step: 1780, loss: 0.08385608345270157\n","step: 1790, loss: 0.04052035138010979\n","step: 1800, loss: 0.04910443350672722\n","step: 1810, loss: 0.12356232106685638\n","step: 1820, loss: 0.03697912395000458\n","step: 1830, loss: 0.04357033222913742\n","step: 1840, loss: 0.1136128157377243\n","step: 1850, loss: 0.1231878399848938\n","step: 1860, loss: 0.07422856241464615\n","step: 1870, loss: 0.07102809846401215\n","step: 1880, loss: 0.13567161560058594\n","step: 1890, loss: 0.1446171998977661\n","step: 1900, loss: 0.08437269926071167\n","step: 1910, loss: 0.03050120174884796\n","step: 1920, loss: 0.08128052949905396\n","step: 1930, loss: 0.07490834593772888\n","step: 1940, loss: 0.06311982870101929\n","step: 1950, loss: 0.10051064938306808\n","step: 1960, loss: 0.17353983223438263\n","step: 1970, loss: 0.10122781991958618\n","step: 1980, loss: 0.11729402095079422\n","step: 1990, loss: 0.08634676039218903\n","step: 2000, loss: 0.09268875420093536\n","step: 2010, loss: 0.10545573383569717\n","step: 2020, loss: 0.2044820487499237\n","step: 2030, loss: 0.07659108936786652\n","step: 2040, loss: 0.10313975811004639\n","step: 2050, loss: 0.10831820964813232\n","step: 2060, loss: 0.17996028065681458\n","step: 2070, loss: 0.08101511001586914\n","step: 2080, loss: 0.18128591775894165\n","step: 2090, loss: 0.05567692592740059\n","step: 2100, loss: 0.08851533383131027\n","step: 2110, loss: 0.07517026364803314\n","step: 2120, loss: 0.14408747851848602\n","step: 2130, loss: 0.13715779781341553\n","step: 2140, loss: 0.12009157985448837\n","step: 2150, loss: 0.05634130537509918\n","step: 2160, loss: 0.08544949442148209\n","step: 2170, loss: 0.0739859864115715\n","step: 2180, loss: 0.038066986948251724\n","step: 2190, loss: 0.05815154314041138\n","step: 2200, loss: 0.08444370329380035\n","step: 2210, loss: 0.10808946937322617\n","step: 2220, loss: 0.17551149427890778\n","step: 2230, loss: 0.06304467469453812\n","step: 2240, loss: 0.0787106603384018\n","step: 2250, loss: 0.0985589474439621\n","step: 2260, loss: 0.11422792077064514\n","step: 2270, loss: 0.12562867999076843\n","step: 2280, loss: 0.06646937876939774\n","step: 2290, loss: 0.04360776022076607\n","step: 2300, loss: 0.10862508416175842\n","step: 2310, loss: 0.04554702714085579\n","step: 2320, loss: 0.050405655056238174\n","step: 2330, loss: 0.07690458744764328\n","step: 2340, loss: 0.16227364540100098\n","step: 2350, loss: 0.09363923221826553\n","step: 2360, loss: 0.05652347207069397\n","step: 2370, loss: 0.0558597594499588\n","step: 2380, loss: 0.1870335191488266\n","step: 2390, loss: 0.08060886710882187\n","step: 2400, loss: 0.07807499915361404\n","step: 2410, loss: 0.03966152295470238\n","step: 2420, loss: 0.04488867521286011\n","step: 2430, loss: 0.15493741631507874\n","step: 2440, loss: 0.11776801198720932\n","step: 2450, loss: 0.12626688182353973\n","step: 2460, loss: 0.15492016077041626\n","step: 2470, loss: 0.033308226615190506\n","step: 2480, loss: 0.11136548966169357\n","step: 2490, loss: 0.05324501171708107\n","step: 2500, loss: 0.08345568180084229\n","step: 2510, loss: 0.08344924449920654\n","step: 2520, loss: 0.06808365136384964\n","step: 2530, loss: 0.098298579454422\n","step: 2540, loss: 0.1831800788640976\n","step: 2550, loss: 0.08627322316169739\n","step: 2560, loss: 0.07187629491090775\n","step: 2570, loss: 0.06665384024381638\n","step: 2580, loss: 0.054424531757831573\n","step: 2590, loss: 0.24911436438560486\n","step: 2600, loss: 0.1298954337835312\n","step: 2610, loss: 0.09806348383426666\n","step: 2620, loss: 0.0355374813079834\n","step: 2630, loss: 0.08425422012805939\n","step: 2640, loss: 0.045757051557302475\n","step: 2650, loss: 0.14059238135814667\n","step: 2660, loss: 0.0623006671667099\n","step: 2670, loss: 0.13991183042526245\n","step: 2680, loss: 0.11504726111888885\n","step: 2690, loss: 0.06017044559121132\n","step: 2700, loss: 0.10303840041160583\n","step: 2710, loss: 0.08528564870357513\n","step: 2720, loss: 0.11155234277248383\n","step: 2730, loss: 0.06743698567152023\n","step: 2740, loss: 0.11714135110378265\n","step: 2750, loss: 0.08776389062404633\n","step: 2760, loss: 0.05612265318632126\n","step: 2770, loss: 0.08252175152301788\n","step: 2780, loss: 0.07860907912254333\n","step: 2790, loss: 0.02597552351653576\n","step: 2800, loss: 0.14478729665279388\n","step: 2810, loss: 0.1343931406736374\n","step: 2820, loss: 0.0849316343665123\n","step: 2830, loss: 0.03865918517112732\n","step: 2840, loss: 0.06292847543954849\n","step: 2850, loss: 0.048320259898900986\n","step: 2860, loss: 0.16054721176624298\n","step: 2870, loss: 0.06440312415361404\n","step: 2880, loss: 0.0551958866417408\n","step: 2890, loss: 0.10172661393880844\n","step: 2900, loss: 0.0858982503414154\n","step: 2910, loss: 0.05900149419903755\n","step: 2920, loss: 0.0942641943693161\n","step: 2930, loss: 0.06146913394331932\n","step: 2940, loss: 0.25466489791870117\n","step: 2950, loss: 0.0461067371070385\n","step: 2960, loss: 0.0652742013335228\n","step: 2970, loss: 0.14609485864639282\n","step: 2980, loss: 0.10777357220649719\n","step: 2990, loss: 0.059911102056503296\n","step: 3000, loss: 0.05301419273018837\n","step: 3010, loss: 0.03781977295875549\n","step: 3020, loss: 0.006783524993807077\n","step: 3030, loss: 0.1349073052406311\n","step: 3040, loss: 0.05699079856276512\n","step: 3050, loss: 0.05086879804730415\n","step: 3060, loss: 0.0357402041554451\n","step: 3070, loss: 0.05031450465321541\n","step: 3080, loss: 0.09910717606544495\n","step: 3090, loss: 0.0751739889383316\n","step: 3100, loss: 0.03564230725169182\n","step: 3110, loss: 0.12452097237110138\n","step: 3120, loss: 0.023802166804671288\n","step: 3130, loss: 0.0617692656815052\n","step: 3140, loss: 0.11546223610639572\n","step: 3150, loss: 0.06616103649139404\n","step: 3160, loss: 0.023234521970152855\n","step: 3170, loss: 0.05724075436592102\n","step: 3180, loss: 0.06507130712270737\n","step: 3190, loss: 0.040680959820747375\n","step: 3200, loss: 0.0637282207608223\n","step: 3210, loss: 0.054102808237075806\n","step: 3220, loss: 0.10498516261577606\n","step: 3230, loss: 0.11011947691440582\n","step: 3240, loss: 0.08984136581420898\n","step: 3250, loss: 0.07002746313810349\n","step: 3260, loss: 0.0472271591424942\n","step: 3270, loss: 0.08233508467674255\n","step: 3280, loss: 0.04225814342498779\n","step: 3290, loss: 0.12582407891750336\n","step: 3300, loss: 0.13478752970695496\n","step: 3310, loss: 0.13235269486904144\n","step: 3320, loss: 0.03274175897240639\n","step: 3330, loss: 0.09057985991239548\n","step: 3340, loss: 0.06189156696200371\n","step: 3350, loss: 0.05107158049941063\n","step: 3360, loss: 0.03763004392385483\n","step: 3370, loss: 0.0914011225104332\n","step: 3380, loss: 0.06911621242761612\n","step: 3390, loss: 0.05702447518706322\n","step: 3400, loss: 0.019327795132994652\n","step: 3410, loss: 0.0879463404417038\n","step: 3420, loss: 0.02200642228126526\n","step: 3430, loss: 0.0968208760023117\n","step: 3440, loss: 0.06692159920930862\n","step: 3450, loss: 0.08751353621482849\n","step: 3460, loss: 0.11171919107437134\n","step: 3470, loss: 0.13231004774570465\n","step: 3480, loss: 0.1375073939561844\n","step: 3490, loss: 0.23479828238487244\n","step: 3500, loss: 0.07616322487592697\n","step: 3510, loss: 0.13137972354888916\n","step: 3520, loss: 0.07452202588319778\n","step: 3530, loss: 0.034494537860155106\n","step: 3540, loss: 0.04638656601309776\n","step: 3550, loss: 0.10506797581911087\n","step: 3560, loss: 0.11062468588352203\n","step: 3570, loss: 0.08864770084619522\n","step: 3580, loss: 0.13661618530750275\n","step: 3590, loss: 0.06727343052625656\n","step: 3600, loss: 0.10810275375843048\n","step: 3610, loss: 0.052435580641031265\n","step: 3620, loss: 0.026482002809643745\n","step: 3630, loss: 0.04953698441386223\n","step: 3640, loss: 0.031226715072989464\n","step: 3650, loss: 0.1977793425321579\n","step: 3660, loss: 0.07174373418092728\n","step: 3670, loss: 0.03713477775454521\n","step: 3680, loss: 0.032419633120298386\n","step: 3690, loss: 0.1004515141248703\n","step: 3700, loss: 0.09028243273496628\n","step: 3710, loss: 0.09340362250804901\n","step: 3720, loss: 0.04465882107615471\n","step: 3730, loss: 0.12637221813201904\n","step: 3740, loss: 0.061623524874448776\n","step: 3750, loss: 0.10619517415761948\n","step: 3760, loss: 0.07444891333580017\n","step: 3770, loss: 0.09680088609457016\n","step: 3780, loss: 0.13206326961517334\n","step: 3790, loss: 0.17109231650829315\n","step: 3800, loss: 0.06208854913711548\n","step: 3810, loss: 0.06488863378763199\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.95      1.00      0.97        35\n","           2       0.79      0.48      0.60        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.98      0.84      0.90       291\n","           5       0.89      0.84      0.86       294\n","           6       0.98      0.99      0.99      1570\n","           7       0.56      0.94      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      1.00      0.99       689\n","          10       0.93      0.98      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.73      0.62      0.67        13\n","          14       0.31      1.00      0.48        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.90      0.83      0.87      1151\n","          17       0.95      0.93      0.94        41\n","          18       0.78      0.97      0.86        32\n","          19       0.00      0.00      0.00        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.70      0.97      0.81      2253\n","          24       0.37      0.39      0.38        44\n","          25       0.87      0.93      0.90       888\n","          26       1.00      0.33      0.50         9\n","          27       0.97      0.97      0.97        69\n","          28       0.99      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.87      0.84      0.85      1136\n","          31       0.71      0.63      0.67        19\n","          32       0.75      0.38      0.50         8\n","          33       0.46      1.00      0.63        86\n","          34       0.27      0.62      0.38        32\n","          35       0.93      0.99      0.96       474\n","          36       1.00      0.11      0.20       182\n","          37       0.84      0.94      0.89      1592\n","          38       0.96      0.96      0.96       404\n","          39       0.96      0.96      0.96       485\n","          40       0.92      0.89      0.91       573\n","          41       0.91      0.94      0.93       841\n","          42       0.98      0.99      0.99       575\n","          43       0.94      0.93      0.94       152\n","          44       0.86      0.92      0.89        75\n","          46       1.00      0.96      0.98        82\n","          48       0.60      0.08      0.13        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.77      0.75     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 458\n","\n","Loop 23\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8107614517211914\n","step: 10, loss: 1.8875535726547241\n","step: 20, loss: 0.6978365778923035\n","step: 30, loss: 0.3610649108886719\n","step: 40, loss: 0.45142269134521484\n","step: 50, loss: 0.34203168749809265\n","step: 60, loss: 0.22080263495445251\n","step: 70, loss: 0.2925768792629242\n","step: 80, loss: 0.15585942566394806\n","step: 90, loss: 0.23336441814899445\n","step: 100, loss: 0.13129231333732605\n","step: 110, loss: 0.18979166448116302\n","step: 120, loss: 0.13795176148414612\n","step: 130, loss: 0.19230501353740692\n","step: 140, loss: 0.1669684499502182\n","step: 150, loss: 0.15912456810474396\n","step: 160, loss: 0.27239561080932617\n","step: 170, loss: 0.12979331612586975\n","step: 180, loss: 0.17691054940223694\n","step: 190, loss: 0.12235379964113235\n","step: 200, loss: 0.26672983169555664\n","step: 210, loss: 0.11715410649776459\n","step: 220, loss: 0.06588133424520493\n","step: 230, loss: 0.28958114981651306\n","step: 240, loss: 0.1442154496908188\n","step: 250, loss: 0.2652602791786194\n","step: 260, loss: 0.15256699919700623\n","step: 270, loss: 0.09510307013988495\n","step: 280, loss: 0.04077305272221565\n","step: 290, loss: 0.12727676331996918\n","step: 300, loss: 0.16693834960460663\n","step: 310, loss: 0.060694094747304916\n","step: 320, loss: 0.06932559609413147\n","step: 330, loss: 0.06186271086335182\n","step: 340, loss: 0.07610708475112915\n","step: 350, loss: 0.07519955188035965\n","step: 360, loss: 0.06163833290338516\n","step: 370, loss: 0.14804400503635406\n","step: 380, loss: 0.09903717041015625\n","step: 390, loss: 0.07891694456338882\n","step: 400, loss: 0.07458381354808807\n","step: 410, loss: 0.09273986518383026\n","step: 420, loss: 0.16398416459560394\n","step: 430, loss: 0.15994800627231598\n","step: 440, loss: 0.1994040161371231\n","step: 450, loss: 0.1735951155424118\n","step: 460, loss: 0.0793994665145874\n","step: 470, loss: 0.09191568940877914\n","step: 480, loss: 0.19533364474773407\n","step: 490, loss: 0.05326120927929878\n","step: 500, loss: 0.08080251514911652\n","step: 510, loss: 0.0479397252202034\n","step: 520, loss: 0.21893076598644257\n","step: 530, loss: 0.06292642652988434\n","step: 540, loss: 0.16120731830596924\n","step: 550, loss: 0.15879417955875397\n","step: 560, loss: 0.07394129782915115\n","step: 570, loss: 0.1047380194067955\n","step: 580, loss: 0.10829412937164307\n","step: 590, loss: 0.056604549288749695\n","step: 600, loss: 0.16039752960205078\n","step: 610, loss: 0.16176679730415344\n","step: 620, loss: 0.11098574101924896\n","step: 630, loss: 0.08365148305892944\n","step: 640, loss: 0.0488717257976532\n","step: 650, loss: 0.09495307505130768\n","step: 660, loss: 0.10036122798919678\n","step: 670, loss: 0.0514877513051033\n","step: 680, loss: 0.12888158857822418\n","step: 690, loss: 0.17264673113822937\n","step: 700, loss: 0.07193366438150406\n","step: 710, loss: 0.09395451098680496\n","step: 720, loss: 0.10603063553571701\n","step: 730, loss: 0.1479932814836502\n","step: 740, loss: 0.24381667375564575\n","step: 750, loss: 0.07298065721988678\n","step: 760, loss: 0.0898803323507309\n","step: 770, loss: 0.07456550002098083\n","step: 780, loss: 0.0493735671043396\n","step: 790, loss: 0.053741589188575745\n","step: 800, loss: 0.08932693302631378\n","step: 810, loss: 0.04913219437003136\n","step: 820, loss: 0.13115504384040833\n","step: 830, loss: 0.07679081708192825\n","step: 840, loss: 0.11219586431980133\n","step: 850, loss: 0.14941205084323883\n","step: 860, loss: 0.13819848001003265\n","step: 870, loss: 0.14083914458751678\n","step: 880, loss: 0.09305651485919952\n","step: 890, loss: 0.0582091324031353\n","step: 900, loss: 0.0477692112326622\n","step: 910, loss: 0.029577501118183136\n","step: 920, loss: 0.11966197937726974\n","step: 930, loss: 0.07729968428611755\n","step: 940, loss: 0.12654255330562592\n","step: 950, loss: 0.04480082169175148\n","step: 960, loss: 0.21238550543785095\n","step: 970, loss: 0.06625542789697647\n","step: 980, loss: 0.13194388151168823\n","step: 990, loss: 0.12133168429136276\n","step: 1000, loss: 0.3649989366531372\n","step: 1010, loss: 0.1864982396364212\n","step: 1020, loss: 0.08768532425165176\n","step: 1030, loss: 0.06275473535060883\n","step: 1040, loss: 0.1076287105679512\n","step: 1050, loss: 0.13647639751434326\n","step: 1060, loss: 0.07378767430782318\n","step: 1070, loss: 0.06911377608776093\n","step: 1080, loss: 0.09258463978767395\n","step: 1090, loss: 0.15258368849754333\n","step: 1100, loss: 0.037979912012815475\n","step: 1110, loss: 0.0349302813410759\n","step: 1120, loss: 0.0893973633646965\n","step: 1130, loss: 0.11313636600971222\n","step: 1140, loss: 0.11562687903642654\n","step: 1150, loss: 0.07930973172187805\n","step: 1160, loss: 0.04364157095551491\n","step: 1170, loss: 0.16347183287143707\n","step: 1180, loss: 0.09437550604343414\n","step: 1190, loss: 0.08569984138011932\n","step: 1200, loss: 0.06569483876228333\n","step: 1210, loss: 0.10236421972513199\n","step: 1220, loss: 0.03998042643070221\n","step: 1230, loss: 0.05862410366535187\n","step: 1240, loss: 0.019937599077820778\n","step: 1250, loss: 0.09828594326972961\n","step: 1260, loss: 0.1331607848405838\n","step: 1270, loss: 0.039448171854019165\n","step: 1280, loss: 0.24953526258468628\n","step: 1290, loss: 0.08184345066547394\n","step: 1300, loss: 0.1215282455086708\n","step: 1310, loss: 0.05422702431678772\n","step: 1320, loss: 0.04267377406358719\n","step: 1330, loss: 0.04530809074640274\n","step: 1340, loss: 0.025836516171693802\n","step: 1350, loss: 0.29635119438171387\n","step: 1360, loss: 0.06296920776367188\n","step: 1370, loss: 0.045981958508491516\n","step: 1380, loss: 0.10358602553606033\n","step: 1390, loss: 0.07317955046892166\n","step: 1400, loss: 0.13593243062496185\n","step: 1410, loss: 0.1127469539642334\n","step: 1420, loss: 0.10965054482221603\n","step: 1430, loss: 0.16859783232212067\n","step: 1440, loss: 0.05699460953474045\n","step: 1450, loss: 0.11203508824110031\n","step: 1460, loss: 0.06389109790325165\n","step: 1470, loss: 0.09462159872055054\n","step: 1480, loss: 0.022618496790528297\n","step: 1490, loss: 0.03910505026578903\n","step: 1500, loss: 0.06117723509669304\n","step: 1510, loss: 0.040531307458877563\n","step: 1520, loss: 0.11456551402807236\n","step: 1530, loss: 0.13603350520133972\n","step: 1540, loss: 0.09548569470643997\n","step: 1550, loss: 0.08012381941080093\n","step: 1560, loss: 0.0807114914059639\n","step: 1570, loss: 0.045764241367578506\n","step: 1580, loss: 0.047148704528808594\n","step: 1590, loss: 0.06093956157565117\n","step: 1600, loss: 0.20125803351402283\n","step: 1610, loss: 0.08354281634092331\n","step: 1620, loss: 0.10402330011129379\n","step: 1630, loss: 0.056213680654764175\n","step: 1640, loss: 0.061582762748003006\n","step: 1650, loss: 0.0574519969522953\n","step: 1660, loss: 0.05562574043869972\n","step: 1670, loss: 0.05930643901228905\n","step: 1680, loss: 0.12742213904857635\n","step: 1690, loss: 0.19038605690002441\n","step: 1700, loss: 0.10804826766252518\n","step: 1710, loss: 0.1701851785182953\n","step: 1720, loss: 0.11128779500722885\n","step: 1730, loss: 0.07399711012840271\n","step: 1740, loss: 0.0562114343047142\n","step: 1750, loss: 0.12530702352523804\n","step: 1760, loss: 0.1169227883219719\n","step: 1770, loss: 0.05140030011534691\n","step: 1780, loss: 0.29896852374076843\n","step: 1790, loss: 0.09791945666074753\n","step: 1800, loss: 0.07703327387571335\n","step: 1810, loss: 0.06833068281412125\n","step: 1820, loss: 0.13134680688381195\n","step: 1830, loss: 0.24707871675491333\n","step: 1840, loss: 0.07171989232301712\n","step: 1850, loss: 0.1114611029624939\n","step: 1860, loss: 0.059948842972517014\n","step: 1870, loss: 0.051674965769052505\n","step: 1880, loss: 0.10440907627344131\n","step: 1890, loss: 0.06320460885763168\n","step: 1900, loss: 0.2134859710931778\n","step: 1910, loss: 0.12411735951900482\n","step: 1920, loss: 0.10317162424325943\n","step: 1930, loss: 0.114844411611557\n","step: 1940, loss: 0.11736109107732773\n","step: 1950, loss: 0.12796662747859955\n","step: 1960, loss: 0.13868969678878784\n","step: 1970, loss: 0.04887726902961731\n","step: 1980, loss: 0.0873604416847229\n","step: 1990, loss: 0.05192376300692558\n","step: 2000, loss: 0.04214366525411606\n","step: 2010, loss: 0.0625394731760025\n","step: 2020, loss: 0.06797915697097778\n","step: 2030, loss: 0.08073053508996964\n","step: 2040, loss: 0.08550882339477539\n","step: 2050, loss: 0.10137206315994263\n","step: 2060, loss: 0.09253456443548203\n","step: 2070, loss: 0.13619081676006317\n","step: 2080, loss: 0.040058594197034836\n","step: 2090, loss: 0.09233803302049637\n","step: 2100, loss: 0.10912497341632843\n","step: 2110, loss: 0.07131732255220413\n","step: 2120, loss: 0.061663154512643814\n","step: 2130, loss: 0.04331735521554947\n","step: 2140, loss: 0.07150691747665405\n","step: 2150, loss: 0.12326622009277344\n","step: 2160, loss: 0.09192796796560287\n","step: 2170, loss: 0.04705243185162544\n","step: 2180, loss: 0.08353013545274734\n","step: 2190, loss: 0.11182130873203278\n","step: 2200, loss: 0.05743182823061943\n","step: 2210, loss: 0.12014954537153244\n","step: 2220, loss: 0.054585035890340805\n","step: 2230, loss: 0.01909548230469227\n","step: 2240, loss: 0.07916950434446335\n","step: 2250, loss: 0.03031129576265812\n","step: 2260, loss: 0.04609908536076546\n","step: 2270, loss: 0.06613634526729584\n","step: 2280, loss: 0.04068101942539215\n","step: 2290, loss: 0.16498370468616486\n","step: 2300, loss: 0.12383251637220383\n","step: 2310, loss: 0.07288669794797897\n","step: 2320, loss: 0.030738264322280884\n","step: 2330, loss: 0.12084800750017166\n","step: 2340, loss: 0.16108457744121552\n","step: 2350, loss: 0.07741344720125198\n","step: 2360, loss: 0.11338606476783752\n","step: 2370, loss: 0.15147684514522552\n","step: 2380, loss: 0.047880709171295166\n","step: 2390, loss: 0.11538102477788925\n","step: 2400, loss: 0.04524692893028259\n","step: 2410, loss: 0.05136021971702576\n","step: 2420, loss: 0.1850738525390625\n","step: 2430, loss: 0.06549295037984848\n","step: 2440, loss: 0.09661398082971573\n","step: 2450, loss: 0.1328207403421402\n","step: 2460, loss: 0.07476626336574554\n","step: 2470, loss: 0.11077176034450531\n","step: 2480, loss: 0.05449438840150833\n","step: 2490, loss: 0.05169336125254631\n","step: 2500, loss: 0.13256895542144775\n","step: 2510, loss: 0.06393104046583176\n","step: 2520, loss: 0.09656970947980881\n","step: 2530, loss: 0.10984476655721664\n","step: 2540, loss: 0.05172063782811165\n","step: 2550, loss: 0.0667462944984436\n","step: 2560, loss: 0.10177143663167953\n","step: 2570, loss: 0.10171785205602646\n","step: 2580, loss: 0.0872146263718605\n","step: 2590, loss: 0.05993257462978363\n","step: 2600, loss: 0.06523201614618301\n","step: 2610, loss: 0.07569216191768646\n","step: 2620, loss: 0.15123724937438965\n","step: 2630, loss: 0.094756118953228\n","step: 2640, loss: 0.06979382038116455\n","step: 2650, loss: 0.05296357348561287\n","step: 2660, loss: 0.10306383669376373\n","step: 2670, loss: 0.036584917455911636\n","step: 2680, loss: 0.06226819381117821\n","step: 2690, loss: 0.06745000183582306\n","step: 2700, loss: 0.12445086240768433\n","step: 2710, loss: 0.20257805287837982\n","step: 2720, loss: 0.07676683366298676\n","step: 2730, loss: 0.13418272137641907\n","step: 2740, loss: 0.044673144817352295\n","step: 2750, loss: 0.1209714487195015\n","step: 2760, loss: 0.059777699410915375\n","step: 2770, loss: 0.06493554264307022\n","step: 2780, loss: 0.039302676916122437\n","step: 2790, loss: 0.14109447598457336\n","step: 2800, loss: 0.10390637069940567\n","step: 2810, loss: 0.1507955938577652\n","step: 2820, loss: 0.08573349565267563\n","step: 2830, loss: 0.07158039510250092\n","step: 2840, loss: 0.09588024020195007\n","step: 2850, loss: 0.07531775534152985\n","step: 2860, loss: 0.0767880529165268\n","step: 2870, loss: 0.0370054729282856\n","step: 2880, loss: 0.06232524290680885\n","step: 2890, loss: 0.13405963778495789\n","step: 2900, loss: 0.030375869944691658\n","step: 2910, loss: 0.10476439446210861\n","step: 2920, loss: 0.1176096573472023\n","step: 2930, loss: 0.09770023822784424\n","step: 2940, loss: 0.23154880106449127\n","step: 2950, loss: 0.0757267028093338\n","step: 2960, loss: 0.2042078971862793\n","step: 2970, loss: 0.23873165249824524\n","step: 2980, loss: 0.053325355052948\n","step: 2990, loss: 0.12392297387123108\n","step: 3000, loss: 0.05999217554926872\n","step: 3010, loss: 0.08236365765333176\n","step: 3020, loss: 0.13357855379581451\n","step: 3030, loss: 0.0690121129155159\n","step: 3040, loss: 0.13045094907283783\n","step: 3050, loss: 0.03948396071791649\n","step: 3060, loss: 0.0882875919342041\n","step: 3070, loss: 0.15967382490634918\n","step: 3080, loss: 0.046041905879974365\n","step: 3090, loss: 0.05651751905679703\n","step: 3100, loss: 0.12909220159053802\n","step: 3110, loss: 0.14835673570632935\n","step: 3120, loss: 0.12162922322750092\n","step: 3130, loss: 0.10224097222089767\n","step: 3140, loss: 0.13908548653125763\n","step: 3150, loss: 0.06276596337556839\n","step: 3160, loss: 0.20410777628421783\n","step: 3170, loss: 0.12702396512031555\n","step: 3180, loss: 0.05906728655099869\n","step: 3190, loss: 0.09199919551610947\n","step: 3200, loss: 0.07865523546934128\n","step: 3210, loss: 0.12778104841709137\n","step: 3220, loss: 0.09035495668649673\n","step: 3230, loss: 0.056567005813121796\n","step: 3240, loss: 0.06357298791408539\n","step: 3250, loss: 0.035331401973962784\n","step: 3260, loss: 0.08240313827991486\n","step: 3270, loss: 0.0668012797832489\n","step: 3280, loss: 0.06640449166297913\n","step: 3290, loss: 0.03679143264889717\n","step: 3300, loss: 0.05010475963354111\n","step: 3310, loss: 0.12859348952770233\n","step: 3320, loss: 0.19996078312397003\n","step: 3330, loss: 0.12553560733795166\n","step: 3340, loss: 0.06211274117231369\n","step: 3350, loss: 0.04305166006088257\n","step: 3360, loss: 0.0737677738070488\n","step: 3370, loss: 0.11703526228666306\n","step: 3380, loss: 0.1214229017496109\n","step: 3390, loss: 0.06357213854789734\n","step: 3400, loss: 0.1462370902299881\n","step: 3410, loss: 0.04858119413256645\n","step: 3420, loss: 0.07646738737821579\n","step: 3430, loss: 0.15019719302654266\n","step: 3440, loss: 0.08656622469425201\n","step: 3450, loss: 0.11900422722101212\n","step: 3460, loss: 0.16457870602607727\n","step: 3470, loss: 0.0967148020863533\n","step: 3480, loss: 0.01549307256937027\n","step: 3490, loss: 0.07166886329650879\n","step: 3500, loss: 0.0513840913772583\n","step: 3510, loss: 0.03878907114267349\n","step: 3520, loss: 0.036514054983854294\n","step: 3530, loss: 0.1542961150407791\n","step: 3540, loss: 0.09510429948568344\n","step: 3550, loss: 0.032818373292684555\n","step: 3560, loss: 0.049405280500650406\n","step: 3570, loss: 0.12844359874725342\n","step: 3580, loss: 0.08164230734109879\n","step: 3590, loss: 0.09971759468317032\n","step: 3600, loss: 0.0873316153883934\n","step: 3610, loss: 0.22584915161132812\n","step: 3620, loss: 0.08991018682718277\n","step: 3630, loss: 0.14350828528404236\n","step: 3640, loss: 0.02462286502122879\n","step: 3650, loss: 0.09174112230539322\n","step: 3660, loss: 0.00942459236830473\n","step: 3670, loss: 0.10350970923900604\n","step: 3680, loss: 0.044334761798381805\n","step: 3690, loss: 0.09117384254932404\n","step: 3700, loss: 0.24440555274486542\n","step: 3710, loss: 0.060987118631601334\n","step: 3720, loss: 0.07999104261398315\n","step: 3730, loss: 0.12576697766780853\n","step: 3740, loss: 0.1336537003517151\n","step: 3750, loss: 0.0991588830947876\n","step: 3760, loss: 0.0758586972951889\n","step: 3770, loss: 0.07352957129478455\n","step: 3780, loss: 0.07112475484609604\n","step: 3790, loss: 0.09143397212028503\n","step: 3800, loss: 0.04706979915499687\n","step: 3810, loss: 0.06088915094733238\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.97      0.87        35\n","           2       0.64      0.95      0.76        77\n","           3       1.00      0.80      0.89      1030\n","           4       0.96      0.84      0.90       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.57      0.92      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       0.97      0.99      0.98       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.29      0.31      0.30        13\n","          14       0.31      1.00      0.47        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.86      0.86      0.86      1151\n","          17       0.89      0.95      0.92        41\n","          18       0.91      0.94      0.92        32\n","          19       0.67      0.85      0.75        40\n","          20       1.00      1.00      1.00       584\n","          21       0.22      0.19      0.21        52\n","          22       0.96      0.73      0.83      4175\n","          23       0.74      0.95      0.83      2253\n","          24       0.32      0.66      0.43        44\n","          25       0.86      0.91      0.88       888\n","          26       0.90      1.00      0.95         9\n","          27       0.96      0.99      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.80      0.86      0.83      1136\n","          31       0.50      0.63      0.56        19\n","          32       1.00      0.62      0.77         8\n","          33       0.76      0.95      0.85        86\n","          34       0.19      0.59      0.29        32\n","          35       0.98      0.99      0.99       474\n","          36       0.97      0.15      0.27       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.97      0.98      0.98       404\n","          39       0.93      0.98      0.95       485\n","          40       0.92      0.96      0.94       573\n","          41       0.96      0.93      0.94       841\n","          42       0.99      0.98      0.99       575\n","          43       0.96      0.90      0.93       152\n","          44       0.87      0.92      0.90        75\n","          46       0.88      0.99      0.93        82\n","          48       0.97      0.48      0.64        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.84      0.80     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 464\n","\n","Loop 24\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9582996368408203\n","step: 10, loss: 1.8249481916427612\n","step: 20, loss: 0.6700311899185181\n","step: 30, loss: 0.6046101450920105\n","step: 40, loss: 0.2301972657442093\n","step: 50, loss: 0.18572214245796204\n","step: 60, loss: 0.10507126152515411\n","step: 70, loss: 0.17649157345294952\n","step: 80, loss: 0.19417905807495117\n","step: 90, loss: 0.15601064264774323\n","step: 100, loss: 0.08921883255243301\n","step: 110, loss: 0.044629473239183426\n","step: 120, loss: 0.09504897892475128\n","step: 130, loss: 0.31219345331192017\n","step: 140, loss: 0.09930957108736038\n","step: 150, loss: 0.27321740984916687\n","step: 160, loss: 0.1466110199689865\n","step: 170, loss: 0.12360028177499771\n","step: 180, loss: 0.11975203454494476\n","step: 190, loss: 0.16042503714561462\n","step: 200, loss: 0.07567974179983139\n","step: 210, loss: 0.12180677801370621\n","step: 220, loss: 0.10523597151041031\n","step: 230, loss: 0.07382869720458984\n","step: 240, loss: 0.27817124128341675\n","step: 250, loss: 0.1614522933959961\n","step: 260, loss: 0.05022516846656799\n","step: 270, loss: 0.08100301772356033\n","step: 280, loss: 0.08571234345436096\n","step: 290, loss: 0.11669910699129105\n","step: 300, loss: 0.16738952696323395\n","step: 310, loss: 0.12040752172470093\n","step: 320, loss: 0.21988438069820404\n","step: 330, loss: 0.06305110454559326\n","step: 340, loss: 0.03807997331023216\n","step: 350, loss: 0.07853278517723083\n","step: 360, loss: 0.08661995828151703\n","step: 370, loss: 0.13034959137439728\n","step: 380, loss: 0.11239170283079147\n","step: 390, loss: 0.10734216123819351\n","step: 400, loss: 0.13301022350788116\n","step: 410, loss: 0.03312710300087929\n","step: 420, loss: 0.09800074994564056\n","step: 430, loss: 0.10963195562362671\n","step: 440, loss: 0.11007387191057205\n","step: 450, loss: 0.05661577358841896\n","step: 460, loss: 0.02424723654985428\n","step: 470, loss: 0.06289191544055939\n","step: 480, loss: 0.2543667256832123\n","step: 490, loss: 0.13510586321353912\n","step: 500, loss: 0.21070225536823273\n","step: 510, loss: 0.07983529567718506\n","step: 520, loss: 0.09510163962841034\n","step: 530, loss: 0.05935094133019447\n","step: 540, loss: 0.0702904462814331\n","step: 550, loss: 0.1228097528219223\n","step: 560, loss: 0.0676022469997406\n","step: 570, loss: 0.06373479217290878\n","step: 580, loss: 0.06654887646436691\n","step: 590, loss: 0.07335983961820602\n","step: 600, loss: 0.14800910651683807\n","step: 610, loss: 0.1487504094839096\n","step: 620, loss: 0.195576012134552\n","step: 630, loss: 0.0899396687746048\n","step: 640, loss: 0.11207716912031174\n","step: 650, loss: 0.11243686079978943\n","step: 660, loss: 0.21580126881599426\n","step: 670, loss: 0.02679329365491867\n","step: 680, loss: 0.08344786614179611\n","step: 690, loss: 0.12420374900102615\n","step: 700, loss: 0.11363770067691803\n","step: 710, loss: 0.11672177910804749\n","step: 720, loss: 0.08286811411380768\n","step: 730, loss: 0.13872259855270386\n","step: 740, loss: 0.13584478199481964\n","step: 750, loss: 0.16559815406799316\n","step: 760, loss: 0.08151844888925552\n","step: 770, loss: 0.05446252226829529\n","step: 780, loss: 0.08735457807779312\n","step: 790, loss: 0.1358034312725067\n","step: 800, loss: 0.08849547803401947\n","step: 810, loss: 0.07453157007694244\n","step: 820, loss: 0.13147886097431183\n","step: 830, loss: 0.04409174248576164\n","step: 840, loss: 0.16835269331932068\n","step: 850, loss: 0.11395736783742905\n","step: 860, loss: 0.11992213129997253\n","step: 870, loss: 0.13194620609283447\n","step: 880, loss: 0.06695610284805298\n","step: 890, loss: 0.05450251325964928\n","step: 900, loss: 0.03741399943828583\n","step: 910, loss: 0.13234880566596985\n","step: 920, loss: 0.08514268696308136\n","step: 930, loss: 0.07284494489431381\n","step: 940, loss: 0.09459497779607773\n","step: 950, loss: 0.13040180504322052\n","step: 960, loss: 0.22765542566776276\n","step: 970, loss: 0.10739758610725403\n","step: 980, loss: 0.1238679587841034\n","step: 990, loss: 0.10176394134759903\n","step: 1000, loss: 0.1030346229672432\n","step: 1010, loss: 0.09108725935220718\n","step: 1020, loss: 0.12155861407518387\n","step: 1030, loss: 0.12609916925430298\n","step: 1040, loss: 0.0673726499080658\n","step: 1050, loss: 0.12949441373348236\n","step: 1060, loss: 0.04858439415693283\n","step: 1070, loss: 0.1681125909090042\n","step: 1080, loss: 0.07950514554977417\n","step: 1090, loss: 0.12202726304531097\n","step: 1100, loss: 0.0658569484949112\n","step: 1110, loss: 0.06916902959346771\n","step: 1120, loss: 0.13460493087768555\n","step: 1130, loss: 0.0681447833776474\n","step: 1140, loss: 0.12830305099487305\n","step: 1150, loss: 0.17093734443187714\n","step: 1160, loss: 0.1745581179857254\n","step: 1170, loss: 0.10923706740140915\n","step: 1180, loss: 0.08187943696975708\n","step: 1190, loss: 0.164134681224823\n","step: 1200, loss: 0.1499527096748352\n","step: 1210, loss: 0.034637171775102615\n","step: 1220, loss: 0.06561291962862015\n","step: 1230, loss: 0.10384783148765564\n","step: 1240, loss: 0.16688357293605804\n","step: 1250, loss: 0.09948607534170151\n","step: 1260, loss: 0.05884871259331703\n","step: 1270, loss: 0.19103220105171204\n","step: 1280, loss: 0.08845362812280655\n","step: 1290, loss: 0.3049001693725586\n","step: 1300, loss: 0.09508935362100601\n","step: 1310, loss: 0.11257705092430115\n","step: 1320, loss: 0.059303246438503265\n","step: 1330, loss: 0.05770782753825188\n","step: 1340, loss: 0.08223716914653778\n","step: 1350, loss: 0.11294984817504883\n","step: 1360, loss: 0.09260934591293335\n","step: 1370, loss: 0.04638081416487694\n","step: 1380, loss: 0.11250359565019608\n","step: 1390, loss: 0.12715718150138855\n","step: 1400, loss: 0.14338195323944092\n","step: 1410, loss: 0.10369893163442612\n","step: 1420, loss: 0.07645551860332489\n","step: 1430, loss: 0.05699702352285385\n","step: 1440, loss: 0.10563001781702042\n","step: 1450, loss: 0.1997152715921402\n","step: 1460, loss: 0.01918894611299038\n","step: 1470, loss: 0.07575229555368423\n","step: 1480, loss: 0.13785548508167267\n","step: 1490, loss: 0.039613988250494\n","step: 1500, loss: 0.10973317176103592\n","step: 1510, loss: 0.19215986132621765\n","step: 1520, loss: 0.169693723320961\n","step: 1530, loss: 0.1174388900399208\n","step: 1540, loss: 0.06597699224948883\n","step: 1550, loss: 0.11527266353368759\n","step: 1560, loss: 0.08029095828533173\n","step: 1570, loss: 0.08910035341978073\n","step: 1580, loss: 0.14013534784317017\n","step: 1590, loss: 0.17209358513355255\n","step: 1600, loss: 0.03271246701478958\n","step: 1610, loss: 0.10564634203910828\n","step: 1620, loss: 0.16736029088497162\n","step: 1630, loss: 0.12168841063976288\n","step: 1640, loss: 0.06294023245573044\n","step: 1650, loss: 0.11244424432516098\n","step: 1660, loss: 0.10432245582342148\n","step: 1670, loss: 0.11313427239656448\n","step: 1680, loss: 0.1289406716823578\n","step: 1690, loss: 0.07986598461866379\n","step: 1700, loss: 0.11049134284257889\n","step: 1710, loss: 0.1237563043832779\n","step: 1720, loss: 0.09722164273262024\n","step: 1730, loss: 0.040042612701654434\n","step: 1740, loss: 0.06340567767620087\n","step: 1750, loss: 0.024660851806402206\n","step: 1760, loss: 0.12590451538562775\n","step: 1770, loss: 0.17640241980552673\n","step: 1780, loss: 0.044234104454517365\n","step: 1790, loss: 0.1376713216304779\n","step: 1800, loss: 0.07590525597333908\n","step: 1810, loss: 0.059842344373464584\n","step: 1820, loss: 0.09197115153074265\n","step: 1830, loss: 0.11429178714752197\n","step: 1840, loss: 0.08008448034524918\n","step: 1850, loss: 0.12593944370746613\n","step: 1860, loss: 0.10566087812185287\n","step: 1870, loss: 0.11769578605890274\n","step: 1880, loss: 0.0525122806429863\n","step: 1890, loss: 0.08291976153850555\n","step: 1900, loss: 0.022646905854344368\n","step: 1910, loss: 0.036820463836193085\n","step: 1920, loss: 0.11050893366336823\n","step: 1930, loss: 0.020401380956172943\n","step: 1940, loss: 0.1009647399187088\n","step: 1950, loss: 0.07668950408697128\n","step: 1960, loss: 0.12725886702537537\n","step: 1970, loss: 0.045378249138593674\n","step: 1980, loss: 0.06864776462316513\n","step: 1990, loss: 0.17648328840732574\n","step: 2000, loss: 0.05933034047484398\n","step: 2010, loss: 0.15878508985042572\n","step: 2020, loss: 0.06789890676736832\n","step: 2030, loss: 0.08016044646501541\n","step: 2040, loss: 0.00883141253143549\n","step: 2050, loss: 0.029511814936995506\n","step: 2060, loss: 0.11656902730464935\n","step: 2070, loss: 0.107877217233181\n","step: 2080, loss: 0.07726918160915375\n","step: 2090, loss: 0.03695346787571907\n","step: 2100, loss: 0.11707772314548492\n","step: 2110, loss: 0.07992017269134521\n","step: 2120, loss: 0.06431703269481659\n","step: 2130, loss: 0.06697310507297516\n","step: 2140, loss: 0.048260558396577835\n","step: 2150, loss: 0.0630764290690422\n","step: 2160, loss: 0.03518189489841461\n","step: 2170, loss: 0.17292428016662598\n","step: 2180, loss: 0.07487821578979492\n","step: 2190, loss: 0.07570990920066833\n","step: 2200, loss: 0.051354289054870605\n","step: 2210, loss: 0.10704082995653152\n","step: 2220, loss: 0.0539090633392334\n","step: 2230, loss: 0.0847773402929306\n","step: 2240, loss: 0.14406804740428925\n","step: 2250, loss: 0.053515296429395676\n","step: 2260, loss: 0.08868970721960068\n","step: 2270, loss: 0.03344404697418213\n","step: 2280, loss: 0.061887361109256744\n","step: 2290, loss: 0.11115878820419312\n","step: 2300, loss: 0.12677593529224396\n","step: 2310, loss: 0.07965487986803055\n","step: 2320, loss: 0.08952149003744125\n","step: 2330, loss: 0.1504828929901123\n","step: 2340, loss: 0.11425738036632538\n","step: 2350, loss: 0.10493408888578415\n","step: 2360, loss: 0.08779606223106384\n","step: 2370, loss: 0.053232453763484955\n","step: 2380, loss: 0.20721067488193512\n","step: 2390, loss: 0.0566168949007988\n","step: 2400, loss: 0.12122534215450287\n","step: 2410, loss: 0.039107128977775574\n","step: 2420, loss: 0.22746507823467255\n","step: 2430, loss: 0.11062424629926682\n","step: 2440, loss: 0.04553787782788277\n","step: 2450, loss: 0.09908635169267654\n","step: 2460, loss: 0.04691005125641823\n","step: 2470, loss: 0.12745380401611328\n","step: 2480, loss: 0.07988528162240982\n","step: 2490, loss: 0.03758292645215988\n","step: 2500, loss: 0.0906827300786972\n","step: 2510, loss: 0.059782158583402634\n","step: 2520, loss: 0.08940472453832626\n","step: 2530, loss: 0.08204428106546402\n","step: 2540, loss: 0.06161312013864517\n","step: 2550, loss: 0.1968582421541214\n","step: 2560, loss: 0.12687233090400696\n","step: 2570, loss: 0.04679981246590614\n","step: 2580, loss: 0.05392397940158844\n","step: 2590, loss: 0.11042273044586182\n","step: 2600, loss: 0.10411392897367477\n","step: 2610, loss: 0.04590217024087906\n","step: 2620, loss: 0.13746459782123566\n","step: 2630, loss: 0.1585092544555664\n","step: 2640, loss: 0.06167878583073616\n","step: 2650, loss: 0.041755933314561844\n","step: 2660, loss: 0.11854372173547745\n","step: 2670, loss: 0.09850601851940155\n","step: 2680, loss: 0.13739079236984253\n","step: 2690, loss: 0.036052536219358444\n","step: 2700, loss: 0.12253598123788834\n","step: 2710, loss: 0.08002091944217682\n","step: 2720, loss: 0.14336571097373962\n","step: 2730, loss: 0.044402383267879486\n","step: 2740, loss: 0.07273448258638382\n","step: 2750, loss: 0.1606554090976715\n","step: 2760, loss: 0.09033700823783875\n","step: 2770, loss: 0.03947960212826729\n","step: 2780, loss: 0.08908599615097046\n","step: 2790, loss: 0.08653586357831955\n","step: 2800, loss: 0.04532996192574501\n","step: 2810, loss: 0.12065853923559189\n","step: 2820, loss: 0.22047922015190125\n","step: 2830, loss: 0.11538102477788925\n","step: 2840, loss: 0.05033409595489502\n","step: 2850, loss: 0.11881928145885468\n","step: 2860, loss: 0.10404108464717865\n","step: 2870, loss: 0.09823748469352722\n","step: 2880, loss: 0.14406031370162964\n","step: 2890, loss: 0.12184374779462814\n","step: 2900, loss: 0.10998346656560898\n","step: 2910, loss: 0.037755612283945084\n","step: 2920, loss: 0.09702257812023163\n","step: 2930, loss: 0.07799006998538971\n","step: 2940, loss: 0.06738598644733429\n","step: 2950, loss: 0.07806169986724854\n","step: 2960, loss: 0.05726137384772301\n","step: 2970, loss: 0.04406328126788139\n","step: 2980, loss: 0.05485987663269043\n","step: 2990, loss: 0.1700565367937088\n","step: 3000, loss: 0.07037807255983353\n","step: 3010, loss: 0.06736655533313751\n","step: 3020, loss: 0.07848090678453445\n","step: 3030, loss: 0.0960899144411087\n","step: 3040, loss: 0.10650891810655594\n","step: 3050, loss: 0.09315852075815201\n","step: 3060, loss: 0.08611942082643509\n","step: 3070, loss: 0.10119376331567764\n","step: 3080, loss: 0.040521424263715744\n","step: 3090, loss: 0.12789586186408997\n","step: 3100, loss: 0.11078542470932007\n","step: 3110, loss: 0.1429356038570404\n","step: 3120, loss: 0.11164059489965439\n","step: 3130, loss: 0.12051793187856674\n","step: 3140, loss: 0.06895297765731812\n","step: 3150, loss: 0.02123846299946308\n","step: 3160, loss: 0.1382937729358673\n","step: 3170, loss: 0.08900652080774307\n","step: 3180, loss: 0.08897126466035843\n","step: 3190, loss: 0.16329169273376465\n","step: 3200, loss: 0.07225386053323746\n","step: 3210, loss: 0.1909177601337433\n","step: 3220, loss: 0.06504051387310028\n","step: 3230, loss: 0.07149454951286316\n","step: 3240, loss: 0.13073907792568207\n","step: 3250, loss: 0.05506538972258568\n","step: 3260, loss: 0.11795619130134583\n","step: 3270, loss: 0.04102283716201782\n","step: 3280, loss: 0.032719291746616364\n","step: 3290, loss: 0.09230756014585495\n","step: 3300, loss: 0.03943219035863876\n","step: 3310, loss: 0.027074059471488\n","step: 3320, loss: 0.022067943587899208\n","step: 3330, loss: 0.12911465764045715\n","step: 3340, loss: 0.033183805644512177\n","step: 3350, loss: 0.08892560005187988\n","step: 3360, loss: 0.09067292511463165\n","step: 3370, loss: 0.0174004677683115\n","step: 3380, loss: 0.03812342882156372\n","step: 3390, loss: 0.05105818063020706\n","step: 3400, loss: 0.06155749037861824\n","step: 3410, loss: 0.07908375561237335\n","step: 3420, loss: 0.06673293560743332\n","step: 3430, loss: 0.126059427857399\n","step: 3440, loss: 0.06925260275602341\n","step: 3450, loss: 0.13312484323978424\n","step: 3460, loss: 0.11030175536870956\n","step: 3470, loss: 0.05276995897293091\n","step: 3480, loss: 0.09504765272140503\n","step: 3490, loss: 0.06366267055273056\n","step: 3500, loss: 0.20894324779510498\n","step: 3510, loss: 0.06182040646672249\n","step: 3520, loss: 0.07713273167610168\n","step: 3530, loss: 0.0618373267352581\n","step: 3540, loss: 0.10586097836494446\n","step: 3550, loss: 0.07532918453216553\n","step: 3560, loss: 0.05044173076748848\n","step: 3570, loss: 0.03968910500407219\n","step: 3580, loss: 0.10545258224010468\n","step: 3590, loss: 0.08573494851589203\n","step: 3600, loss: 0.08675876259803772\n","step: 3610, loss: 0.05871015414595604\n","step: 3620, loss: 0.06391483545303345\n","step: 3630, loss: 0.0838703066110611\n","step: 3640, loss: 0.1312999129295349\n","step: 3650, loss: 0.10367494821548462\n","step: 3660, loss: 0.1321992427110672\n","step: 3670, loss: 0.07542566955089569\n","step: 3680, loss: 0.06789040565490723\n","step: 3690, loss: 0.13483232259750366\n","step: 3700, loss: 0.05017562210559845\n","step: 3710, loss: 0.08722952753305435\n","step: 3720, loss: 0.05638311803340912\n","step: 3730, loss: 0.12635937333106995\n","step: 3740, loss: 0.10353076457977295\n","step: 3750, loss: 0.15789794921875\n","step: 3760, loss: 0.05535592883825302\n","step: 3770, loss: 0.13874799013137817\n","step: 3780, loss: 0.16871954500675201\n","step: 3790, loss: 0.138539120554924\n","step: 3800, loss: 0.09980276972055435\n","step: 3810, loss: 0.0906047523021698\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      0.97      0.99        35\n","           2       0.35      0.08      0.13        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.93      0.85      0.89       291\n","           5       1.00      0.84      0.91       294\n","           6       0.97      0.98      0.97      1570\n","           7       0.44      0.95      0.60       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.96      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       1.00      0.92      0.96        13\n","          14       0.33      1.00      0.50        43\n","          15       0.95      0.99      0.97      2778\n","          16       0.88      0.84      0.86      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.94      0.97      0.95        32\n","          19       0.38      0.57      0.46        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.72      0.96      0.82      2253\n","          24       0.47      0.20      0.29        44\n","          25       0.84      0.96      0.89       888\n","          26       1.00      0.78      0.88         9\n","          27       1.00      0.90      0.95        69\n","          28       1.00      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.81      0.87      0.84      1136\n","          31       0.61      0.58      0.59        19\n","          32       1.00      0.62      0.77         8\n","          33       0.83      0.88      0.85        86\n","          34       0.27      0.69      0.38        32\n","          35       0.98      0.98      0.98       474\n","          36       0.86      0.10      0.19       182\n","          37       0.90      0.95      0.92      1592\n","          38       0.95      0.97      0.96       404\n","          39       0.97      0.94      0.96       485\n","          40       0.90      0.97      0.94       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.98      0.98       575\n","          43       0.97      0.91      0.94       152\n","          44       0.90      0.95      0.92        75\n","          46       1.00      0.96      0.98        82\n","          48       0.62      0.13      0.21        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 449\n","\n","Loop 25\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.967308282852173\n","step: 10, loss: 1.9493290185928345\n","step: 20, loss: 0.7577653527259827\n","step: 30, loss: 0.5611835718154907\n","step: 40, loss: 0.3393835723400116\n","step: 50, loss: 0.3219708204269409\n","step: 60, loss: 0.1268589198589325\n","step: 70, loss: 0.17318446934223175\n","step: 80, loss: 0.2311050444841385\n","step: 90, loss: 0.21086324751377106\n","step: 100, loss: 0.17789986729621887\n","step: 110, loss: 0.19529615342617035\n","step: 120, loss: 0.2778151035308838\n","step: 130, loss: 0.1102503165602684\n","step: 140, loss: 0.19874577224254608\n","step: 150, loss: 0.1664084494113922\n","step: 160, loss: 0.1721031367778778\n","step: 170, loss: 0.06932882964611053\n","step: 180, loss: 0.1116962730884552\n","step: 190, loss: 0.18274717032909393\n","step: 200, loss: 0.10897097736597061\n","step: 210, loss: 0.1421630084514618\n","step: 220, loss: 0.0823134332895279\n","step: 230, loss: 0.08477891981601715\n","step: 240, loss: 0.07477150857448578\n","step: 250, loss: 0.06118211895227432\n","step: 260, loss: 0.18078981339931488\n","step: 270, loss: 0.15692749619483948\n","step: 280, loss: 0.11037005484104156\n","step: 290, loss: 0.16109870374202728\n","step: 300, loss: 0.053192440420389175\n","step: 310, loss: 0.09486034512519836\n","step: 320, loss: 0.051890838891267776\n","step: 330, loss: 0.1839166134595871\n","step: 340, loss: 0.17092488706111908\n","step: 350, loss: 0.09566577523946762\n","step: 360, loss: 0.039507608860731125\n","step: 370, loss: 0.10208331048488617\n","step: 380, loss: 0.143980011343956\n","step: 390, loss: 0.14275576174259186\n","step: 400, loss: 0.176046684384346\n","step: 410, loss: 0.1951427012681961\n","step: 420, loss: 0.11437543481588364\n","step: 430, loss: 0.08274871110916138\n","step: 440, loss: 0.06343527138233185\n","step: 450, loss: 0.19204476475715637\n","step: 460, loss: 0.08867217600345612\n","step: 470, loss: 0.12141489237546921\n","step: 480, loss: 0.09063094854354858\n","step: 490, loss: 0.07065246999263763\n","step: 500, loss: 0.0907556340098381\n","step: 510, loss: 0.11054316908121109\n","step: 520, loss: 0.11545750498771667\n","step: 530, loss: 0.11698497831821442\n","step: 540, loss: 0.0871180072426796\n","step: 550, loss: 0.11076076328754425\n","step: 560, loss: 0.1366201490163803\n","step: 570, loss: 0.10767006129026413\n","step: 580, loss: 0.13700169324874878\n","step: 590, loss: 0.05055117607116699\n","step: 600, loss: 0.10596955567598343\n","step: 610, loss: 0.1255856603384018\n","step: 620, loss: 0.18471534550189972\n","step: 630, loss: 0.12585994601249695\n","step: 640, loss: 0.14548376202583313\n","step: 650, loss: 0.09270352125167847\n","step: 660, loss: 0.12162542343139648\n","step: 670, loss: 0.08766043931245804\n","step: 680, loss: 0.06923791021108627\n","step: 690, loss: 0.0193819347769022\n","step: 700, loss: 0.13215363025665283\n","step: 710, loss: 0.11589516699314117\n","step: 720, loss: 0.1860322803258896\n","step: 730, loss: 0.06219087913632393\n","step: 740, loss: 0.12021096050739288\n","step: 750, loss: 0.06028047204017639\n","step: 760, loss: 0.11123038828372955\n","step: 770, loss: 0.08659633994102478\n","step: 780, loss: 0.13375447690486908\n","step: 790, loss: 0.09751231968402863\n","step: 800, loss: 0.053478289395570755\n","step: 810, loss: 0.1880084127187729\n","step: 820, loss: 0.026375407353043556\n","step: 830, loss: 0.04064996540546417\n","step: 840, loss: 0.08278108388185501\n","step: 850, loss: 0.16065078973770142\n","step: 860, loss: 0.111183300614357\n","step: 870, loss: 0.125584214925766\n","step: 880, loss: 0.13310813903808594\n","step: 890, loss: 0.034913159906864166\n","step: 900, loss: 0.08880162984132767\n","step: 910, loss: 0.061313919723033905\n","step: 920, loss: 0.12071861326694489\n","step: 930, loss: 0.06491602212190628\n","step: 940, loss: 0.21066337823867798\n","step: 950, loss: 0.08564495295286179\n","step: 960, loss: 0.16951115429401398\n","step: 970, loss: 0.09962211549282074\n","step: 980, loss: 0.14905580878257751\n","step: 990, loss: 0.02691996470093727\n","step: 1000, loss: 0.020871957764029503\n","step: 1010, loss: 0.08425165712833405\n","step: 1020, loss: 0.1359935998916626\n","step: 1030, loss: 0.10041534900665283\n","step: 1040, loss: 0.06274974346160889\n","step: 1050, loss: 0.21634447574615479\n","step: 1060, loss: 0.06051349639892578\n","step: 1070, loss: 0.2258281111717224\n","step: 1080, loss: 0.04252344369888306\n","step: 1090, loss: 0.11154916137456894\n","step: 1100, loss: 0.12679851055145264\n","step: 1110, loss: 0.1127038523554802\n","step: 1120, loss: 0.10697443783283234\n","step: 1130, loss: 0.11011875420808792\n","step: 1140, loss: 0.11189266294240952\n","step: 1150, loss: 0.12671029567718506\n","step: 1160, loss: 0.08841091394424438\n","step: 1170, loss: 0.13436593115329742\n","step: 1180, loss: 0.11792455613613129\n","step: 1190, loss: 0.06409718841314316\n","step: 1200, loss: 0.039233721792697906\n","step: 1210, loss: 0.04954766854643822\n","step: 1220, loss: 0.03614160045981407\n","step: 1230, loss: 0.10470311343669891\n","step: 1240, loss: 0.05439729988574982\n","step: 1250, loss: 0.09945322573184967\n","step: 1260, loss: 0.041682809591293335\n","step: 1270, loss: 0.06345181167125702\n","step: 1280, loss: 0.1065860465168953\n","step: 1290, loss: 0.11104106158018112\n","step: 1300, loss: 0.10832948982715607\n","step: 1310, loss: 0.08805674314498901\n","step: 1320, loss: 0.13510607182979584\n","step: 1330, loss: 0.12429340928792953\n","step: 1340, loss: 0.028063707053661346\n","step: 1350, loss: 0.07769926637411118\n","step: 1360, loss: 0.1204964742064476\n","step: 1370, loss: 0.058978497982025146\n","step: 1380, loss: 0.019536403939127922\n","step: 1390, loss: 0.13521896302700043\n","step: 1400, loss: 0.08802099525928497\n","step: 1410, loss: 0.0746418908238411\n","step: 1420, loss: 0.19641441106796265\n","step: 1430, loss: 0.09682798385620117\n","step: 1440, loss: 0.04646443948149681\n","step: 1450, loss: 0.04723501577973366\n","step: 1460, loss: 0.10636185109615326\n","step: 1470, loss: 0.04835427552461624\n","step: 1480, loss: 0.042974814772605896\n","step: 1490, loss: 0.03627193346619606\n","step: 1500, loss: 0.03000599890947342\n","step: 1510, loss: 0.07120764255523682\n","step: 1520, loss: 0.15156689286231995\n","step: 1530, loss: 0.05976330488920212\n","step: 1540, loss: 0.12082129716873169\n","step: 1550, loss: 0.09872131794691086\n","step: 1560, loss: 0.06428288668394089\n","step: 1570, loss: 0.1784922331571579\n","step: 1580, loss: 0.16643284261226654\n","step: 1590, loss: 0.05244899541139603\n","step: 1600, loss: 0.1703881472349167\n","step: 1610, loss: 0.13405901193618774\n","step: 1620, loss: 0.1335614174604416\n","step: 1630, loss: 0.1681976169347763\n","step: 1640, loss: 0.03234271705150604\n","step: 1650, loss: 0.05662977695465088\n","step: 1660, loss: 0.1293422281742096\n","step: 1670, loss: 0.09729432314634323\n","step: 1680, loss: 0.08695974200963974\n","step: 1690, loss: 0.130131796002388\n","step: 1700, loss: 0.1502893567085266\n","step: 1710, loss: 0.05208829417824745\n","step: 1720, loss: 0.10324054211378098\n","step: 1730, loss: 0.2348480224609375\n","step: 1740, loss: 0.1932309865951538\n","step: 1750, loss: 0.052640486508607864\n","step: 1760, loss: 0.07795624434947968\n","step: 1770, loss: 0.09179919958114624\n","step: 1780, loss: 0.13080908358097076\n","step: 1790, loss: 0.12194810807704926\n","step: 1800, loss: 0.11868797242641449\n","step: 1810, loss: 0.16778519749641418\n","step: 1820, loss: 0.08484598994255066\n","step: 1830, loss: 0.07323119789361954\n","step: 1840, loss: 0.13364416360855103\n","step: 1850, loss: 0.05541616678237915\n","step: 1860, loss: 0.11375152319669724\n","step: 1870, loss: 0.0959591269493103\n","step: 1880, loss: 0.057737112045288086\n","step: 1890, loss: 0.0725395455956459\n","step: 1900, loss: 0.08743974566459656\n","step: 1910, loss: 0.17068415880203247\n","step: 1920, loss: 0.06849005073308945\n","step: 1930, loss: 0.061552535742521286\n","step: 1940, loss: 0.07791359722614288\n","step: 1950, loss: 0.12956050038337708\n","step: 1960, loss: 0.08161594718694687\n","step: 1970, loss: 0.05972540006041527\n","step: 1980, loss: 0.048363376408815384\n","step: 1990, loss: 0.10332327336072922\n","step: 2000, loss: 0.15593090653419495\n","step: 2010, loss: 0.033026233315467834\n","step: 2020, loss: 0.09730035066604614\n","step: 2030, loss: 0.04566517472267151\n","step: 2040, loss: 0.07203080505132675\n","step: 2050, loss: 0.07775438576936722\n","step: 2060, loss: 0.08230600506067276\n","step: 2070, loss: 0.059732191264629364\n","step: 2080, loss: 0.08702509850263596\n","step: 2090, loss: 0.09942296147346497\n","step: 2100, loss: 0.15397869050502777\n","step: 2110, loss: 0.11344262957572937\n","step: 2120, loss: 0.1613912284374237\n","step: 2130, loss: 0.06982657313346863\n","step: 2140, loss: 0.05449394881725311\n","step: 2150, loss: 0.09370778501033783\n","step: 2160, loss: 0.0607045516371727\n","step: 2170, loss: 0.18161319196224213\n","step: 2180, loss: 0.1265030801296234\n","step: 2190, loss: 0.19670787453651428\n","step: 2200, loss: 0.06349127739667892\n","step: 2210, loss: 0.1723787486553192\n","step: 2220, loss: 0.19277842342853546\n","step: 2230, loss: 0.023024816066026688\n","step: 2240, loss: 0.09350227564573288\n","step: 2250, loss: 0.021953241899609566\n","step: 2260, loss: 0.07062388211488724\n","step: 2270, loss: 0.10497207939624786\n","step: 2280, loss: 0.09364964812994003\n","step: 2290, loss: 0.11568629741668701\n","step: 2300, loss: 0.15009146928787231\n","step: 2310, loss: 0.04961651563644409\n","step: 2320, loss: 0.159999281167984\n","step: 2330, loss: 0.07712128013372421\n","step: 2340, loss: 0.12824885547161102\n","step: 2350, loss: 0.12921109795570374\n","step: 2360, loss: 0.043148189783096313\n","step: 2370, loss: 0.07874427735805511\n","step: 2380, loss: 0.05745776742696762\n","step: 2390, loss: 0.1676402986049652\n","step: 2400, loss: 0.11562900990247726\n","step: 2410, loss: 0.03847764804959297\n","step: 2420, loss: 0.04642314836382866\n","step: 2430, loss: 0.03674064949154854\n","step: 2440, loss: 0.06693392992019653\n","step: 2450, loss: 0.07682637870311737\n","step: 2460, loss: 0.06652966141700745\n","step: 2470, loss: 0.08072692155838013\n","step: 2480, loss: 0.08060237765312195\n","step: 2490, loss: 0.05612875148653984\n","step: 2500, loss: 0.023771118372678757\n","step: 2510, loss: 0.13874730467796326\n","step: 2520, loss: 0.10642873495817184\n","step: 2530, loss: 0.08755314350128174\n","step: 2540, loss: 0.017460282891988754\n","step: 2550, loss: 0.056998301297426224\n","step: 2560, loss: 0.14863459765911102\n","step: 2570, loss: 0.06761622428894043\n","step: 2580, loss: 0.0726843923330307\n","step: 2590, loss: 0.08751742541790009\n","step: 2600, loss: 0.0982193648815155\n","step: 2610, loss: 0.08420634269714355\n","step: 2620, loss: 0.049335211515426636\n","step: 2630, loss: 0.0789329931139946\n","step: 2640, loss: 0.17047612369060516\n","step: 2650, loss: 0.054733339697122574\n","step: 2660, loss: 0.12482303380966187\n","step: 2670, loss: 0.05610312148928642\n","step: 2680, loss: 0.0756034404039383\n","step: 2690, loss: 0.0483446829020977\n","step: 2700, loss: 0.07845094054937363\n","step: 2710, loss: 0.1547437161207199\n","step: 2720, loss: 0.15403586626052856\n","step: 2730, loss: 0.12911948561668396\n","step: 2740, loss: 0.07512544095516205\n","step: 2750, loss: 0.03017129749059677\n","step: 2760, loss: 0.039768122136592865\n","step: 2770, loss: 0.1497090756893158\n","step: 2780, loss: 0.06783414632081985\n","step: 2790, loss: 0.028255391865968704\n","step: 2800, loss: 0.204247385263443\n","step: 2810, loss: 0.10860133916139603\n","step: 2820, loss: 0.06839621812105179\n","step: 2830, loss: 0.06762652099132538\n","step: 2840, loss: 0.09640342742204666\n","step: 2850, loss: 0.07869935035705566\n","step: 2860, loss: 0.16489458084106445\n","step: 2870, loss: 0.10516528785228729\n","step: 2880, loss: 0.02660292759537697\n","step: 2890, loss: 0.04781289026141167\n","step: 2900, loss: 0.13511289656162262\n","step: 2910, loss: 0.18430466949939728\n","step: 2920, loss: 0.026867911219596863\n","step: 2930, loss: 0.10710076242685318\n","step: 2940, loss: 0.06051812693476677\n","step: 2950, loss: 0.06922072917222977\n","step: 2960, loss: 0.04095332697033882\n","step: 2970, loss: 0.11414024978876114\n","step: 2980, loss: 0.051251232624053955\n","step: 2990, loss: 0.05016623064875603\n","step: 3000, loss: 0.07315019518136978\n","step: 3010, loss: 0.13335706293582916\n","step: 3020, loss: 0.0746232196688652\n","step: 3030, loss: 0.061815015971660614\n","step: 3040, loss: 0.11096364259719849\n","step: 3050, loss: 0.06909198313951492\n","step: 3060, loss: 0.07393523305654526\n","step: 3070, loss: 0.13518977165222168\n","step: 3080, loss: 0.13604779541492462\n","step: 3090, loss: 0.05922285094857216\n","step: 3100, loss: 0.07196073979139328\n","step: 3110, loss: 0.16243821382522583\n","step: 3120, loss: 0.07119264453649521\n","step: 3130, loss: 0.07551496475934982\n","step: 3140, loss: 0.047586869448423386\n","step: 3150, loss: 0.1096724271774292\n","step: 3160, loss: 0.10281714051961899\n","step: 3170, loss: 0.11854111403226852\n","step: 3180, loss: 0.07030265033245087\n","step: 3190, loss: 0.12082228809595108\n","step: 3200, loss: 0.09582681208848953\n","step: 3210, loss: 0.09121423959732056\n","step: 3220, loss: 0.13310419023036957\n","step: 3230, loss: 0.10602571070194244\n","step: 3240, loss: 0.12749478220939636\n","step: 3250, loss: 0.13565964996814728\n","step: 3260, loss: 0.05714532360434532\n","step: 3270, loss: 0.13837215304374695\n","step: 3280, loss: 0.08580494672060013\n","step: 3290, loss: 0.03538753464818001\n","step: 3300, loss: 0.127408966422081\n","step: 3310, loss: 0.04658598452806473\n","step: 3320, loss: 0.14160171151161194\n","step: 3330, loss: 0.08250442147254944\n","step: 3340, loss: 0.06312748044729233\n","step: 3350, loss: 0.09554160386323929\n","step: 3360, loss: 0.06060429662466049\n","step: 3370, loss: 0.14353609085083008\n","step: 3380, loss: 0.040439486503601074\n","step: 3390, loss: 0.045919228345155716\n","step: 3400, loss: 0.056829579174518585\n","step: 3410, loss: 0.12993668019771576\n","step: 3420, loss: 0.06991669535636902\n","step: 3430, loss: 0.15306442975997925\n","step: 3440, loss: 0.15625162422657013\n","step: 3450, loss: 0.1108093410730362\n","step: 3460, loss: 0.06646210700273514\n","step: 3470, loss: 0.13959036767482758\n","step: 3480, loss: 0.10183612257242203\n","step: 3490, loss: 0.05223656818270683\n","step: 3500, loss: 0.07772807776927948\n","step: 3510, loss: 0.08338145166635513\n","step: 3520, loss: 0.03240591287612915\n","step: 3530, loss: 0.1083822250366211\n","step: 3540, loss: 0.13211123645305634\n","step: 3550, loss: 0.10624035447835922\n","step: 3560, loss: 0.14943364262580872\n","step: 3570, loss: 0.05480992794036865\n","step: 3580, loss: 0.10568419098854065\n","step: 3590, loss: 0.11271700263023376\n","step: 3600, loss: 0.07613696902990341\n","step: 3610, loss: 0.13044816255569458\n","step: 3620, loss: 0.08098122477531433\n","step: 3630, loss: 0.1517372578382492\n","step: 3640, loss: 0.11704830825328827\n","step: 3650, loss: 0.18464022874832153\n","step: 3660, loss: 0.08642253279685974\n","step: 3670, loss: 0.10146580636501312\n","step: 3680, loss: 0.04197179526090622\n","step: 3690, loss: 0.07362761348485947\n","step: 3700, loss: 0.0886349305510521\n","step: 3710, loss: 0.15615905821323395\n","step: 3720, loss: 0.02512936294078827\n","step: 3730, loss: 0.04839935898780823\n","step: 3740, loss: 0.07822015881538391\n","step: 3750, loss: 0.049926724284887314\n","step: 3760, loss: 0.09049030393362045\n","step: 3770, loss: 0.1493583470582962\n","step: 3780, loss: 0.025174086913466454\n","step: 3790, loss: 0.0313124842941761\n","step: 3800, loss: 0.07306897640228271\n","step: 3810, loss: 0.13493959605693817\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.72      0.97      0.83        35\n","           2       0.61      0.36      0.46        77\n","           3       1.00      0.80      0.89      1030\n","           4       0.99      0.83      0.90       291\n","           5       0.94      0.84      0.89       294\n","           6       0.97      0.98      0.98      1570\n","           7       0.58      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.96      0.98      0.97       689\n","          10       0.93      0.98      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.44      0.92      0.60        13\n","          14       0.31      1.00      0.48        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.86      0.86      0.86      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.89      1.00      0.94        32\n","          19       0.00      0.00      0.00        40\n","          20       1.00      1.00      1.00       584\n","          21       0.18      0.15      0.17        52\n","          22       0.93      0.76      0.84      4175\n","          23       0.72      0.95      0.82      2253\n","          24       0.31      0.61      0.42        44\n","          25       0.87      0.91      0.89       888\n","          26       1.00      1.00      1.00         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.86      0.90      1136\n","          31       0.53      0.53      0.53        19\n","          32       1.00      0.50      0.67         8\n","          33       0.70      0.93      0.80        86\n","          34       0.26      0.78      0.39        32\n","          35       0.99      0.98      0.99       474\n","          36       1.00      0.13      0.22       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.94      0.99      0.96       404\n","          39       0.96      0.95      0.96       485\n","          40       0.90      0.87      0.89       573\n","          41       0.96      0.93      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.97      0.80      0.88       152\n","          44       0.86      0.93      0.90        75\n","          46       0.99      0.98      0.98        82\n","          48       1.00      0.03      0.05        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.80      0.77     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","Difference 441\n","\n","Loop 26\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.916905641555786\n","step: 10, loss: 1.9728736877441406\n","step: 20, loss: 0.7337070107460022\n","step: 30, loss: 0.5660843253135681\n","step: 40, loss: 0.24938593804836273\n","step: 50, loss: 0.18021315336227417\n","step: 60, loss: 0.2640312910079956\n","step: 70, loss: 0.2390705943107605\n","step: 80, loss: 0.16996939480304718\n","step: 90, loss: 0.12459852546453476\n","step: 100, loss: 0.11759120970964432\n","step: 110, loss: 0.16793830692768097\n","step: 120, loss: 0.18493913114070892\n","step: 130, loss: 0.2119285762310028\n","step: 140, loss: 0.07813221216201782\n","step: 150, loss: 0.09309204667806625\n","step: 160, loss: 0.14472314715385437\n","step: 170, loss: 0.1431066244840622\n","step: 180, loss: 0.16751140356063843\n","step: 190, loss: 0.12334993481636047\n","step: 200, loss: 0.14797501266002655\n","step: 210, loss: 0.11178842186927795\n","step: 220, loss: 0.15251192450523376\n","step: 230, loss: 0.04640892148017883\n","step: 240, loss: 0.11469638347625732\n","step: 250, loss: 0.0943935215473175\n","step: 260, loss: 0.10998808592557907\n","step: 270, loss: 0.13102424144744873\n","step: 280, loss: 0.17268076539039612\n","step: 290, loss: 0.0874003991484642\n","step: 300, loss: 0.16216519474983215\n","step: 310, loss: 0.049830660223960876\n","step: 320, loss: 0.07239432632923126\n","step: 330, loss: 0.16912595927715302\n","step: 340, loss: 0.07754860818386078\n","step: 350, loss: 0.044589247554540634\n","step: 360, loss: 0.12962952256202698\n","step: 370, loss: 0.08483174443244934\n","step: 380, loss: 0.2637917399406433\n","step: 390, loss: 0.11529694497585297\n","step: 400, loss: 0.02854270674288273\n","step: 410, loss: 0.1021578311920166\n","step: 420, loss: 0.12967607378959656\n","step: 430, loss: 0.18280048668384552\n","step: 440, loss: 0.07277858257293701\n","step: 450, loss: 0.0761333554983139\n","step: 460, loss: 0.08221963793039322\n","step: 470, loss: 0.26156359910964966\n","step: 480, loss: 0.05235825479030609\n","step: 490, loss: 0.15782561898231506\n","step: 500, loss: 0.13652896881103516\n","step: 510, loss: 0.08176849037408829\n","step: 520, loss: 0.12375130504369736\n","step: 530, loss: 0.13160455226898193\n","step: 540, loss: 0.06924325227737427\n","step: 550, loss: 0.05405082181096077\n","step: 560, loss: 0.12932656705379486\n","step: 570, loss: 0.1733822077512741\n","step: 580, loss: 0.039506543427705765\n","step: 590, loss: 0.06520911306142807\n","step: 600, loss: 0.08436880260705948\n","step: 610, loss: 0.08398712426424026\n","step: 620, loss: 0.08987341076135635\n","step: 630, loss: 0.1243479773402214\n","step: 640, loss: 0.07032985985279083\n","step: 650, loss: 0.028460271656513214\n","step: 660, loss: 0.19928914308547974\n","step: 670, loss: 0.12918388843536377\n","step: 680, loss: 0.252009779214859\n","step: 690, loss: 0.04287540167570114\n","step: 700, loss: 0.024016687646508217\n","step: 710, loss: 0.1481933444738388\n","step: 720, loss: 0.062037140130996704\n","step: 730, loss: 0.0414682999253273\n","step: 740, loss: 0.18093422055244446\n","step: 750, loss: 0.12162651121616364\n","step: 760, loss: 0.04224998131394386\n","step: 770, loss: 0.06315015256404877\n","step: 780, loss: 0.11338774859905243\n","step: 790, loss: 0.09468129277229309\n","step: 800, loss: 0.05631948262453079\n","step: 810, loss: 0.08889498561620712\n","step: 820, loss: 0.07544754445552826\n","step: 830, loss: 0.048063021153211594\n","step: 840, loss: 0.23732613027095795\n","step: 850, loss: 0.16888615489006042\n","step: 860, loss: 0.052354972809553146\n","step: 870, loss: 0.2041020691394806\n","step: 880, loss: 0.13016964495182037\n","step: 890, loss: 0.11046017706394196\n","step: 900, loss: 0.062280379235744476\n","step: 910, loss: 0.0859370082616806\n","step: 920, loss: 0.025213154032826424\n","step: 930, loss: 0.1102667823433876\n","step: 940, loss: 0.09707304835319519\n","step: 950, loss: 0.25184354186058044\n","step: 960, loss: 0.12659011781215668\n","step: 970, loss: 0.07069617509841919\n","step: 980, loss: 0.10491088777780533\n","step: 990, loss: 0.17030827701091766\n","step: 1000, loss: 0.1229357048869133\n","step: 1010, loss: 0.09219487011432648\n","step: 1020, loss: 0.08298371732234955\n","step: 1030, loss: 0.11280027776956558\n","step: 1040, loss: 0.06746026873588562\n","step: 1050, loss: 0.11874474585056305\n","step: 1060, loss: 0.14907579123973846\n","step: 1070, loss: 0.05827770382165909\n","step: 1080, loss: 0.10016504675149918\n","step: 1090, loss: 0.02536742575466633\n","step: 1100, loss: 0.1437617838382721\n","step: 1110, loss: 0.07010739296674728\n","step: 1120, loss: 0.14266137778759003\n","step: 1130, loss: 0.07430989295244217\n","step: 1140, loss: 0.11975986510515213\n","step: 1150, loss: 0.07686568051576614\n","step: 1160, loss: 0.04823259636759758\n","step: 1170, loss: 0.03805273026227951\n","step: 1180, loss: 0.06450402736663818\n","step: 1190, loss: 0.06727094203233719\n","step: 1200, loss: 0.03864946588873863\n","step: 1210, loss: 0.14410658180713654\n","step: 1220, loss: 0.0595409981906414\n","step: 1230, loss: 0.06912390887737274\n","step: 1240, loss: 0.14552581310272217\n","step: 1250, loss: 0.083233542740345\n","step: 1260, loss: 0.12101402133703232\n","step: 1270, loss: 0.1630920171737671\n","step: 1280, loss: 0.20969973504543304\n","step: 1290, loss: 0.10885411500930786\n","step: 1300, loss: 0.059758950024843216\n","step: 1310, loss: 0.08210199326276779\n","step: 1320, loss: 0.036025308072566986\n","step: 1330, loss: 0.06494422256946564\n","step: 1340, loss: 0.1399841457605362\n","step: 1350, loss: 0.10012049973011017\n","step: 1360, loss: 0.0620608814060688\n","step: 1370, loss: 0.051177978515625\n","step: 1380, loss: 0.09135248512029648\n","step: 1390, loss: 0.052204303443431854\n","step: 1400, loss: 0.08723922073841095\n","step: 1410, loss: 0.08603493124246597\n","step: 1420, loss: 0.06239083409309387\n","step: 1430, loss: 0.07394325733184814\n","step: 1440, loss: 0.10793279111385345\n","step: 1450, loss: 0.10155685991048813\n","step: 1460, loss: 0.11679068952798843\n","step: 1470, loss: 0.10113232582807541\n","step: 1480, loss: 0.08966910094022751\n","step: 1490, loss: 0.04060840234160423\n","step: 1500, loss: 0.1344502717256546\n","step: 1510, loss: 0.12474917620420456\n","step: 1520, loss: 0.097662553191185\n","step: 1530, loss: 0.07310837507247925\n","step: 1540, loss: 0.07972107082605362\n","step: 1550, loss: 0.15493029356002808\n","step: 1560, loss: 0.06243430823087692\n","step: 1570, loss: 0.03601536154747009\n","step: 1580, loss: 0.06926387548446655\n","step: 1590, loss: 0.06874468177556992\n","step: 1600, loss: 0.10617753118276596\n","step: 1610, loss: 0.07583045214414597\n","step: 1620, loss: 0.03501494973897934\n","step: 1630, loss: 0.06588810682296753\n","step: 1640, loss: 0.23147819936275482\n","step: 1650, loss: 0.06027324125170708\n","step: 1660, loss: 0.12615640461444855\n","step: 1670, loss: 0.06519722193479538\n","step: 1680, loss: 0.1976442039012909\n","step: 1690, loss: 0.024768764153122902\n","step: 1700, loss: 0.12458407878875732\n","step: 1710, loss: 0.14138519763946533\n","step: 1720, loss: 0.031250517815351486\n","step: 1730, loss: 0.07698581367731094\n","step: 1740, loss: 0.08481219410896301\n","step: 1750, loss: 0.10103372484445572\n","step: 1760, loss: 0.09812282770872116\n","step: 1770, loss: 0.03990951180458069\n","step: 1780, loss: 0.11538265645503998\n","step: 1790, loss: 0.1367042064666748\n","step: 1800, loss: 0.043770115822553635\n","step: 1810, loss: 0.06755807995796204\n","step: 1820, loss: 0.03042818047106266\n","step: 1830, loss: 0.20720742642879486\n","step: 1840, loss: 0.0674760639667511\n","step: 1850, loss: 0.09275668859481812\n","step: 1860, loss: 0.0831199362874031\n","step: 1870, loss: 0.06275777518749237\n","step: 1880, loss: 0.11096211522817612\n","step: 1890, loss: 0.06931532174348831\n","step: 1900, loss: 0.2042454332113266\n","step: 1910, loss: 0.043414462357759476\n","step: 1920, loss: 0.09935817867517471\n","step: 1930, loss: 0.02447582222521305\n","step: 1940, loss: 0.03796710446476936\n","step: 1950, loss: 0.07175325602293015\n","step: 1960, loss: 0.14288508892059326\n","step: 1970, loss: 0.1521957814693451\n","step: 1980, loss: 0.0516684465110302\n","step: 1990, loss: 0.06917302310466766\n","step: 2000, loss: 0.1345442682504654\n","step: 2010, loss: 0.11798925697803497\n","step: 2020, loss: 0.19597317278385162\n","step: 2030, loss: 0.17001856863498688\n","step: 2040, loss: 0.068227618932724\n","step: 2050, loss: 0.030912529677152634\n","step: 2060, loss: 0.053735584020614624\n","step: 2070, loss: 0.07600532472133636\n","step: 2080, loss: 0.1476718932390213\n","step: 2090, loss: 0.06766865402460098\n","step: 2100, loss: 0.0889732837677002\n","step: 2110, loss: 0.1284702718257904\n","step: 2120, loss: 0.0432637520134449\n","step: 2130, loss: 0.03400363773107529\n","step: 2140, loss: 0.04073108360171318\n","step: 2150, loss: 0.10885964334011078\n","step: 2160, loss: 0.06787049770355225\n","step: 2170, loss: 0.155965656042099\n","step: 2180, loss: 0.10635628551244736\n","step: 2190, loss: 0.1376609355211258\n","step: 2200, loss: 0.18428796529769897\n","step: 2210, loss: 0.11768586933612823\n","step: 2220, loss: 0.04249093681573868\n","step: 2230, loss: 0.15578940510749817\n","step: 2240, loss: 0.052898187190294266\n","step: 2250, loss: 0.1926754117012024\n","step: 2260, loss: 0.016236823052167892\n","step: 2270, loss: 0.09431613981723785\n","step: 2280, loss: 0.061360493302345276\n","step: 2290, loss: 0.04551984369754791\n","step: 2300, loss: 0.11276225000619888\n","step: 2310, loss: 0.059469882398843765\n","step: 2320, loss: 0.050233736634254456\n","step: 2330, loss: 0.11631366610527039\n","step: 2340, loss: 0.12630541622638702\n","step: 2350, loss: 0.06117336452007294\n","step: 2360, loss: 0.07030457258224487\n","step: 2370, loss: 0.03803340718150139\n","step: 2380, loss: 0.16813884675502777\n","step: 2390, loss: 0.1280342936515808\n","step: 2400, loss: 0.1159706860780716\n","step: 2410, loss: 0.052417926490306854\n","step: 2420, loss: 0.11051712185144424\n","step: 2430, loss: 0.08437217772006989\n","step: 2440, loss: 0.046393875032663345\n","step: 2450, loss: 0.037382837384939194\n","step: 2460, loss: 0.14116252958774567\n","step: 2470, loss: 0.08890500664710999\n","step: 2480, loss: 0.12685008347034454\n","step: 2490, loss: 0.037351153790950775\n","step: 2500, loss: 0.23149074614048004\n","step: 2510, loss: 0.02482590638101101\n","step: 2520, loss: 0.06576525419950485\n","step: 2530, loss: 0.057738061994314194\n","step: 2540, loss: 0.0637751892209053\n","step: 2550, loss: 0.08068817108869553\n","step: 2560, loss: 0.052871547639369965\n","step: 2570, loss: 0.15748059749603271\n","step: 2580, loss: 0.12625330686569214\n","step: 2590, loss: 0.08595191687345505\n","step: 2600, loss: 0.10865326225757599\n","step: 2610, loss: 0.06374408304691315\n","step: 2620, loss: 0.10328465700149536\n","step: 2630, loss: 0.15019746124744415\n","step: 2640, loss: 0.12097612768411636\n","step: 2650, loss: 0.06753081828355789\n","step: 2660, loss: 0.19929462671279907\n","step: 2670, loss: 0.05254998058080673\n","step: 2680, loss: 0.12988829612731934\n","step: 2690, loss: 0.032226696610450745\n","step: 2700, loss: 0.07553228735923767\n","step: 2710, loss: 0.2023618221282959\n","step: 2720, loss: 0.04604622349143028\n","step: 2730, loss: 0.0844639465212822\n","step: 2740, loss: 0.13038082420825958\n","step: 2750, loss: 0.0785868763923645\n","step: 2760, loss: 0.07460197806358337\n","step: 2770, loss: 0.04794744774699211\n","step: 2780, loss: 0.06446097791194916\n","step: 2790, loss: 0.09008948504924774\n","step: 2800, loss: 0.15568138659000397\n","step: 2810, loss: 0.11777372658252716\n","step: 2820, loss: 0.06836888939142227\n","step: 2830, loss: 0.05095512792468071\n","step: 2840, loss: 0.10591260343790054\n","step: 2850, loss: 0.047655362635850906\n","step: 2860, loss: 0.08201677352190018\n","step: 2870, loss: 0.0401063933968544\n","step: 2880, loss: 0.07593662291765213\n","step: 2890, loss: 0.04165984317660332\n","step: 2900, loss: 0.04631168767809868\n","step: 2910, loss: 0.11971462517976761\n","step: 2920, loss: 0.040359675884246826\n","step: 2930, loss: 0.06283074617385864\n","step: 2940, loss: 0.02263941802084446\n","step: 2950, loss: 0.05011459067463875\n","step: 2960, loss: 0.08555921912193298\n","step: 2970, loss: 0.10727126151323318\n","step: 2980, loss: 0.09363587945699692\n","step: 2990, loss: 0.1339367777109146\n","step: 3000, loss: 0.10150877386331558\n","step: 3010, loss: 0.11483895033597946\n","step: 3020, loss: 0.08110945671796799\n","step: 3030, loss: 0.04368429258465767\n","step: 3040, loss: 0.12626022100448608\n","step: 3050, loss: 0.10584825277328491\n","step: 3060, loss: 0.1574033796787262\n","step: 3070, loss: 0.07705902308225632\n","step: 3080, loss: 0.04167519137263298\n","step: 3090, loss: 0.0904870480298996\n","step: 3100, loss: 0.07086004316806793\n","step: 3110, loss: 0.07036308199167252\n","step: 3120, loss: 0.06512032449245453\n","step: 3130, loss: 0.022203588858246803\n","step: 3140, loss: 0.19119063019752502\n","step: 3150, loss: 0.038010649383068085\n","step: 3160, loss: 0.06588622182607651\n","step: 3170, loss: 0.18822872638702393\n","step: 3180, loss: 0.07447244226932526\n","step: 3190, loss: 0.0522150844335556\n","step: 3200, loss: 0.09086330235004425\n","step: 3210, loss: 0.09574901312589645\n","step: 3220, loss: 0.12896130979061127\n","step: 3230, loss: 0.11434382945299149\n","step: 3240, loss: 0.11928647756576538\n","step: 3250, loss: 0.08273512125015259\n","step: 3260, loss: 0.09335216134786606\n","step: 3270, loss: 0.11781089007854462\n","step: 3280, loss: 0.0646154060959816\n","step: 3290, loss: 0.09879962354898453\n","step: 3300, loss: 0.05434533208608627\n","step: 3310, loss: 0.12134727090597153\n","step: 3320, loss: 0.04171491041779518\n","step: 3330, loss: 0.1802268922328949\n","step: 3340, loss: 0.12189291417598724\n","step: 3350, loss: 0.12079735845327377\n","step: 3360, loss: 0.05918111652135849\n","step: 3370, loss: 0.04305307939648628\n","step: 3380, loss: 0.09159325808286667\n","step: 3390, loss: 0.04082157835364342\n","step: 3400, loss: 0.057354480028152466\n","step: 3410, loss: 0.09922517091035843\n","step: 3420, loss: 0.02046525478363037\n","step: 3430, loss: 0.0515190064907074\n","step: 3440, loss: 0.10679280757904053\n","step: 3450, loss: 0.04085606336593628\n","step: 3460, loss: 0.19672466814517975\n","step: 3470, loss: 0.05044003203511238\n","step: 3480, loss: 0.10646898299455643\n","step: 3490, loss: 0.06120589002966881\n","step: 3500, loss: 0.12992189824581146\n","step: 3510, loss: 0.08869875222444534\n","step: 3520, loss: 0.0605536550283432\n","step: 3530, loss: 0.020016903057694435\n","step: 3540, loss: 0.11334095895290375\n","step: 3550, loss: 0.18126755952835083\n","step: 3560, loss: 0.12361366301774979\n","step: 3570, loss: 0.18497693538665771\n","step: 3580, loss: 0.0359349250793457\n","step: 3590, loss: 0.11125804483890533\n","step: 3600, loss: 0.054465580731630325\n","step: 3610, loss: 0.10535576939582825\n","step: 3620, loss: 0.05379697307944298\n","step: 3630, loss: 0.05727139860391617\n","step: 3640, loss: 0.12352465093135834\n","step: 3650, loss: 0.06015687808394432\n","step: 3660, loss: 0.05353861674666405\n","step: 3670, loss: 0.09468238800764084\n","step: 3680, loss: 0.08607374131679535\n","step: 3690, loss: 0.07800672948360443\n","step: 3700, loss: 0.10345634073019028\n","step: 3710, loss: 0.05997481197118759\n","step: 3720, loss: 0.1210322231054306\n","step: 3730, loss: 0.08826704323291779\n","step: 3740, loss: 0.08103875815868378\n","step: 3750, loss: 0.11878207325935364\n","step: 3760, loss: 0.03900255262851715\n","step: 3770, loss: 0.08365169167518616\n","step: 3780, loss: 0.048297613859176636\n","step: 3790, loss: 0.1024843379855156\n","step: 3800, loss: 0.04671359434723854\n","step: 3810, loss: 0.11583808809518814\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.89      0.84        35\n","           2       0.77      0.44      0.56        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.96      0.84      0.90       291\n","           5       0.85      0.84      0.85       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      1.00      0.99       689\n","          10       0.96      0.97      0.96       901\n","          11       0.99      0.99      0.99      2111\n","          12       0.87      0.98      0.92        47\n","          13       0.39      0.85      0.54        13\n","          14       0.43      1.00      0.60        43\n","          15       0.95      0.99      0.97      2778\n","          16       0.88      0.84      0.86      1151\n","          17       0.93      0.93      0.93        41\n","          18       1.00      0.84      0.92        32\n","          19       0.21      0.93      0.34        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.73      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.38      0.25      0.30        44\n","          25       0.88      0.89      0.88       888\n","          26       0.78      0.78      0.78         9\n","          27       0.92      0.97      0.94        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.90      0.87      0.88      1136\n","          31       0.65      0.68      0.67        19\n","          32       0.62      1.00      0.76         8\n","          33       0.71      0.95      0.81        86\n","          34       0.26      0.62      0.37        32\n","          35       0.99      0.99      0.99       474\n","          36       0.94      0.16      0.28       182\n","          37       0.91      0.93      0.92      1592\n","          38       0.95      0.98      0.96       404\n","          39       0.97      0.93      0.95       485\n","          40       0.93      0.93      0.93       573\n","          41       0.93      0.94      0.94       841\n","          42       0.97      0.99      0.98       575\n","          43       0.96      0.88      0.92       152\n","          44       0.95      0.92      0.93        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.81      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 440\n","\n","Loop 27\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.950421094894409\n","step: 10, loss: 1.9944342374801636\n","step: 20, loss: 0.7868780493736267\n","step: 30, loss: 0.4284162223339081\n","step: 40, loss: 0.35986849665641785\n","step: 50, loss: 0.34620729088783264\n","step: 60, loss: 0.1546086072921753\n","step: 70, loss: 0.3955244719982147\n","step: 80, loss: 0.2307206094264984\n","step: 90, loss: 0.1331174075603485\n","step: 100, loss: 0.10227174311876297\n","step: 110, loss: 0.17241176962852478\n","step: 120, loss: 0.17073655128479004\n","step: 130, loss: 0.2542482018470764\n","step: 140, loss: 0.14258812367916107\n","step: 150, loss: 0.13812586665153503\n","step: 160, loss: 0.07987601310014725\n","step: 170, loss: 0.09497424215078354\n","step: 180, loss: 0.183591827750206\n","step: 190, loss: 0.051736053079366684\n","step: 200, loss: 0.21867252886295319\n","step: 210, loss: 0.1471271961927414\n","step: 220, loss: 0.22160254418849945\n","step: 230, loss: 0.2823318839073181\n","step: 240, loss: 0.04842458292841911\n","step: 250, loss: 0.06621947884559631\n","step: 260, loss: 0.07880479842424393\n","step: 270, loss: 0.0973336473107338\n","step: 280, loss: 0.06062095984816551\n","step: 290, loss: 0.0745343342423439\n","step: 300, loss: 0.08920907229185104\n","step: 310, loss: 0.06751645356416702\n","step: 320, loss: 0.15382136404514313\n","step: 330, loss: 0.21448706090450287\n","step: 340, loss: 0.06344488263130188\n","step: 350, loss: 0.2050761580467224\n","step: 360, loss: 0.13726040720939636\n","step: 370, loss: 0.19916801154613495\n","step: 380, loss: 0.07752302289009094\n","step: 390, loss: 0.16551415622234344\n","step: 400, loss: 0.25212278962135315\n","step: 410, loss: 0.05173274129629135\n","step: 420, loss: 0.1004684790968895\n","step: 430, loss: 0.15500670671463013\n","step: 440, loss: 0.08075486123561859\n","step: 450, loss: 0.08010369539260864\n","step: 460, loss: 0.07718253135681152\n","step: 470, loss: 0.1414838284254074\n","step: 480, loss: 0.0968490019440651\n","step: 490, loss: 0.10052010416984558\n","step: 500, loss: 0.055521972477436066\n","step: 510, loss: 0.08604002743959427\n","step: 520, loss: 0.09187161922454834\n","step: 530, loss: 0.05624976381659508\n","step: 540, loss: 0.1309153139591217\n","step: 550, loss: 0.03369952365756035\n","step: 560, loss: 0.1265614628791809\n","step: 570, loss: 0.07798709720373154\n","step: 580, loss: 0.1367742270231247\n","step: 590, loss: 0.06550926715135574\n","step: 600, loss: 0.14798788726329803\n","step: 610, loss: 0.06642530858516693\n","step: 620, loss: 0.09210271388292313\n","step: 630, loss: 0.07068795710802078\n","step: 640, loss: 0.17923709750175476\n","step: 650, loss: 0.11618050932884216\n","step: 660, loss: 0.03489908576011658\n","step: 670, loss: 0.20092274248600006\n","step: 680, loss: 0.03908186033368111\n","step: 690, loss: 0.04434078186750412\n","step: 700, loss: 0.07446331530809402\n","step: 710, loss: 0.22276148200035095\n","step: 720, loss: 0.1195003092288971\n","step: 730, loss: 0.18811403214931488\n","step: 740, loss: 0.11785335093736649\n","step: 750, loss: 0.17503131926059723\n","step: 760, loss: 0.1332978755235672\n","step: 770, loss: 0.02626137062907219\n","step: 780, loss: 0.08540861308574677\n","step: 790, loss: 0.08761042356491089\n","step: 800, loss: 0.07932862639427185\n","step: 810, loss: 0.15561813116073608\n","step: 820, loss: 0.1307193785905838\n","step: 830, loss: 0.11356054246425629\n","step: 840, loss: 0.10009892284870148\n","step: 850, loss: 0.1518562138080597\n","step: 860, loss: 0.15584537386894226\n","step: 870, loss: 0.13002809882164001\n","step: 880, loss: 0.050719454884529114\n","step: 890, loss: 0.07618814706802368\n","step: 900, loss: 0.06602133065462112\n","step: 910, loss: 0.12501846253871918\n","step: 920, loss: 0.026102034375071526\n","step: 930, loss: 0.194375142455101\n","step: 940, loss: 0.08435961604118347\n","step: 950, loss: 0.21059779822826385\n","step: 960, loss: 0.07809137552976608\n","step: 970, loss: 0.14445453882217407\n","step: 980, loss: 0.1332317590713501\n","step: 990, loss: 0.07671808451414108\n","step: 1000, loss: 0.07309459894895554\n","step: 1010, loss: 0.0564316064119339\n","step: 1020, loss: 0.12588004767894745\n","step: 1030, loss: 0.10630020499229431\n","step: 1040, loss: 0.09136361628770828\n","step: 1050, loss: 0.08496883511543274\n","step: 1060, loss: 0.07965537905693054\n","step: 1070, loss: 0.08244026452302933\n","step: 1080, loss: 0.10117695480585098\n","step: 1090, loss: 0.11129608005285263\n","step: 1100, loss: 0.13400205969810486\n","step: 1110, loss: 0.19404849410057068\n","step: 1120, loss: 0.054796572774648666\n","step: 1130, loss: 0.04851904511451721\n","step: 1140, loss: 0.04011029377579689\n","step: 1150, loss: 0.06498107314109802\n","step: 1160, loss: 0.12416622787714005\n","step: 1170, loss: 0.09413217008113861\n","step: 1180, loss: 0.13953983783721924\n","step: 1190, loss: 0.12082713842391968\n","step: 1200, loss: 0.07850738614797592\n","step: 1210, loss: 0.0974181517958641\n","step: 1220, loss: 0.15768541395664215\n","step: 1230, loss: 0.07940392941236496\n","step: 1240, loss: 0.09626656770706177\n","step: 1250, loss: 0.1029340922832489\n","step: 1260, loss: 0.12831635773181915\n","step: 1270, loss: 0.06921873986721039\n","step: 1280, loss: 0.08612269163131714\n","step: 1290, loss: 0.09062336385250092\n","step: 1300, loss: 0.11379490792751312\n","step: 1310, loss: 0.0792471319437027\n","step: 1320, loss: 0.01939551532268524\n","step: 1330, loss: 0.15468934178352356\n","step: 1340, loss: 0.06715630739927292\n","step: 1350, loss: 0.1011827141046524\n","step: 1360, loss: 0.04844177886843681\n","step: 1370, loss: 0.14325468242168427\n","step: 1380, loss: 0.04888425022363663\n","step: 1390, loss: 0.11626734584569931\n","step: 1400, loss: 0.10488063842058182\n","step: 1410, loss: 0.08957640081644058\n","step: 1420, loss: 0.11818712949752808\n","step: 1430, loss: 0.09906430542469025\n","step: 1440, loss: 0.10934162884950638\n","step: 1450, loss: 0.1500290334224701\n","step: 1460, loss: 0.1763617992401123\n","step: 1470, loss: 0.1390293687582016\n","step: 1480, loss: 0.09092753380537033\n","step: 1490, loss: 0.1376177966594696\n","step: 1500, loss: 0.09937035292387009\n","step: 1510, loss: 0.11049053817987442\n","step: 1520, loss: 0.23666594922542572\n","step: 1530, loss: 0.042906902730464935\n","step: 1540, loss: 0.08811412006616592\n","step: 1550, loss: 0.06600502133369446\n","step: 1560, loss: 0.0806199312210083\n","step: 1570, loss: 0.05370514094829559\n","step: 1580, loss: 0.05890817940235138\n","step: 1590, loss: 0.04232423007488251\n","step: 1600, loss: 0.08317983150482178\n","step: 1610, loss: 0.02999921329319477\n","step: 1620, loss: 0.12928877770900726\n","step: 1630, loss: 0.13700620830059052\n","step: 1640, loss: 0.09749604761600494\n","step: 1650, loss: 0.03706282377243042\n","step: 1660, loss: 0.10829105973243713\n","step: 1670, loss: 0.11265072971582413\n","step: 1680, loss: 0.057725053280591965\n","step: 1690, loss: 0.0561845637857914\n","step: 1700, loss: 0.08053665608167648\n","step: 1710, loss: 0.13533416390419006\n","step: 1720, loss: 0.09519454836845398\n","step: 1730, loss: 0.07823671400547028\n","step: 1740, loss: 0.10736773908138275\n","step: 1750, loss: 0.18006214499473572\n","step: 1760, loss: 0.0779685229063034\n","step: 1770, loss: 0.057665664702653885\n","step: 1780, loss: 0.11600036174058914\n","step: 1790, loss: 0.09728170931339264\n","step: 1800, loss: 0.11024459451436996\n","step: 1810, loss: 0.1315680593252182\n","step: 1820, loss: 0.018135391175746918\n","step: 1830, loss: 0.12982133030891418\n","step: 1840, loss: 0.02317817322909832\n","step: 1850, loss: 0.1504322737455368\n","step: 1860, loss: 0.030921075493097305\n","step: 1870, loss: 0.1016460508108139\n","step: 1880, loss: 0.09170790016651154\n","step: 1890, loss: 0.0829256922006607\n","step: 1900, loss: 0.05215698480606079\n","step: 1910, loss: 0.06625168025493622\n","step: 1920, loss: 0.05651245638728142\n","step: 1930, loss: 0.09950949251651764\n","step: 1940, loss: 0.12973572313785553\n","step: 1950, loss: 0.09046246856451035\n","step: 1960, loss: 0.12783995270729065\n","step: 1970, loss: 0.04630294814705849\n","step: 1980, loss: 0.08261121809482574\n","step: 1990, loss: 0.0845143273472786\n","step: 2000, loss: 0.09042579680681229\n","step: 2010, loss: 0.11186864972114563\n","step: 2020, loss: 0.12656007707118988\n","step: 2030, loss: 0.0630398839712143\n","step: 2040, loss: 0.10318474471569061\n","step: 2050, loss: 0.11125878989696503\n","step: 2060, loss: 0.05854742228984833\n","step: 2070, loss: 0.12623970210552216\n","step: 2080, loss: 0.05160052701830864\n","step: 2090, loss: 0.10683239996433258\n","step: 2100, loss: 0.13289065659046173\n","step: 2110, loss: 0.10627247393131256\n","step: 2120, loss: 0.09400627017021179\n","step: 2130, loss: 0.06795654445886612\n","step: 2140, loss: 0.13314571976661682\n","step: 2150, loss: 0.06107238680124283\n","step: 2160, loss: 0.06639493256807327\n","step: 2170, loss: 0.07561145722866058\n","step: 2180, loss: 0.05835912376642227\n","step: 2190, loss: 0.04832573980093002\n","step: 2200, loss: 0.03938855975866318\n","step: 2210, loss: 0.05578817427158356\n","step: 2220, loss: 0.03784559294581413\n","step: 2230, loss: 0.06419753283262253\n","step: 2240, loss: 0.10235855728387833\n","step: 2250, loss: 0.028320888057351112\n","step: 2260, loss: 0.10815981775522232\n","step: 2270, loss: 0.10766690969467163\n","step: 2280, loss: 0.04473981633782387\n","step: 2290, loss: 0.1679670661687851\n","step: 2300, loss: 0.11268544942140579\n","step: 2310, loss: 0.09288112819194794\n","step: 2320, loss: 0.06029937416315079\n","step: 2330, loss: 0.09830879420042038\n","step: 2340, loss: 0.09157402068376541\n","step: 2350, loss: 0.07816412299871445\n","step: 2360, loss: 0.07306497544050217\n","step: 2370, loss: 0.09757611155509949\n","step: 2380, loss: 0.07369345426559448\n","step: 2390, loss: 0.08500853925943375\n","step: 2400, loss: 0.11543794721364975\n","step: 2410, loss: 0.11507919430732727\n","step: 2420, loss: 0.08126037567853928\n","step: 2430, loss: 0.23056018352508545\n","step: 2440, loss: 0.11547361314296722\n","step: 2450, loss: 0.14719125628471375\n","step: 2460, loss: 0.05586947128176689\n","step: 2470, loss: 0.06419836729764938\n","step: 2480, loss: 0.06777890771627426\n","step: 2490, loss: 0.06389138847589493\n","step: 2500, loss: 0.06583132594823837\n","step: 2510, loss: 0.04877138510346413\n","step: 2520, loss: 0.07687515765428543\n","step: 2530, loss: 0.11300598084926605\n","step: 2540, loss: 0.13352930545806885\n","step: 2550, loss: 0.06596332043409348\n","step: 2560, loss: 0.07924135029315948\n","step: 2570, loss: 0.06052207574248314\n","step: 2580, loss: 0.19740241765975952\n","step: 2590, loss: 0.09669137001037598\n","step: 2600, loss: 0.0420570969581604\n","step: 2610, loss: 0.07155558466911316\n","step: 2620, loss: 0.16349934041500092\n","step: 2630, loss: 0.05471210181713104\n","step: 2640, loss: 0.11841274052858353\n","step: 2650, loss: 0.03446469455957413\n","step: 2660, loss: 0.05985158681869507\n","step: 2670, loss: 0.09608228504657745\n","step: 2680, loss: 0.06758876144886017\n","step: 2690, loss: 0.1043301597237587\n","step: 2700, loss: 0.06247471645474434\n","step: 2710, loss: 0.10162117332220078\n","step: 2720, loss: 0.07229023426771164\n","step: 2730, loss: 0.07439204305410385\n","step: 2740, loss: 0.08999435603618622\n","step: 2750, loss: 0.04369521141052246\n","step: 2760, loss: 0.13690917193889618\n","step: 2770, loss: 0.0731591209769249\n","step: 2780, loss: 0.21589520573616028\n","step: 2790, loss: 0.06809356063604355\n","step: 2800, loss: 0.14038372039794922\n","step: 2810, loss: 0.06653505563735962\n","step: 2820, loss: 0.039593156427145004\n","step: 2830, loss: 0.034207116812467575\n","step: 2840, loss: 0.08064211905002594\n","step: 2850, loss: 0.08210888504981995\n","step: 2860, loss: 0.05685464292764664\n","step: 2870, loss: 0.04588723182678223\n","step: 2880, loss: 0.16039922833442688\n","step: 2890, loss: 0.052028730511665344\n","step: 2900, loss: 0.07727383822202682\n","step: 2910, loss: 0.028478603810071945\n","step: 2920, loss: 0.16624553501605988\n","step: 2930, loss: 0.08123063296079636\n","step: 2940, loss: 0.05828133225440979\n","step: 2950, loss: 0.1753804087638855\n","step: 2960, loss: 0.11747881770133972\n","step: 2970, loss: 0.05960235744714737\n","step: 2980, loss: 0.08593049645423889\n","step: 2990, loss: 0.2184889167547226\n","step: 3000, loss: 0.03421181067824364\n","step: 3010, loss: 0.11134468764066696\n","step: 3020, loss: 0.05658770352602005\n","step: 3030, loss: 0.08942240476608276\n","step: 3040, loss: 0.06532949954271317\n","step: 3050, loss: 0.09965993463993073\n","step: 3060, loss: 0.16693523526191711\n","step: 3070, loss: 0.049867160618305206\n","step: 3080, loss: 0.02394605427980423\n","step: 3090, loss: 0.014524162746965885\n","step: 3100, loss: 0.030133817344903946\n","step: 3110, loss: 0.10969206690788269\n","step: 3120, loss: 0.11655396223068237\n","step: 3130, loss: 0.09724454581737518\n","step: 3140, loss: 0.06944312155246735\n","step: 3150, loss: 0.0662144273519516\n","step: 3160, loss: 0.07868785411119461\n","step: 3170, loss: 0.06570658832788467\n","step: 3180, loss: 0.07333934307098389\n","step: 3190, loss: 0.09447761625051498\n","step: 3200, loss: 0.08365719765424728\n","step: 3210, loss: 0.13242581486701965\n","step: 3220, loss: 0.11052155494689941\n","step: 3230, loss: 0.09991271793842316\n","step: 3240, loss: 0.15351873636245728\n","step: 3250, loss: 0.10607557743787766\n","step: 3260, loss: 0.14777438342571259\n","step: 3270, loss: 0.08116406947374344\n","step: 3280, loss: 0.1173897460103035\n","step: 3290, loss: 0.10100127011537552\n","step: 3300, loss: 0.056561872363090515\n","step: 3310, loss: 0.09832461178302765\n","step: 3320, loss: 0.017887571826577187\n","step: 3330, loss: 0.09269698709249496\n","step: 3340, loss: 0.06358934938907623\n","step: 3350, loss: 0.1473475992679596\n","step: 3360, loss: 0.04003777727484703\n","step: 3370, loss: 0.06106332316994667\n","step: 3380, loss: 0.06511852145195007\n","step: 3390, loss: 0.061933666467666626\n","step: 3400, loss: 0.07784813642501831\n","step: 3410, loss: 0.12090663611888885\n","step: 3420, loss: 0.09598105400800705\n","step: 3430, loss: 0.10646851360797882\n","step: 3440, loss: 0.11135145276784897\n","step: 3450, loss: 0.16665205359458923\n","step: 3460, loss: 0.12072495371103287\n","step: 3470, loss: 0.12714926898479462\n","step: 3480, loss: 0.09092573821544647\n","step: 3490, loss: 0.1299332082271576\n","step: 3500, loss: 0.12343363463878632\n","step: 3510, loss: 0.08534280210733414\n","step: 3520, loss: 0.10442708432674408\n","step: 3530, loss: 0.09403656423091888\n","step: 3540, loss: 0.17799672484397888\n","step: 3550, loss: 0.06916766613721848\n","step: 3560, loss: 0.03974282369017601\n","step: 3570, loss: 0.025543296709656715\n","step: 3580, loss: 0.04294343292713165\n","step: 3590, loss: 0.018673667684197426\n","step: 3600, loss: 0.13493169844150543\n","step: 3610, loss: 0.101793572306633\n","step: 3620, loss: 0.0614636205136776\n","step: 3630, loss: 0.0847383663058281\n","step: 3640, loss: 0.06253904849290848\n","step: 3650, loss: 0.22707189619541168\n","step: 3660, loss: 0.018158381804823875\n","step: 3670, loss: 0.09196075052022934\n","step: 3680, loss: 0.021640505641698837\n","step: 3690, loss: 0.1222328245639801\n","step: 3700, loss: 0.169861301779747\n","step: 3710, loss: 0.041071102023124695\n","step: 3720, loss: 0.07778304815292358\n","step: 3730, loss: 0.07466237992048264\n","step: 3740, loss: 0.07082589715719223\n","step: 3750, loss: 0.0560004860162735\n","step: 3760, loss: 0.017363818362355232\n","step: 3770, loss: 0.09236041456460953\n","step: 3780, loss: 0.12897008657455444\n","step: 3790, loss: 0.08553136140108109\n","step: 3800, loss: 0.15480370819568634\n","step: 3810, loss: 0.06835249811410904\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      1.00      0.89        35\n","           2       0.26      0.12      0.16        77\n","           3       0.97      0.79      0.87      1030\n","           4       0.95      0.84      0.89       291\n","           5       0.99      0.81      0.89       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.54      0.94      0.69       186\n","           8       0.00      0.00      0.00        11\n","           9       0.94      0.99      0.97       689\n","          10       0.93      0.98      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.67      0.77      0.71        13\n","          14       0.23      1.00      0.37        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.84      0.84      0.84      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.89      1.00      0.94        32\n","          19       0.14      0.20      0.16        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.74      0.83      4175\n","          23       0.71      0.96      0.81      2253\n","          24       0.36      0.20      0.26        44\n","          25       0.85      0.95      0.90       888\n","          26       0.90      1.00      0.95         9\n","          27       0.96      0.99      0.97        69\n","          28       0.99      0.97      0.98      1864\n","          29       0.98      0.99      0.99       344\n","          30       0.88      0.86      0.87      1136\n","          31       0.62      0.68      0.65        19\n","          32       1.00      0.75      0.86         8\n","          33       0.67      0.88      0.76        86\n","          34       0.21      0.53      0.30        32\n","          35       0.99      0.99      0.99       474\n","          36       1.00      0.10      0.19       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.98      0.97      0.97       404\n","          39       0.99      0.90      0.94       485\n","          40       0.91      0.92      0.92       573\n","          41       0.96      0.93      0.94       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.86      0.91       152\n","          44       0.87      0.92      0.90        75\n","          46       0.99      0.99      0.99        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.77      0.79      0.76     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 441\n","\n","Loop 28\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.865830421447754\n","step: 10, loss: 1.869805932044983\n","step: 20, loss: 0.6822465658187866\n","step: 30, loss: 0.4888271987438202\n","step: 40, loss: 0.23511338233947754\n","step: 50, loss: 0.2991226017475128\n","step: 60, loss: 0.21645094454288483\n","step: 70, loss: 0.2098425328731537\n","step: 80, loss: 0.12764345109462738\n","step: 90, loss: 0.16210715472698212\n","step: 100, loss: 0.15273278951644897\n","step: 110, loss: 0.19344276189804077\n","step: 120, loss: 0.1773684322834015\n","step: 130, loss: 0.1261894553899765\n","step: 140, loss: 0.3228941857814789\n","step: 150, loss: 0.17845939099788666\n","step: 160, loss: 0.08669593930244446\n","step: 170, loss: 0.07739907503128052\n","step: 180, loss: 0.17026059329509735\n","step: 190, loss: 0.1441430300474167\n","step: 200, loss: 0.15276585519313812\n","step: 210, loss: 0.1181754395365715\n","step: 220, loss: 0.06466468423604965\n","step: 230, loss: 0.10600832104682922\n","step: 240, loss: 0.14771506190299988\n","step: 250, loss: 0.1073315367102623\n","step: 260, loss: 0.1391608715057373\n","step: 270, loss: 0.16171374917030334\n","step: 280, loss: 0.11345255374908447\n","step: 290, loss: 0.21032576262950897\n","step: 300, loss: 0.21634888648986816\n","step: 310, loss: 0.11228367686271667\n","step: 320, loss: 0.09826841205358505\n","step: 330, loss: 0.12212542444467545\n","step: 340, loss: 0.10395471751689911\n","step: 350, loss: 0.06878345459699631\n","step: 360, loss: 0.0858737975358963\n","step: 370, loss: 0.13039200007915497\n","step: 380, loss: 0.1053071841597557\n","step: 390, loss: 0.0857434943318367\n","step: 400, loss: 0.15484851598739624\n","step: 410, loss: 0.1292489469051361\n","step: 420, loss: 0.1270947903394699\n","step: 430, loss: 0.08011142164468765\n","step: 440, loss: 0.02947072871029377\n","step: 450, loss: 0.09735802561044693\n","step: 460, loss: 0.1426040679216385\n","step: 470, loss: 0.04739989712834358\n","step: 480, loss: 0.012990145944058895\n","step: 490, loss: 0.13838914036750793\n","step: 500, loss: 0.1530454307794571\n","step: 510, loss: 0.12060385942459106\n","step: 520, loss: 0.1867952048778534\n","step: 530, loss: 0.1225326731801033\n","step: 540, loss: 0.1271613985300064\n","step: 550, loss: 0.03681853041052818\n","step: 560, loss: 0.15759974718093872\n","step: 570, loss: 0.19825948774814606\n","step: 580, loss: 0.02167411334812641\n","step: 590, loss: 0.0606331042945385\n","step: 600, loss: 0.08447733521461487\n","step: 610, loss: 0.05388982594013214\n","step: 620, loss: 0.09063238650560379\n","step: 630, loss: 0.11761985719203949\n","step: 640, loss: 0.08172726631164551\n","step: 650, loss: 0.12265287339687347\n","step: 660, loss: 0.04134256765246391\n","step: 670, loss: 0.12572512030601501\n","step: 680, loss: 0.1445106863975525\n","step: 690, loss: 0.10871893912553787\n","step: 700, loss: 0.11680985987186432\n","step: 710, loss: 0.049542468041181564\n","step: 720, loss: 0.06301885098218918\n","step: 730, loss: 0.155633807182312\n","step: 740, loss: 0.050042230635881424\n","step: 750, loss: 0.0573238767683506\n","step: 760, loss: 0.07881373912096024\n","step: 770, loss: 0.07632491737604141\n","step: 780, loss: 0.04991205036640167\n","step: 790, loss: 0.14384092390537262\n","step: 800, loss: 0.07242050766944885\n","step: 810, loss: 0.05695665627717972\n","step: 820, loss: 0.030842795968055725\n","step: 830, loss: 0.10317918658256531\n","step: 840, loss: 0.07475687563419342\n","step: 850, loss: 0.10905809700489044\n","step: 860, loss: 0.10957008600234985\n","step: 870, loss: 0.0777755156159401\n","step: 880, loss: 0.06119473651051521\n","step: 890, loss: 0.07421012967824936\n","step: 900, loss: 0.07347098737955093\n","step: 910, loss: 0.11964675039052963\n","step: 920, loss: 0.12709300220012665\n","step: 930, loss: 0.13801954686641693\n","step: 940, loss: 0.15408875048160553\n","step: 950, loss: 0.13897265493869781\n","step: 960, loss: 0.08905542641878128\n","step: 970, loss: 0.14024683833122253\n","step: 980, loss: 0.13129477202892303\n","step: 990, loss: 0.03680814802646637\n","step: 1000, loss: 0.13049110770225525\n","step: 1010, loss: 0.10422047972679138\n","step: 1020, loss: 0.09249232709407806\n","step: 1030, loss: 0.07249537110328674\n","step: 1040, loss: 0.04556950181722641\n","step: 1050, loss: 0.09590472280979156\n","step: 1060, loss: 0.07638800144195557\n","step: 1070, loss: 0.15734505653381348\n","step: 1080, loss: 0.1542634218931198\n","step: 1090, loss: 0.1520196795463562\n","step: 1100, loss: 0.04394960403442383\n","step: 1110, loss: 0.08333852142095566\n","step: 1120, loss: 0.06508149206638336\n","step: 1130, loss: 0.07114706933498383\n","step: 1140, loss: 0.02724802866578102\n","step: 1150, loss: 0.05592990666627884\n","step: 1160, loss: 0.045232586562633514\n","step: 1170, loss: 0.08708526939153671\n","step: 1180, loss: 0.10148395597934723\n","step: 1190, loss: 0.08123141527175903\n","step: 1200, loss: 0.1626257449388504\n","step: 1210, loss: 0.046208035200834274\n","step: 1220, loss: 0.06479837745428085\n","step: 1230, loss: 0.11150304228067398\n","step: 1240, loss: 0.0813007652759552\n","step: 1250, loss: 0.07222074270248413\n","step: 1260, loss: 0.10752984881401062\n","step: 1270, loss: 0.12501661479473114\n","step: 1280, loss: 0.08107230812311172\n","step: 1290, loss: 0.04508482664823532\n","step: 1300, loss: 0.12791971862316132\n","step: 1310, loss: 0.06598344445228577\n","step: 1320, loss: 0.04427356645464897\n","step: 1330, loss: 0.07016897946596146\n","step: 1340, loss: 0.05792240798473358\n","step: 1350, loss: 0.08482906967401505\n","step: 1360, loss: 0.14491085708141327\n","step: 1370, loss: 0.0894053503870964\n","step: 1380, loss: 0.07075444608926773\n","step: 1390, loss: 0.08336157351732254\n","step: 1400, loss: 0.08550439774990082\n","step: 1410, loss: 0.11918482929468155\n","step: 1420, loss: 0.1815839558839798\n","step: 1430, loss: 0.05193942040205002\n","step: 1440, loss: 0.09451422095298767\n","step: 1450, loss: 0.06233710050582886\n","step: 1460, loss: 0.0862022116780281\n","step: 1470, loss: 0.06794747710227966\n","step: 1480, loss: 0.11438104510307312\n","step: 1490, loss: 0.059407442808151245\n","step: 1500, loss: 0.1849704384803772\n","step: 1510, loss: 0.1210377886891365\n","step: 1520, loss: 0.05624140799045563\n","step: 1530, loss: 0.1061859279870987\n","step: 1540, loss: 0.09797118604183197\n","step: 1550, loss: 0.04085332527756691\n","step: 1560, loss: 0.15997257828712463\n","step: 1570, loss: 0.07555383443832397\n","step: 1580, loss: 0.1336061805486679\n","step: 1590, loss: 0.13134092092514038\n","step: 1600, loss: 0.030047252774238586\n","step: 1610, loss: 0.06673859804868698\n","step: 1620, loss: 0.11718085408210754\n","step: 1630, loss: 0.08675200492143631\n","step: 1640, loss: 0.10037223994731903\n","step: 1650, loss: 0.13516585528850555\n","step: 1660, loss: 0.08597122877836227\n","step: 1670, loss: 0.04202575981616974\n","step: 1680, loss: 0.0876254290342331\n","step: 1690, loss: 0.0808904692530632\n","step: 1700, loss: 0.05141480267047882\n","step: 1710, loss: 0.1254267692565918\n","step: 1720, loss: 0.04353920742869377\n","step: 1730, loss: 0.14720876514911652\n","step: 1740, loss: 0.08614449203014374\n","step: 1750, loss: 0.11680534482002258\n","step: 1760, loss: 0.08411232382059097\n","step: 1770, loss: 0.19806335866451263\n","step: 1780, loss: 0.030298832803964615\n","step: 1790, loss: 0.048652537167072296\n","step: 1800, loss: 0.03527626395225525\n","step: 1810, loss: 0.1021939367055893\n","step: 1820, loss: 0.046183377504348755\n","step: 1830, loss: 0.12646089494228363\n","step: 1840, loss: 0.10618660598993301\n","step: 1850, loss: 0.13870437443256378\n","step: 1860, loss: 0.2041284739971161\n","step: 1870, loss: 0.23582984507083893\n","step: 1880, loss: 0.12552852928638458\n","step: 1890, loss: 0.10802964121103287\n","step: 1900, loss: 0.20951704680919647\n","step: 1910, loss: 0.14217440783977509\n","step: 1920, loss: 0.05885159224271774\n","step: 1930, loss: 0.09580414742231369\n","step: 1940, loss: 0.25806179642677307\n","step: 1950, loss: 0.04634516313672066\n","step: 1960, loss: 0.05276040360331535\n","step: 1970, loss: 0.14360499382019043\n","step: 1980, loss: 0.05963175371289253\n","step: 1990, loss: 0.06593623012304306\n","step: 2000, loss: 0.15291127562522888\n","step: 2010, loss: 0.13166223466396332\n","step: 2020, loss: 0.06516133248806\n","step: 2030, loss: 0.12558159232139587\n","step: 2040, loss: 0.16461341083049774\n","step: 2050, loss: 0.10033168643712997\n","step: 2060, loss: 0.034968383610248566\n","step: 2070, loss: 0.1145523339509964\n","step: 2080, loss: 0.13212274014949799\n","step: 2090, loss: 0.144795760512352\n","step: 2100, loss: 0.20246046781539917\n","step: 2110, loss: 0.17523902654647827\n","step: 2120, loss: 0.06113957613706589\n","step: 2130, loss: 0.15424157679080963\n","step: 2140, loss: 0.11960494518280029\n","step: 2150, loss: 0.11778184026479721\n","step: 2160, loss: 0.039959825575351715\n","step: 2170, loss: 0.09039274603128433\n","step: 2180, loss: 0.09620625525712967\n","step: 2190, loss: 0.04817880317568779\n","step: 2200, loss: 0.09485992789268494\n","step: 2210, loss: 0.022902024909853935\n","step: 2220, loss: 0.04219388589262962\n","step: 2230, loss: 0.018301641568541527\n","step: 2240, loss: 0.16429850459098816\n","step: 2250, loss: 0.16333746910095215\n","step: 2260, loss: 0.06665771454572678\n","step: 2270, loss: 0.029682185500860214\n","step: 2280, loss: 0.04907411336898804\n","step: 2290, loss: 0.11997506767511368\n","step: 2300, loss: 0.05816347897052765\n","step: 2310, loss: 0.11674046516418457\n","step: 2320, loss: 0.0800861194729805\n","step: 2330, loss: 0.05043166130781174\n","step: 2340, loss: 0.04781120643019676\n","step: 2350, loss: 0.08625486493110657\n","step: 2360, loss: 0.04413873702287674\n","step: 2370, loss: 0.10206229239702225\n","step: 2380, loss: 0.10057510435581207\n","step: 2390, loss: 0.04174335300922394\n","step: 2400, loss: 0.10013782232999802\n","step: 2410, loss: 0.08584441989660263\n","step: 2420, loss: 0.08432623744010925\n","step: 2430, loss: 0.08846569061279297\n","step: 2440, loss: 0.10190296918153763\n","step: 2450, loss: 0.12620940804481506\n","step: 2460, loss: 0.060802094638347626\n","step: 2470, loss: 0.11095990240573883\n","step: 2480, loss: 0.17884016036987305\n","step: 2490, loss: 0.10141213983297348\n","step: 2500, loss: 0.06000054255127907\n","step: 2510, loss: 0.09879812598228455\n","step: 2520, loss: 0.23096828162670135\n","step: 2530, loss: 0.07396664470434189\n","step: 2540, loss: 0.1477346122264862\n","step: 2550, loss: 0.1495026797056198\n","step: 2560, loss: 0.043343622237443924\n","step: 2570, loss: 0.0826789140701294\n","step: 2580, loss: 0.05109713599085808\n","step: 2590, loss: 0.028013627976179123\n","step: 2600, loss: 0.11889725923538208\n","step: 2610, loss: 0.050946708768606186\n","step: 2620, loss: 0.09531048685312271\n","step: 2630, loss: 0.058787379413843155\n","step: 2640, loss: 0.09987323731184006\n","step: 2650, loss: 0.06738487631082535\n","step: 2660, loss: 0.10060375183820724\n","step: 2670, loss: 0.11162185668945312\n","step: 2680, loss: 0.07660818099975586\n","step: 2690, loss: 0.0642157793045044\n","step: 2700, loss: 0.17867310345172882\n","step: 2710, loss: 0.10912176966667175\n","step: 2720, loss: 0.1570737063884735\n","step: 2730, loss: 0.08193603157997131\n","step: 2740, loss: 0.1083492636680603\n","step: 2750, loss: 0.06513939052820206\n","step: 2760, loss: 0.04978732764720917\n","step: 2770, loss: 0.12352681905031204\n","step: 2780, loss: 0.08428944647312164\n","step: 2790, loss: 0.043305061757564545\n","step: 2800, loss: 0.08499882370233536\n","step: 2810, loss: 0.0989793911576271\n","step: 2820, loss: 0.05511561036109924\n","step: 2830, loss: 0.2191455066204071\n","step: 2840, loss: 0.08199408650398254\n","step: 2850, loss: 0.07103248685598373\n","step: 2860, loss: 0.04006921872496605\n","step: 2870, loss: 0.14396482706069946\n","step: 2880, loss: 0.16412363946437836\n","step: 2890, loss: 0.13596351444721222\n","step: 2900, loss: 0.08595984429121017\n","step: 2910, loss: 0.1375676989555359\n","step: 2920, loss: 0.0994010791182518\n","step: 2930, loss: 0.053230565041303635\n","step: 2940, loss: 0.10050389170646667\n","step: 2950, loss: 0.06375407427549362\n","step: 2960, loss: 0.10594986379146576\n","step: 2970, loss: 0.05972445756196976\n","step: 2980, loss: 0.04201122745871544\n","step: 2990, loss: 0.11465861648321152\n","step: 3000, loss: 0.08918178081512451\n","step: 3010, loss: 0.18187707662582397\n","step: 3020, loss: 0.05809905752539635\n","step: 3030, loss: 0.10146818310022354\n","step: 3040, loss: 0.0968305915594101\n","step: 3050, loss: 0.07970890402793884\n","step: 3060, loss: 0.08171732723712921\n","step: 3070, loss: 0.05426414683461189\n","step: 3080, loss: 0.12277084589004517\n","step: 3090, loss: 0.02975703589618206\n","step: 3100, loss: 0.024121984839439392\n","step: 3110, loss: 0.11411968618631363\n","step: 3120, loss: 0.036483488976955414\n","step: 3130, loss: 0.07866951078176498\n","step: 3140, loss: 0.16360542178153992\n","step: 3150, loss: 0.07501361519098282\n","step: 3160, loss: 0.021847739815711975\n","step: 3170, loss: 0.0778321623802185\n","step: 3180, loss: 0.19881044328212738\n","step: 3190, loss: 0.058556411415338516\n","step: 3200, loss: 0.17326247692108154\n","step: 3210, loss: 0.14459654688835144\n","step: 3220, loss: 0.10348778963088989\n","step: 3230, loss: 0.04163672402501106\n","step: 3240, loss: 0.12124144285917282\n","step: 3250, loss: 0.07270093262195587\n","step: 3260, loss: 0.09811899065971375\n","step: 3270, loss: 0.04958204925060272\n","step: 3280, loss: 0.14524666965007782\n","step: 3290, loss: 0.08117782324552536\n","step: 3300, loss: 0.10002422332763672\n","step: 3310, loss: 0.055045243352651596\n","step: 3320, loss: 0.11457424610853195\n","step: 3330, loss: 0.05778000131249428\n","step: 3340, loss: 0.06912314146757126\n","step: 3350, loss: 0.08357710391283035\n","step: 3360, loss: 0.08671645820140839\n","step: 3370, loss: 0.09061786532402039\n","step: 3380, loss: 0.061782896518707275\n","step: 3390, loss: 0.09798536449670792\n","step: 3400, loss: 0.13417230546474457\n","step: 3410, loss: 0.04274865612387657\n","step: 3420, loss: 0.07579763233661652\n","step: 3430, loss: 0.056234344840049744\n","step: 3440, loss: 0.0614321231842041\n","step: 3450, loss: 0.09478746354579926\n","step: 3460, loss: 0.06537505239248276\n","step: 3470, loss: 0.1000250056385994\n","step: 3480, loss: 0.08673951774835587\n","step: 3490, loss: 0.044888414442539215\n","step: 3500, loss: 0.08584953844547272\n","step: 3510, loss: 0.1965692788362503\n","step: 3520, loss: 0.03193575516343117\n","step: 3530, loss: 0.034653451293706894\n","step: 3540, loss: 0.07567895948886871\n","step: 3550, loss: 0.08320413529872894\n","step: 3560, loss: 0.09627261012792587\n","step: 3570, loss: 0.14438608288764954\n","step: 3580, loss: 0.08564282208681107\n","step: 3590, loss: 0.058799706399440765\n","step: 3600, loss: 0.07188396155834198\n","step: 3610, loss: 0.05345086380839348\n","step: 3620, loss: 0.11634130775928497\n","step: 3630, loss: 0.014236882328987122\n","step: 3640, loss: 0.13700351119041443\n","step: 3650, loss: 0.10160082578659058\n","step: 3660, loss: 0.07279238849878311\n","step: 3670, loss: 0.07691209018230438\n","step: 3680, loss: 0.16082826256752014\n","step: 3690, loss: 0.0394585095345974\n","step: 3700, loss: 0.1108328253030777\n","step: 3710, loss: 0.09228646755218506\n","step: 3720, loss: 0.13765963912010193\n","step: 3730, loss: 0.08812691271305084\n","step: 3740, loss: 0.03976793959736824\n","step: 3750, loss: 0.011545388959348202\n","step: 3760, loss: 0.1248965933918953\n","step: 3770, loss: 0.11579013615846634\n","step: 3780, loss: 0.05611563101410866\n","step: 3790, loss: 0.04231724888086319\n","step: 3800, loss: 0.181748628616333\n","step: 3810, loss: 0.13782894611358643\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.71      0.06      0.12        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.88      0.83      0.85       291\n","           5       0.74      0.89      0.81       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.63      0.94      0.75       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.93      0.98      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.53      0.77      0.62        13\n","          14       0.40      1.00      0.57        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.90      0.83      0.86      1151\n","          17       0.95      0.95      0.95        41\n","          18       0.94      0.97      0.95        32\n","          19       0.00      0.00      0.00        40\n","          20       1.00      1.00      1.00       584\n","          21       0.11      0.06      0.08        52\n","          22       0.92      0.78      0.84      4175\n","          23       0.69      0.96      0.81      2253\n","          24       0.17      0.18      0.18        44\n","          25       0.87      0.90      0.89       888\n","          26       1.00      0.11      0.20         9\n","          27       0.93      0.97      0.95        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.96      0.82      0.88      1136\n","          31       0.57      0.84      0.68        19\n","          32       1.00      0.62      0.77         8\n","          33       0.66      0.98      0.79        86\n","          34       0.26      0.78      0.39        32\n","          35       0.99      0.99      0.99       474\n","          36       0.88      0.12      0.21       182\n","          37       0.90      0.94      0.92      1592\n","          38       0.95      0.98      0.97       404\n","          39       0.99      0.90      0.94       485\n","          40       0.91      0.97      0.94       573\n","          41       0.94      0.95      0.94       841\n","          42       0.99      0.99      0.99       575\n","          43       0.93      0.91      0.92       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.04      0.07        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.77      0.75     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","Difference 443\n","\n","Loop 29\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9922983646392822\n","step: 10, loss: 1.6437197923660278\n","step: 20, loss: 0.8478151559829712\n","step: 30, loss: 0.5150139331817627\n","step: 40, loss: 0.22727078199386597\n","step: 50, loss: 0.18755610287189484\n","step: 60, loss: 0.20724275708198547\n","step: 70, loss: 0.13899581134319305\n","step: 80, loss: 0.22636911273002625\n","step: 90, loss: 0.14528191089630127\n","step: 100, loss: 0.23487268388271332\n","step: 110, loss: 0.1257224678993225\n","step: 120, loss: 0.18211515247821808\n","step: 130, loss: 0.22295139729976654\n","step: 140, loss: 0.10150113701820374\n","step: 150, loss: 0.2754174470901489\n","step: 160, loss: 0.13848818838596344\n","step: 170, loss: 0.13168363273143768\n","step: 180, loss: 0.06020205467939377\n","step: 190, loss: 0.17599567770957947\n","step: 200, loss: 0.16285201907157898\n","step: 210, loss: 0.1301746815443039\n","step: 220, loss: 0.1297040730714798\n","step: 230, loss: 0.06382784247398376\n","step: 240, loss: 0.11433518677949905\n","step: 250, loss: 0.1872488111257553\n","step: 260, loss: 0.16935577988624573\n","step: 270, loss: 0.194815993309021\n","step: 280, loss: 0.0868341326713562\n","step: 290, loss: 0.041205838322639465\n","step: 300, loss: 0.0891852006316185\n","step: 310, loss: 0.08789276331663132\n","step: 320, loss: 0.15177737176418304\n","step: 330, loss: 0.06960386782884598\n","step: 340, loss: 0.06633870303630829\n","step: 350, loss: 0.16552287340164185\n","step: 360, loss: 0.15876927971839905\n","step: 370, loss: 0.16515228152275085\n","step: 380, loss: 0.07612832635641098\n","step: 390, loss: 0.1430196315050125\n","step: 400, loss: 0.17517389357089996\n","step: 410, loss: 0.12408173084259033\n","step: 420, loss: 0.05715334042906761\n","step: 430, loss: 0.11783625930547714\n","step: 440, loss: 0.0888887494802475\n","step: 450, loss: 0.05208443105220795\n","step: 460, loss: 0.14174479246139526\n","step: 470, loss: 0.12067276239395142\n","step: 480, loss: 0.05515274778008461\n","step: 490, loss: 0.16953924298286438\n","step: 500, loss: 0.15912987291812897\n","step: 510, loss: 0.11519714444875717\n","step: 520, loss: 0.14709597826004028\n","step: 530, loss: 0.06377911567687988\n","step: 540, loss: 0.1704869270324707\n","step: 550, loss: 0.05717433989048004\n","step: 560, loss: 0.10713497549295425\n","step: 570, loss: 0.07913287729024887\n","step: 580, loss: 0.12847505509853363\n","step: 590, loss: 0.08033978193998337\n","step: 600, loss: 0.1155022457242012\n","step: 610, loss: 0.16018737852573395\n","step: 620, loss: 0.1498601734638214\n","step: 630, loss: 0.1301041692495346\n","step: 640, loss: 0.12208609282970428\n","step: 650, loss: 0.04581330344080925\n","step: 660, loss: 0.04278762638568878\n","step: 670, loss: 0.22780351340770721\n","step: 680, loss: 0.11205703765153885\n","step: 690, loss: 0.10667363554239273\n","step: 700, loss: 0.09022390842437744\n","step: 710, loss: 0.20307336747646332\n","step: 720, loss: 0.17717981338500977\n","step: 730, loss: 0.09266045689582825\n","step: 740, loss: 0.09977441281080246\n","step: 750, loss: 0.13998708128929138\n","step: 760, loss: 0.06960546970367432\n","step: 770, loss: 0.024359408766031265\n","step: 780, loss: 0.08544556796550751\n","step: 790, loss: 0.11757239699363708\n","step: 800, loss: 0.13835026323795319\n","step: 810, loss: 0.09032931923866272\n","step: 820, loss: 0.13737767934799194\n","step: 830, loss: 0.14632517099380493\n","step: 840, loss: 0.09400779008865356\n","step: 850, loss: 0.08010736107826233\n","step: 860, loss: 0.11621209233999252\n","step: 870, loss: 0.05644722282886505\n","step: 880, loss: 0.04978369176387787\n","step: 890, loss: 0.0698503628373146\n","step: 900, loss: 0.03647608682513237\n","step: 910, loss: 0.10355287790298462\n","step: 920, loss: 0.12116006761789322\n","step: 930, loss: 0.11506342142820358\n","step: 940, loss: 0.07974329590797424\n","step: 950, loss: 0.10707524418830872\n","step: 960, loss: 0.05887620896100998\n","step: 970, loss: 0.1491343379020691\n","step: 980, loss: 0.05908743664622307\n","step: 990, loss: 0.09350130707025528\n","step: 1000, loss: 0.07148987799882889\n","step: 1010, loss: 0.05087780952453613\n","step: 1020, loss: 0.12830768525600433\n","step: 1030, loss: 0.02297012321650982\n","step: 1040, loss: 0.019217493012547493\n","step: 1050, loss: 0.21499314904212952\n","step: 1060, loss: 0.10970600694417953\n","step: 1070, loss: 0.08642542362213135\n","step: 1080, loss: 0.09790085256099701\n","step: 1090, loss: 0.09019146114587784\n","step: 1100, loss: 0.21129456162452698\n","step: 1110, loss: 0.10845274478197098\n","step: 1120, loss: 0.07929050177335739\n","step: 1130, loss: 0.10853936523199081\n","step: 1140, loss: 0.2245495468378067\n","step: 1150, loss: 0.08530924469232559\n","step: 1160, loss: 0.08177343755960464\n","step: 1170, loss: 0.1056710034608841\n","step: 1180, loss: 0.10089762508869171\n","step: 1190, loss: 0.1120825856924057\n","step: 1200, loss: 0.04765963926911354\n","step: 1210, loss: 0.05924706161022186\n","step: 1220, loss: 0.16805201768875122\n","step: 1230, loss: 0.0951758474111557\n","step: 1240, loss: 0.12980659306049347\n","step: 1250, loss: 0.11970715969800949\n","step: 1260, loss: 0.06076584383845329\n","step: 1270, loss: 0.08780325204133987\n","step: 1280, loss: 0.08253250271081924\n","step: 1290, loss: 0.1144975796341896\n","step: 1300, loss: 0.06834527105093002\n","step: 1310, loss: 0.06299492716789246\n","step: 1320, loss: 0.12244701385498047\n","step: 1330, loss: 0.1722184270620346\n","step: 1340, loss: 0.04721147567033768\n","step: 1350, loss: 0.11262672394514084\n","step: 1360, loss: 0.09207948297262192\n","step: 1370, loss: 0.1606074869632721\n","step: 1380, loss: 0.05194655433297157\n","step: 1390, loss: 0.19472971558570862\n","step: 1400, loss: 0.0625133141875267\n","step: 1410, loss: 0.024887435138225555\n","step: 1420, loss: 0.11570677161216736\n","step: 1430, loss: 0.1592818796634674\n","step: 1440, loss: 0.12905187904834747\n","step: 1450, loss: 0.08849725127220154\n","step: 1460, loss: 0.08372972905635834\n","step: 1470, loss: 0.13779675960540771\n","step: 1480, loss: 0.0652649775147438\n","step: 1490, loss: 0.06560250371694565\n","step: 1500, loss: 0.0872441902756691\n","step: 1510, loss: 0.06207611784338951\n","step: 1520, loss: 0.23699218034744263\n","step: 1530, loss: 0.09798026084899902\n","step: 1540, loss: 0.1444021463394165\n","step: 1550, loss: 0.0712391808629036\n","step: 1560, loss: 0.22133205831050873\n","step: 1570, loss: 0.09267858415842056\n","step: 1580, loss: 0.0911136269569397\n","step: 1590, loss: 0.12154807895421982\n","step: 1600, loss: 0.1468150019645691\n","step: 1610, loss: 0.08248092979192734\n","step: 1620, loss: 0.2219940572977066\n","step: 1630, loss: 0.1109333485364914\n","step: 1640, loss: 0.07263709604740143\n","step: 1650, loss: 0.08646497875452042\n","step: 1660, loss: 0.15329793095588684\n","step: 1670, loss: 0.12198863923549652\n","step: 1680, loss: 0.15825045108795166\n","step: 1690, loss: 0.1291327178478241\n","step: 1700, loss: 0.060197681188583374\n","step: 1710, loss: 0.10742024332284927\n","step: 1720, loss: 0.10196752846240997\n","step: 1730, loss: 0.02594606764614582\n","step: 1740, loss: 0.10123465210199356\n","step: 1750, loss: 0.06921619176864624\n","step: 1760, loss: 0.18237651884555817\n","step: 1770, loss: 0.11847205460071564\n","step: 1780, loss: 0.10720723867416382\n","step: 1790, loss: 0.08071746677160263\n","step: 1800, loss: 0.09612390398979187\n","step: 1810, loss: 0.0517984963953495\n","step: 1820, loss: 0.06129484251141548\n","step: 1830, loss: 0.06314589083194733\n","step: 1840, loss: 0.056571293622255325\n","step: 1850, loss: 0.04109135642647743\n","step: 1860, loss: 0.08941954374313354\n","step: 1870, loss: 0.08591640740633011\n","step: 1880, loss: 0.09700936079025269\n","step: 1890, loss: 0.13890668749809265\n","step: 1900, loss: 0.05870651826262474\n","step: 1910, loss: 0.06291499733924866\n","step: 1920, loss: 0.05796745792031288\n","step: 1930, loss: 0.0434056892991066\n","step: 1940, loss: 0.053114503622055054\n","step: 1950, loss: 0.09429492056369781\n","step: 1960, loss: 0.07294759899377823\n","step: 1970, loss: 0.16112950444221497\n","step: 1980, loss: 0.06664983928203583\n","step: 1990, loss: 0.10107491910457611\n","step: 2000, loss: 0.06541089713573456\n","step: 2010, loss: 0.15389497578144073\n","step: 2020, loss: 0.1133890375494957\n","step: 2030, loss: 0.08389294147491455\n","step: 2040, loss: 0.08472027629613876\n","step: 2050, loss: 0.04714583978056908\n","step: 2060, loss: 0.09284473955631256\n","step: 2070, loss: 0.14001864194869995\n","step: 2080, loss: 0.1107914075255394\n","step: 2090, loss: 0.04974276199936867\n","step: 2100, loss: 0.08083780854940414\n","step: 2110, loss: 0.17707327008247375\n","step: 2120, loss: 0.08720766752958298\n","step: 2130, loss: 0.11495598405599594\n","step: 2140, loss: 0.1283441036939621\n","step: 2150, loss: 0.16624590754508972\n","step: 2160, loss: 0.07285190373659134\n","step: 2170, loss: 0.09195871651172638\n","step: 2180, loss: 0.11084851622581482\n","step: 2190, loss: 0.19038026034832\n","step: 2200, loss: 0.10327282547950745\n","step: 2210, loss: 0.1230950579047203\n","step: 2220, loss: 0.1256074160337448\n","step: 2230, loss: 0.04240628704428673\n","step: 2240, loss: 0.08779153227806091\n","step: 2250, loss: 0.10374002903699875\n","step: 2260, loss: 0.08839382231235504\n","step: 2270, loss: 0.035273272544145584\n","step: 2280, loss: 0.05551157519221306\n","step: 2290, loss: 0.07475612312555313\n","step: 2300, loss: 0.08212806284427643\n","step: 2310, loss: 0.037119101732969284\n","step: 2320, loss: 0.07292505353689194\n","step: 2330, loss: 0.07154716551303864\n","step: 2340, loss: 0.20310735702514648\n","step: 2350, loss: 0.10343306511640549\n","step: 2360, loss: 0.21173569560050964\n","step: 2370, loss: 0.039100080728530884\n","step: 2380, loss: 0.06281911581754684\n","step: 2390, loss: 0.1258012354373932\n","step: 2400, loss: 0.061953458935022354\n","step: 2410, loss: 0.1874811202287674\n","step: 2420, loss: 0.06849198788404465\n","step: 2430, loss: 0.06613480299711227\n","step: 2440, loss: 0.03219357877969742\n","step: 2450, loss: 0.16234388947486877\n","step: 2460, loss: 0.1396673619747162\n","step: 2470, loss: 0.0830182358622551\n","step: 2480, loss: 0.046589236706495285\n","step: 2490, loss: 0.09053215384483337\n","step: 2500, loss: 0.06328946352005005\n","step: 2510, loss: 0.06543677300214767\n","step: 2520, loss: 0.13706478476524353\n","step: 2530, loss: 0.1127948984503746\n","step: 2540, loss: 0.10018741339445114\n","step: 2550, loss: 0.08167685568332672\n","step: 2560, loss: 0.07644256949424744\n","step: 2570, loss: 0.09306157380342484\n","step: 2580, loss: 0.0661904364824295\n","step: 2590, loss: 0.06164293363690376\n","step: 2600, loss: 0.08197006583213806\n","step: 2610, loss: 0.08591990917921066\n","step: 2620, loss: 0.06609468907117844\n","step: 2630, loss: 0.11587069183588028\n","step: 2640, loss: 0.038664788007736206\n","step: 2650, loss: 0.0850713923573494\n","step: 2660, loss: 0.10969225317239761\n","step: 2670, loss: 0.06458810716867447\n","step: 2680, loss: 0.0832354798913002\n","step: 2690, loss: 0.12694397568702698\n","step: 2700, loss: 0.14554724097251892\n","step: 2710, loss: 0.06056464836001396\n","step: 2720, loss: 0.10799950361251831\n","step: 2730, loss: 0.107246994972229\n","step: 2740, loss: 0.0474892184138298\n","step: 2750, loss: 0.06755296140909195\n","step: 2760, loss: 0.03805871307849884\n","step: 2770, loss: 0.08515210449695587\n","step: 2780, loss: 0.08130493760108948\n","step: 2790, loss: 0.01865629479289055\n","step: 2800, loss: 0.09333115816116333\n","step: 2810, loss: 0.07830840349197388\n","step: 2820, loss: 0.1458219438791275\n","step: 2830, loss: 0.19388647377490997\n","step: 2840, loss: 0.08815266191959381\n","step: 2850, loss: 0.11892429739236832\n","step: 2860, loss: 0.23495320975780487\n","step: 2870, loss: 0.0649140477180481\n","step: 2880, loss: 0.06132807955145836\n","step: 2890, loss: 0.06549103558063507\n","step: 2900, loss: 0.045749444514513016\n","step: 2910, loss: 0.16494899988174438\n","step: 2920, loss: 0.10959722846746445\n","step: 2930, loss: 0.019425133243203163\n","step: 2940, loss: 0.10577438026666641\n","step: 2950, loss: 0.08766450732946396\n","step: 2960, loss: 0.096673883497715\n","step: 2970, loss: 0.0970756858587265\n","step: 2980, loss: 0.0955689400434494\n","step: 2990, loss: 0.12712383270263672\n","step: 3000, loss: 0.10103359073400497\n","step: 3010, loss: 0.02836301364004612\n","step: 3020, loss: 0.046736206859350204\n","step: 3030, loss: 0.22857312858104706\n","step: 3040, loss: 0.033333927392959595\n","step: 3050, loss: 0.06021847948431969\n","step: 3060, loss: 0.09454772621393204\n","step: 3070, loss: 0.09137862175703049\n","step: 3080, loss: 0.1065608337521553\n","step: 3090, loss: 0.0875365361571312\n","step: 3100, loss: 0.1257520169019699\n","step: 3110, loss: 0.1631280481815338\n","step: 3120, loss: 0.060992222279310226\n","step: 3130, loss: 0.03912506625056267\n","step: 3140, loss: 0.06729164719581604\n","step: 3150, loss: 0.1613398939371109\n","step: 3160, loss: 0.08754780888557434\n","step: 3170, loss: 0.03832332789897919\n","step: 3180, loss: 0.08115871250629425\n","step: 3190, loss: 0.11187399923801422\n","step: 3200, loss: 0.053342148661613464\n","step: 3210, loss: 0.13425111770629883\n","step: 3220, loss: 0.07351691275835037\n","step: 3230, loss: 0.11569671332836151\n","step: 3240, loss: 0.08866448700428009\n","step: 3250, loss: 0.1250251978635788\n","step: 3260, loss: 0.046426694840192795\n","step: 3270, loss: 0.06203833967447281\n","step: 3280, loss: 0.0762920156121254\n","step: 3290, loss: 0.07507068663835526\n","step: 3300, loss: 0.07829778641462326\n","step: 3310, loss: 0.0472424179315567\n","step: 3320, loss: 0.10626456141471863\n","step: 3330, loss: 0.050652891397476196\n","step: 3340, loss: 0.1810959279537201\n","step: 3350, loss: 0.05446438491344452\n","step: 3360, loss: 0.02145276963710785\n","step: 3370, loss: 0.065914586186409\n","step: 3380, loss: 0.12277768552303314\n","step: 3390, loss: 0.09095136821269989\n","step: 3400, loss: 0.0558483861386776\n","step: 3410, loss: 0.11091311275959015\n","step: 3420, loss: 0.05645938590168953\n","step: 3430, loss: 0.11320360749959946\n","step: 3440, loss: 0.05766903981566429\n","step: 3450, loss: 0.11293265223503113\n","step: 3460, loss: 0.09277115017175674\n","step: 3470, loss: 0.11271849274635315\n","step: 3480, loss: 0.0864417776465416\n","step: 3490, loss: 0.045904695987701416\n","step: 3500, loss: 0.034666843712329865\n","step: 3510, loss: 0.02857719361782074\n","step: 3520, loss: 0.09512798488140106\n","step: 3530, loss: 0.12307772040367126\n","step: 3540, loss: 0.08452790975570679\n","step: 3550, loss: 0.14553692936897278\n","step: 3560, loss: 0.10147460550069809\n","step: 3570, loss: 0.045255210250616074\n","step: 3580, loss: 0.030440054833889008\n","step: 3590, loss: 0.0805056095123291\n","step: 3600, loss: 0.09923049062490463\n","step: 3610, loss: 0.13133388757705688\n","step: 3620, loss: 0.12376321852207184\n","step: 3630, loss: 0.03570472449064255\n","step: 3640, loss: 0.0981730967760086\n","step: 3650, loss: 0.08613292872905731\n","step: 3660, loss: 0.08571416884660721\n","step: 3670, loss: 0.15650250017642975\n","step: 3680, loss: 0.036415133625268936\n","step: 3690, loss: 0.12260288000106812\n","step: 3700, loss: 0.057588718831539154\n","step: 3710, loss: 0.09465940296649933\n","step: 3720, loss: 0.08510726690292358\n","step: 3730, loss: 0.21189434826374054\n","step: 3740, loss: 0.05133777856826782\n","step: 3750, loss: 0.0617842860519886\n","step: 3760, loss: 0.1559896320104599\n","step: 3770, loss: 0.09946867823600769\n","step: 3780, loss: 0.06720634549856186\n","step: 3790, loss: 0.052023325115442276\n","step: 3800, loss: 0.10830102860927582\n","step: 3810, loss: 0.0385332927107811\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.48      0.64      0.55        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.79      0.84      0.81       291\n","           5       0.89      0.84      0.87       294\n","           6       0.98      0.99      0.99      1570\n","           7       0.66      0.94      0.78       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.94      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.85      0.85      0.85        13\n","          14       0.49      1.00      0.66        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.87      0.84      0.85      1151\n","          17       0.90      0.90      0.90        41\n","          18       0.94      1.00      0.97        32\n","          19       0.54      0.33      0.41        40\n","          20       1.00      0.97      0.98       584\n","          21       0.20      0.19      0.20        52\n","          22       0.95      0.75      0.84      4175\n","          23       0.71      0.96      0.82      2253\n","          24       0.39      0.43      0.41        44\n","          25       0.85      0.94      0.89       888\n","          26       0.89      0.89      0.89         9\n","          27       1.00      0.99      0.99        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.91      0.87      0.89      1136\n","          31       0.52      0.74      0.61        19\n","          32       1.00      0.62      0.77         8\n","          33       0.66      0.95      0.78        86\n","          34       0.25      0.56      0.35        32\n","          35       0.98      0.99      0.99       474\n","          36       1.00      0.08      0.14       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.95      0.97      0.96       404\n","          39       0.96      0.98      0.97       485\n","          40       0.92      0.93      0.92       573\n","          41       0.94      0.93      0.94       841\n","          42       0.97      0.99      0.98       575\n","          43       0.96      0.89      0.92       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.99      0.99        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.81      0.79     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 459\n","\n","Loop 30\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9833672046661377\n","step: 10, loss: 2.0012624263763428\n","step: 20, loss: 0.9333800673484802\n","step: 30, loss: 0.46426212787628174\n","step: 40, loss: 0.2991935908794403\n","step: 50, loss: 0.28763866424560547\n","step: 60, loss: 0.24691373109817505\n","step: 70, loss: 0.24274155497550964\n","step: 80, loss: 0.24511593580245972\n","step: 90, loss: 0.28137826919555664\n","step: 100, loss: 0.23452970385551453\n","step: 110, loss: 0.1657952070236206\n","step: 120, loss: 0.047282252460718155\n","step: 130, loss: 0.12113465368747711\n","step: 140, loss: 0.1300913393497467\n","step: 150, loss: 0.14493998885154724\n","step: 160, loss: 0.19895969331264496\n","step: 170, loss: 0.1449916511774063\n","step: 180, loss: 0.1812533587217331\n","step: 190, loss: 0.0907856673002243\n","step: 200, loss: 0.1667603999376297\n","step: 210, loss: 0.22557903826236725\n","step: 220, loss: 0.07394960522651672\n","step: 230, loss: 0.17610138654708862\n","step: 240, loss: 0.07996619492769241\n","step: 250, loss: 0.26975491642951965\n","step: 260, loss: 0.1561480313539505\n","step: 270, loss: 0.20844915509223938\n","step: 280, loss: 0.07139154523611069\n","step: 290, loss: 0.1606910079717636\n","step: 300, loss: 0.0960291400551796\n","step: 310, loss: 0.1889077126979828\n","step: 320, loss: 0.10089951008558273\n","step: 330, loss: 0.10468635708093643\n","step: 340, loss: 0.049926355481147766\n","step: 350, loss: 0.12160110473632812\n","step: 360, loss: 0.15809397399425507\n","step: 370, loss: 0.1684851050376892\n","step: 380, loss: 0.1339232325553894\n","step: 390, loss: 0.0507194958627224\n","step: 400, loss: 0.1613319218158722\n","step: 410, loss: 0.044168949127197266\n","step: 420, loss: 0.14736483991146088\n","step: 430, loss: 0.12775741517543793\n","step: 440, loss: 0.18702548742294312\n","step: 450, loss: 0.1404501050710678\n","step: 460, loss: 0.04448588192462921\n","step: 470, loss: 0.09396828711032867\n","step: 480, loss: 0.19525761902332306\n","step: 490, loss: 0.1485401690006256\n","step: 500, loss: 0.2020716816186905\n","step: 510, loss: 0.09387912601232529\n","step: 520, loss: 0.05808938294649124\n","step: 530, loss: 0.10431625694036484\n","step: 540, loss: 0.05933212488889694\n","step: 550, loss: 0.10745004564523697\n","step: 560, loss: 0.12278018891811371\n","step: 570, loss: 0.14019939303398132\n","step: 580, loss: 0.10278144478797913\n","step: 590, loss: 0.2526188790798187\n","step: 600, loss: 0.14090794324874878\n","step: 610, loss: 0.048992399126291275\n","step: 620, loss: 0.07186334580183029\n","step: 630, loss: 0.13394998013973236\n","step: 640, loss: 0.2183447629213333\n","step: 650, loss: 0.0952104702591896\n","step: 660, loss: 0.11514084041118622\n","step: 670, loss: 0.11251744627952576\n","step: 680, loss: 0.05798962712287903\n","step: 690, loss: 0.16028141975402832\n","step: 700, loss: 0.10199512541294098\n","step: 710, loss: 0.05535620078444481\n","step: 720, loss: 0.05265384539961815\n","step: 730, loss: 0.06677405536174774\n","step: 740, loss: 0.15706606209278107\n","step: 750, loss: 0.1195746660232544\n","step: 760, loss: 0.10966025292873383\n","step: 770, loss: 0.04244397208094597\n","step: 780, loss: 0.284416139125824\n","step: 790, loss: 0.1356244534254074\n","step: 800, loss: 0.03219976648688316\n","step: 810, loss: 0.10493821650743484\n","step: 820, loss: 0.11043158918619156\n","step: 830, loss: 0.22738203406333923\n","step: 840, loss: 0.1792287975549698\n","step: 850, loss: 0.13951581716537476\n","step: 860, loss: 0.12189209461212158\n","step: 870, loss: 0.04192839562892914\n","step: 880, loss: 0.08909554779529572\n","step: 890, loss: 0.058816488832235336\n","step: 900, loss: 0.17698100209236145\n","step: 910, loss: 0.11279098689556122\n","step: 920, loss: 0.2604789733886719\n","step: 930, loss: 0.08267156034708023\n","step: 940, loss: 0.06730037927627563\n","step: 950, loss: 0.08904792368412018\n","step: 960, loss: 0.12086409330368042\n","step: 970, loss: 0.09463831037282944\n","step: 980, loss: 0.11999359726905823\n","step: 990, loss: 0.10328039526939392\n","step: 1000, loss: 0.07287716120481491\n","step: 1010, loss: 0.10266389697790146\n","step: 1020, loss: 0.08222942054271698\n","step: 1030, loss: 0.1526666134595871\n","step: 1040, loss: 0.08861981332302094\n","step: 1050, loss: 0.07395883649587631\n","step: 1060, loss: 0.04649323225021362\n","step: 1070, loss: 0.0761033445596695\n","step: 1080, loss: 0.10211236029863358\n","step: 1090, loss: 0.07041451334953308\n","step: 1100, loss: 0.08478710055351257\n","step: 1110, loss: 0.04169318452477455\n","step: 1120, loss: 0.0525740347802639\n","step: 1130, loss: 0.03518955782055855\n","step: 1140, loss: 0.055050816386938095\n","step: 1150, loss: 0.05102615803480148\n","step: 1160, loss: 0.0959073007106781\n","step: 1170, loss: 0.14148950576782227\n","step: 1180, loss: 0.10332708805799484\n","step: 1190, loss: 0.1329232007265091\n","step: 1200, loss: 0.11543567478656769\n","step: 1210, loss: 0.14381490647792816\n","step: 1220, loss: 0.09070922434329987\n","step: 1230, loss: 0.04198209568858147\n","step: 1240, loss: 0.035269442945718765\n","step: 1250, loss: 0.05917174369096756\n","step: 1260, loss: 0.045723289251327515\n","step: 1270, loss: 0.11903420835733414\n","step: 1280, loss: 0.16326236724853516\n","step: 1290, loss: 0.1073513999581337\n","step: 1300, loss: 0.09543021768331528\n","step: 1310, loss: 0.10750848799943924\n","step: 1320, loss: 0.05588384345173836\n","step: 1330, loss: 0.19714830815792084\n","step: 1340, loss: 0.08790720254182816\n","step: 1350, loss: 0.14633311331272125\n","step: 1360, loss: 0.06526343524456024\n","step: 1370, loss: 0.2659137547016144\n","step: 1380, loss: 0.07758444547653198\n","step: 1390, loss: 0.082119882106781\n","step: 1400, loss: 0.05114917829632759\n","step: 1410, loss: 0.08135480433702469\n","step: 1420, loss: 0.14053703844547272\n","step: 1430, loss: 0.14851829409599304\n","step: 1440, loss: 0.0650753378868103\n","step: 1450, loss: 0.05811046436429024\n","step: 1460, loss: 0.06393060833215714\n","step: 1470, loss: 0.0943932831287384\n","step: 1480, loss: 0.07585956901311874\n","step: 1490, loss: 0.05944986268877983\n","step: 1500, loss: 0.13175533711910248\n","step: 1510, loss: 0.15660323202610016\n","step: 1520, loss: 0.14931516349315643\n","step: 1530, loss: 0.11704066395759583\n","step: 1540, loss: 0.08791366219520569\n","step: 1550, loss: 0.1525580734014511\n","step: 1560, loss: 0.07432276010513306\n","step: 1570, loss: 0.05564301833510399\n","step: 1580, loss: 0.06895899027585983\n","step: 1590, loss: 0.09795922040939331\n","step: 1600, loss: 0.11320622265338898\n","step: 1610, loss: 0.0951862707734108\n","step: 1620, loss: 0.036300063133239746\n","step: 1630, loss: 0.02576584182679653\n","step: 1640, loss: 0.09380455315113068\n","step: 1650, loss: 0.08852413296699524\n","step: 1660, loss: 0.08291368931531906\n","step: 1670, loss: 0.053825199604034424\n","step: 1680, loss: 0.04012078791856766\n","step: 1690, loss: 0.09054536372423172\n","step: 1700, loss: 0.08661507070064545\n","step: 1710, loss: 0.1473386436700821\n","step: 1720, loss: 0.10942711681127548\n","step: 1730, loss: 0.08154930919408798\n","step: 1740, loss: 0.0808396115899086\n","step: 1750, loss: 0.10413803905248642\n","step: 1760, loss: 0.06771519035100937\n","step: 1770, loss: 0.15877139568328857\n","step: 1780, loss: 0.1312878131866455\n","step: 1790, loss: 0.12141160666942596\n","step: 1800, loss: 0.053670335561037064\n","step: 1810, loss: 0.02747996151447296\n","step: 1820, loss: 0.09126003086566925\n","step: 1830, loss: 0.06820309907197952\n","step: 1840, loss: 0.11943414062261581\n","step: 1850, loss: 0.078386589884758\n","step: 1860, loss: 0.05726911500096321\n","step: 1870, loss: 0.0445365235209465\n","step: 1880, loss: 0.06495621055364609\n","step: 1890, loss: 0.12949135899543762\n","step: 1900, loss: 0.23064786195755005\n","step: 1910, loss: 0.04784853011369705\n","step: 1920, loss: 0.06252938508987427\n","step: 1930, loss: 0.093001589179039\n","step: 1940, loss: 0.11901989579200745\n","step: 1950, loss: 0.07010728120803833\n","step: 1960, loss: 0.07491838186979294\n","step: 1970, loss: 0.0796419009566307\n","step: 1980, loss: 0.09510672837495804\n","step: 1990, loss: 0.10059550404548645\n","step: 2000, loss: 0.06263147294521332\n","step: 2010, loss: 0.04501693323254585\n","step: 2020, loss: 0.06918511539697647\n","step: 2030, loss: 0.08065149188041687\n","step: 2040, loss: 0.12072236835956573\n","step: 2050, loss: 0.07896380126476288\n","step: 2060, loss: 0.059580542147159576\n","step: 2070, loss: 0.037203844636678696\n","step: 2080, loss: 0.2061769813299179\n","step: 2090, loss: 0.03613272309303284\n","step: 2100, loss: 0.09309819340705872\n","step: 2110, loss: 0.1485213190317154\n","step: 2120, loss: 0.100507453083992\n","step: 2130, loss: 0.04515192285180092\n","step: 2140, loss: 0.12149491161108017\n","step: 2150, loss: 0.029715295881032944\n","step: 2160, loss: 0.15027309954166412\n","step: 2170, loss: 0.08636453747749329\n","step: 2180, loss: 0.06357714533805847\n","step: 2190, loss: 0.1017880067229271\n","step: 2200, loss: 0.08969534188508987\n","step: 2210, loss: 0.05810077488422394\n","step: 2220, loss: 0.09021179378032684\n","step: 2230, loss: 0.10404418408870697\n","step: 2240, loss: 0.05133781209588051\n","step: 2250, loss: 0.03430188447237015\n","step: 2260, loss: 0.0403067022562027\n","step: 2270, loss: 0.09498166292905807\n","step: 2280, loss: 0.09737792611122131\n","step: 2290, loss: 0.03706621006131172\n","step: 2300, loss: 0.02741522341966629\n","step: 2310, loss: 0.058763228356838226\n","step: 2320, loss: 0.08517322689294815\n","step: 2330, loss: 0.09582719951868057\n","step: 2340, loss: 0.12882107496261597\n","step: 2350, loss: 0.061841949820518494\n","step: 2360, loss: 0.10607348382472992\n","step: 2370, loss: 0.13450117409229279\n","step: 2380, loss: 0.06937795877456665\n","step: 2390, loss: 0.04840464890003204\n","step: 2400, loss: 0.11621197313070297\n","step: 2410, loss: 0.11435646563768387\n","step: 2420, loss: 0.02736942283809185\n","step: 2430, loss: 0.03506774455308914\n","step: 2440, loss: 0.07073801755905151\n","step: 2450, loss: 0.1594458669424057\n","step: 2460, loss: 0.10759837180376053\n","step: 2470, loss: 0.11452716588973999\n","step: 2480, loss: 0.08959553390741348\n","step: 2490, loss: 0.09216122329235077\n","step: 2500, loss: 0.06040086969733238\n","step: 2510, loss: 0.06005915254354477\n","step: 2520, loss: 0.051583416759967804\n","step: 2530, loss: 0.12656231224536896\n","step: 2540, loss: 0.051177095621824265\n","step: 2550, loss: 0.13680623471736908\n","step: 2560, loss: 0.0646243765950203\n","step: 2570, loss: 0.03733129799365997\n","step: 2580, loss: 0.09316705912351608\n","step: 2590, loss: 0.02374127320945263\n","step: 2600, loss: 0.1075880378484726\n","step: 2610, loss: 0.0799984261393547\n","step: 2620, loss: 0.0909913182258606\n","step: 2630, loss: 0.12290965020656586\n","step: 2640, loss: 0.11192650347948074\n","step: 2650, loss: 0.0742737278342247\n","step: 2660, loss: 0.10997679829597473\n","step: 2670, loss: 0.13910986483097076\n","step: 2680, loss: 0.10659782588481903\n","step: 2690, loss: 0.07922975718975067\n","step: 2700, loss: 0.11308968812227249\n","step: 2710, loss: 0.04654018208384514\n","step: 2720, loss: 0.09676747769117355\n","step: 2730, loss: 0.162218376994133\n","step: 2740, loss: 0.14168712496757507\n","step: 2750, loss: 0.13994167745113373\n","step: 2760, loss: 0.13000831007957458\n","step: 2770, loss: 0.10430939495563507\n","step: 2780, loss: 0.023072833195328712\n","step: 2790, loss: 0.12545454502105713\n","step: 2800, loss: 0.11187971383333206\n","step: 2810, loss: 0.13555489480495453\n","step: 2820, loss: 0.03497357293963432\n","step: 2830, loss: 0.07773551344871521\n","step: 2840, loss: 0.17155006527900696\n","step: 2850, loss: 0.1250430941581726\n","step: 2860, loss: 0.16148781776428223\n","step: 2870, loss: 0.10955585539340973\n","step: 2880, loss: 0.08322801440954208\n","step: 2890, loss: 0.0791313648223877\n","step: 2900, loss: 0.14165979623794556\n","step: 2910, loss: 0.040007926523685455\n","step: 2920, loss: 0.08825904130935669\n","step: 2930, loss: 0.10227146744728088\n","step: 2940, loss: 0.06890560686588287\n","step: 2950, loss: 0.09682229906320572\n","step: 2960, loss: 0.16131269931793213\n","step: 2970, loss: 0.08800230175256729\n","step: 2980, loss: 0.08895988017320633\n","step: 2990, loss: 0.028539378196001053\n","step: 3000, loss: 0.0609382688999176\n","step: 3010, loss: 0.07846713066101074\n","step: 3020, loss: 0.0395582914352417\n","step: 3030, loss: 0.048745304346084595\n","step: 3040, loss: 0.1184106394648552\n","step: 3050, loss: 0.16957400739192963\n","step: 3060, loss: 0.04556051641702652\n","step: 3070, loss: 0.19364869594573975\n","step: 3080, loss: 0.17394964396953583\n","step: 3090, loss: 0.15406110882759094\n","step: 3100, loss: 0.04143844544887543\n","step: 3110, loss: 0.16297902166843414\n","step: 3120, loss: 0.0660577043890953\n","step: 3130, loss: 0.03794623166322708\n","step: 3140, loss: 0.08442473411560059\n","step: 3150, loss: 0.05141667649149895\n","step: 3160, loss: 0.1397678554058075\n","step: 3170, loss: 0.04533802717924118\n","step: 3180, loss: 0.08759130537509918\n","step: 3190, loss: 0.052473340183496475\n","step: 3200, loss: 0.08942026644945145\n","step: 3210, loss: 0.20383745431900024\n","step: 3220, loss: 0.09705580770969391\n","step: 3230, loss: 0.07509816437959671\n","step: 3240, loss: 0.06662023067474365\n","step: 3250, loss: 0.02685713954269886\n","step: 3260, loss: 0.12389802932739258\n","step: 3270, loss: 0.03176121413707733\n","step: 3280, loss: 0.10200611501932144\n","step: 3290, loss: 0.08547857403755188\n","step: 3300, loss: 0.09414396435022354\n","step: 3310, loss: 0.148115873336792\n","step: 3320, loss: 0.12122198194265366\n","step: 3330, loss: 0.1024489775300026\n","step: 3340, loss: 0.13567745685577393\n","step: 3350, loss: 0.08513282984495163\n","step: 3360, loss: 0.042571160942316055\n","step: 3370, loss: 0.05296581611037254\n","step: 3380, loss: 0.16547413170337677\n","step: 3390, loss: 0.030337197706103325\n","step: 3400, loss: 0.13807934522628784\n","step: 3410, loss: 0.1256965845823288\n","step: 3420, loss: 0.12341775000095367\n","step: 3430, loss: 0.0814494714140892\n","step: 3440, loss: 0.11435163021087646\n","step: 3450, loss: 0.10479713976383209\n","step: 3460, loss: 0.06610020995140076\n","step: 3470, loss: 0.09304434806108475\n","step: 3480, loss: 0.032801881432533264\n","step: 3490, loss: 0.1573774367570877\n","step: 3500, loss: 0.10099104791879654\n","step: 3510, loss: 0.015081091783940792\n","step: 3520, loss: 0.1186249777674675\n","step: 3530, loss: 0.07347696274518967\n","step: 3540, loss: 0.0772537887096405\n","step: 3550, loss: 0.0959128886461258\n","step: 3560, loss: 0.0836348608136177\n","step: 3570, loss: 0.04960335046052933\n","step: 3580, loss: 0.09305999428033829\n","step: 3590, loss: 0.18729372322559357\n","step: 3600, loss: 0.07887853682041168\n","step: 3610, loss: 0.15336903929710388\n","step: 3620, loss: 0.13279955089092255\n","step: 3630, loss: 0.4023118317127228\n","step: 3640, loss: 0.1351996660232544\n","step: 3650, loss: 0.08420038968324661\n","step: 3660, loss: 0.07730413228273392\n","step: 3670, loss: 0.1130659356713295\n","step: 3680, loss: 0.07055801898241043\n","step: 3690, loss: 0.1216498389840126\n","step: 3700, loss: 0.09673801809549332\n","step: 3710, loss: 0.07445921748876572\n","step: 3720, loss: 0.15116441249847412\n","step: 3730, loss: 0.11948394775390625\n","step: 3740, loss: 0.11763053387403488\n","step: 3750, loss: 0.05717155709862709\n","step: 3760, loss: 0.059066858142614365\n","step: 3770, loss: 0.04735015332698822\n","step: 3780, loss: 0.08534422516822815\n","step: 3790, loss: 0.08787985146045685\n","step: 3800, loss: 0.08966884762048721\n","step: 3810, loss: 0.03899294510483742\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.49      1.00      0.66        35\n","           2       0.52      0.91      0.66        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.85      0.91       291\n","           5       0.97      0.84      0.90       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.53      0.94      0.67       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.98       689\n","          10       0.92      0.98      0.95       901\n","          11       0.99      0.99      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.00      0.00      0.00        13\n","          14       0.39      1.00      0.56        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.79      0.86      0.82      1151\n","          17       0.97      0.95      0.96        41\n","          18       0.94      0.97      0.95        32\n","          19       0.16      0.17      0.16        40\n","          20       0.99      1.00      1.00       584\n","          21       0.28      0.10      0.14        52\n","          22       0.93      0.81      0.87      4175\n","          23       0.79      0.91      0.85      2253\n","          24       0.52      0.34      0.41        44\n","          25       0.84      0.95      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       0.95      1.00      0.97        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.87      0.90      0.88      1136\n","          31       0.56      0.74      0.64        19\n","          32       0.86      0.75      0.80         8\n","          33       0.66      0.95      0.78        86\n","          34       0.21      0.81      0.33        32\n","          35       0.99      0.99      0.99       474\n","          36       0.40      0.01      0.02       182\n","          37       0.89      0.97      0.93      1592\n","          38       0.98      0.96      0.97       404\n","          39       0.97      0.93      0.95       485\n","          40       0.92      0.81      0.86       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       1.00      0.75      0.86       152\n","          44       0.90      0.96      0.93        75\n","          46       0.99      0.98      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.76      0.80      0.76     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 459\n","\n","Loop 31\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.935161590576172\n","step: 10, loss: 1.911209225654602\n","step: 20, loss: 0.9037948250770569\n","step: 30, loss: 0.3635736107826233\n","step: 40, loss: 0.2953256368637085\n","step: 50, loss: 0.32126733660697937\n","step: 60, loss: 0.22760558128356934\n","step: 70, loss: 0.1371592879295349\n","step: 80, loss: 0.12471862882375717\n","step: 90, loss: 0.12261476367712021\n","step: 100, loss: 0.11061687767505646\n","step: 110, loss: 0.16220317780971527\n","step: 120, loss: 0.11777196079492569\n","step: 130, loss: 0.1073988527059555\n","step: 140, loss: 0.11261854320764542\n","step: 150, loss: 0.1403658241033554\n","step: 160, loss: 0.11712750792503357\n","step: 170, loss: 0.2865191102027893\n","step: 180, loss: 0.2186758816242218\n","step: 190, loss: 0.13984200358390808\n","step: 200, loss: 0.08780822157859802\n","step: 210, loss: 0.08354240655899048\n","step: 220, loss: 0.11134849488735199\n","step: 230, loss: 0.11070156842470169\n","step: 240, loss: 0.1174364760518074\n","step: 250, loss: 0.12061665952205658\n","step: 260, loss: 0.0942346528172493\n","step: 270, loss: 0.1845572143793106\n","step: 280, loss: 0.10599497705698013\n","step: 290, loss: 0.12575440108776093\n","step: 300, loss: 0.11633414775133133\n","step: 310, loss: 0.09719192981719971\n","step: 320, loss: 0.25253063440322876\n","step: 330, loss: 0.09242518991231918\n","step: 340, loss: 0.15169240534305573\n","step: 350, loss: 0.12324249744415283\n","step: 360, loss: 0.0571027509868145\n","step: 370, loss: 0.047434162348508835\n","step: 380, loss: 0.04192941263318062\n","step: 390, loss: 0.12076199054718018\n","step: 400, loss: 0.06254138052463531\n","step: 410, loss: 0.18824677169322968\n","step: 420, loss: 0.10392331331968307\n","step: 430, loss: 0.10701107233762741\n","step: 440, loss: 0.13725030422210693\n","step: 450, loss: 0.11003366112709045\n","step: 460, loss: 0.23456034064292908\n","step: 470, loss: 0.10882475227117538\n","step: 480, loss: 0.1180102676153183\n","step: 490, loss: 0.10472028702497482\n","step: 500, loss: 0.12486964464187622\n","step: 510, loss: 0.09339888393878937\n","step: 520, loss: 0.07797829806804657\n","step: 530, loss: 0.09837262332439423\n","step: 540, loss: 0.06965600699186325\n","step: 550, loss: 0.16035205125808716\n","step: 560, loss: 0.22322343289852142\n","step: 570, loss: 0.10872045904397964\n","step: 580, loss: 0.13534411787986755\n","step: 590, loss: 0.1006460040807724\n","step: 600, loss: 0.20025081932544708\n","step: 610, loss: 0.1327134519815445\n","step: 620, loss: 0.04972074553370476\n","step: 630, loss: 0.09994228184223175\n","step: 640, loss: 0.2082871049642563\n","step: 650, loss: 0.08919983357191086\n","step: 660, loss: 0.11560837924480438\n","step: 670, loss: 0.11725853383541107\n","step: 680, loss: 0.13439784944057465\n","step: 690, loss: 0.10046481341123581\n","step: 700, loss: 0.12266159057617188\n","step: 710, loss: 0.09342391043901443\n","step: 720, loss: 0.049605436623096466\n","step: 730, loss: 0.013411757536232471\n","step: 740, loss: 0.14212073385715485\n","step: 750, loss: 0.08073293417692184\n","step: 760, loss: 0.08943799138069153\n","step: 770, loss: 0.04977152496576309\n","step: 780, loss: 0.061782170087099075\n","step: 790, loss: 0.22071988880634308\n","step: 800, loss: 0.05916239693760872\n","step: 810, loss: 0.08698301017284393\n","step: 820, loss: 0.15759393572807312\n","step: 830, loss: 0.06916991621255875\n","step: 840, loss: 0.10987390577793121\n","step: 850, loss: 0.08127887547016144\n","step: 860, loss: 0.08695485442876816\n","step: 870, loss: 0.08816100656986237\n","step: 880, loss: 0.1347305178642273\n","step: 890, loss: 0.07325040549039841\n","step: 900, loss: 0.0925118625164032\n","step: 910, loss: 0.07798320800065994\n","step: 920, loss: 0.07528484612703323\n","step: 930, loss: 0.12790587544441223\n","step: 940, loss: 0.20535658299922943\n","step: 950, loss: 0.07429284602403641\n","step: 960, loss: 0.0709400475025177\n","step: 970, loss: 0.10560520738363266\n","step: 980, loss: 0.09055247902870178\n","step: 990, loss: 0.07995187491178513\n","step: 1000, loss: 0.0784464105963707\n","step: 1010, loss: 0.07937177270650864\n","step: 1020, loss: 0.12102117389440536\n","step: 1030, loss: 0.09918252378702164\n","step: 1040, loss: 0.09081805497407913\n","step: 1050, loss: 0.04523053392767906\n","step: 1060, loss: 0.12457592040300369\n","step: 1070, loss: 0.08846055716276169\n","step: 1080, loss: 0.054845962673425674\n","step: 1090, loss: 0.15287995338439941\n","step: 1100, loss: 0.15941183269023895\n","step: 1110, loss: 0.05495145544409752\n","step: 1120, loss: 0.1095195785164833\n","step: 1130, loss: 0.09259751439094543\n","step: 1140, loss: 0.07298143208026886\n","step: 1150, loss: 0.09870186448097229\n","step: 1160, loss: 0.04627598822116852\n","step: 1170, loss: 0.06890317052602768\n","step: 1180, loss: 0.06259644776582718\n","step: 1190, loss: 0.04560496285557747\n","step: 1200, loss: 0.24467332661151886\n","step: 1210, loss: 0.06035631150007248\n","step: 1220, loss: 0.0729386955499649\n","step: 1230, loss: 0.0825078934431076\n","step: 1240, loss: 0.0922483503818512\n","step: 1250, loss: 0.05904117971658707\n","step: 1260, loss: 0.1772095113992691\n","step: 1270, loss: 0.09147568047046661\n","step: 1280, loss: 0.12773893773555756\n","step: 1290, loss: 0.1211729645729065\n","step: 1300, loss: 0.05501864477992058\n","step: 1310, loss: 0.06120041012763977\n","step: 1320, loss: 0.06808853894472122\n","step: 1330, loss: 0.04464023932814598\n","step: 1340, loss: 0.08461230993270874\n","step: 1350, loss: 0.04307541996240616\n","step: 1360, loss: 0.08519247174263\n","step: 1370, loss: 0.06147124990820885\n","step: 1380, loss: 0.08448494970798492\n","step: 1390, loss: 0.11420357972383499\n","step: 1400, loss: 0.0505819097161293\n","step: 1410, loss: 0.10097581893205643\n","step: 1420, loss: 0.19777773320674896\n","step: 1430, loss: 0.12615489959716797\n","step: 1440, loss: 0.11413100361824036\n","step: 1450, loss: 0.04629041254520416\n","step: 1460, loss: 0.11475253850221634\n","step: 1470, loss: 0.041355032473802567\n","step: 1480, loss: 0.0520186573266983\n","step: 1490, loss: 0.07515256851911545\n","step: 1500, loss: 0.14147432148456573\n","step: 1510, loss: 0.09038648754358292\n","step: 1520, loss: 0.05773298069834709\n","step: 1530, loss: 0.08411524444818497\n","step: 1540, loss: 0.05376956984400749\n","step: 1550, loss: 0.09190202504396439\n","step: 1560, loss: 0.1304631233215332\n","step: 1570, loss: 0.12030963599681854\n","step: 1580, loss: 0.05881249159574509\n","step: 1590, loss: 0.1089073196053505\n","step: 1600, loss: 0.06961579620838165\n","step: 1610, loss: 0.0423981249332428\n","step: 1620, loss: 0.11206165701150894\n","step: 1630, loss: 0.13143578171730042\n","step: 1640, loss: 0.08559531718492508\n","step: 1650, loss: 0.08015717566013336\n","step: 1660, loss: 0.03825179487466812\n","step: 1670, loss: 0.042801886796951294\n","step: 1680, loss: 0.06728001683950424\n","step: 1690, loss: 0.1829732060432434\n","step: 1700, loss: 0.07077375799417496\n","step: 1710, loss: 0.09106625616550446\n","step: 1720, loss: 0.06666352599859238\n","step: 1730, loss: 0.06021959334611893\n","step: 1740, loss: 0.1395442932844162\n","step: 1750, loss: 0.10571931302547455\n","step: 1760, loss: 0.05068637430667877\n","step: 1770, loss: 0.21474532783031464\n","step: 1780, loss: 0.10554999858140945\n","step: 1790, loss: 0.1562841683626175\n","step: 1800, loss: 0.08015407621860504\n","step: 1810, loss: 0.12961487472057343\n","step: 1820, loss: 0.15039151906967163\n","step: 1830, loss: 0.09092433005571365\n","step: 1840, loss: 0.09267538785934448\n","step: 1850, loss: 0.06590710580348969\n","step: 1860, loss: 0.02842860482633114\n","step: 1870, loss: 0.1336381435394287\n","step: 1880, loss: 0.09566375613212585\n","step: 1890, loss: 0.1211642324924469\n","step: 1900, loss: 0.06917223334312439\n","step: 1910, loss: 0.06420671939849854\n","step: 1920, loss: 0.052403341978788376\n","step: 1930, loss: 0.10109401494264603\n","step: 1940, loss: 0.06824708729982376\n","step: 1950, loss: 0.10507133603096008\n","step: 1960, loss: 0.03229288384318352\n","step: 1970, loss: 0.06265592575073242\n","step: 1980, loss: 0.1454474925994873\n","step: 1990, loss: 0.18989108502864838\n","step: 2000, loss: 0.07965406775474548\n","step: 2010, loss: 0.06602261960506439\n","step: 2020, loss: 0.17712171375751495\n","step: 2030, loss: 0.05555504187941551\n","step: 2040, loss: 0.06263560801744461\n","step: 2050, loss: 0.08190279453992844\n","step: 2060, loss: 0.10882379114627838\n","step: 2070, loss: 0.07065805792808533\n","step: 2080, loss: 0.1266741156578064\n","step: 2090, loss: 0.08035630732774734\n","step: 2100, loss: 0.09443222731351852\n","step: 2110, loss: 0.05064505711197853\n","step: 2120, loss: 0.07428322732448578\n","step: 2130, loss: 0.041001562029123306\n","step: 2140, loss: 0.040767867118120193\n","step: 2150, loss: 0.05864984542131424\n","step: 2160, loss: 0.13519568741321564\n","step: 2170, loss: 0.06025002896785736\n","step: 2180, loss: 0.050121236592531204\n","step: 2190, loss: 0.15136319398880005\n","step: 2200, loss: 0.13558898866176605\n","step: 2210, loss: 0.1192549541592598\n","step: 2220, loss: 0.061279453337192535\n","step: 2230, loss: 0.1475844532251358\n","step: 2240, loss: 0.06901118904352188\n","step: 2250, loss: 0.10165780782699585\n","step: 2260, loss: 0.10044454783201218\n","step: 2270, loss: 0.22832657396793365\n","step: 2280, loss: 0.0809902623295784\n","step: 2290, loss: 0.033014897257089615\n","step: 2300, loss: 0.03381327912211418\n","step: 2310, loss: 0.05924631282687187\n","step: 2320, loss: 0.030053945258259773\n","step: 2330, loss: 0.12357179075479507\n","step: 2340, loss: 0.06524741649627686\n","step: 2350, loss: 0.14321765303611755\n","step: 2360, loss: 0.11315146833658218\n","step: 2370, loss: 0.058157820254564285\n","step: 2380, loss: 0.060942642390728\n","step: 2390, loss: 0.12482351064682007\n","step: 2400, loss: 0.09798714518547058\n","step: 2410, loss: 0.05231542885303497\n","step: 2420, loss: 0.06336785107851028\n","step: 2430, loss: 0.06915215402841568\n","step: 2440, loss: 0.08198333531618118\n","step: 2450, loss: 0.09724944829940796\n","step: 2460, loss: 0.18633075058460236\n","step: 2470, loss: 0.09220649302005768\n","step: 2480, loss: 0.04484383389353752\n","step: 2490, loss: 0.027579473331570625\n","step: 2500, loss: 0.14095138013362885\n","step: 2510, loss: 0.07330179214477539\n","step: 2520, loss: 0.09013397246599197\n","step: 2530, loss: 0.030488744378089905\n","step: 2540, loss: 0.05465326085686684\n","step: 2550, loss: 0.07354544848203659\n","step: 2560, loss: 0.1376737505197525\n","step: 2570, loss: 0.08914068341255188\n","step: 2580, loss: 0.09442952275276184\n","step: 2590, loss: 0.07929417490959167\n","step: 2600, loss: 0.0690474584698677\n","step: 2610, loss: 0.09932470321655273\n","step: 2620, loss: 0.07425090670585632\n","step: 2630, loss: 0.1082521304488182\n","step: 2640, loss: 0.09870095551013947\n","step: 2650, loss: 0.1713312417268753\n","step: 2660, loss: 0.21865810453891754\n","step: 2670, loss: 0.0892953872680664\n","step: 2680, loss: 0.09393026679754257\n","step: 2690, loss: 0.09075146168470383\n","step: 2700, loss: 0.11028681695461273\n","step: 2710, loss: 0.0796290934085846\n","step: 2720, loss: 0.09950917214155197\n","step: 2730, loss: 0.0419800840318203\n","step: 2740, loss: 0.032373201102018356\n","step: 2750, loss: 0.03900330513715744\n","step: 2760, loss: 0.085508793592453\n","step: 2770, loss: 0.09503114223480225\n","step: 2780, loss: 0.0545242615044117\n","step: 2790, loss: 0.08402087539434433\n","step: 2800, loss: 0.06295797973871231\n","step: 2810, loss: 0.09050653129816055\n","step: 2820, loss: 0.11549115180969238\n","step: 2830, loss: 0.07950913161039352\n","step: 2840, loss: 0.06391038000583649\n","step: 2850, loss: 0.11096968501806259\n","step: 2860, loss: 0.07478027045726776\n","step: 2870, loss: 0.07891086488962173\n","step: 2880, loss: 0.06537104398012161\n","step: 2890, loss: 0.169268399477005\n","step: 2900, loss: 0.09417029470205307\n","step: 2910, loss: 0.06937757879495621\n","step: 2920, loss: 0.08647163957357407\n","step: 2930, loss: 0.10495345294475555\n","step: 2940, loss: 0.03693392127752304\n","step: 2950, loss: 0.11892726272344589\n","step: 2960, loss: 0.03600286319851875\n","step: 2970, loss: 0.034192945808172226\n","step: 2980, loss: 0.1293603777885437\n","step: 2990, loss: 0.07708323001861572\n","step: 3000, loss: 0.02711431309580803\n","step: 3010, loss: 0.08878662437200546\n","step: 3020, loss: 0.03646065294742584\n","step: 3030, loss: 0.09263040125370026\n","step: 3040, loss: 0.0626276433467865\n","step: 3050, loss: 0.08733222633600235\n","step: 3060, loss: 0.13323421776294708\n","step: 3070, loss: 0.029808741062879562\n","step: 3080, loss: 0.07503388077020645\n","step: 3090, loss: 0.09714344888925552\n","step: 3100, loss: 0.04009980708360672\n","step: 3110, loss: 0.07999680191278458\n","step: 3120, loss: 0.06015278771519661\n","step: 3130, loss: 0.04657368361949921\n","step: 3140, loss: 0.09593211859464645\n","step: 3150, loss: 0.13180124759674072\n","step: 3160, loss: 0.04247644543647766\n","step: 3170, loss: 0.12588928639888763\n","step: 3180, loss: 0.0565226785838604\n","step: 3190, loss: 0.08830250799655914\n","step: 3200, loss: 0.07478390634059906\n","step: 3210, loss: 0.07295986264944077\n","step: 3220, loss: 0.11645279079675674\n","step: 3230, loss: 0.10493895411491394\n","step: 3240, loss: 0.09658783674240112\n","step: 3250, loss: 0.0391751304268837\n","step: 3260, loss: 0.15099111199378967\n","step: 3270, loss: 0.03432922810316086\n","step: 3280, loss: 0.05442396178841591\n","step: 3290, loss: 0.10959181189537048\n","step: 3300, loss: 0.08726564049720764\n","step: 3310, loss: 0.05396600067615509\n","step: 3320, loss: 0.11333265155553818\n","step: 3330, loss: 0.09765218198299408\n","step: 3340, loss: 0.02231043390929699\n","step: 3350, loss: 0.10277584940195084\n","step: 3360, loss: 0.054166410118341446\n","step: 3370, loss: 0.16021110117435455\n","step: 3380, loss: 0.027152933180332184\n","step: 3390, loss: 0.09638238698244095\n","step: 3400, loss: 0.16594332456588745\n","step: 3410, loss: 0.040717337280511856\n","step: 3420, loss: 0.08734135329723358\n","step: 3430, loss: 0.20220984518527985\n","step: 3440, loss: 0.13228639960289001\n","step: 3450, loss: 0.18238086998462677\n","step: 3460, loss: 0.05626556649804115\n","step: 3470, loss: 0.05448038876056671\n","step: 3480, loss: 0.12371278554201126\n","step: 3490, loss: 0.16355524957180023\n","step: 3500, loss: 0.11059599369764328\n","step: 3510, loss: 0.05297233536839485\n","step: 3520, loss: 0.04906385764479637\n","step: 3530, loss: 0.12484302371740341\n","step: 3540, loss: 0.022233271971344948\n","step: 3550, loss: 0.031012050807476044\n","step: 3560, loss: 0.055279385298490524\n","step: 3570, loss: 0.09727299213409424\n","step: 3580, loss: 0.13849030435085297\n","step: 3590, loss: 0.043811582028865814\n","step: 3600, loss: 0.05105285346508026\n","step: 3610, loss: 0.10261749476194382\n","step: 3620, loss: 0.16221807897090912\n","step: 3630, loss: 0.10157877206802368\n","step: 3640, loss: 0.12312854826450348\n","step: 3650, loss: 0.13483789563179016\n","step: 3660, loss: 0.045382726937532425\n","step: 3670, loss: 0.09224522113800049\n","step: 3680, loss: 0.22313019633293152\n","step: 3690, loss: 0.0927949845790863\n","step: 3700, loss: 0.09406325221061707\n","step: 3710, loss: 0.1635139435529709\n","step: 3720, loss: 0.07209863513708115\n","step: 3730, loss: 0.04724373295903206\n","step: 3740, loss: 0.05903170630335808\n","step: 3750, loss: 0.058855559676885605\n","step: 3760, loss: 0.06013713404536247\n","step: 3770, loss: 0.0913153812289238\n","step: 3780, loss: 0.12604498863220215\n","step: 3790, loss: 0.09430016577243805\n","step: 3800, loss: 0.06354711204767227\n","step: 3810, loss: 0.07971996068954468\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.65      1.00      0.79        35\n","           2       0.53      0.62      0.57        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.92      0.84      0.88       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.96      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.24      0.92      0.38        13\n","          14       0.29      1.00      0.45        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.91      0.83      0.87      1151\n","          17       0.89      0.95      0.92        41\n","          18       0.91      1.00      0.96        32\n","          19       0.41      0.60      0.48        40\n","          20       0.99      1.00      0.99       584\n","          21       0.29      0.19      0.23        52\n","          22       0.95      0.72      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.21      0.27      0.24        44\n","          25       0.86      0.92      0.89       888\n","          26       1.00      1.00      1.00         9\n","          27       0.88      0.97      0.92        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.96      0.99      0.98       344\n","          30       0.90      0.89      0.90      1136\n","          31       0.59      0.68      0.63        19\n","          32       1.00      0.50      0.67         8\n","          33       0.71      0.95      0.82        86\n","          34       0.15      0.38      0.21        32\n","          35       0.98      0.99      0.98       474\n","          36       1.00      0.10      0.18       182\n","          37       0.89      0.97      0.93      1592\n","          38       0.98      0.97      0.97       404\n","          39       0.97      0.95      0.96       485\n","          40       0.92      0.95      0.94       573\n","          41       0.98      0.94      0.96       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.84      0.89       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.95      0.97        82\n","          48       0.97      0.37      0.53        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.82      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 451\n","\n","Loop 32\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.933379888534546\n","step: 10, loss: 2.079237699508667\n","step: 20, loss: 0.7082272171974182\n","step: 30, loss: 0.3658095598220825\n","step: 40, loss: 0.39438706636428833\n","step: 50, loss: 0.2855231463909149\n","step: 60, loss: 0.273346871137619\n","step: 70, loss: 0.257242351770401\n","step: 80, loss: 0.20762771368026733\n","step: 90, loss: 0.12094555795192719\n","step: 100, loss: 0.25085729360580444\n","step: 110, loss: 0.16700465977191925\n","step: 120, loss: 0.13564090430736542\n","step: 130, loss: 0.18133509159088135\n","step: 140, loss: 0.14227987825870514\n","step: 150, loss: 0.1679641306400299\n","step: 160, loss: 0.08405518531799316\n","step: 170, loss: 0.17879851162433624\n","step: 180, loss: 0.16967813670635223\n","step: 190, loss: 0.21257784962654114\n","step: 200, loss: 0.10305246710777283\n","step: 210, loss: 0.17060287296772003\n","step: 220, loss: 0.0778423622250557\n","step: 230, loss: 0.1276039034128189\n","step: 240, loss: 0.07280922681093216\n","step: 250, loss: 0.135254368185997\n","step: 260, loss: 0.23587730526924133\n","step: 270, loss: 0.23539628088474274\n","step: 280, loss: 0.09545274078845978\n","step: 290, loss: 0.09029696881771088\n","step: 300, loss: 0.050167325884103775\n","step: 310, loss: 0.07183616608381271\n","step: 320, loss: 0.08544029295444489\n","step: 330, loss: 0.10319946706295013\n","step: 340, loss: 0.15834785997867584\n","step: 350, loss: 0.07616160809993744\n","step: 360, loss: 0.15258941054344177\n","step: 370, loss: 0.12606273591518402\n","step: 380, loss: 0.08832669258117676\n","step: 390, loss: 0.05563250184059143\n","step: 400, loss: 0.12700006365776062\n","step: 410, loss: 0.214767724275589\n","step: 420, loss: 0.1310245841741562\n","step: 430, loss: 0.08828737586736679\n","step: 440, loss: 0.1407027244567871\n","step: 450, loss: 0.08653344959020615\n","step: 460, loss: 0.10040786862373352\n","step: 470, loss: 0.021066749468445778\n","step: 480, loss: 0.042982589453458786\n","step: 490, loss: 0.14817839860916138\n","step: 500, loss: 0.1806947886943817\n","step: 510, loss: 0.057165782898664474\n","step: 520, loss: 0.12223558127880096\n","step: 530, loss: 0.1738952398300171\n","step: 540, loss: 0.1979190707206726\n","step: 550, loss: 0.12805770337581635\n","step: 560, loss: 0.0969664454460144\n","step: 570, loss: 0.1392001211643219\n","step: 580, loss: 0.07248082011938095\n","step: 590, loss: 0.10005458444356918\n","step: 600, loss: 0.034843675792217255\n","step: 610, loss: 0.06108459457755089\n","step: 620, loss: 0.12052246183156967\n","step: 630, loss: 0.10546714067459106\n","step: 640, loss: 0.15466731786727905\n","step: 650, loss: 0.06386160105466843\n","step: 660, loss: 0.07550512999296188\n","step: 670, loss: 0.10289689898490906\n","step: 680, loss: 0.030502021312713623\n","step: 690, loss: 0.15415360033512115\n","step: 700, loss: 0.08590149879455566\n","step: 710, loss: 0.1298275738954544\n","step: 720, loss: 0.05055135861039162\n","step: 730, loss: 0.2307228147983551\n","step: 740, loss: 0.10718654096126556\n","step: 750, loss: 0.14226040244102478\n","step: 760, loss: 0.22013463079929352\n","step: 770, loss: 0.13483528792858124\n","step: 780, loss: 0.11474025994539261\n","step: 790, loss: 0.06089356169104576\n","step: 800, loss: 0.10336702316999435\n","step: 810, loss: 0.16823600232601166\n","step: 820, loss: 0.083277128636837\n","step: 830, loss: 0.10662807524204254\n","step: 840, loss: 0.10342541337013245\n","step: 850, loss: 0.04888598620891571\n","step: 860, loss: 0.07333818823099136\n","step: 870, loss: 0.08565826714038849\n","step: 880, loss: 0.0736749917268753\n","step: 890, loss: 0.08838120102882385\n","step: 900, loss: 0.07281547039747238\n","step: 910, loss: 0.1070905551314354\n","step: 920, loss: 0.13340942561626434\n","step: 930, loss: 0.10683856159448624\n","step: 940, loss: 0.03356213495135307\n","step: 950, loss: 0.030575137585401535\n","step: 960, loss: 0.09484264254570007\n","step: 970, loss: 0.07677658647298813\n","step: 980, loss: 0.16488592326641083\n","step: 990, loss: 0.09541145712137222\n","step: 1000, loss: 0.1278042048215866\n","step: 1010, loss: 0.15201207995414734\n","step: 1020, loss: 0.07446872442960739\n","step: 1030, loss: 0.05252939462661743\n","step: 1040, loss: 0.05407116934657097\n","step: 1050, loss: 0.21512450277805328\n","step: 1060, loss: 0.06543503701686859\n","step: 1070, loss: 0.024414602667093277\n","step: 1080, loss: 0.049045175313949585\n","step: 1090, loss: 0.09255347400903702\n","step: 1100, loss: 0.0751316249370575\n","step: 1110, loss: 0.09458126127719879\n","step: 1120, loss: 0.042183101177215576\n","step: 1130, loss: 0.0997140184044838\n","step: 1140, loss: 0.05914934724569321\n","step: 1150, loss: 0.13898822665214539\n","step: 1160, loss: 0.023402979597449303\n","step: 1170, loss: 0.062065575271844864\n","step: 1180, loss: 0.20042632520198822\n","step: 1190, loss: 0.09950438886880875\n","step: 1200, loss: 0.06138039380311966\n","step: 1210, loss: 0.19147039949893951\n","step: 1220, loss: 0.03752345219254494\n","step: 1230, loss: 0.07632843405008316\n","step: 1240, loss: 0.05024304613471031\n","step: 1250, loss: 0.12216982245445251\n","step: 1260, loss: 0.05930503085255623\n","step: 1270, loss: 0.022221921011805534\n","step: 1280, loss: 0.11645156145095825\n","step: 1290, loss: 0.0923733115196228\n","step: 1300, loss: 0.049476489424705505\n","step: 1310, loss: 0.11695071309804916\n","step: 1320, loss: 0.04729139059782028\n","step: 1330, loss: 0.06003376841545105\n","step: 1340, loss: 0.06443603336811066\n","step: 1350, loss: 0.12064661085605621\n","step: 1360, loss: 0.044801849871873856\n","step: 1370, loss: 0.10685379803180695\n","step: 1380, loss: 0.12212032079696655\n","step: 1390, loss: 0.03409266844391823\n","step: 1400, loss: 0.05396483466029167\n","step: 1410, loss: 0.11623239517211914\n","step: 1420, loss: 0.14305289089679718\n","step: 1430, loss: 0.12534676492214203\n","step: 1440, loss: 0.06695283949375153\n","step: 1450, loss: 0.024693049490451813\n","step: 1460, loss: 0.11105659604072571\n","step: 1470, loss: 0.07120272517204285\n","step: 1480, loss: 0.20476441085338593\n","step: 1490, loss: 0.11182598769664764\n","step: 1500, loss: 0.10227854549884796\n","step: 1510, loss: 0.10545273870229721\n","step: 1520, loss: 0.03874826803803444\n","step: 1530, loss: 0.1073211133480072\n","step: 1540, loss: 0.10255403816699982\n","step: 1550, loss: 0.05787890404462814\n","step: 1560, loss: 0.07085935026407242\n","step: 1570, loss: 0.09600772708654404\n","step: 1580, loss: 0.06564608961343765\n","step: 1590, loss: 0.0524149015545845\n","step: 1600, loss: 0.14057554304599762\n","step: 1610, loss: 0.12782683968544006\n","step: 1620, loss: 0.07936933636665344\n","step: 1630, loss: 0.11602940410375595\n","step: 1640, loss: 0.06041891500353813\n","step: 1650, loss: 0.09205465018749237\n","step: 1660, loss: 0.17008064687252045\n","step: 1670, loss: 0.07862144708633423\n","step: 1680, loss: 0.07812023162841797\n","step: 1690, loss: 0.032801393419504166\n","step: 1700, loss: 0.11520986258983612\n","step: 1710, loss: 0.12460042536258698\n","step: 1720, loss: 0.13020539283752441\n","step: 1730, loss: 0.11436604708433151\n","step: 1740, loss: 0.1947317123413086\n","step: 1750, loss: 0.0842851921916008\n","step: 1760, loss: 0.049080684781074524\n","step: 1770, loss: 0.13285985589027405\n","step: 1780, loss: 0.09047738462686539\n","step: 1790, loss: 0.0861557349562645\n","step: 1800, loss: 0.06935305893421173\n","step: 1810, loss: 0.10751565545797348\n","step: 1820, loss: 0.20066018402576447\n","step: 1830, loss: 0.09467728435993195\n","step: 1840, loss: 0.07158730179071426\n","step: 1850, loss: 0.057980019599199295\n","step: 1860, loss: 0.09052783995866776\n","step: 1870, loss: 0.2600594758987427\n","step: 1880, loss: 0.11987759917974472\n","step: 1890, loss: 0.08592177927494049\n","step: 1900, loss: 0.11525963246822357\n","step: 1910, loss: 0.09323638677597046\n","step: 1920, loss: 0.05404861643910408\n","step: 1930, loss: 0.11607896536588669\n","step: 1940, loss: 0.061157215386629105\n","step: 1950, loss: 0.13643604516983032\n","step: 1960, loss: 0.059827882796525955\n","step: 1970, loss: 0.09941429644823074\n","step: 1980, loss: 0.07778596878051758\n","step: 1990, loss: 0.0483902245759964\n","step: 2000, loss: 0.08322471380233765\n","step: 2010, loss: 0.09174798429012299\n","step: 2020, loss: 0.05981770530343056\n","step: 2030, loss: 0.10583636909723282\n","step: 2040, loss: 0.12056084722280502\n","step: 2050, loss: 0.1285867542028427\n","step: 2060, loss: 0.08526583760976791\n","step: 2070, loss: 0.1033550500869751\n","step: 2080, loss: 0.059252750128507614\n","step: 2090, loss: 0.056979063898324966\n","step: 2100, loss: 0.07458144426345825\n","step: 2110, loss: 0.22517742216587067\n","step: 2120, loss: 0.21929368376731873\n","step: 2130, loss: 0.14908546209335327\n","step: 2140, loss: 0.0365777462720871\n","step: 2150, loss: 0.05729144439101219\n","step: 2160, loss: 0.07156231999397278\n","step: 2170, loss: 0.06059429422020912\n","step: 2180, loss: 0.11707016080617905\n","step: 2190, loss: 0.051453303545713425\n","step: 2200, loss: 0.046243298798799515\n","step: 2210, loss: 0.20576111972332\n","step: 2220, loss: 0.044140905141830444\n","step: 2230, loss: 0.1072746068239212\n","step: 2240, loss: 0.07358012348413467\n","step: 2250, loss: 0.10283108800649643\n","step: 2260, loss: 0.0645219087600708\n","step: 2270, loss: 0.0769636332988739\n","step: 2280, loss: 0.06958666443824768\n","step: 2290, loss: 0.11777276545763016\n","step: 2300, loss: 0.07654354721307755\n","step: 2310, loss: 0.08128459006547928\n","step: 2320, loss: 0.03418444097042084\n","step: 2330, loss: 0.13259437680244446\n","step: 2340, loss: 0.04586034640669823\n","step: 2350, loss: 0.08848929405212402\n","step: 2360, loss: 0.03878103569149971\n","step: 2370, loss: 0.13148503005504608\n","step: 2380, loss: 0.02844304032623768\n","step: 2390, loss: 0.11322296410799026\n","step: 2400, loss: 0.2177501916885376\n","step: 2410, loss: 0.05831417441368103\n","step: 2420, loss: 0.16780070960521698\n","step: 2430, loss: 0.0635099709033966\n","step: 2440, loss: 0.07274529337882996\n","step: 2450, loss: 0.04984676092863083\n","step: 2460, loss: 0.05365457013249397\n","step: 2470, loss: 0.023462539538741112\n","step: 2480, loss: 0.07672174274921417\n","step: 2490, loss: 0.10108406096696854\n","step: 2500, loss: 0.07388950884342194\n","step: 2510, loss: 0.050745777785778046\n","step: 2520, loss: 0.0439254455268383\n","step: 2530, loss: 0.04119091480970383\n","step: 2540, loss: 0.10045404732227325\n","step: 2550, loss: 0.08573833107948303\n","step: 2560, loss: 0.06223396956920624\n","step: 2570, loss: 0.054404761642217636\n","step: 2580, loss: 0.09880362451076508\n","step: 2590, loss: 0.0503256656229496\n","step: 2600, loss: 0.024902667850255966\n","step: 2610, loss: 0.13399796187877655\n","step: 2620, loss: 0.028463641181588173\n","step: 2630, loss: 0.07487499713897705\n","step: 2640, loss: 0.05448246747255325\n","step: 2650, loss: 0.30155009031295776\n","step: 2660, loss: 0.07293527573347092\n","step: 2670, loss: 0.030867066234350204\n","step: 2680, loss: 0.04912710562348366\n","step: 2690, loss: 0.06526114046573639\n","step: 2700, loss: 0.07397226989269257\n","step: 2710, loss: 0.14820966124534607\n","step: 2720, loss: 0.12232467532157898\n","step: 2730, loss: 0.08323036134243011\n","step: 2740, loss: 0.16951221227645874\n","step: 2750, loss: 0.10792529582977295\n","step: 2760, loss: 0.16651380062103271\n","step: 2770, loss: 0.06151365861296654\n","step: 2780, loss: 0.2026345580816269\n","step: 2790, loss: 0.05103406310081482\n","step: 2800, loss: 0.06500143557786942\n","step: 2810, loss: 0.1726474016904831\n","step: 2820, loss: 0.06756880134344101\n","step: 2830, loss: 0.08952946215867996\n","step: 2840, loss: 0.031897008419036865\n","step: 2850, loss: 0.026641368865966797\n","step: 2860, loss: 0.042417339980602264\n","step: 2870, loss: 0.12057247012853622\n","step: 2880, loss: 0.04054519534111023\n","step: 2890, loss: 0.035208750516176224\n","step: 2900, loss: 0.14222615957260132\n","step: 2910, loss: 0.08628690242767334\n","step: 2920, loss: 0.06907514482736588\n","step: 2930, loss: 0.056551434099674225\n","step: 2940, loss: 0.07774626463651657\n","step: 2950, loss: 0.07903064042329788\n","step: 2960, loss: 0.11302466690540314\n","step: 2970, loss: 0.10529226064682007\n","step: 2980, loss: 0.05227632820606232\n","step: 2990, loss: 0.10846289992332458\n","step: 3000, loss: 0.059581562876701355\n","step: 3010, loss: 0.10757256299257278\n","step: 3020, loss: 0.1501816362142563\n","step: 3030, loss: 0.1094280555844307\n","step: 3040, loss: 0.05738820135593414\n","step: 3050, loss: 0.01017332635819912\n","step: 3060, loss: 0.06667320430278778\n","step: 3070, loss: 0.10058444738388062\n","step: 3080, loss: 0.14495065808296204\n","step: 3090, loss: 0.04989904165267944\n","step: 3100, loss: 0.043729059398174286\n","step: 3110, loss: 0.09075785428285599\n","step: 3120, loss: 0.026131218299269676\n","step: 3130, loss: 0.11786548793315887\n","step: 3140, loss: 0.08317242562770844\n","step: 3150, loss: 0.04835480451583862\n","step: 3160, loss: 0.08938492089509964\n","step: 3170, loss: 0.10757340490818024\n","step: 3180, loss: 0.0678505152463913\n","step: 3190, loss: 0.08420448750257492\n","step: 3200, loss: 0.13783471286296844\n","step: 3210, loss: 0.07387684285640717\n","step: 3220, loss: 0.0411895215511322\n","step: 3230, loss: 0.12766104936599731\n","step: 3240, loss: 0.07193443924188614\n","step: 3250, loss: 0.06299751251935959\n","step: 3260, loss: 0.11762149631977081\n","step: 3270, loss: 0.07906528562307358\n","step: 3280, loss: 0.17950499057769775\n","step: 3290, loss: 0.09744669497013092\n","step: 3300, loss: 0.04481539875268936\n","step: 3310, loss: 0.13144546747207642\n","step: 3320, loss: 0.11234524846076965\n","step: 3330, loss: 0.10866157710552216\n","step: 3340, loss: 0.015702463686466217\n","step: 3350, loss: 0.10412687063217163\n","step: 3360, loss: 0.11671382188796997\n","step: 3370, loss: 0.1294996738433838\n","step: 3380, loss: 0.2109079658985138\n","step: 3390, loss: 0.0707964226603508\n","step: 3400, loss: 0.22617223858833313\n","step: 3410, loss: 0.1697969138622284\n","step: 3420, loss: 0.05345721170306206\n","step: 3430, loss: 0.026310306042432785\n","step: 3440, loss: 0.0653633251786232\n","step: 3450, loss: 0.08175000548362732\n","step: 3460, loss: 0.08695942163467407\n","step: 3470, loss: 0.05118231847882271\n","step: 3480, loss: 0.08481725305318832\n","step: 3490, loss: 0.06488523632287979\n","step: 3500, loss: 0.1925090253353119\n","step: 3510, loss: 0.092301145195961\n","step: 3520, loss: 0.08871468901634216\n","step: 3530, loss: 0.03506080061197281\n","step: 3540, loss: 0.0819060429930687\n","step: 3550, loss: 0.053580909967422485\n","step: 3560, loss: 0.02836071141064167\n","step: 3570, loss: 0.06846972554922104\n","step: 3580, loss: 0.06804828345775604\n","step: 3590, loss: 0.03769008815288544\n","step: 3600, loss: 0.030325012281537056\n","step: 3610, loss: 0.0897723138332367\n","step: 3620, loss: 0.1681370586156845\n","step: 3630, loss: 0.07008396834135056\n","step: 3640, loss: 0.10229014605283737\n","step: 3650, loss: 0.07740344107151031\n","step: 3660, loss: 0.12201951444149017\n","step: 3670, loss: 0.07951076328754425\n","step: 3680, loss: 0.11613325774669647\n","step: 3690, loss: 0.048488181084394455\n","step: 3700, loss: 0.09265395998954773\n","step: 3710, loss: 0.15510399639606476\n","step: 3720, loss: 0.045691125094890594\n","step: 3730, loss: 0.06680500507354736\n","step: 3740, loss: 0.1128111481666565\n","step: 3750, loss: 0.06639143824577332\n","step: 3760, loss: 0.029332101345062256\n","step: 3770, loss: 0.09220224618911743\n","step: 3780, loss: 0.16870324313640594\n","step: 3790, loss: 0.1868281215429306\n","step: 3800, loss: 0.09383374452590942\n","step: 3810, loss: 0.07502327859401703\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.94      0.86        35\n","           2       0.61      0.48      0.54        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.87      0.86      0.87       291\n","           5       0.98      0.84      0.90       294\n","           6       1.00      0.98      0.99      1570\n","           7       0.49      0.97      0.65       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.98       689\n","          10       0.96      0.98      0.97       901\n","          11       0.96      1.00      0.98      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.40      0.62      0.48        13\n","          14       0.67      0.98      0.79        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.88      0.82      0.85      1151\n","          17       0.91      0.95      0.93        41\n","          18       0.93      0.81      0.87        32\n","          19       0.70      0.70      0.70        40\n","          20       0.99      1.00      0.99       584\n","          21       0.32      0.12      0.17        52\n","          22       0.97      0.70      0.82      4175\n","          23       0.67      0.97      0.79      2253\n","          24       0.44      0.48      0.46        44\n","          25       0.87      0.93      0.90       888\n","          26       0.69      1.00      0.82         9\n","          27       0.99      1.00      0.99        69\n","          28       1.00      0.99      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.90      0.86      0.88      1136\n","          31       0.57      0.68      0.62        19\n","          32       0.50      0.75      0.60         8\n","          33       0.71      0.99      0.83        86\n","          34       0.29      0.72      0.41        32\n","          35       0.94      1.00      0.97       474\n","          36       0.55      0.16      0.25       182\n","          37       0.84      0.96      0.89      1592\n","          38       0.94      0.98      0.96       404\n","          39       0.94      0.97      0.96       485\n","          40       0.89      0.94      0.91       573\n","          41       0.93      0.94      0.93       841\n","          42       0.98      0.99      0.98       575\n","          43       0.96      0.73      0.83       152\n","          44       0.87      0.92      0.90        75\n","          46       0.99      0.99      0.99        82\n","          48       1.00      0.04      0.07        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 452\n","\n","Loop 33\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.972285032272339\n","step: 10, loss: 1.881079912185669\n","step: 20, loss: 0.8686563968658447\n","step: 30, loss: 0.4632740914821625\n","step: 40, loss: 0.3136829435825348\n","step: 50, loss: 0.21951790153980255\n","step: 60, loss: 0.2424425333738327\n","step: 70, loss: 0.22672469913959503\n","step: 80, loss: 0.22567012906074524\n","step: 90, loss: 0.1412728875875473\n","step: 100, loss: 0.17538921535015106\n","step: 110, loss: 0.11442257463932037\n","step: 120, loss: 0.1897497922182083\n","step: 130, loss: 0.2295593023300171\n","step: 140, loss: 0.18233737349510193\n","step: 150, loss: 0.11299975216388702\n","step: 160, loss: 0.10710879415273666\n","step: 170, loss: 0.09027916193008423\n","step: 180, loss: 0.10736306756734848\n","step: 190, loss: 0.09539572894573212\n","step: 200, loss: 0.20191554725170135\n","step: 210, loss: 0.12055662274360657\n","step: 220, loss: 0.2992651164531708\n","step: 230, loss: 0.0982879027724266\n","step: 240, loss: 0.20909830927848816\n","step: 250, loss: 0.17327097058296204\n","step: 260, loss: 0.08199142664670944\n","step: 270, loss: 0.037100084125995636\n","step: 280, loss: 0.269647479057312\n","step: 290, loss: 0.1190720871090889\n","step: 300, loss: 0.05053776502609253\n","step: 310, loss: 0.11844787746667862\n","step: 320, loss: 0.26213711500167847\n","step: 330, loss: 0.04952072352170944\n","step: 340, loss: 0.2526286542415619\n","step: 350, loss: 0.15505030751228333\n","step: 360, loss: 0.20942330360412598\n","step: 370, loss: 0.11399314552545547\n","step: 380, loss: 0.0763489380478859\n","step: 390, loss: 0.08645831793546677\n","step: 400, loss: 0.10991561412811279\n","step: 410, loss: 0.11151383072137833\n","step: 420, loss: 0.0724424347281456\n","step: 430, loss: 0.12819962203502655\n","step: 440, loss: 0.15909750759601593\n","step: 450, loss: 0.1638931930065155\n","step: 460, loss: 0.09451641142368317\n","step: 470, loss: 0.12619860470294952\n","step: 480, loss: 0.09304554760456085\n","step: 490, loss: 0.13807283341884613\n","step: 500, loss: 0.05958733335137367\n","step: 510, loss: 0.08963501453399658\n","step: 520, loss: 0.06267419457435608\n","step: 530, loss: 0.13609862327575684\n","step: 540, loss: 0.08194974809885025\n","step: 550, loss: 0.06454029679298401\n","step: 560, loss: 0.0852578803896904\n","step: 570, loss: 0.12707945704460144\n","step: 580, loss: 0.0807286873459816\n","step: 590, loss: 0.07437648624181747\n","step: 600, loss: 0.03854003921151161\n","step: 610, loss: 0.09587889909744263\n","step: 620, loss: 0.14912082254886627\n","step: 630, loss: 0.07677992433309555\n","step: 640, loss: 0.061375245451927185\n","step: 650, loss: 0.165505513548851\n","step: 660, loss: 0.13284100592136383\n","step: 670, loss: 0.08406385034322739\n","step: 680, loss: 0.082819364964962\n","step: 690, loss: 0.13251625001430511\n","step: 700, loss: 0.11172857880592346\n","step: 710, loss: 0.0953427329659462\n","step: 720, loss: 0.05842346325516701\n","step: 730, loss: 0.12469058483839035\n","step: 740, loss: 0.04454534873366356\n","step: 750, loss: 0.046483054757118225\n","step: 760, loss: 0.34752973914146423\n","step: 770, loss: 0.05011874437332153\n","step: 780, loss: 0.12424731999635696\n","step: 790, loss: 0.11169789731502533\n","step: 800, loss: 0.11333224177360535\n","step: 810, loss: 0.09633993357419968\n","step: 820, loss: 0.12822136282920837\n","step: 830, loss: 0.08941072225570679\n","step: 840, loss: 0.10134528577327728\n","step: 850, loss: 0.16596470773220062\n","step: 860, loss: 0.09910175949335098\n","step: 870, loss: 0.10026637464761734\n","step: 880, loss: 0.10140243172645569\n","step: 890, loss: 0.07731392979621887\n","step: 900, loss: 0.10629859566688538\n","step: 910, loss: 0.10787476599216461\n","step: 920, loss: 0.07030870765447617\n","step: 930, loss: 0.11367783695459366\n","step: 940, loss: 0.13369838893413544\n","step: 950, loss: 0.11111372709274292\n","step: 960, loss: 0.07027789205312729\n","step: 970, loss: 0.04679369553923607\n","step: 980, loss: 0.04324599355459213\n","step: 990, loss: 0.057967718690633774\n","step: 1000, loss: 0.07932371646165848\n","step: 1010, loss: 0.06808464974164963\n","step: 1020, loss: 0.04090895131230354\n","step: 1030, loss: 0.07279719412326813\n","step: 1040, loss: 0.037424035370349884\n","step: 1050, loss: 0.16567035019397736\n","step: 1060, loss: 0.11410892009735107\n","step: 1070, loss: 0.1745361089706421\n","step: 1080, loss: 0.1093972772359848\n","step: 1090, loss: 0.09320703148841858\n","step: 1100, loss: 0.02731955051422119\n","step: 1110, loss: 0.1454375684261322\n","step: 1120, loss: 0.0369863398373127\n","step: 1130, loss: 0.06112923100590706\n","step: 1140, loss: 0.0806582048535347\n","step: 1150, loss: 0.05902348831295967\n","step: 1160, loss: 0.1906646490097046\n","step: 1170, loss: 0.07129572331905365\n","step: 1180, loss: 0.04558756202459335\n","step: 1190, loss: 0.044332392513751984\n","step: 1200, loss: 0.11378121376037598\n","step: 1210, loss: 0.05698970705270767\n","step: 1220, loss: 0.08769720047712326\n","step: 1230, loss: 0.10393939167261124\n","step: 1240, loss: 0.15293467044830322\n","step: 1250, loss: 0.10277241468429565\n","step: 1260, loss: 0.02733142301440239\n","step: 1270, loss: 0.07991215586662292\n","step: 1280, loss: 0.07595113664865494\n","step: 1290, loss: 0.09628616273403168\n","step: 1300, loss: 0.13086411356925964\n","step: 1310, loss: 0.03193924203515053\n","step: 1320, loss: 0.135172039270401\n","step: 1330, loss: 0.13087767362594604\n","step: 1340, loss: 0.09565775096416473\n","step: 1350, loss: 0.08031611144542694\n","step: 1360, loss: 0.0775042399764061\n","step: 1370, loss: 0.1039654016494751\n","step: 1380, loss: 0.11791806668043137\n","step: 1390, loss: 0.10608160495758057\n","step: 1400, loss: 0.03833688795566559\n","step: 1410, loss: 0.08397021144628525\n","step: 1420, loss: 0.07102178782224655\n","step: 1430, loss: 0.15645283460617065\n","step: 1440, loss: 0.14871945977210999\n","step: 1450, loss: 0.10985741019248962\n","step: 1460, loss: 0.09293040633201599\n","step: 1470, loss: 0.1041739284992218\n","step: 1480, loss: 0.03725090250372887\n","step: 1490, loss: 0.10684031993150711\n","step: 1500, loss: 0.14035649597644806\n","step: 1510, loss: 0.14057552814483643\n","step: 1520, loss: 0.05964137241244316\n","step: 1530, loss: 0.19668157398700714\n","step: 1540, loss: 0.16219322383403778\n","step: 1550, loss: 0.08280631899833679\n","step: 1560, loss: 0.041630037128925323\n","step: 1570, loss: 0.13585886359214783\n","step: 1580, loss: 0.1168292909860611\n","step: 1590, loss: 0.08354829996824265\n","step: 1600, loss: 0.06403400003910065\n","step: 1610, loss: 0.1145152822136879\n","step: 1620, loss: 0.1447863131761551\n","step: 1630, loss: 0.19754056632518768\n","step: 1640, loss: 0.10620011389255524\n","step: 1650, loss: 0.030201340094208717\n","step: 1660, loss: 0.08775344491004944\n","step: 1670, loss: 0.03229231759905815\n","step: 1680, loss: 0.06689721345901489\n","step: 1690, loss: 0.10573168843984604\n","step: 1700, loss: 0.11526883393526077\n","step: 1710, loss: 0.0713970884680748\n","step: 1720, loss: 0.06620726734399796\n","step: 1730, loss: 0.08800365030765533\n","step: 1740, loss: 0.1068943440914154\n","step: 1750, loss: 0.13254915177822113\n","step: 1760, loss: 0.0925830528140068\n","step: 1770, loss: 0.1161399781703949\n","step: 1780, loss: 0.09431782364845276\n","step: 1790, loss: 0.08781996369361877\n","step: 1800, loss: 0.039914779365062714\n","step: 1810, loss: 0.08943070471286774\n","step: 1820, loss: 0.07350724935531616\n","step: 1830, loss: 0.11167822033166885\n","step: 1840, loss: 0.0483073852956295\n","step: 1850, loss: 0.16336940228939056\n","step: 1860, loss: 0.04637414962053299\n","step: 1870, loss: 0.09101207554340363\n","step: 1880, loss: 0.09563431143760681\n","step: 1890, loss: 0.15943200886249542\n","step: 1900, loss: 0.17684610188007355\n","step: 1910, loss: 0.06372211873531342\n","step: 1920, loss: 0.06940992176532745\n","step: 1930, loss: 0.14181698858737946\n","step: 1940, loss: 0.10379966348409653\n","step: 1950, loss: 0.04722990468144417\n","step: 1960, loss: 0.06211467087268829\n","step: 1970, loss: 0.06101086735725403\n","step: 1980, loss: 0.05759580433368683\n","step: 1990, loss: 0.11971399188041687\n","step: 2000, loss: 0.09382254630327225\n","step: 2010, loss: 0.09493480622768402\n","step: 2020, loss: 0.1823008805513382\n","step: 2030, loss: 0.14121021330356598\n","step: 2040, loss: 0.058873530477285385\n","step: 2050, loss: 0.14902116358280182\n","step: 2060, loss: 0.02729322947561741\n","step: 2070, loss: 0.08148057758808136\n","step: 2080, loss: 0.10132967680692673\n","step: 2090, loss: 0.04153798148036003\n","step: 2100, loss: 0.08083797991275787\n","step: 2110, loss: 0.1323055475950241\n","step: 2120, loss: 0.09326059371232986\n","step: 2130, loss: 0.03956623747944832\n","step: 2140, loss: 0.15383084118366241\n","step: 2150, loss: 0.11302435398101807\n","step: 2160, loss: 0.019791144877672195\n","step: 2170, loss: 0.03189590945839882\n","step: 2180, loss: 0.1258281171321869\n","step: 2190, loss: 0.07940433919429779\n","step: 2200, loss: 0.10641413927078247\n","step: 2210, loss: 0.10319061577320099\n","step: 2220, loss: 0.09953957051038742\n","step: 2230, loss: 0.035206619650125504\n","step: 2240, loss: 0.03303421661257744\n","step: 2250, loss: 0.13282178342342377\n","step: 2260, loss: 0.0837034061551094\n","step: 2270, loss: 0.1649056226015091\n","step: 2280, loss: 0.14707016944885254\n","step: 2290, loss: 0.05476534739136696\n","step: 2300, loss: 0.04772306978702545\n","step: 2310, loss: 0.06601584702730179\n","step: 2320, loss: 0.12454870343208313\n","step: 2330, loss: 0.0615978017449379\n","step: 2340, loss: 0.037375207990407944\n","step: 2350, loss: 0.024817366153001785\n","step: 2360, loss: 0.12354296445846558\n","step: 2370, loss: 0.07430104911327362\n","step: 2380, loss: 0.06644431501626968\n","step: 2390, loss: 0.15774954855442047\n","step: 2400, loss: 0.0825335755944252\n","step: 2410, loss: 0.10222291201353073\n","step: 2420, loss: 0.017353059723973274\n","step: 2430, loss: 0.16567552089691162\n","step: 2440, loss: 0.10263214260339737\n","step: 2450, loss: 0.06455765664577484\n","step: 2460, loss: 0.14007654786109924\n","step: 2470, loss: 0.10138209909200668\n","step: 2480, loss: 0.08850765973329544\n","step: 2490, loss: 0.08594731241464615\n","step: 2500, loss: 0.1255360245704651\n","step: 2510, loss: 0.05635056272149086\n","step: 2520, loss: 0.1754273921251297\n","step: 2530, loss: 0.015527698211371899\n","step: 2540, loss: 0.10633569210767746\n","step: 2550, loss: 0.05486651137471199\n","step: 2560, loss: 0.06235345080494881\n","step: 2570, loss: 0.08212196081876755\n","step: 2580, loss: 0.05053107813000679\n","step: 2590, loss: 0.027485569939017296\n","step: 2600, loss: 0.05739567428827286\n","step: 2610, loss: 0.12895487248897552\n","step: 2620, loss: 0.196133553981781\n","step: 2630, loss: 0.07272224128246307\n","step: 2640, loss: 0.03639823943376541\n","step: 2650, loss: 0.055269669741392136\n","step: 2660, loss: 0.12913423776626587\n","step: 2670, loss: 0.09299248456954956\n","step: 2680, loss: 0.0317375510931015\n","step: 2690, loss: 0.0352134183049202\n","step: 2700, loss: 0.09835989773273468\n","step: 2710, loss: 0.19415618479251862\n","step: 2720, loss: 0.04024071991443634\n","step: 2730, loss: 0.17041979730129242\n","step: 2740, loss: 0.09834371507167816\n","step: 2750, loss: 0.17866024374961853\n","step: 2760, loss: 0.11005304008722305\n","step: 2770, loss: 0.06297080963850021\n","step: 2780, loss: 0.03168775886297226\n","step: 2790, loss: 0.028601286932826042\n","step: 2800, loss: 0.08767161518335342\n","step: 2810, loss: 0.12952971458435059\n","step: 2820, loss: 0.1553749293088913\n","step: 2830, loss: 0.07337291538715363\n","step: 2840, loss: 0.08962193131446838\n","step: 2850, loss: 0.033089496195316315\n","step: 2860, loss: 0.10067607462406158\n","step: 2870, loss: 0.10141586512327194\n","step: 2880, loss: 0.1320982128381729\n","step: 2890, loss: 0.07953906804323196\n","step: 2900, loss: 0.09103738516569138\n","step: 2910, loss: 0.13365811109542847\n","step: 2920, loss: 0.05765467882156372\n","step: 2930, loss: 0.097442626953125\n","step: 2940, loss: 0.1179690808057785\n","step: 2950, loss: 0.07156045734882355\n","step: 2960, loss: 0.13764987885951996\n","step: 2970, loss: 0.09528501331806183\n","step: 2980, loss: 0.07885421812534332\n","step: 2990, loss: 0.08296938240528107\n","step: 3000, loss: 0.10533522069454193\n","step: 3010, loss: 0.03950747102499008\n","step: 3020, loss: 0.062381260097026825\n","step: 3030, loss: 0.08820242434740067\n","step: 3040, loss: 0.09017614275217056\n","step: 3050, loss: 0.09057309478521347\n","step: 3060, loss: 0.049473464488983154\n","step: 3070, loss: 0.11734932661056519\n","step: 3080, loss: 0.07157448679208755\n","step: 3090, loss: 0.20729874074459076\n","step: 3100, loss: 0.060784466564655304\n","step: 3110, loss: 0.09321058541536331\n","step: 3120, loss: 0.1305573284626007\n","step: 3130, loss: 0.07081758975982666\n","step: 3140, loss: 0.06015972048044205\n","step: 3150, loss: 0.14926598966121674\n","step: 3160, loss: 0.09783422946929932\n","step: 3170, loss: 0.08748096227645874\n","step: 3180, loss: 0.11473317444324493\n","step: 3190, loss: 0.26107677817344666\n","step: 3200, loss: 0.06894057244062424\n","step: 3210, loss: 0.06610424816608429\n","step: 3220, loss: 0.1034545972943306\n","step: 3230, loss: 0.08852985501289368\n","step: 3240, loss: 0.10193950682878494\n","step: 3250, loss: 0.07438263297080994\n","step: 3260, loss: 0.08453245460987091\n","step: 3270, loss: 0.07237694412469864\n","step: 3280, loss: 0.05918128415942192\n","step: 3290, loss: 0.06682007759809494\n","step: 3300, loss: 0.08524926751852036\n","step: 3310, loss: 0.08839591592550278\n","step: 3320, loss: 0.07882459461688995\n","step: 3330, loss: 0.12951895594596863\n","step: 3340, loss: 0.12928850948810577\n","step: 3350, loss: 0.037461038678884506\n","step: 3360, loss: 0.0977417603135109\n","step: 3370, loss: 0.05948413908481598\n","step: 3380, loss: 0.035173408687114716\n","step: 3390, loss: 0.07562639564275742\n","step: 3400, loss: 0.0929129347205162\n","step: 3410, loss: 0.07565692067146301\n","step: 3420, loss: 0.08546444773674011\n","step: 3430, loss: 0.060765136033296585\n","step: 3440, loss: 0.11976449191570282\n","step: 3450, loss: 0.11326687783002853\n","step: 3460, loss: 0.08067914843559265\n","step: 3470, loss: 0.060949478298425674\n","step: 3480, loss: 0.08404514938592911\n","step: 3490, loss: 0.1657661497592926\n","step: 3500, loss: 0.0714741200208664\n","step: 3510, loss: 0.05079322308301926\n","step: 3520, loss: 0.017059534788131714\n","step: 3530, loss: 0.07963836193084717\n","step: 3540, loss: 0.08879974484443665\n","step: 3550, loss: 0.06216812878847122\n","step: 3560, loss: 0.05915137007832527\n","step: 3570, loss: 0.13461285829544067\n","step: 3580, loss: 0.09868768602609634\n","step: 3590, loss: 0.09233161062002182\n","step: 3600, loss: 0.05142482370138168\n","step: 3610, loss: 0.14775532484054565\n","step: 3620, loss: 0.059593696147203445\n","step: 3630, loss: 0.13315655291080475\n","step: 3640, loss: 0.08578605204820633\n","step: 3650, loss: 0.054038144648075104\n","step: 3660, loss: 0.1518488973379135\n","step: 3670, loss: 0.09783069044351578\n","step: 3680, loss: 0.07082296907901764\n","step: 3690, loss: 0.1461154818534851\n","step: 3700, loss: 0.07988202571868896\n","step: 3710, loss: 0.05680624395608902\n","step: 3720, loss: 0.1171342208981514\n","step: 3730, loss: 0.11987960338592529\n","step: 3740, loss: 0.04034285619854927\n","step: 3750, loss: 0.08942069113254547\n","step: 3760, loss: 0.1194964200258255\n","step: 3770, loss: 0.31205204129219055\n","step: 3780, loss: 0.016807347536087036\n","step: 3790, loss: 0.047176700085401535\n","step: 3800, loss: 0.08634288609027863\n","step: 3810, loss: 0.030285976827144623\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.84      0.89      0.86        35\n","           2       0.64      0.09      0.16        77\n","           3       0.99      0.79      0.88      1030\n","           4       0.93      0.85      0.89       291\n","           5       0.98      0.84      0.90       294\n","           6       0.96      0.98      0.97      1570\n","           7       0.50      0.96      0.66       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.96      0.98      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.19      0.31      0.24        13\n","          14       0.54      1.00      0.70        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.97      0.95      0.96        41\n","          18       0.91      0.97      0.94        32\n","          19       0.43      0.65      0.51        40\n","          20       0.99      1.00      0.99       584\n","          21       0.23      0.06      0.09        52\n","          22       0.95      0.72      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.29      0.61      0.40        44\n","          25       0.85      0.88      0.87       888\n","          26       0.90      1.00      0.95         9\n","          27       0.96      0.97      0.96        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.93      0.87      0.90      1136\n","          31       0.58      0.74      0.65        19\n","          32       1.00      0.62      0.77         8\n","          33       0.76      0.91      0.83        86\n","          34       0.24      0.56      0.34        32\n","          35       1.00      0.97      0.98       474\n","          36       0.88      0.12      0.20       182\n","          37       0.89      0.92      0.91      1592\n","          38       0.95      0.98      0.96       404\n","          39       0.97      0.97      0.97       485\n","          40       0.91      0.91      0.91       573\n","          41       0.90      0.95      0.92       841\n","          42       0.98      0.99      0.99       575\n","          43       0.93      0.78      0.85       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.33      0.01      0.02        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.79      0.77     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 452\n","\n","Loop 34\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9889211654663086\n","step: 10, loss: 1.8902932405471802\n","step: 20, loss: 0.8721663355827332\n","step: 30, loss: 0.4634251296520233\n","step: 40, loss: 0.2630622982978821\n","step: 50, loss: 0.3083701431751251\n","step: 60, loss: 0.19311611354351044\n","step: 70, loss: 0.20632493495941162\n","step: 80, loss: 0.21555358171463013\n","step: 90, loss: 0.13461995124816895\n","step: 100, loss: 0.19053933024406433\n","step: 110, loss: 0.17524878680706024\n","step: 120, loss: 0.17663361132144928\n","step: 130, loss: 0.21215607225894928\n","step: 140, loss: 0.14651650190353394\n","step: 150, loss: 0.13787151873111725\n","step: 160, loss: 0.07736410945653915\n","step: 170, loss: 0.16541312634944916\n","step: 180, loss: 0.17952395975589752\n","step: 190, loss: 0.12273160368204117\n","step: 200, loss: 0.09034425765275955\n","step: 210, loss: 0.10374200344085693\n","step: 220, loss: 0.14182940125465393\n","step: 230, loss: 0.1353885978460312\n","step: 240, loss: 0.10320206731557846\n","step: 250, loss: 0.0990682989358902\n","step: 260, loss: 0.19339816272258759\n","step: 270, loss: 0.14129193127155304\n","step: 280, loss: 0.1205151379108429\n","step: 290, loss: 0.20719410479068756\n","step: 300, loss: 0.10166412591934204\n","step: 310, loss: 0.10804083943367004\n","step: 320, loss: 0.09194468706846237\n","step: 330, loss: 0.21502144634723663\n","step: 340, loss: 0.13858549296855927\n","step: 350, loss: 0.15860828757286072\n","step: 360, loss: 0.2201932668685913\n","step: 370, loss: 0.1978895217180252\n","step: 380, loss: 0.10072793811559677\n","step: 390, loss: 0.12065646052360535\n","step: 400, loss: 0.10059911012649536\n","step: 410, loss: 0.10724101960659027\n","step: 420, loss: 0.10154929012060165\n","step: 430, loss: 0.09977717697620392\n","step: 440, loss: 0.03785431757569313\n","step: 450, loss: 0.06556558609008789\n","step: 460, loss: 0.04750676825642586\n","step: 470, loss: 0.1055113673210144\n","step: 480, loss: 0.10572392493486404\n","step: 490, loss: 0.04587456211447716\n","step: 500, loss: 0.05110142379999161\n","step: 510, loss: 0.11888183653354645\n","step: 520, loss: 0.07828007638454437\n","step: 530, loss: 0.13133610785007477\n","step: 540, loss: 0.23677828907966614\n","step: 550, loss: 0.13308288156986237\n","step: 560, loss: 0.08517399430274963\n","step: 570, loss: 0.09139864891767502\n","step: 580, loss: 0.1085287556052208\n","step: 590, loss: 0.08730751276016235\n","step: 600, loss: 0.21737317740917206\n","step: 610, loss: 0.1755622923374176\n","step: 620, loss: 0.0705505758523941\n","step: 630, loss: 0.08848964422941208\n","step: 640, loss: 0.13245977461338043\n","step: 650, loss: 0.08654812723398209\n","step: 660, loss: 0.07726644724607468\n","step: 670, loss: 0.21941056847572327\n","step: 680, loss: 0.06595961004495621\n","step: 690, loss: 0.06035592779517174\n","step: 700, loss: 0.10786489397287369\n","step: 710, loss: 0.057965438812971115\n","step: 720, loss: 0.10646475851535797\n","step: 730, loss: 0.11191466450691223\n","step: 740, loss: 0.10569338500499725\n","step: 750, loss: 0.032135289162397385\n","step: 760, loss: 0.2053440511226654\n","step: 770, loss: 0.06566332280635834\n","step: 780, loss: 0.10271505266427994\n","step: 790, loss: 0.060315705835819244\n","step: 800, loss: 0.05965688079595566\n","step: 810, loss: 0.17183555662631989\n","step: 820, loss: 0.19617696106433868\n","step: 830, loss: 0.057841017842292786\n","step: 840, loss: 0.11190567165613174\n","step: 850, loss: 0.06956315785646439\n","step: 860, loss: 0.05750030651688576\n","step: 870, loss: 0.11352169513702393\n","step: 880, loss: 0.09648537635803223\n","step: 890, loss: 0.048244375735521317\n","step: 900, loss: 0.08643646538257599\n","step: 910, loss: 0.0721747875213623\n","step: 920, loss: 0.07590114325284958\n","step: 930, loss: 0.11582078784704208\n","step: 940, loss: 0.14173045754432678\n","step: 950, loss: 0.07590266317129135\n","step: 960, loss: 0.13060860335826874\n","step: 970, loss: 0.09126647561788559\n","step: 980, loss: 0.06461234390735626\n","step: 990, loss: 0.042179446667432785\n","step: 1000, loss: 0.06209095939993858\n","step: 1010, loss: 0.06273286789655685\n","step: 1020, loss: 0.12204615026712418\n","step: 1030, loss: 0.060995958745479584\n","step: 1040, loss: 0.12491188198328018\n","step: 1050, loss: 0.09429479390382767\n","step: 1060, loss: 0.1095629408955574\n","step: 1070, loss: 0.024368125945329666\n","step: 1080, loss: 0.13103392720222473\n","step: 1090, loss: 0.053379323333501816\n","step: 1100, loss: 0.18020014464855194\n","step: 1110, loss: 0.0793788880109787\n","step: 1120, loss: 0.06182103604078293\n","step: 1130, loss: 0.07612887024879456\n","step: 1140, loss: 0.12029112130403519\n","step: 1150, loss: 0.03447144106030464\n","step: 1160, loss: 0.050948623567819595\n","step: 1170, loss: 0.19300775229930878\n","step: 1180, loss: 0.11410818994045258\n","step: 1190, loss: 0.07282908260822296\n","step: 1200, loss: 0.16221678256988525\n","step: 1210, loss: 0.13860948383808136\n","step: 1220, loss: 0.09122814983129501\n","step: 1230, loss: 0.05653241276741028\n","step: 1240, loss: 0.08446455001831055\n","step: 1250, loss: 0.06689318269491196\n","step: 1260, loss: 0.056067369878292084\n","step: 1270, loss: 0.08061237633228302\n","step: 1280, loss: 0.12694188952445984\n","step: 1290, loss: 0.08171296119689941\n","step: 1300, loss: 0.12221423536539078\n","step: 1310, loss: 0.15671344101428986\n","step: 1320, loss: 0.1516864150762558\n","step: 1330, loss: 0.16629119217395782\n","step: 1340, loss: 0.03742772713303566\n","step: 1350, loss: 0.10615712404251099\n","step: 1360, loss: 0.1561020463705063\n","step: 1370, loss: 0.12640251219272614\n","step: 1380, loss: 0.08550455421209335\n","step: 1390, loss: 0.07510688900947571\n","step: 1400, loss: 0.19844938814640045\n","step: 1410, loss: 0.052294570952653885\n","step: 1420, loss: 0.051663268357515335\n","step: 1430, loss: 0.06576985120773315\n","step: 1440, loss: 0.06112102419137955\n","step: 1450, loss: 0.18419621884822845\n","step: 1460, loss: 0.07450782507658005\n","step: 1470, loss: 0.058940332382917404\n","step: 1480, loss: 0.07720181345939636\n","step: 1490, loss: 0.16820693016052246\n","step: 1500, loss: 0.06439084559679031\n","step: 1510, loss: 0.16658826172351837\n","step: 1520, loss: 0.06454002857208252\n","step: 1530, loss: 0.1308487206697464\n","step: 1540, loss: 0.1445917934179306\n","step: 1550, loss: 0.10964909195899963\n","step: 1560, loss: 0.10034217685461044\n","step: 1570, loss: 0.03145907446742058\n","step: 1580, loss: 0.13006483018398285\n","step: 1590, loss: 0.102620430290699\n","step: 1600, loss: 0.14929747581481934\n","step: 1610, loss: 0.07025005668401718\n","step: 1620, loss: 0.020467156544327736\n","step: 1630, loss: 0.10523536056280136\n","step: 1640, loss: 0.05567654222249985\n","step: 1650, loss: 0.20885975658893585\n","step: 1660, loss: 0.07888015359640121\n","step: 1670, loss: 0.07468684017658234\n","step: 1680, loss: 0.06779149919748306\n","step: 1690, loss: 0.0462089367210865\n","step: 1700, loss: 0.08650727570056915\n","step: 1710, loss: 0.0941799134016037\n","step: 1720, loss: 0.07423710078001022\n","step: 1730, loss: 0.1035957783460617\n","step: 1740, loss: 0.062159813940525055\n","step: 1750, loss: 0.2034338265657425\n","step: 1760, loss: 0.02306458353996277\n","step: 1770, loss: 0.12426187843084335\n","step: 1780, loss: 0.15715324878692627\n","step: 1790, loss: 0.09139209240674973\n","step: 1800, loss: 0.11182079464197159\n","step: 1810, loss: 0.09825623780488968\n","step: 1820, loss: 0.09626870602369308\n","step: 1830, loss: 0.3487040400505066\n","step: 1840, loss: 0.1017787903547287\n","step: 1850, loss: 0.03812045603990555\n","step: 1860, loss: 0.08374499529600143\n","step: 1870, loss: 0.06768293678760529\n","step: 1880, loss: 0.06395430117845535\n","step: 1890, loss: 0.05677776783704758\n","step: 1900, loss: 0.06597363948822021\n","step: 1910, loss: 0.22175776958465576\n","step: 1920, loss: 0.0763697475194931\n","step: 1930, loss: 0.11156599223613739\n","step: 1940, loss: 0.05265989154577255\n","step: 1950, loss: 0.08152490109205246\n","step: 1960, loss: 0.07174208015203476\n","step: 1970, loss: 0.17089051008224487\n","step: 1980, loss: 0.05952265486121178\n","step: 1990, loss: 0.06348162144422531\n","step: 2000, loss: 0.07385251671075821\n","step: 2010, loss: 0.057767946273088455\n","step: 2020, loss: 0.09170207381248474\n","step: 2030, loss: 0.178764209151268\n","step: 2040, loss: 0.0701122060418129\n","step: 2050, loss: 0.12705081701278687\n","step: 2060, loss: 0.11895029991865158\n","step: 2070, loss: 0.08636833727359772\n","step: 2080, loss: 0.04937964677810669\n","step: 2090, loss: 0.0605739988386631\n","step: 2100, loss: 0.1787453144788742\n","step: 2110, loss: 0.16578760743141174\n","step: 2120, loss: 0.07832559198141098\n","step: 2130, loss: 0.07799577713012695\n","step: 2140, loss: 0.16623471677303314\n","step: 2150, loss: 0.054329995065927505\n","step: 2160, loss: 0.05739524960517883\n","step: 2170, loss: 0.17016229033470154\n","step: 2180, loss: 0.07769294083118439\n","step: 2190, loss: 0.0951097384095192\n","step: 2200, loss: 0.06898817420005798\n","step: 2210, loss: 0.09634141623973846\n","step: 2220, loss: 0.07421063631772995\n","step: 2230, loss: 0.19156520068645477\n","step: 2240, loss: 0.0889170840382576\n","step: 2250, loss: 0.07258235663175583\n","step: 2260, loss: 0.14581061899662018\n","step: 2270, loss: 0.09979026019573212\n","step: 2280, loss: 0.07137572020292282\n","step: 2290, loss: 0.06819328665733337\n","step: 2300, loss: 0.15612444281578064\n","step: 2310, loss: 0.06708931177854538\n","step: 2320, loss: 0.07460721582174301\n","step: 2330, loss: 0.08540257811546326\n","step: 2340, loss: 0.1484997272491455\n","step: 2350, loss: 0.1436251401901245\n","step: 2360, loss: 0.15308886766433716\n","step: 2370, loss: 0.15600457787513733\n","step: 2380, loss: 0.033215202391147614\n","step: 2390, loss: 0.0356232225894928\n","step: 2400, loss: 0.05491478368639946\n","step: 2410, loss: 0.07856966555118561\n","step: 2420, loss: 0.049186401069164276\n","step: 2430, loss: 0.13455258309841156\n","step: 2440, loss: 0.03761330991983414\n","step: 2450, loss: 0.0745997428894043\n","step: 2460, loss: 0.07094922661781311\n","step: 2470, loss: 0.10435933619737625\n","step: 2480, loss: 0.11286187171936035\n","step: 2490, loss: 0.09801573306322098\n","step: 2500, loss: 0.09218807518482208\n","step: 2510, loss: 0.10103477537631989\n","step: 2520, loss: 0.04904186725616455\n","step: 2530, loss: 0.11315271258354187\n","step: 2540, loss: 0.06642892211675644\n","step: 2550, loss: 0.08285839110612869\n","step: 2560, loss: 0.05789913237094879\n","step: 2570, loss: 0.05086444318294525\n","step: 2580, loss: 0.03315964713692665\n","step: 2590, loss: 0.18403765559196472\n","step: 2600, loss: 0.06783141940832138\n","step: 2610, loss: 0.08143450319766998\n","step: 2620, loss: 0.034422460943460464\n","step: 2630, loss: 0.06807795912027359\n","step: 2640, loss: 0.09552602469921112\n","step: 2650, loss: 0.07058726251125336\n","step: 2660, loss: 0.040857944637537\n","step: 2670, loss: 0.11042600870132446\n","step: 2680, loss: 0.11475049704313278\n","step: 2690, loss: 0.13981269299983978\n","step: 2700, loss: 0.03377849608659744\n","step: 2710, loss: 0.08651265501976013\n","step: 2720, loss: 0.08840545266866684\n","step: 2730, loss: 0.10659103095531464\n","step: 2740, loss: 0.17510688304901123\n","step: 2750, loss: 0.05976386368274689\n","step: 2760, loss: 0.055641546845436096\n","step: 2770, loss: 0.12002929300069809\n","step: 2780, loss: 0.08839620649814606\n","step: 2790, loss: 0.11712798476219177\n","step: 2800, loss: 0.10641653090715408\n","step: 2810, loss: 0.03341056779026985\n","step: 2820, loss: 0.09519973397254944\n","step: 2830, loss: 0.12243872135877609\n","step: 2840, loss: 0.18908259272575378\n","step: 2850, loss: 0.07518921047449112\n","step: 2860, loss: 0.2166864573955536\n","step: 2870, loss: 0.051748186349868774\n","step: 2880, loss: 0.19120121002197266\n","step: 2890, loss: 0.04516537860035896\n","step: 2900, loss: 0.10531935095787048\n","step: 2910, loss: 0.016513843089342117\n","step: 2920, loss: 0.15012310445308685\n","step: 2930, loss: 0.11112915724515915\n","step: 2940, loss: 0.07637636363506317\n","step: 2950, loss: 0.07436847686767578\n","step: 2960, loss: 0.07752290368080139\n","step: 2970, loss: 0.18043002486228943\n","step: 2980, loss: 0.09470221400260925\n","step: 2990, loss: 0.08838929235935211\n","step: 3000, loss: 0.06247345730662346\n","step: 3010, loss: 0.10048811882734299\n","step: 3020, loss: 0.06459031999111176\n","step: 3030, loss: 0.1037626788020134\n","step: 3040, loss: 0.12437035888433456\n","step: 3050, loss: 0.07950171083211899\n","step: 3060, loss: 0.17176391184329987\n","step: 3070, loss: 0.031886544078588486\n","step: 3080, loss: 0.14088639616966248\n","step: 3090, loss: 0.10164149850606918\n","step: 3100, loss: 0.04335987940430641\n","step: 3110, loss: 0.1276203840970993\n","step: 3120, loss: 0.08599569648504257\n","step: 3130, loss: 0.04208063334226608\n","step: 3140, loss: 0.09492354094982147\n","step: 3150, loss: 0.1711016744375229\n","step: 3160, loss: 0.10262463241815567\n","step: 3170, loss: 0.04956292733550072\n","step: 3180, loss: 0.07528087496757507\n","step: 3190, loss: 0.08798771351575851\n","step: 3200, loss: 0.06607875227928162\n","step: 3210, loss: 0.18935425579547882\n","step: 3220, loss: 0.09436793625354767\n","step: 3230, loss: 0.06658578664064407\n","step: 3240, loss: 0.1047363206744194\n","step: 3250, loss: 0.047483667731285095\n","step: 3260, loss: 0.10622502118349075\n","step: 3270, loss: 0.0729067474603653\n","step: 3280, loss: 0.1379222571849823\n","step: 3290, loss: 0.054478250443935394\n","step: 3300, loss: 0.15704283118247986\n","step: 3310, loss: 0.08616122603416443\n","step: 3320, loss: 0.03951806575059891\n","step: 3330, loss: 0.13411791622638702\n","step: 3340, loss: 0.11310421675443649\n","step: 3350, loss: 0.1367594599723816\n","step: 3360, loss: 0.08374112844467163\n","step: 3370, loss: 0.11383134871721268\n","step: 3380, loss: 0.13033582270145416\n","step: 3390, loss: 0.04677693918347359\n","step: 3400, loss: 0.0394166074693203\n","step: 3410, loss: 0.03576020523905754\n","step: 3420, loss: 0.0619051493704319\n","step: 3430, loss: 0.07419092953205109\n","step: 3440, loss: 0.133412703871727\n","step: 3450, loss: 0.04339224845170975\n","step: 3460, loss: 0.0691191628575325\n","step: 3470, loss: 0.11418058723211288\n","step: 3480, loss: 0.06448625773191452\n","step: 3490, loss: 0.06378810107707977\n","step: 3500, loss: 0.10733586549758911\n","step: 3510, loss: 0.05276232957839966\n","step: 3520, loss: 0.05648664012551308\n","step: 3530, loss: 0.07905630022287369\n","step: 3540, loss: 0.028229938820004463\n","step: 3550, loss: 0.0796479657292366\n","step: 3560, loss: 0.11803451180458069\n","step: 3570, loss: 0.07558546215295792\n","step: 3580, loss: 0.09158676117658615\n","step: 3590, loss: 0.06677556782960892\n","step: 3600, loss: 0.07189933955669403\n","step: 3610, loss: 0.06255059689283371\n","step: 3620, loss: 0.11374305933713913\n","step: 3630, loss: 0.060075949877500534\n","step: 3640, loss: 0.0490933395922184\n","step: 3650, loss: 0.08052859455347061\n","step: 3660, loss: 0.061636652797460556\n","step: 3670, loss: 0.08403737843036652\n","step: 3680, loss: 0.12701216340065002\n","step: 3690, loss: 0.18501590192317963\n","step: 3700, loss: 0.08869238942861557\n","step: 3710, loss: 0.10588543117046356\n","step: 3720, loss: 0.07634520530700684\n","step: 3730, loss: 0.03919891268014908\n","step: 3740, loss: 0.06560838967561722\n","step: 3750, loss: 0.10042143613100052\n","step: 3760, loss: 0.14150558412075043\n","step: 3770, loss: 0.11001928895711899\n","step: 3780, loss: 0.06020425632596016\n","step: 3790, loss: 0.13828566670417786\n","step: 3800, loss: 0.12060233950614929\n","step: 3810, loss: 0.07751818746328354\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.73      0.94      0.83        35\n","           2       0.58      0.52      0.55        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.96      0.84      0.90       291\n","           5       0.94      0.84      0.88       294\n","           6       1.00      0.98      0.99      1570\n","           7       0.55      0.94      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.92      0.97      0.94       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.00      0.00      0.00        13\n","          14       0.28      1.00      0.44        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.89      0.81      0.85      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.89      0.97      0.93        32\n","          19       0.11      0.12      0.12        40\n","          20       0.99      1.00      0.99       584\n","          21       0.06      0.04      0.05        52\n","          22       0.95      0.73      0.82      4175\n","          23       0.67      0.97      0.79      2253\n","          24       0.31      0.34      0.32        44\n","          25       0.86      0.90      0.88       888\n","          26       0.82      1.00      0.90         9\n","          27       0.92      0.97      0.94        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.89      0.84      0.87      1136\n","          31       0.60      0.63      0.62        19\n","          32       1.00      0.38      0.55         8\n","          33       0.69      0.94      0.80        86\n","          34       0.25      0.69      0.37        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.01      0.02       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.98      0.98      0.98       404\n","          39       0.94      0.98      0.96       485\n","          40       0.90      0.92      0.91       573\n","          41       0.95      0.93      0.94       841\n","          42       0.98      0.99      0.98       575\n","          43       0.96      0.93      0.95       152\n","          44       0.96      0.92      0.94        75\n","          46       1.00      0.96      0.98        82\n","          48       0.53      0.13      0.20        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.78      0.75     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 438\n","\n","Loop 35\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9744536876678467\n","step: 10, loss: 1.8813841342926025\n","step: 20, loss: 0.8601671457290649\n","step: 30, loss: 0.25061285495758057\n","step: 40, loss: 0.3138974606990814\n","step: 50, loss: 0.24492688477039337\n","step: 60, loss: 0.2361849844455719\n","step: 70, loss: 0.14505547285079956\n","step: 80, loss: 0.14649318158626556\n","step: 90, loss: 0.14387916028499603\n","step: 100, loss: 0.26877284049987793\n","step: 110, loss: 0.1308128535747528\n","step: 120, loss: 0.1805248111486435\n","step: 130, loss: 0.19299860298633575\n","step: 140, loss: 0.1818201243877411\n","step: 150, loss: 0.27566584944725037\n","step: 160, loss: 0.0750981941819191\n","step: 170, loss: 0.21244047582149506\n","step: 180, loss: 0.4024541974067688\n","step: 190, loss: 0.07883187383413315\n","step: 200, loss: 0.05764206871390343\n","step: 210, loss: 0.11974114924669266\n","step: 220, loss: 0.22525708377361298\n","step: 230, loss: 0.18757307529449463\n","step: 240, loss: 0.1549552083015442\n","step: 250, loss: 0.10043161362409592\n","step: 260, loss: 0.14222991466522217\n","step: 270, loss: 0.0620904415845871\n","step: 280, loss: 0.23595349490642548\n","step: 290, loss: 0.11987926065921783\n","step: 300, loss: 0.08529333025217056\n","step: 310, loss: 0.14915010333061218\n","step: 320, loss: 0.06339078396558762\n","step: 330, loss: 0.10580258816480637\n","step: 340, loss: 0.08851680159568787\n","step: 350, loss: 0.11154131591320038\n","step: 360, loss: 0.0982392281293869\n","step: 370, loss: 0.09183573722839355\n","step: 380, loss: 0.0865856260061264\n","step: 390, loss: 0.05430153012275696\n","step: 400, loss: 0.1085750088095665\n","step: 410, loss: 0.10940501093864441\n","step: 420, loss: 0.03658684343099594\n","step: 430, loss: 0.13447648286819458\n","step: 440, loss: 0.12404462695121765\n","step: 450, loss: 0.10207861661911011\n","step: 460, loss: 0.11945035308599472\n","step: 470, loss: 0.04304887354373932\n","step: 480, loss: 0.07821392267942429\n","step: 490, loss: 0.095242939889431\n","step: 500, loss: 0.2016972303390503\n","step: 510, loss: 0.06892011314630508\n","step: 520, loss: 0.08733808249235153\n","step: 530, loss: 0.08660698682069778\n","step: 540, loss: 0.10785312205553055\n","step: 550, loss: 0.1291179209947586\n","step: 560, loss: 0.12122626602649689\n","step: 570, loss: 0.0920916274189949\n","step: 580, loss: 0.05019400268793106\n","step: 590, loss: 0.1500016748905182\n","step: 600, loss: 0.13340729475021362\n","step: 610, loss: 0.13677482306957245\n","step: 620, loss: 0.05384932458400726\n","step: 630, loss: 0.1549306958913803\n","step: 640, loss: 0.05223185569047928\n","step: 650, loss: 0.07519816607236862\n","step: 660, loss: 0.07075615972280502\n","step: 670, loss: 0.12298240512609482\n","step: 680, loss: 0.05930357053875923\n","step: 690, loss: 0.09907709807157516\n","step: 700, loss: 0.04584739729762077\n","step: 710, loss: 0.05237572640180588\n","step: 720, loss: 0.055307500064373016\n","step: 730, loss: 0.04772724583745003\n","step: 740, loss: 0.04262981191277504\n","step: 750, loss: 0.1368732750415802\n","step: 760, loss: 0.08512932062149048\n","step: 770, loss: 0.06358040124177933\n","step: 780, loss: 0.0850013718008995\n","step: 790, loss: 0.11528203636407852\n","step: 800, loss: 0.18997366726398468\n","step: 810, loss: 0.036947593092918396\n","step: 820, loss: 0.08185584843158722\n","step: 830, loss: 0.06561461836099625\n","step: 840, loss: 0.0592624768614769\n","step: 850, loss: 0.08803977817296982\n","step: 860, loss: 0.0556759275496006\n","step: 870, loss: 0.11960045993328094\n","step: 880, loss: 0.15442444384098053\n","step: 890, loss: 0.12570905685424805\n","step: 900, loss: 0.05176655948162079\n","step: 910, loss: 0.13147543370723724\n","step: 920, loss: 0.01912456937134266\n","step: 930, loss: 0.08709564059972763\n","step: 940, loss: 0.06735072284936905\n","step: 950, loss: 0.04046998172998428\n","step: 960, loss: 0.09769932180643082\n","step: 970, loss: 0.09072118252515793\n","step: 980, loss: 0.14513897895812988\n","step: 990, loss: 0.05712015554308891\n","step: 1000, loss: 0.06125693768262863\n","step: 1010, loss: 0.11406535655260086\n","step: 1020, loss: 0.10986462235450745\n","step: 1030, loss: 0.11777681112289429\n","step: 1040, loss: 0.01706363819539547\n","step: 1050, loss: 0.048406291753053665\n","step: 1060, loss: 0.041766438633203506\n","step: 1070, loss: 0.08397411555051804\n","step: 1080, loss: 0.1506054699420929\n","step: 1090, loss: 0.04979326203465462\n","step: 1100, loss: 0.10088413208723068\n","step: 1110, loss: 0.06924169510602951\n","step: 1120, loss: 0.14167796075344086\n","step: 1130, loss: 0.024348577484488487\n","step: 1140, loss: 0.03837141767144203\n","step: 1150, loss: 0.05086316540837288\n","step: 1160, loss: 0.08414693921804428\n","step: 1170, loss: 0.11008943617343903\n","step: 1180, loss: 0.07083836197853088\n","step: 1190, loss: 0.12557055056095123\n","step: 1200, loss: 0.09977030009031296\n","step: 1210, loss: 0.058824051171541214\n","step: 1220, loss: 0.1130862608551979\n","step: 1230, loss: 0.06633021682500839\n","step: 1240, loss: 0.049810994416475296\n","step: 1250, loss: 1.0864473581314087\n","step: 1260, loss: 0.1986486166715622\n","step: 1270, loss: 0.10088897496461868\n","step: 1280, loss: 0.038618382066488266\n","step: 1290, loss: 0.09606768935918808\n","step: 1300, loss: 0.15975253283977509\n","step: 1310, loss: 0.10950294137001038\n","step: 1320, loss: 0.06957358866930008\n","step: 1330, loss: 0.030046649277210236\n","step: 1340, loss: 0.14476913213729858\n","step: 1350, loss: 0.08545880764722824\n","step: 1360, loss: 0.062158048152923584\n","step: 1370, loss: 0.18124566972255707\n","step: 1380, loss: 0.0880727767944336\n","step: 1390, loss: 0.23681850731372833\n","step: 1400, loss: 0.06966926157474518\n","step: 1410, loss: 0.13372959196567535\n","step: 1420, loss: 0.12333983927965164\n","step: 1430, loss: 0.20381496846675873\n","step: 1440, loss: 0.05572108179330826\n","step: 1450, loss: 0.1074235737323761\n","step: 1460, loss: 0.07179762423038483\n","step: 1470, loss: 0.11444137245416641\n","step: 1480, loss: 0.08368511497974396\n","step: 1490, loss: 0.08532490581274033\n","step: 1500, loss: 0.10759951174259186\n","step: 1510, loss: 0.03827224671840668\n","step: 1520, loss: 0.07087769359350204\n","step: 1530, loss: 0.14580509066581726\n","step: 1540, loss: 0.03421157971024513\n","step: 1550, loss: 0.12327367812395096\n","step: 1560, loss: 0.10096937417984009\n","step: 1570, loss: 0.056938931345939636\n","step: 1580, loss: 0.09656916558742523\n","step: 1590, loss: 0.17440713942050934\n","step: 1600, loss: 0.10502023249864578\n","step: 1610, loss: 0.06978557258844376\n","step: 1620, loss: 0.11127587407827377\n","step: 1630, loss: 0.1480780839920044\n","step: 1640, loss: 0.08030364662408829\n","step: 1650, loss: 0.1122027337551117\n","step: 1660, loss: 0.16733701527118683\n","step: 1670, loss: 0.15353135764598846\n","step: 1680, loss: 0.07125098258256912\n","step: 1690, loss: 0.028953276574611664\n","step: 1700, loss: 0.05332869291305542\n","step: 1710, loss: 0.084550641477108\n","step: 1720, loss: 0.07385469973087311\n","step: 1730, loss: 0.1326507329940796\n","step: 1740, loss: 0.06605201214551926\n","step: 1750, loss: 0.07283492386341095\n","step: 1760, loss: 0.13500146567821503\n","step: 1770, loss: 0.0970105230808258\n","step: 1780, loss: 0.1300986111164093\n","step: 1790, loss: 0.11548171192407608\n","step: 1800, loss: 0.06387174129486084\n","step: 1810, loss: 0.03690822049975395\n","step: 1820, loss: 0.01741001382470131\n","step: 1830, loss: 0.12837497889995575\n","step: 1840, loss: 0.0997624397277832\n","step: 1850, loss: 0.05621131882071495\n","step: 1860, loss: 0.07436337321996689\n","step: 1870, loss: 0.12068165838718414\n","step: 1880, loss: 0.039811331778764725\n","step: 1890, loss: 0.10139086842536926\n","step: 1900, loss: 0.13826708495616913\n","step: 1910, loss: 0.03445671126246452\n","step: 1920, loss: 0.21594609320163727\n","step: 1930, loss: 0.05719568207859993\n","step: 1940, loss: 0.15192313492298126\n","step: 1950, loss: 0.05790932476520538\n","step: 1960, loss: 0.043563831597566605\n","step: 1970, loss: 0.04974853992462158\n","step: 1980, loss: 0.08832167834043503\n","step: 1990, loss: 0.06824001669883728\n","step: 2000, loss: 0.05684725567698479\n","step: 2010, loss: 0.06848055869340897\n","step: 2020, loss: 0.04349355399608612\n","step: 2030, loss: 0.040460530668497086\n","step: 2040, loss: 0.15891195833683014\n","step: 2050, loss: 0.1812562495470047\n","step: 2060, loss: 0.10764554888010025\n","step: 2070, loss: 0.06701487302780151\n","step: 2080, loss: 0.15050891041755676\n","step: 2090, loss: 0.13920912146568298\n","step: 2100, loss: 0.059159886091947556\n","step: 2110, loss: 0.07350312918424606\n","step: 2120, loss: 0.05874066427350044\n","step: 2130, loss: 0.0967927798628807\n","step: 2140, loss: 0.102706179022789\n","step: 2150, loss: 0.0866323933005333\n","step: 2160, loss: 0.10022414475679398\n","step: 2170, loss: 0.075776606798172\n","step: 2180, loss: 0.07084430754184723\n","step: 2190, loss: 0.05764876678586006\n","step: 2200, loss: 0.04258830100297928\n","step: 2210, loss: 0.11793354153633118\n","step: 2220, loss: 0.149732306599617\n","step: 2230, loss: 0.06903274357318878\n","step: 2240, loss: 0.053303588181734085\n","step: 2250, loss: 0.08897671848535538\n","step: 2260, loss: 0.10022352635860443\n","step: 2270, loss: 0.03230671212077141\n","step: 2280, loss: 0.091959148645401\n","step: 2290, loss: 0.04149603843688965\n","step: 2300, loss: 0.06709329783916473\n","step: 2310, loss: 0.10786115378141403\n","step: 2320, loss: 0.06108621135354042\n","step: 2330, loss: 0.08277688175439835\n","step: 2340, loss: 0.06333504617214203\n","step: 2350, loss: 0.06595879793167114\n","step: 2360, loss: 0.11115527898073196\n","step: 2370, loss: 0.08427643030881882\n","step: 2380, loss: 0.13982656598091125\n","step: 2390, loss: 0.12866538763046265\n","step: 2400, loss: 0.16434505581855774\n","step: 2410, loss: 0.1140795648097992\n","step: 2420, loss: 0.07012500613927841\n","step: 2430, loss: 0.09447578340768814\n","step: 2440, loss: 0.059883635491132736\n","step: 2450, loss: 0.11240164935588837\n","step: 2460, loss: 0.14614582061767578\n","step: 2470, loss: 0.06269991397857666\n","step: 2480, loss: 0.1016741544008255\n","step: 2490, loss: 0.08052141964435577\n","step: 2500, loss: 0.07461909204721451\n","step: 2510, loss: 0.06934688985347748\n","step: 2520, loss: 0.060559745877981186\n","step: 2530, loss: 0.1600361466407776\n","step: 2540, loss: 0.104782335460186\n","step: 2550, loss: 0.14248794317245483\n","step: 2560, loss: 0.08681263774633408\n","step: 2570, loss: 0.06726779043674469\n","step: 2580, loss: 0.10715707391500473\n","step: 2590, loss: 0.17809492349624634\n","step: 2600, loss: 0.04846278950572014\n","step: 2610, loss: 0.036307722330093384\n","step: 2620, loss: 0.11931679397821426\n","step: 2630, loss: 0.10344896465539932\n","step: 2640, loss: 0.17961278557777405\n","step: 2650, loss: 0.10916227847337723\n","step: 2660, loss: 0.02595951221883297\n","step: 2670, loss: 0.14568744599819183\n","step: 2680, loss: 0.07273462414741516\n","step: 2690, loss: 0.1111246645450592\n","step: 2700, loss: 0.07529787719249725\n","step: 2710, loss: 0.08891306817531586\n","step: 2720, loss: 0.04583503678441048\n","step: 2730, loss: 0.08160339295864105\n","step: 2740, loss: 0.0778619647026062\n","step: 2750, loss: 0.09124197065830231\n","step: 2760, loss: 0.1258935183286667\n","step: 2770, loss: 0.0938258022069931\n","step: 2780, loss: 0.13931910693645477\n","step: 2790, loss: 0.17167581617832184\n","step: 2800, loss: 0.0838506817817688\n","step: 2810, loss: 0.044831160455942154\n","step: 2820, loss: 0.06994277238845825\n","step: 2830, loss: 0.05360608175396919\n","step: 2840, loss: 0.024856770411133766\n","step: 2850, loss: 0.0997476577758789\n","step: 2860, loss: 0.06294168531894684\n","step: 2870, loss: 0.14265380799770355\n","step: 2880, loss: 0.08995208889245987\n","step: 2890, loss: 0.05985955893993378\n","step: 2900, loss: 0.03014104627072811\n","step: 2910, loss: 0.07698273658752441\n","step: 2920, loss: 0.01824375055730343\n","step: 2930, loss: 0.027705060318112373\n","step: 2940, loss: 0.1100759357213974\n","step: 2950, loss: 0.03250052034854889\n","step: 2960, loss: 0.03333013504743576\n","step: 2970, loss: 0.08505653589963913\n","step: 2980, loss: 0.05681390315294266\n","step: 2990, loss: 0.12850849330425262\n","step: 3000, loss: 0.027993137016892433\n","step: 3010, loss: 0.20590223371982574\n","step: 3020, loss: 0.05329305678606033\n","step: 3030, loss: 0.05373462289571762\n","step: 3040, loss: 0.02573251724243164\n","step: 3050, loss: 0.028437377884984016\n","step: 3060, loss: 0.04841797798871994\n","step: 3070, loss: 0.09857950359582901\n","step: 3080, loss: 0.11985673755407333\n","step: 3090, loss: 0.12452578544616699\n","step: 3100, loss: 0.10174678266048431\n","step: 3110, loss: 0.14090169966220856\n","step: 3120, loss: 0.06886302679777145\n","step: 3130, loss: 0.1067461296916008\n","step: 3140, loss: 0.06676007062196732\n","step: 3150, loss: 0.03736026585102081\n","step: 3160, loss: 0.1008828654885292\n","step: 3170, loss: 0.11076171696186066\n","step: 3180, loss: 0.08593431115150452\n","step: 3190, loss: 0.11709539592266083\n","step: 3200, loss: 0.1546972095966339\n","step: 3210, loss: 0.053156014531850815\n","step: 3220, loss: 0.07403501868247986\n","step: 3230, loss: 0.05063673481345177\n","step: 3240, loss: 0.08543568849563599\n","step: 3250, loss: 0.055558666586875916\n","step: 3260, loss: 0.07389925420284271\n","step: 3270, loss: 0.06700854003429413\n","step: 3280, loss: 0.03350738063454628\n","step: 3290, loss: 0.06817308813333511\n","step: 3300, loss: 0.04575314000248909\n","step: 3310, loss: 0.05862252041697502\n","step: 3320, loss: 0.1433594673871994\n","step: 3330, loss: 0.025889035314321518\n","step: 3340, loss: 0.09981369972229004\n","step: 3350, loss: 0.07355699688196182\n","step: 3360, loss: 0.0369747132062912\n","step: 3370, loss: 0.05839535593986511\n","step: 3380, loss: 0.050026435405015945\n","step: 3390, loss: 0.12507081031799316\n","step: 3400, loss: 0.02788829244673252\n","step: 3410, loss: 0.04522179439663887\n","step: 3420, loss: 0.06403478235006332\n","step: 3430, loss: 0.0740436464548111\n","step: 3440, loss: 0.05348769202828407\n","step: 3450, loss: 0.0387900285422802\n","step: 3460, loss: 0.09686047583818436\n","step: 3470, loss: 0.08334611356258392\n","step: 3480, loss: 0.11422114074230194\n","step: 3490, loss: 0.039753686636686325\n","step: 3500, loss: 0.04486070200800896\n","step: 3510, loss: 0.27674394845962524\n","step: 3520, loss: 0.12267005443572998\n","step: 3530, loss: 0.15384766459465027\n","step: 3540, loss: 0.16439950466156006\n","step: 3550, loss: 0.09127232432365417\n","step: 3560, loss: 0.12380486726760864\n","step: 3570, loss: 0.05817753076553345\n","step: 3580, loss: 0.06500447541475296\n","step: 3590, loss: 0.061951298266649246\n","step: 3600, loss: 0.13414891064167023\n","step: 3610, loss: 0.07284776866436005\n","step: 3620, loss: 0.049941252917051315\n","step: 3630, loss: 0.07852863520383835\n","step: 3640, loss: 0.10068349540233612\n","step: 3650, loss: 0.08651242405176163\n","step: 3660, loss: 0.047265030443668365\n","step: 3670, loss: 0.052300434559583664\n","step: 3680, loss: 0.1791049689054489\n","step: 3690, loss: 0.0928267240524292\n","step: 3700, loss: 0.1342850625514984\n","step: 3710, loss: 0.04013630375266075\n","step: 3720, loss: 0.1297987848520279\n","step: 3730, loss: 0.031874846667051315\n","step: 3740, loss: 0.07441051304340363\n","step: 3750, loss: 0.06575687974691391\n","step: 3760, loss: 0.013114931993186474\n","step: 3770, loss: 0.046607837080955505\n","step: 3780, loss: 0.19098301231861115\n","step: 3790, loss: 0.0613488145172596\n","step: 3800, loss: 0.05960455164313316\n","step: 3810, loss: 0.07315424084663391\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.83      1.00      0.91        35\n","           2       0.68      0.90      0.78        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.89      0.84      0.87       291\n","           5       0.98      0.83      0.90       294\n","           6       0.96      1.00      0.98      1570\n","           7       0.58      0.95      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.92      0.98      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.77      0.77      0.77        13\n","          14       0.46      1.00      0.63        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.82      0.84      0.83      1151\n","          17       0.91      0.95      0.93        41\n","          18       0.94      0.94      0.94        32\n","          19       0.49      0.50      0.49        40\n","          20       1.00      1.00      1.00       584\n","          21       0.36      0.10      0.15        52\n","          22       0.97      0.74      0.84      4175\n","          23       0.72      0.95      0.82      2253\n","          24       0.28      0.73      0.40        44\n","          25       0.85      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.92      0.97      0.94        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.90      0.86      0.88      1136\n","          31       0.67      0.53      0.59        19\n","          32       0.75      0.75      0.75         8\n","          33       0.75      0.95      0.84        86\n","          34       0.22      0.66      0.33        32\n","          35       0.97      1.00      0.98       474\n","          36       0.91      0.17      0.29       182\n","          37       0.86      0.95      0.91      1592\n","          38       0.95      0.98      0.97       404\n","          39       0.92      0.93      0.93       485\n","          40       0.91      0.84      0.87       573\n","          41       0.93      0.94      0.93       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.87      0.91       152\n","          44       0.87      0.91      0.89        75\n","          46       1.00      0.96      0.98        82\n","          48       0.92      0.46      0.61        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.82      0.83      0.81     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","Difference 443\n","\n","Loop 36\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.921952486038208\n","step: 10, loss: 1.9553453922271729\n","step: 20, loss: 0.8228297829627991\n","step: 30, loss: 0.546245276927948\n","step: 40, loss: 0.4095013439655304\n","step: 50, loss: 0.34957775473594666\n","step: 60, loss: 0.17030252516269684\n","step: 70, loss: 0.3192024230957031\n","step: 80, loss: 0.257066935300827\n","step: 90, loss: 0.21964257955551147\n","step: 100, loss: 0.08713547140359879\n","step: 110, loss: 0.133710578083992\n","step: 120, loss: 0.07465558499097824\n","step: 130, loss: 0.20182396471500397\n","step: 140, loss: 0.0979204997420311\n","step: 150, loss: 0.11746446043252945\n","step: 160, loss: 0.14733202755451202\n","step: 170, loss: 0.13915866613388062\n","step: 180, loss: 0.15032973885536194\n","step: 190, loss: 0.14940519630908966\n","step: 200, loss: 0.08555871248245239\n","step: 210, loss: 0.11843817681074142\n","step: 220, loss: 0.23812337219715118\n","step: 230, loss: 0.0839167982339859\n","step: 240, loss: 0.07623381912708282\n","step: 250, loss: 0.10840439796447754\n","step: 260, loss: 0.2695624828338623\n","step: 270, loss: 0.09975043684244156\n","step: 280, loss: 0.1685653030872345\n","step: 290, loss: 0.09312038123607635\n","step: 300, loss: 0.0562034510076046\n","step: 310, loss: 0.056913554668426514\n","step: 320, loss: 0.09001428633928299\n","step: 330, loss: 0.08221246302127838\n","step: 340, loss: 0.06488141417503357\n","step: 350, loss: 0.19048760831356049\n","step: 360, loss: 0.17295458912849426\n","step: 370, loss: 0.07246027141809464\n","step: 380, loss: 0.1362089067697525\n","step: 390, loss: 0.12149660289287567\n","step: 400, loss: 0.16811172664165497\n","step: 410, loss: 0.11949910968542099\n","step: 420, loss: 0.12009309977293015\n","step: 430, loss: 0.01739172637462616\n","step: 440, loss: 0.08943157643079758\n","step: 450, loss: 0.1340908706188202\n","step: 460, loss: 0.12635363638401031\n","step: 470, loss: 0.18135669827461243\n","step: 480, loss: 0.12507876753807068\n","step: 490, loss: 0.15963627398014069\n","step: 500, loss: 0.17289935052394867\n","step: 510, loss: 0.13364438712596893\n","step: 520, loss: 0.11804993450641632\n","step: 530, loss: 0.19951461255550385\n","step: 540, loss: 0.10618644952774048\n","step: 550, loss: 0.118231400847435\n","step: 560, loss: 0.05832783132791519\n","step: 570, loss: 0.0701090395450592\n","step: 580, loss: 0.10351491719484329\n","step: 590, loss: 0.060520682483911514\n","step: 600, loss: 0.16821634769439697\n","step: 610, loss: 0.10481817275285721\n","step: 620, loss: 0.10779237747192383\n","step: 630, loss: 0.04962597414851189\n","step: 640, loss: 0.12807613611221313\n","step: 650, loss: 0.06015579774975777\n","step: 660, loss: 0.1741277426481247\n","step: 670, loss: 0.07965469360351562\n","step: 680, loss: 0.05009420961141586\n","step: 690, loss: 0.11019532382488251\n","step: 700, loss: 0.19241581857204437\n","step: 710, loss: 0.1006695032119751\n","step: 720, loss: 0.053758490830659866\n","step: 730, loss: 0.09742885828018188\n","step: 740, loss: 0.04627532511949539\n","step: 750, loss: 0.11312026530504227\n","step: 760, loss: 0.1082567647099495\n","step: 770, loss: 0.06089617311954498\n","step: 780, loss: 0.04140115901827812\n","step: 790, loss: 0.07298865169286728\n","step: 800, loss: 0.13592229783535004\n","step: 810, loss: 0.07290956377983093\n","step: 820, loss: 0.06709178537130356\n","step: 830, loss: 0.05657133460044861\n","step: 840, loss: 0.0799427255988121\n","step: 850, loss: 0.10624391585588455\n","step: 860, loss: 0.2362094521522522\n","step: 870, loss: 0.10678587853908539\n","step: 880, loss: 0.12372690439224243\n","step: 890, loss: 0.07095992565155029\n","step: 900, loss: 0.11214281618595123\n","step: 910, loss: 0.1023622378706932\n","step: 920, loss: 0.05876573175191879\n","step: 930, loss: 0.06920858472585678\n","step: 940, loss: 0.05721720680594444\n","step: 950, loss: 0.14393097162246704\n","step: 960, loss: 0.05905818194150925\n","step: 970, loss: 0.0776011273264885\n","step: 980, loss: 0.1709345281124115\n","step: 990, loss: 0.0990082398056984\n","step: 1000, loss: 0.16362901031970978\n","step: 1010, loss: 0.14878442883491516\n","step: 1020, loss: 0.0495566762983799\n","step: 1030, loss: 0.060867637395858765\n","step: 1040, loss: 0.15462419390678406\n","step: 1050, loss: 0.03716917335987091\n","step: 1060, loss: 0.13700969517230988\n","step: 1070, loss: 0.042510151863098145\n","step: 1080, loss: 0.05613477900624275\n","step: 1090, loss: 0.10430862009525299\n","step: 1100, loss: 0.1264948546886444\n","step: 1110, loss: 0.08050879091024399\n","step: 1120, loss: 0.1672772467136383\n","step: 1130, loss: 0.05636081472039223\n","step: 1140, loss: 0.059132181107997894\n","step: 1150, loss: 0.12378595024347305\n","step: 1160, loss: 0.04184510186314583\n","step: 1170, loss: 0.07546459138393402\n","step: 1180, loss: 0.10850317776203156\n","step: 1190, loss: 0.21005576848983765\n","step: 1200, loss: 0.08081437647342682\n","step: 1210, loss: 0.05283907800912857\n","step: 1220, loss: 0.13570041954517365\n","step: 1230, loss: 0.06570816785097122\n","step: 1240, loss: 0.15178240835666656\n","step: 1250, loss: 0.06306285411119461\n","step: 1260, loss: 0.05938759446144104\n","step: 1270, loss: 0.13706977665424347\n","step: 1280, loss: 0.07536677271127701\n","step: 1290, loss: 0.08463399112224579\n","step: 1300, loss: 0.03469184413552284\n","step: 1310, loss: 0.07242105156183243\n","step: 1320, loss: 0.12702931463718414\n","step: 1330, loss: 0.07776664197444916\n","step: 1340, loss: 0.07824389636516571\n","step: 1350, loss: 0.13352596759796143\n","step: 1360, loss: 0.06444454938173294\n","step: 1370, loss: 0.10021884739398956\n","step: 1380, loss: 0.10419230163097382\n","step: 1390, loss: 0.16181449592113495\n","step: 1400, loss: 0.09439922869205475\n","step: 1410, loss: 0.05318581685423851\n","step: 1420, loss: 0.11096969991922379\n","step: 1430, loss: 0.11595872044563293\n","step: 1440, loss: 0.05922355875372887\n","step: 1450, loss: 0.11701430380344391\n","step: 1460, loss: 0.040761128067970276\n","step: 1470, loss: 0.08827229589223862\n","step: 1480, loss: 0.0734030082821846\n","step: 1490, loss: 0.024885544553399086\n","step: 1500, loss: 0.13481636345386505\n","step: 1510, loss: 0.07946392893791199\n","step: 1520, loss: 0.047203946858644485\n","step: 1530, loss: 0.10704900324344635\n","step: 1540, loss: 0.18279796838760376\n","step: 1550, loss: 0.13585318624973297\n","step: 1560, loss: 0.0947304517030716\n","step: 1570, loss: 0.06318943202495575\n","step: 1580, loss: 0.08074606955051422\n","step: 1590, loss: 0.11922544240951538\n","step: 1600, loss: 0.14751726388931274\n","step: 1610, loss: 0.12326126545667648\n","step: 1620, loss: 0.1922232210636139\n","step: 1630, loss: 0.08580274879932404\n","step: 1640, loss: 0.14418300986289978\n","step: 1650, loss: 0.08976680785417557\n","step: 1660, loss: 0.0765906497836113\n","step: 1670, loss: 0.11602228134870529\n","step: 1680, loss: 0.06173703819513321\n","step: 1690, loss: 0.08375512063503265\n","step: 1700, loss: 0.08203313499689102\n","step: 1710, loss: 0.0687045156955719\n","step: 1720, loss: 0.12376430630683899\n","step: 1730, loss: 0.13126398622989655\n","step: 1740, loss: 0.1228589415550232\n","step: 1750, loss: 0.052525028586387634\n","step: 1760, loss: 0.08188136667013168\n","step: 1770, loss: 0.12788279354572296\n","step: 1780, loss: 0.16939584910869598\n","step: 1790, loss: 0.0762610137462616\n","step: 1800, loss: 0.13131242990493774\n","step: 1810, loss: 0.1546400636434555\n","step: 1820, loss: 0.18455220758914948\n","step: 1830, loss: 0.12019532173871994\n","step: 1840, loss: 0.06296223402023315\n","step: 1850, loss: 0.04970903322100639\n","step: 1860, loss: 0.12798066437244415\n","step: 1870, loss: 0.0806027352809906\n","step: 1880, loss: 0.05838851258158684\n","step: 1890, loss: 0.050117917358875275\n","step: 1900, loss: 0.06994564831256866\n","step: 1910, loss: 0.06220603361725807\n","step: 1920, loss: 0.1287207156419754\n","step: 1930, loss: 0.06683431565761566\n","step: 1940, loss: 0.08682886511087418\n","step: 1950, loss: 0.1573120355606079\n","step: 1960, loss: 0.11151137202978134\n","step: 1970, loss: 0.12356941401958466\n","step: 1980, loss: 0.09176814556121826\n","step: 1990, loss: 0.04647118225693703\n","step: 2000, loss: 0.13058938086032867\n","step: 2010, loss: 0.0956760048866272\n","step: 2020, loss: 0.11563874036073685\n","step: 2030, loss: 0.1452147513628006\n","step: 2040, loss: 0.038717031478881836\n","step: 2050, loss: 0.06087398901581764\n","step: 2060, loss: 0.07336551696062088\n","step: 2070, loss: 0.032477956265211105\n","step: 2080, loss: 0.08079873770475388\n","step: 2090, loss: 0.068518728017807\n","step: 2100, loss: 0.06873658299446106\n","step: 2110, loss: 0.06872362643480301\n","step: 2120, loss: 0.09579432010650635\n","step: 2130, loss: 0.1386311650276184\n","step: 2140, loss: 0.07686235010623932\n","step: 2150, loss: 0.02599469944834709\n","step: 2160, loss: 0.11928524821996689\n","step: 2170, loss: 0.17825649678707123\n","step: 2180, loss: 0.1293715387582779\n","step: 2190, loss: 0.12177839130163193\n","step: 2200, loss: 0.050333816558122635\n","step: 2210, loss: 0.0679667741060257\n","step: 2220, loss: 0.13744346797466278\n","step: 2230, loss: 0.1280633509159088\n","step: 2240, loss: 0.06129329651594162\n","step: 2250, loss: 0.11459943652153015\n","step: 2260, loss: 0.12303157150745392\n","step: 2270, loss: 0.13202448189258575\n","step: 2280, loss: 0.06372885406017303\n","step: 2290, loss: 0.05646498128771782\n","step: 2300, loss: 0.11437605321407318\n","step: 2310, loss: 0.18767978250980377\n","step: 2320, loss: 0.0702681690454483\n","step: 2330, loss: 0.16443318128585815\n","step: 2340, loss: 0.17840662598609924\n","step: 2350, loss: 0.1657998412847519\n","step: 2360, loss: 0.11178536713123322\n","step: 2370, loss: 0.05213501304388046\n","step: 2380, loss: 0.0691990926861763\n","step: 2390, loss: 0.08734389394521713\n","step: 2400, loss: 0.1263967901468277\n","step: 2410, loss: 0.16013182699680328\n","step: 2420, loss: 0.07037036865949631\n","step: 2430, loss: 0.06541283428668976\n","step: 2440, loss: 0.1140761524438858\n","step: 2450, loss: 0.04710773751139641\n","step: 2460, loss: 0.07633484154939651\n","step: 2470, loss: 0.10126245766878128\n","step: 2480, loss: 0.04352966696023941\n","step: 2490, loss: 0.10757958143949509\n","step: 2500, loss: 0.07031209766864777\n","step: 2510, loss: 0.04431968927383423\n","step: 2520, loss: 0.17309410870075226\n","step: 2530, loss: 0.12781019508838654\n","step: 2540, loss: 0.05407034233212471\n","step: 2550, loss: 0.10912531614303589\n","step: 2560, loss: 0.027686091139912605\n","step: 2570, loss: 0.08413056284189224\n","step: 2580, loss: 0.16711051762104034\n","step: 2590, loss: 0.10459441691637039\n","step: 2600, loss: 0.055378999561071396\n","step: 2610, loss: 0.10992281883955002\n","step: 2620, loss: 0.06681132316589355\n","step: 2630, loss: 0.06673113256692886\n","step: 2640, loss: 0.06264558434486389\n","step: 2650, loss: 0.14855146408081055\n","step: 2660, loss: 0.13916915655136108\n","step: 2670, loss: 0.07325953245162964\n","step: 2680, loss: 0.13906629383563995\n","step: 2690, loss: 0.0919816717505455\n","step: 2700, loss: 0.09147781878709793\n","step: 2710, loss: 0.10012944787740707\n","step: 2720, loss: 0.24822212755680084\n","step: 2730, loss: 0.045460399240255356\n","step: 2740, loss: 0.035700760781764984\n","step: 2750, loss: 0.10930346697568893\n","step: 2760, loss: 0.09413761645555496\n","step: 2770, loss: 0.05457422882318497\n","step: 2780, loss: 0.17909079790115356\n","step: 2790, loss: 0.17458321154117584\n","step: 2800, loss: 0.059934306889772415\n","step: 2810, loss: 0.13316120207309723\n","step: 2820, loss: 0.0856558084487915\n","step: 2830, loss: 0.09462853521108627\n","step: 2840, loss: 0.0703219398856163\n","step: 2850, loss: 0.13400186598300934\n","step: 2860, loss: 0.09147725999355316\n","step: 2870, loss: 0.04335277900099754\n","step: 2880, loss: 0.12543919682502747\n","step: 2890, loss: 0.1098599061369896\n","step: 2900, loss: 0.07195219397544861\n","step: 2910, loss: 0.08930332958698273\n","step: 2920, loss: 0.0627584308385849\n","step: 2930, loss: 0.12157933413982391\n","step: 2940, loss: 0.2020220160484314\n","step: 2950, loss: 0.13576431572437286\n","step: 2960, loss: 0.10014741867780685\n","step: 2970, loss: 0.11209065467119217\n","step: 2980, loss: 0.06193431466817856\n","step: 2990, loss: 0.08231019973754883\n","step: 3000, loss: 0.1454455405473709\n","step: 3010, loss: 0.09225249290466309\n","step: 3020, loss: 0.10336964577436447\n","step: 3030, loss: 0.10736178606748581\n","step: 3040, loss: 0.09278830885887146\n","step: 3050, loss: 0.0699782744050026\n","step: 3060, loss: 0.08208996802568436\n","step: 3070, loss: 0.16237768530845642\n","step: 3080, loss: 0.04810786992311478\n","step: 3090, loss: 0.11704748123884201\n","step: 3100, loss: 0.056269560009241104\n","step: 3110, loss: 0.10446751862764359\n","step: 3120, loss: 0.05415641516447067\n","step: 3130, loss: 0.07301243394613266\n","step: 3140, loss: 0.07837951183319092\n","step: 3150, loss: 0.15164081752300262\n","step: 3160, loss: 0.06830516457557678\n","step: 3170, loss: 0.049845512956380844\n","step: 3180, loss: 0.09455041587352753\n","step: 3190, loss: 0.09982947260141373\n","step: 3200, loss: 0.11540476977825165\n","step: 3210, loss: 0.11384626477956772\n","step: 3220, loss: 0.03526937961578369\n","step: 3230, loss: 0.13383997976779938\n","step: 3240, loss: 0.13748426735401154\n","step: 3250, loss: 0.06350357085466385\n","step: 3260, loss: 0.04342003911733627\n","step: 3270, loss: 0.12785282731056213\n","step: 3280, loss: 0.17603915929794312\n","step: 3290, loss: 0.11779224872589111\n","step: 3300, loss: 0.09745126217603683\n","step: 3310, loss: 0.08835706114768982\n","step: 3320, loss: 0.13467955589294434\n","step: 3330, loss: 0.05279308184981346\n","step: 3340, loss: 0.11280427873134613\n","step: 3350, loss: 0.11349514126777649\n","step: 3360, loss: 0.05753221735358238\n","step: 3370, loss: 0.088149294257164\n","step: 3380, loss: 0.1448567807674408\n","step: 3390, loss: 0.10459824651479721\n","step: 3400, loss: 0.10738738626241684\n","step: 3410, loss: 0.09825461357831955\n","step: 3420, loss: 0.10916207730770111\n","step: 3430, loss: 0.15883177518844604\n","step: 3440, loss: 0.05792135372757912\n","step: 3450, loss: 0.2157873958349228\n","step: 3460, loss: 0.10872385650873184\n","step: 3470, loss: 0.11730209738016129\n","step: 3480, loss: 0.11009925603866577\n","step: 3490, loss: 0.14047223329544067\n","step: 3500, loss: 0.06428883969783783\n","step: 3510, loss: 0.11923936754465103\n","step: 3520, loss: 0.05385192856192589\n","step: 3530, loss: 0.04251132532954216\n","step: 3540, loss: 0.08229894191026688\n","step: 3550, loss: 0.15319335460662842\n","step: 3560, loss: 0.08807257562875748\n","step: 3570, loss: 0.1672179400920868\n","step: 3580, loss: 0.047122981399297714\n","step: 3590, loss: 0.04170387610793114\n","step: 3600, loss: 0.05274933576583862\n","step: 3610, loss: 0.9530831575393677\n","step: 3620, loss: 0.12796248495578766\n","step: 3630, loss: 0.20644083619117737\n","step: 3640, loss: 0.07166903465986252\n","step: 3650, loss: 0.2021958976984024\n","step: 3660, loss: 0.17219063639640808\n","step: 3670, loss: 0.06192287802696228\n","step: 3680, loss: 0.12234661728143692\n","step: 3690, loss: 0.05776914581656456\n","step: 3700, loss: 0.04679138958454132\n","step: 3710, loss: 0.06135089695453644\n","step: 3720, loss: 0.12362989783287048\n","step: 3730, loss: 0.11736627668142319\n","step: 3740, loss: 0.15239864587783813\n","step: 3750, loss: 0.0933663621544838\n","step: 3760, loss: 0.13792060315608978\n","step: 3770, loss: 0.0894852951169014\n","step: 3780, loss: 0.12011386454105377\n","step: 3790, loss: 0.06106874719262123\n","step: 3800, loss: 0.08707395941019058\n","step: 3810, loss: 0.1257789582014084\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      1.00      0.89        35\n","           2       0.40      0.78      0.53        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.77      0.85      0.81       291\n","           5       0.93      0.78      0.85       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.64      0.94      0.76       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.99      0.99       689\n","          10       0.97      0.96      0.97       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.82      0.69      0.75        13\n","          14       0.38      1.00      0.55        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.85      0.87      0.86      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.94      0.97      0.95        32\n","          19       0.92      0.60      0.73        40\n","          20       1.00      1.00      1.00       584\n","          21       0.20      0.04      0.06        52\n","          22       0.93      0.74      0.83      4175\n","          23       0.68      0.96      0.80      2253\n","          24       0.31      0.25      0.28        44\n","          25       0.86      0.92      0.89       888\n","          26       1.00      0.89      0.94         9\n","          27       0.92      0.96      0.94        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.96      0.83      0.89      1136\n","          31       0.63      0.63      0.63        19\n","          32       1.00      0.62      0.77         8\n","          33       0.46      1.00      0.63        86\n","          34       0.22      0.56      0.32        32\n","          35       0.99      0.98      0.98       474\n","          36       1.00      0.09      0.16       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.91      0.98      0.94       404\n","          39       0.97      0.96      0.97       485\n","          40       0.93      0.82      0.87       573\n","          41       0.95      0.94      0.95       841\n","          42       0.97      0.99      0.98       575\n","          43       0.97      0.91      0.94       152\n","          44       0.81      0.92      0.86        75\n","          46       1.00      0.93      0.96        82\n","          48       1.00      0.11      0.20        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.82      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 442\n","\n","Loop 37\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.001358985900879\n","step: 10, loss: 2.176811933517456\n","step: 20, loss: 0.778374433517456\n","step: 30, loss: 0.39793869853019714\n","step: 40, loss: 0.2603228688240051\n","step: 50, loss: 0.20918981730937958\n","step: 60, loss: 0.25824499130249023\n","step: 70, loss: 0.2705514132976532\n","step: 80, loss: 0.17983576655387878\n","step: 90, loss: 0.12380404770374298\n","step: 100, loss: 0.17446008324623108\n","step: 110, loss: 0.2079235464334488\n","step: 120, loss: 0.217134490609169\n","step: 130, loss: 0.18322893977165222\n","step: 140, loss: 0.127005934715271\n","step: 150, loss: 0.09769882261753082\n","step: 160, loss: 0.09292875230312347\n","step: 170, loss: 0.16319064795970917\n","step: 180, loss: 0.14336436986923218\n","step: 190, loss: 0.09133639931678772\n","step: 200, loss: 0.1182781308889389\n","step: 210, loss: 0.06775201112031937\n","step: 220, loss: 0.06734900921583176\n","step: 230, loss: 0.08980239182710648\n","step: 240, loss: 0.17466220259666443\n","step: 250, loss: 0.10532821714878082\n","step: 260, loss: 0.0904889851808548\n","step: 270, loss: 0.13916532695293427\n","step: 280, loss: 0.0780564472079277\n","step: 290, loss: 0.08799014985561371\n","step: 300, loss: 0.1230807676911354\n","step: 310, loss: 0.06826801598072052\n","step: 320, loss: 0.08135287463665009\n","step: 330, loss: 0.060467805713415146\n","step: 340, loss: 0.1140928640961647\n","step: 350, loss: 0.14760388433933258\n","step: 360, loss: 0.1599440574645996\n","step: 370, loss: 0.08359877020120621\n","step: 380, loss: 0.06070359796285629\n","step: 390, loss: 0.20939992368221283\n","step: 400, loss: 0.10038956999778748\n","step: 410, loss: 0.13127736747264862\n","step: 420, loss: 0.3136838972568512\n","step: 430, loss: 0.06525012850761414\n","step: 440, loss: 0.16380181908607483\n","step: 450, loss: 0.06698740273714066\n","step: 460, loss: 0.11071841418743134\n","step: 470, loss: 0.18423843383789062\n","step: 480, loss: 0.09538592398166656\n","step: 490, loss: 0.06427941471338272\n","step: 500, loss: 0.1298568695783615\n","step: 510, loss: 0.055603671818971634\n","step: 520, loss: 0.06250616163015366\n","step: 530, loss: 0.03958294540643692\n","step: 540, loss: 0.09843568503856659\n","step: 550, loss: 0.2966011166572571\n","step: 560, loss: 0.1008850708603859\n","step: 570, loss: 0.07160551100969315\n","step: 580, loss: 0.08490416407585144\n","step: 590, loss: 0.0780414342880249\n","step: 600, loss: 0.04483453556895256\n","step: 610, loss: 0.11396054923534393\n","step: 620, loss: 0.146527960896492\n","step: 630, loss: 0.07645900547504425\n","step: 640, loss: 0.12298891693353653\n","step: 650, loss: 0.19166982173919678\n","step: 660, loss: 0.10184018313884735\n","step: 670, loss: 0.12839144468307495\n","step: 680, loss: 0.08013846725225449\n","step: 690, loss: 0.17604413628578186\n","step: 700, loss: 0.1768672913312912\n","step: 710, loss: 0.19987471401691437\n","step: 720, loss: 0.05267618969082832\n","step: 730, loss: 0.11333795636892319\n","step: 740, loss: 0.15366357564926147\n","step: 750, loss: 0.0925910621881485\n","step: 760, loss: 0.2999935746192932\n","step: 770, loss: 0.062121111899614334\n","step: 780, loss: 0.06530820578336716\n","step: 790, loss: 0.1156037449836731\n","step: 800, loss: 0.10072614252567291\n","step: 810, loss: 0.15373681485652924\n","step: 820, loss: 0.11593504995107651\n","step: 830, loss: 0.0672861859202385\n","step: 840, loss: 0.055521972477436066\n","step: 850, loss: 0.11001002043485641\n","step: 860, loss: 0.08835647255182266\n","step: 870, loss: 0.12122353911399841\n","step: 880, loss: 0.028907619416713715\n","step: 890, loss: 0.10985123366117477\n","step: 900, loss: 0.13277843594551086\n","step: 910, loss: 0.02728070132434368\n","step: 920, loss: 0.08047932386398315\n","step: 930, loss: 0.16348285973072052\n","step: 940, loss: 0.1336684226989746\n","step: 950, loss: 0.07147987931966782\n","step: 960, loss: 0.08723419904708862\n","step: 970, loss: 0.07165829837322235\n","step: 980, loss: 0.03482288494706154\n","step: 990, loss: 0.0911460667848587\n","step: 1000, loss: 0.08440345525741577\n","step: 1010, loss: 0.025762077420949936\n","step: 1020, loss: 0.09856802225112915\n","step: 1030, loss: 0.13109011948108673\n","step: 1040, loss: 0.11476240307092667\n","step: 1050, loss: 0.06662305444478989\n","step: 1060, loss: 0.05673906207084656\n","step: 1070, loss: 0.09291514754295349\n","step: 1080, loss: 0.10410818457603455\n","step: 1090, loss: 0.15321536362171173\n","step: 1100, loss: 0.026885710656642914\n","step: 1110, loss: 0.07413481920957565\n","step: 1120, loss: 0.13283510506153107\n","step: 1130, loss: 0.19403554499149323\n","step: 1140, loss: 0.10259656608104706\n","step: 1150, loss: 0.1335791051387787\n","step: 1160, loss: 0.13506557047367096\n","step: 1170, loss: 0.1283222734928131\n","step: 1180, loss: 0.14473004639148712\n","step: 1190, loss: 0.2164641171693802\n","step: 1200, loss: 0.09626021236181259\n","step: 1210, loss: 0.04610554128885269\n","step: 1220, loss: 0.06779070198535919\n","step: 1230, loss: 0.09841544181108475\n","step: 1240, loss: 0.08013414591550827\n","step: 1250, loss: 0.1446622610092163\n","step: 1260, loss: 0.06309595704078674\n","step: 1270, loss: 0.06748387217521667\n","step: 1280, loss: 0.07073000073432922\n","step: 1290, loss: 0.08425873517990112\n","step: 1300, loss: 0.12796609103679657\n","step: 1310, loss: 0.10317081212997437\n","step: 1320, loss: 0.11216742545366287\n","step: 1330, loss: 0.11378931254148483\n","step: 1340, loss: 0.12311054021120071\n","step: 1350, loss: 0.13839106261730194\n","step: 1360, loss: 0.1304890662431717\n","step: 1370, loss: 0.12136965245008469\n","step: 1380, loss: 0.09848488867282867\n","step: 1390, loss: 0.024993354454636574\n","step: 1400, loss: 0.1262393742799759\n","step: 1410, loss: 0.10709672421216965\n","step: 1420, loss: 0.0742485299706459\n","step: 1430, loss: 0.2637891173362732\n","step: 1440, loss: 0.06950701028108597\n","step: 1450, loss: 0.0774150863289833\n","step: 1460, loss: 0.1179800033569336\n","step: 1470, loss: 0.056955158710479736\n","step: 1480, loss: 0.16905972361564636\n","step: 1490, loss: 0.07421383261680603\n","step: 1500, loss: 0.03283609077334404\n","step: 1510, loss: 0.2207392156124115\n","step: 1520, loss: 0.03904854878783226\n","step: 1530, loss: 0.09245384484529495\n","step: 1540, loss: 0.058500103652477264\n","step: 1550, loss: 0.11821277439594269\n","step: 1560, loss: 0.1043277233839035\n","step: 1570, loss: 0.09010948985815048\n","step: 1580, loss: 0.07461026310920715\n","step: 1590, loss: 0.17407773435115814\n","step: 1600, loss: 0.10094329714775085\n","step: 1610, loss: 0.08775193244218826\n","step: 1620, loss: 0.10885217040777206\n","step: 1630, loss: 0.152474045753479\n","step: 1640, loss: 0.04050137475132942\n","step: 1650, loss: 0.025495124980807304\n","step: 1660, loss: 0.07965593039989471\n","step: 1670, loss: 0.04736301302909851\n","step: 1680, loss: 0.09890630841255188\n","step: 1690, loss: 0.06252254545688629\n","step: 1700, loss: 0.06616149842739105\n","step: 1710, loss: 0.1639348268508911\n","step: 1720, loss: 0.10180525481700897\n","step: 1730, loss: 0.052680205553770065\n","step: 1740, loss: 0.033009499311447144\n","step: 1750, loss: 0.14980904757976532\n","step: 1760, loss: 0.09436104446649551\n","step: 1770, loss: 0.024541441351175308\n","step: 1780, loss: 0.05652714520692825\n","step: 1790, loss: 0.20054170489311218\n","step: 1800, loss: 0.07407225668430328\n","step: 1810, loss: 0.08391766995191574\n","step: 1820, loss: 0.016359686851501465\n","step: 1830, loss: 0.09535998106002808\n","step: 1840, loss: 0.03770767152309418\n","step: 1850, loss: 0.10631997883319855\n","step: 1860, loss: 0.11774221807718277\n","step: 1870, loss: 0.21919551491737366\n","step: 1880, loss: 0.1309577375650406\n","step: 1890, loss: 0.08480427414178848\n","step: 1900, loss: 0.08602507412433624\n","step: 1910, loss: 0.05811363458633423\n","step: 1920, loss: 0.11030400544404984\n","step: 1930, loss: 0.07694519311189651\n","step: 1940, loss: 0.0593317411839962\n","step: 1950, loss: 0.08130762726068497\n","step: 1960, loss: 0.06776238232851028\n","step: 1970, loss: 0.15266086161136627\n","step: 1980, loss: 0.08546009659767151\n","step: 1990, loss: 0.07665406912565231\n","step: 2000, loss: 0.10817588865756989\n","step: 2010, loss: 0.07545127719640732\n","step: 2020, loss: 0.11024503409862518\n","step: 2030, loss: 0.04992149397730827\n","step: 2040, loss: 0.05117866396903992\n","step: 2050, loss: 0.0876714214682579\n","step: 2060, loss: 0.0890897661447525\n","step: 2070, loss: 0.05335941165685654\n","step: 2080, loss: 0.12383110076189041\n","step: 2090, loss: 0.05800655111670494\n","step: 2100, loss: 0.09126020222902298\n","step: 2110, loss: 0.24984091520309448\n","step: 2120, loss: 0.09379193186759949\n","step: 2130, loss: 0.1046246662735939\n","step: 2140, loss: 0.12092806398868561\n","step: 2150, loss: 0.06467533856630325\n","step: 2160, loss: 0.06315255165100098\n","step: 2170, loss: 0.08422757685184479\n","step: 2180, loss: 0.07618515193462372\n","step: 2190, loss: 0.08964227139949799\n","step: 2200, loss: 0.13769306242465973\n","step: 2210, loss: 0.12689673900604248\n","step: 2220, loss: 0.16385389864444733\n","step: 2230, loss: 0.05879609286785126\n","step: 2240, loss: 0.22216132283210754\n","step: 2250, loss: 0.09379648417234421\n","step: 2260, loss: 0.09922841191291809\n","step: 2270, loss: 0.11954326182603836\n","step: 2280, loss: 0.09036204218864441\n","step: 2290, loss: 0.2012661099433899\n","step: 2300, loss: 0.09473702311515808\n","step: 2310, loss: 0.1267186403274536\n","step: 2320, loss: 0.03436131030321121\n","step: 2330, loss: 0.08179717510938644\n","step: 2340, loss: 0.05561754107475281\n","step: 2350, loss: 0.09923180192708969\n","step: 2360, loss: 0.12757094204425812\n","step: 2370, loss: 0.07306666672229767\n","step: 2380, loss: 0.0965472087264061\n","step: 2390, loss: 0.0855901837348938\n","step: 2400, loss: 0.07253282517194748\n","step: 2410, loss: 0.12202807515859604\n","step: 2420, loss: 0.024220585823059082\n","step: 2430, loss: 0.0714300274848938\n","step: 2440, loss: 0.0550493448972702\n","step: 2450, loss: 0.09569493681192398\n","step: 2460, loss: 0.10282918065786362\n","step: 2470, loss: 0.044360190629959106\n","step: 2480, loss: 0.08140043169260025\n","step: 2490, loss: 0.09640495479106903\n","step: 2500, loss: 0.10936228930950165\n","step: 2510, loss: 0.1037578359246254\n","step: 2520, loss: 0.09344668686389923\n","step: 2530, loss: 0.09179560840129852\n","step: 2540, loss: 0.12959152460098267\n","step: 2550, loss: 0.031302936375141144\n","step: 2560, loss: 0.05199551582336426\n","step: 2570, loss: 0.09581585228443146\n","step: 2580, loss: 0.10032319277524948\n","step: 2590, loss: 0.07745197415351868\n","step: 2600, loss: 0.0902191549539566\n","step: 2610, loss: 0.17852100729942322\n","step: 2620, loss: 0.19563041627407074\n","step: 2630, loss: 0.039559539407491684\n","step: 2640, loss: 0.03491382673382759\n","step: 2650, loss: 0.019536061212420464\n","step: 2660, loss: 0.0788223147392273\n","step: 2670, loss: 0.042608633637428284\n","step: 2680, loss: 0.09613969177007675\n","step: 2690, loss: 0.12978403270244598\n","step: 2700, loss: 0.053488414734601974\n","step: 2710, loss: 0.10593019425868988\n","step: 2720, loss: 0.07223301380872726\n","step: 2730, loss: 0.06871964037418365\n","step: 2740, loss: 0.05948128178715706\n","step: 2750, loss: 0.13165922462940216\n","step: 2760, loss: 0.13545864820480347\n","step: 2770, loss: 0.07835022360086441\n","step: 2780, loss: 0.15201275050640106\n","step: 2790, loss: 0.09211882203817368\n","step: 2800, loss: 0.06488412618637085\n","step: 2810, loss: 0.10204411298036575\n","step: 2820, loss: 0.11832183599472046\n","step: 2830, loss: 0.07574687898159027\n","step: 2840, loss: 0.04160318523645401\n","step: 2850, loss: 0.07510481029748917\n","step: 2860, loss: 0.08218567818403244\n","step: 2870, loss: 0.07719717174768448\n","step: 2880, loss: 0.026585007086396217\n","step: 2890, loss: 0.05763091892004013\n","step: 2900, loss: 0.08173620700836182\n","step: 2910, loss: 0.049876902252435684\n","step: 2920, loss: 0.05446393042802811\n","step: 2930, loss: 0.03937723860144615\n","step: 2940, loss: 0.06913208961486816\n","step: 2950, loss: 0.12001349031925201\n","step: 2960, loss: 0.10421641916036606\n","step: 2970, loss: 0.05172554403543472\n","step: 2980, loss: 0.08045371621847153\n","step: 2990, loss: 0.09516028314828873\n","step: 3000, loss: 0.04475392401218414\n","step: 3010, loss: 0.0707700178027153\n","step: 3020, loss: 0.09702259302139282\n","step: 3030, loss: 0.06774412095546722\n","step: 3040, loss: 0.05697730556130409\n","step: 3050, loss: 0.022889768704771996\n","step: 3060, loss: 0.029068101197481155\n","step: 3070, loss: 0.05682847276329994\n","step: 3080, loss: 0.10505753755569458\n","step: 3090, loss: 0.04406781122088432\n","step: 3100, loss: 0.07945679873228073\n","step: 3110, loss: 0.060481730848550797\n","step: 3120, loss: 0.5769982933998108\n","step: 3130, loss: 0.1443624049425125\n","step: 3140, loss: 0.1647854894399643\n","step: 3150, loss: 0.10467302799224854\n","step: 3160, loss: 0.10860946774482727\n","step: 3170, loss: 0.15491318702697754\n","step: 3180, loss: 0.08207226544618607\n","step: 3190, loss: 0.11071981489658356\n","step: 3200, loss: 0.11914762854576111\n","step: 3210, loss: 0.18127259612083435\n","step: 3220, loss: 0.07349050045013428\n","step: 3230, loss: 0.10709501057863235\n","step: 3240, loss: 0.0698089599609375\n","step: 3250, loss: 0.0761723443865776\n","step: 3260, loss: 0.09987325966358185\n","step: 3270, loss: 0.1923639327287674\n","step: 3280, loss: 0.1768866926431656\n","step: 3290, loss: 0.06929285079240799\n","step: 3300, loss: 0.1530960649251938\n","step: 3310, loss: 0.059903334826231\n","step: 3320, loss: 0.0897752046585083\n","step: 3330, loss: 0.08949663490056992\n","step: 3340, loss: 0.02981889247894287\n","step: 3350, loss: 0.1619759202003479\n","step: 3360, loss: 0.11459436267614365\n","step: 3370, loss: 0.06235980987548828\n","step: 3380, loss: 0.23051045835018158\n","step: 3390, loss: 0.06479880213737488\n","step: 3400, loss: 0.11619701236486435\n","step: 3410, loss: 0.047388046979904175\n","step: 3420, loss: 0.13634711503982544\n","step: 3430, loss: 0.12013625353574753\n","step: 3440, loss: 0.06659875810146332\n","step: 3450, loss: 0.11837653815746307\n","step: 3460, loss: 0.0863310694694519\n","step: 3470, loss: 0.028292858973145485\n","step: 3480, loss: 0.061288055032491684\n","step: 3490, loss: 0.07448749989271164\n","step: 3500, loss: 0.025796059519052505\n","step: 3510, loss: 0.1057656854391098\n","step: 3520, loss: 0.122470922768116\n","step: 3530, loss: 0.11814156174659729\n","step: 3540, loss: 0.09848113358020782\n","step: 3550, loss: 0.07571582496166229\n","step: 3560, loss: 0.06523620337247849\n","step: 3570, loss: 0.15595044195652008\n","step: 3580, loss: 0.05874452739953995\n","step: 3590, loss: 0.11596670746803284\n","step: 3600, loss: 0.02991788648068905\n","step: 3610, loss: 0.023369742557406425\n","step: 3620, loss: 0.10067714005708694\n","step: 3630, loss: 0.1831202358007431\n","step: 3640, loss: 0.08574002981185913\n","step: 3650, loss: 0.09152071177959442\n","step: 3660, loss: 0.10085749626159668\n","step: 3670, loss: 0.11111454665660858\n","step: 3680, loss: 0.0764574185013771\n","step: 3690, loss: 0.13034465909004211\n","step: 3700, loss: 0.18024788796901703\n","step: 3710, loss: 0.1362089216709137\n","step: 3720, loss: 0.09175790846347809\n","step: 3730, loss: 0.15013544261455536\n","step: 3740, loss: 0.1268683820962906\n","step: 3750, loss: 0.08309362828731537\n","step: 3760, loss: 0.05399063229560852\n","step: 3770, loss: 0.12584242224693298\n","step: 3780, loss: 0.06262332946062088\n","step: 3790, loss: 0.1336417943239212\n","step: 3800, loss: 0.09334460645914078\n","step: 3810, loss: 0.08174606412649155\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.81      1.00      0.90        35\n","           2       0.55      0.39      0.45        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.89      0.84      0.86       291\n","           5       1.00      0.83      0.91       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.97      0.97      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       1.00      0.54      0.70        13\n","          14       0.35      1.00      0.51        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.76      0.87      0.82      1151\n","          17       0.93      0.95      0.94        41\n","          18       0.88      0.94      0.91        32\n","          19       0.52      0.57      0.55        40\n","          20       1.00      1.00      1.00       584\n","          21       0.33      0.04      0.07        52\n","          22       0.96      0.73      0.83      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.32      0.14      0.19        44\n","          25       0.86      0.92      0.89       888\n","          26       1.00      0.89      0.94         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.95      0.83      0.89      1136\n","          31       0.68      0.68      0.68        19\n","          32       1.00      0.38      0.55         8\n","          33       0.70      0.93      0.80        86\n","          34       0.27      0.66      0.38        32\n","          35       0.99      0.99      0.99       474\n","          36       1.00      0.05      0.09       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.96      0.97      0.96       404\n","          39       0.96      0.95      0.96       485\n","          40       0.87      0.96      0.92       573\n","          41       0.97      0.93      0.95       841\n","          42       0.98      0.99      0.98       575\n","          43       0.97      0.91      0.94       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.79      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 435\n","\n","Loop 38\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8675501346588135\n","step: 10, loss: 2.0546021461486816\n","step: 20, loss: 0.7829028964042664\n","step: 30, loss: 0.47590675950050354\n","step: 40, loss: 0.3435775339603424\n","step: 50, loss: 0.21292619407176971\n","step: 60, loss: 0.25671157240867615\n","step: 70, loss: 0.13401192426681519\n","step: 80, loss: 0.14558568596839905\n","step: 90, loss: 0.1226481944322586\n","step: 100, loss: 0.3375145196914673\n","step: 110, loss: 0.21370398998260498\n","step: 120, loss: 0.18654392659664154\n","step: 130, loss: 0.18362629413604736\n","step: 140, loss: 0.049059074372053146\n","step: 150, loss: 0.08412835001945496\n","step: 160, loss: 0.13183647394180298\n","step: 170, loss: 0.07567910104990005\n","step: 180, loss: 0.16673295199871063\n","step: 190, loss: 0.17998698353767395\n","step: 200, loss: 0.04684961214661598\n","step: 210, loss: 0.1154683455824852\n","step: 220, loss: 0.11786939203739166\n","step: 230, loss: 0.14790967106819153\n","step: 240, loss: 0.09465213865041733\n","step: 250, loss: 0.11259350925683975\n","step: 260, loss: 0.01914226822555065\n","step: 270, loss: 0.17897717654705048\n","step: 280, loss: 0.05099060386419296\n","step: 290, loss: 0.1599854677915573\n","step: 300, loss: 0.10533368587493896\n","step: 310, loss: 0.13869516551494598\n","step: 320, loss: 0.1297961324453354\n","step: 330, loss: 0.14154568314552307\n","step: 340, loss: 0.1447860449552536\n","step: 350, loss: 0.11046433448791504\n","step: 360, loss: 0.08427046984434128\n","step: 370, loss: 0.13439738750457764\n","step: 380, loss: 0.08926071226596832\n","step: 390, loss: 0.13412274420261383\n","step: 400, loss: 0.14674875140190125\n","step: 410, loss: 0.11556397378444672\n","step: 420, loss: 0.08018580079078674\n","step: 430, loss: 0.09362693876028061\n","step: 440, loss: 0.1363975554704666\n","step: 450, loss: 0.165455162525177\n","step: 460, loss: 0.09764044731855392\n","step: 470, loss: 0.053896840661764145\n","step: 480, loss: 0.07663197070360184\n","step: 490, loss: 0.1602325588464737\n","step: 500, loss: 0.17477935552597046\n","step: 510, loss: 0.13373148441314697\n","step: 520, loss: 0.046840060502290726\n","step: 530, loss: 0.12389717996120453\n","step: 540, loss: 0.10377872735261917\n","step: 550, loss: 0.13566891849040985\n","step: 560, loss: 0.06817549467086792\n","step: 570, loss: 0.14442074298858643\n","step: 580, loss: 0.13181914389133453\n","step: 590, loss: 0.09581619501113892\n","step: 600, loss: 0.13265880942344666\n","step: 610, loss: 0.08320294320583344\n","step: 620, loss: 0.054068129509687424\n","step: 630, loss: 0.09895981848239899\n","step: 640, loss: 0.07439836859703064\n","step: 650, loss: 0.15277409553527832\n","step: 660, loss: 0.17262692749500275\n","step: 670, loss: 0.059618812054395676\n","step: 680, loss: 0.04471755400300026\n","step: 690, loss: 0.13970358669757843\n","step: 700, loss: 0.11368381977081299\n","step: 710, loss: 0.09354274719953537\n","step: 720, loss: 0.061674393713474274\n","step: 730, loss: 0.08010885864496231\n","step: 740, loss: 0.04408392310142517\n","step: 750, loss: 0.02690557762980461\n","step: 760, loss: 0.24383202195167542\n","step: 770, loss: 0.03407244756817818\n","step: 780, loss: 0.06034107506275177\n","step: 790, loss: 0.164608895778656\n","step: 800, loss: 0.0963175818324089\n","step: 810, loss: 0.04536836966872215\n","step: 820, loss: 0.08765238523483276\n","step: 830, loss: 0.059557292610406876\n","step: 840, loss: 0.07090771943330765\n","step: 850, loss: 0.08247677236795425\n","step: 860, loss: 0.08023836463689804\n","step: 870, loss: 0.14603020250797272\n","step: 880, loss: 0.09092288464307785\n","step: 890, loss: 0.027480468153953552\n","step: 900, loss: 0.09901712089776993\n","step: 910, loss: 0.07494482398033142\n","step: 920, loss: 0.04949444904923439\n","step: 930, loss: 0.11608567833900452\n","step: 940, loss: 0.07582737505435944\n","step: 950, loss: 0.10019555687904358\n","step: 960, loss: 0.0742408037185669\n","step: 970, loss: 0.08555187284946442\n","step: 980, loss: 0.16301654279232025\n","step: 990, loss: 0.12296737730503082\n","step: 1000, loss: 0.06147870793938637\n","step: 1010, loss: 0.07879505306482315\n","step: 1020, loss: 0.12110605835914612\n","step: 1030, loss: 0.04044895991683006\n","step: 1040, loss: 0.0587049163877964\n","step: 1050, loss: 0.03635140135884285\n","step: 1060, loss: 0.16075538098812103\n","step: 1070, loss: 0.053603608161211014\n","step: 1080, loss: 0.0961725041270256\n","step: 1090, loss: 0.0795830637216568\n","step: 1100, loss: 0.16633473336696625\n","step: 1110, loss: 0.04600810632109642\n","step: 1120, loss: 0.10652892291545868\n","step: 1130, loss: 0.1650242954492569\n","step: 1140, loss: 0.12206718325614929\n","step: 1150, loss: 0.2145266830921173\n","step: 1160, loss: 0.0469743013381958\n","step: 1170, loss: 0.05930223688483238\n","step: 1180, loss: 0.055521752685308456\n","step: 1190, loss: 0.06816437840461731\n","step: 1200, loss: 0.1177084669470787\n","step: 1210, loss: 0.12086167186498642\n","step: 1220, loss: 0.09631515294313431\n","step: 1230, loss: 0.03907632455229759\n","step: 1240, loss: 0.08644366264343262\n","step: 1250, loss: 0.08678758144378662\n","step: 1260, loss: 0.11912427842617035\n","step: 1270, loss: 0.18769855797290802\n","step: 1280, loss: 0.07828051596879959\n","step: 1290, loss: 0.1264972984790802\n","step: 1300, loss: 0.09933356940746307\n","step: 1310, loss: 0.03684613108634949\n","step: 1320, loss: 0.1356406956911087\n","step: 1330, loss: 0.09210853278636932\n","step: 1340, loss: 0.0978861004114151\n","step: 1350, loss: 0.1775892674922943\n","step: 1360, loss: 0.09691870957612991\n","step: 1370, loss: 0.11698552966117859\n","step: 1380, loss: 0.2568248212337494\n","step: 1390, loss: 0.17368386685848236\n","step: 1400, loss: 0.07577560842037201\n","step: 1410, loss: 0.12725110352039337\n","step: 1420, loss: 0.10985413938760757\n","step: 1430, loss: 0.09443290531635284\n","step: 1440, loss: 0.20625369250774384\n","step: 1450, loss: 0.14323458075523376\n","step: 1460, loss: 0.03992392495274544\n","step: 1470, loss: 0.05437898635864258\n","step: 1480, loss: 0.15864841639995575\n","step: 1490, loss: 0.17270781099796295\n","step: 1500, loss: 0.08332893997430801\n","step: 1510, loss: 0.1342257410287857\n","step: 1520, loss: 0.089175745844841\n","step: 1530, loss: 0.08226411044597626\n","step: 1540, loss: 0.14428646862506866\n","step: 1550, loss: 0.04676913097500801\n","step: 1560, loss: 0.09408094733953476\n","step: 1570, loss: 0.046756941825151443\n","step: 1580, loss: 0.16398468613624573\n","step: 1590, loss: 0.022696416825056076\n","step: 1600, loss: 0.16433045268058777\n","step: 1610, loss: 0.0325017087161541\n","step: 1620, loss: 0.09317067265510559\n","step: 1630, loss: 0.1848355382680893\n","step: 1640, loss: 0.1729213148355484\n","step: 1650, loss: 0.15345469117164612\n","step: 1660, loss: 0.10422319173812866\n","step: 1670, loss: 0.11031357944011688\n","step: 1680, loss: 0.09477570652961731\n","step: 1690, loss: 0.10243596881628036\n","step: 1700, loss: 0.06482114642858505\n","step: 1710, loss: 0.0808013305068016\n","step: 1720, loss: 0.09024926275014877\n","step: 1730, loss: 0.07647133618593216\n","step: 1740, loss: 0.06440158933401108\n","step: 1750, loss: 0.09654386341571808\n","step: 1760, loss: 0.09506424516439438\n","step: 1770, loss: 0.05338386446237564\n","step: 1780, loss: 0.032042741775512695\n","step: 1790, loss: 0.15554927289485931\n","step: 1800, loss: 0.05913659930229187\n","step: 1810, loss: 0.13656441867351532\n","step: 1820, loss: 0.07110024988651276\n","step: 1830, loss: 0.04263182729482651\n","step: 1840, loss: 0.08550205081701279\n","step: 1850, loss: 0.06504441797733307\n","step: 1860, loss: 0.05857354402542114\n","step: 1870, loss: 0.08269046247005463\n","step: 1880, loss: 0.08273690938949585\n","step: 1890, loss: 0.05578340217471123\n","step: 1900, loss: 0.06026243418455124\n","step: 1910, loss: 0.07650692760944366\n","step: 1920, loss: 0.05870625004172325\n","step: 1930, loss: 0.08478108048439026\n","step: 1940, loss: 0.023542478680610657\n","step: 1950, loss: 0.16540324687957764\n","step: 1960, loss: 0.05074220895767212\n","step: 1970, loss: 0.07373665273189545\n","step: 1980, loss: 0.05124511197209358\n","step: 1990, loss: 0.08310092985630035\n","step: 2000, loss: 0.08894645422697067\n","step: 2010, loss: 0.0314319021999836\n","step: 2020, loss: 0.11649443209171295\n","step: 2030, loss: 0.03313446417450905\n","step: 2040, loss: 0.05693522468209267\n","step: 2050, loss: 0.06432092934846878\n","step: 2060, loss: 0.1348574310541153\n","step: 2070, loss: 0.15557895600795746\n","step: 2080, loss: 0.08295906335115433\n","step: 2090, loss: 0.1438847780227661\n","step: 2100, loss: 0.11057455092668533\n","step: 2110, loss: 0.2716309726238251\n","step: 2120, loss: 0.1132727786898613\n","step: 2130, loss: 0.045724257826805115\n","step: 2140, loss: 0.09881307929754257\n","step: 2150, loss: 0.0622175931930542\n","step: 2160, loss: 0.03458481281995773\n","step: 2170, loss: 0.09958382695913315\n","step: 2180, loss: 0.06946785002946854\n","step: 2190, loss: 0.04960213229060173\n","step: 2200, loss: 0.15262936055660248\n","step: 2210, loss: 0.11927901208400726\n","step: 2220, loss: 0.08780433237552643\n","step: 2230, loss: 0.08529931306838989\n","step: 2240, loss: 0.28338074684143066\n","step: 2250, loss: 0.1193847507238388\n","step: 2260, loss: 0.047951411455869675\n","step: 2270, loss: 0.1527264416217804\n","step: 2280, loss: 0.1311665177345276\n","step: 2290, loss: 0.07795249670743942\n","step: 2300, loss: 0.15541911125183105\n","step: 2310, loss: 0.05287742242217064\n","step: 2320, loss: 0.09364259243011475\n","step: 2330, loss: 0.11477286368608475\n","step: 2340, loss: 0.14510668814182281\n","step: 2350, loss: 0.09113173186779022\n","step: 2360, loss: 0.18370476365089417\n","step: 2370, loss: 0.13387681543827057\n","step: 2380, loss: 0.13024543225765228\n","step: 2390, loss: 0.08383190631866455\n","step: 2400, loss: 0.03439326584339142\n","step: 2410, loss: 0.08516067266464233\n","step: 2420, loss: 0.022805389016866684\n","step: 2430, loss: 0.11354076117277145\n","step: 2440, loss: 0.0993463546037674\n","step: 2450, loss: 0.07299742847681046\n","step: 2460, loss: 0.11742940545082092\n","step: 2470, loss: 0.23937667906284332\n","step: 2480, loss: 0.08135781437158585\n","step: 2490, loss: 0.057744819670915604\n","step: 2500, loss: 0.0744648277759552\n","step: 2510, loss: 0.10339199006557465\n","step: 2520, loss: 0.10031541436910629\n","step: 2530, loss: 0.07101131975650787\n","step: 2540, loss: 0.14240552484989166\n","step: 2550, loss: 0.03970450907945633\n","step: 2560, loss: 0.11599431931972504\n","step: 2570, loss: 0.03259892761707306\n","step: 2580, loss: 0.05365106835961342\n","step: 2590, loss: 0.07906105369329453\n","step: 2600, loss: 0.058660656213760376\n","step: 2610, loss: 0.04923839494585991\n","step: 2620, loss: 0.07327970117330551\n","step: 2630, loss: 0.1009489893913269\n","step: 2640, loss: 0.04620856046676636\n","step: 2650, loss: 0.0587175227701664\n","step: 2660, loss: 0.0946945920586586\n","step: 2670, loss: 0.08563419431447983\n","step: 2680, loss: 0.16297997534275055\n","step: 2690, loss: 0.08841211348772049\n","step: 2700, loss: 0.0805274099111557\n","step: 2710, loss: 0.17157351970672607\n","step: 2720, loss: 0.15871061384677887\n","step: 2730, loss: 0.10484670847654343\n","step: 2740, loss: 0.07413683086633682\n","step: 2750, loss: 0.09002230316400528\n","step: 2760, loss: 0.07996825873851776\n","step: 2770, loss: 0.03598606958985329\n","step: 2780, loss: 0.07832128554582596\n","step: 2790, loss: 0.09409526735544205\n","step: 2800, loss: 0.09518998116254807\n","step: 2810, loss: 0.11196094751358032\n","step: 2820, loss: 0.11895742267370224\n","step: 2830, loss: 0.10416895896196365\n","step: 2840, loss: 0.06629050523042679\n","step: 2850, loss: 0.06246614083647728\n","step: 2860, loss: 0.1641780287027359\n","step: 2870, loss: 0.13992010056972504\n","step: 2880, loss: 0.13663114607334137\n","step: 2890, loss: 0.09411762654781342\n","step: 2900, loss: 0.13098789751529694\n","step: 2910, loss: 0.07750920206308365\n","step: 2920, loss: 0.05356092005968094\n","step: 2930, loss: 0.08200709521770477\n","step: 2940, loss: 0.06173953041434288\n","step: 2950, loss: 0.07085267454385757\n","step: 2960, loss: 0.13740435242652893\n","step: 2970, loss: 0.0655270516872406\n","step: 2980, loss: 0.0772225484251976\n","step: 2990, loss: 0.10754692554473877\n","step: 3000, loss: 0.06652875989675522\n","step: 3010, loss: 0.041724495589733124\n","step: 3020, loss: 0.055087167769670486\n","step: 3030, loss: 0.04469796642661095\n","step: 3040, loss: 0.045425985008478165\n","step: 3050, loss: 0.025853535160422325\n","step: 3060, loss: 0.10751110315322876\n","step: 3070, loss: 0.08180257678031921\n","step: 3080, loss: 0.08750802278518677\n","step: 3090, loss: 0.10825085639953613\n","step: 3100, loss: 0.07641949504613876\n","step: 3110, loss: 0.10933170467615128\n","step: 3120, loss: 0.13702072203159332\n","step: 3130, loss: 0.133037269115448\n","step: 3140, loss: 0.08882296830415726\n","step: 3150, loss: 0.05021236091852188\n","step: 3160, loss: 0.09238530695438385\n","step: 3170, loss: 0.043413225561380386\n","step: 3180, loss: 0.08990810066461563\n","step: 3190, loss: 0.04317767545580864\n","step: 3200, loss: 0.037944644689559937\n","step: 3210, loss: 0.08559577912092209\n","step: 3220, loss: 0.22847382724285126\n","step: 3230, loss: 0.08897469937801361\n","step: 3240, loss: 0.08062905818223953\n","step: 3250, loss: 0.039718106389045715\n","step: 3260, loss: 0.08798964321613312\n","step: 3270, loss: 0.15762247145175934\n","step: 3280, loss: 0.12522903084754944\n","step: 3290, loss: 0.0353468656539917\n","step: 3300, loss: 0.06787649542093277\n","step: 3310, loss: 0.12034940719604492\n","step: 3320, loss: 0.11438243091106415\n","step: 3330, loss: 0.07981918007135391\n","step: 3340, loss: 0.06724712252616882\n","step: 3350, loss: 0.14285695552825928\n","step: 3360, loss: 0.05691687390208244\n","step: 3370, loss: 0.07440623641014099\n","step: 3380, loss: 0.12002791464328766\n","step: 3390, loss: 0.09214583039283752\n","step: 3400, loss: 0.09330137073993683\n","step: 3410, loss: 0.16601233184337616\n","step: 3420, loss: 0.08370152860879898\n","step: 3430, loss: 0.04305442050099373\n","step: 3440, loss: 0.0387251153588295\n","step: 3450, loss: 0.20142482221126556\n","step: 3460, loss: 0.17734822630882263\n","step: 3470, loss: 0.07169074565172195\n","step: 3480, loss: 0.09292684495449066\n","step: 3490, loss: 0.10999065637588501\n","step: 3500, loss: 0.0862698033452034\n","step: 3510, loss: 0.042798712849617004\n","step: 3520, loss: 0.0631251111626625\n","step: 3530, loss: 0.06115153431892395\n","step: 3540, loss: 0.04890614002943039\n","step: 3550, loss: 0.1257639080286026\n","step: 3560, loss: 0.09004157036542892\n","step: 3570, loss: 0.04236379265785217\n","step: 3580, loss: 0.12343783676624298\n","step: 3590, loss: 0.0684506744146347\n","step: 3600, loss: 0.14413908123970032\n","step: 3610, loss: 0.11964700371026993\n","step: 3620, loss: 0.06595923751592636\n","step: 3630, loss: 0.13803057372570038\n","step: 3640, loss: 0.037220731377601624\n","step: 3650, loss: 0.06501802802085876\n","step: 3660, loss: 0.07311031967401505\n","step: 3670, loss: 0.09682419151067734\n","step: 3680, loss: 0.10385534912347794\n","step: 3690, loss: 0.19624431431293488\n","step: 3700, loss: 0.04346708208322525\n","step: 3710, loss: 0.11288376897573471\n","step: 3720, loss: 0.06154010072350502\n","step: 3730, loss: 0.13892152905464172\n","step: 3740, loss: 0.04075521603226662\n","step: 3750, loss: 0.0886494442820549\n","step: 3760, loss: 0.09749355912208557\n","step: 3770, loss: 0.10903952270746231\n","step: 3780, loss: 0.027437439188361168\n","step: 3790, loss: 0.08385568112134933\n","step: 3800, loss: 0.1600509136915207\n","step: 3810, loss: 0.03591475635766983\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.67      1.00      0.80        35\n","           2       0.55      0.97      0.70        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.94      0.84      0.89       291\n","           5       0.94      0.84      0.88       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.57      0.96      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.94      0.98      0.96       901\n","          11       0.97      1.00      0.98      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.91      0.77      0.83        13\n","          14       0.38      1.00      0.55        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.86      0.87      0.87      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.94      0.97      0.95        32\n","          19       0.60      0.80      0.69        40\n","          20       1.00      0.99      1.00       584\n","          21       0.08      0.04      0.05        52\n","          22       0.95      0.76      0.85      4175\n","          23       0.74      0.95      0.83      2253\n","          24       0.33      0.27      0.30        44\n","          25       0.85      0.94      0.89       888\n","          26       0.45      1.00      0.62         9\n","          27       1.00      0.99      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.94      0.84      0.89      1136\n","          31       0.69      0.58      0.63        19\n","          32       1.00      0.75      0.86         8\n","          33       0.67      0.97      0.79        86\n","          34       0.24      0.62      0.34        32\n","          35       0.99      0.99      0.99       474\n","          36       0.88      0.12      0.20       182\n","          37       0.86      0.97      0.91      1592\n","          38       0.99      0.90      0.94       404\n","          39       0.92      0.97      0.94       485\n","          40       0.93      0.96      0.94       573\n","          41       0.93      0.94      0.94       841\n","          42       0.99      0.98      0.99       575\n","          43       0.95      0.66      0.78       152\n","          44       0.87      0.92      0.90        75\n","          46       0.96      0.99      0.98        82\n","          48       0.87      0.16      0.28        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.83      0.79     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 445\n","\n","Loop 39\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9306347370147705\n","step: 10, loss: 1.8630945682525635\n","step: 20, loss: 0.9501100778579712\n","step: 30, loss: 0.4462183117866516\n","step: 40, loss: 0.2706449627876282\n","step: 50, loss: 0.2950475215911865\n","step: 60, loss: 0.3984825015068054\n","step: 70, loss: 0.10151363164186478\n","step: 80, loss: 0.30601128935813904\n","step: 90, loss: 0.16543038189411163\n","step: 100, loss: 0.23344500362873077\n","step: 110, loss: 0.15280593931674957\n","step: 120, loss: 0.16076704859733582\n","step: 130, loss: 0.1035730391740799\n","step: 140, loss: 0.14258907735347748\n","step: 150, loss: 0.22788938879966736\n","step: 160, loss: 0.14310452342033386\n","step: 170, loss: 0.08486581593751907\n","step: 180, loss: 0.13020752370357513\n","step: 190, loss: 0.09415335208177567\n","step: 200, loss: 0.06965958327054977\n","step: 210, loss: 0.06766773760318756\n","step: 220, loss: 0.14209946990013123\n","step: 230, loss: 0.09482316672801971\n","step: 240, loss: 0.15127825736999512\n","step: 250, loss: 0.07908084243535995\n","step: 260, loss: 0.24681441485881805\n","step: 270, loss: 0.17973732948303223\n","step: 280, loss: 0.1585431843996048\n","step: 290, loss: 0.02978438325226307\n","step: 300, loss: 0.19135195016860962\n","step: 310, loss: 0.08950504660606384\n","step: 320, loss: 0.06304075568914413\n","step: 330, loss: 0.09213413298130035\n","step: 340, loss: 0.16089089214801788\n","step: 350, loss: 0.05570727586746216\n","step: 360, loss: 0.11393404006958008\n","step: 370, loss: 0.07725625485181808\n","step: 380, loss: 0.10773439705371857\n","step: 390, loss: 0.2200583815574646\n","step: 400, loss: 0.10555221140384674\n","step: 410, loss: 0.08376352488994598\n","step: 420, loss: 0.1456707864999771\n","step: 430, loss: 0.08585724979639053\n","step: 440, loss: 0.15756763517856598\n","step: 450, loss: 0.07281602919101715\n","step: 460, loss: 0.048862699419260025\n","step: 470, loss: 0.214222252368927\n","step: 480, loss: 0.0814259946346283\n","step: 490, loss: 0.13813860714435577\n","step: 500, loss: 0.07214929908514023\n","step: 510, loss: 0.08777669072151184\n","step: 520, loss: 0.06877914071083069\n","step: 530, loss: 0.09419097006320953\n","step: 540, loss: 0.14553342759609222\n","step: 550, loss: 0.16251513361930847\n","step: 560, loss: 0.09257171303033829\n","step: 570, loss: 0.1003626212477684\n","step: 580, loss: 0.09423064440488815\n","step: 590, loss: 0.048029884696006775\n","step: 600, loss: 0.12901733815670013\n","step: 610, loss: 0.10393800586462021\n","step: 620, loss: 0.08169280737638474\n","step: 630, loss: 0.0848267450928688\n","step: 640, loss: 0.08131413906812668\n","step: 650, loss: 0.1290847659111023\n","step: 660, loss: 0.1433151662349701\n","step: 670, loss: 0.08387104421854019\n","step: 680, loss: 0.10075558722019196\n","step: 690, loss: 0.0957329273223877\n","step: 700, loss: 0.09592311829328537\n","step: 710, loss: 0.03412077948451042\n","step: 720, loss: 0.09130518138408661\n","step: 730, loss: 0.1487329602241516\n","step: 740, loss: 0.043989814817905426\n","step: 750, loss: 0.07230079174041748\n","step: 760, loss: 0.06569493561983109\n","step: 770, loss: 0.18657027184963226\n","step: 780, loss: 0.12939663231372833\n","step: 790, loss: 0.04038901999592781\n","step: 800, loss: 0.04993603378534317\n","step: 810, loss: 0.07627660781145096\n","step: 820, loss: 0.12548291683197021\n","step: 830, loss: 0.12284132838249207\n","step: 840, loss: 0.08140359073877335\n","step: 850, loss: 0.16831551492214203\n","step: 860, loss: 0.07286760956048965\n","step: 870, loss: 0.09671999514102936\n","step: 880, loss: 0.09418878704309464\n","step: 890, loss: 0.1179971843957901\n","step: 900, loss: 0.12051215767860413\n","step: 910, loss: 0.05849692225456238\n","step: 920, loss: 0.1462707221508026\n","step: 930, loss: 0.09376860409975052\n","step: 940, loss: 0.08292099833488464\n","step: 950, loss: 0.12870989739894867\n","step: 960, loss: 0.15840011835098267\n","step: 970, loss: 0.20347203314304352\n","step: 980, loss: 0.048798516392707825\n","step: 990, loss: 0.05607745796442032\n","step: 1000, loss: 0.0809159055352211\n","step: 1010, loss: 0.11992768943309784\n","step: 1020, loss: 0.08745600283145905\n","step: 1030, loss: 0.08298205584287643\n","step: 1040, loss: 0.10322422534227371\n","step: 1050, loss: 0.09035585075616837\n","step: 1060, loss: 0.09304539859294891\n","step: 1070, loss: 0.08135086297988892\n","step: 1080, loss: 0.08835837244987488\n","step: 1090, loss: 0.12132887542247772\n","step: 1100, loss: 0.033025845885276794\n","step: 1110, loss: 0.122220478951931\n","step: 1120, loss: 0.07865182310342789\n","step: 1130, loss: 0.0932215005159378\n","step: 1140, loss: 0.0803917869925499\n","step: 1150, loss: 0.09253271669149399\n","step: 1160, loss: 0.04936092346906662\n","step: 1170, loss: 0.21399982273578644\n","step: 1180, loss: 0.05743913725018501\n","step: 1190, loss: 0.08368976414203644\n","step: 1200, loss: 0.058512087911367416\n","step: 1210, loss: 0.13877983391284943\n","step: 1220, loss: 0.19033266603946686\n","step: 1230, loss: 0.11065025627613068\n","step: 1240, loss: 0.12644188106060028\n","step: 1250, loss: 0.08274750411510468\n","step: 1260, loss: 0.07578808814287186\n","step: 1270, loss: 0.06605610996484756\n","step: 1280, loss: 0.021027840673923492\n","step: 1290, loss: 0.14487607777118683\n","step: 1300, loss: 0.07847288250923157\n","step: 1310, loss: 0.08971183001995087\n","step: 1320, loss: 0.11638236045837402\n","step: 1330, loss: 0.08322311192750931\n","step: 1340, loss: 0.03947419300675392\n","step: 1350, loss: 0.10485897213220596\n","step: 1360, loss: 0.05666477978229523\n","step: 1370, loss: 0.14569106698036194\n","step: 1380, loss: 0.0978369265794754\n","step: 1390, loss: 0.08014730364084244\n","step: 1400, loss: 0.10623709112405777\n","step: 1410, loss: 0.03888269513845444\n","step: 1420, loss: 0.10699218511581421\n","step: 1430, loss: 0.05432179570198059\n","step: 1440, loss: 0.13967472314834595\n","step: 1450, loss: 0.2138333022594452\n","step: 1460, loss: 0.12232424318790436\n","step: 1470, loss: 0.12963233888149261\n","step: 1480, loss: 0.05711979791522026\n","step: 1490, loss: 0.0983155220746994\n","step: 1500, loss: 0.09019189327955246\n","step: 1510, loss: 0.0797290951013565\n","step: 1520, loss: 0.11288546025753021\n","step: 1530, loss: 0.08408872038125992\n","step: 1540, loss: 0.1392306536436081\n","step: 1550, loss: 0.04244832694530487\n","step: 1560, loss: 0.12406006455421448\n","step: 1570, loss: 0.13722814619541168\n","step: 1580, loss: 0.10472989082336426\n","step: 1590, loss: 0.18747884035110474\n","step: 1600, loss: 0.08733395487070084\n","step: 1610, loss: 0.07358218729496002\n","step: 1620, loss: 0.09895942360162735\n","step: 1630, loss: 0.10427700728178024\n","step: 1640, loss: 0.2519710958003998\n","step: 1650, loss: 0.07162404805421829\n","step: 1660, loss: 0.1591898649930954\n","step: 1670, loss: 0.12032376229763031\n","step: 1680, loss: 0.08466988801956177\n","step: 1690, loss: 0.12608204782009125\n","step: 1700, loss: 0.11701121926307678\n","step: 1710, loss: 0.08044137805700302\n","step: 1720, loss: 0.11897801607847214\n","step: 1730, loss: 0.07913592457771301\n","step: 1740, loss: 0.30416855216026306\n","step: 1750, loss: 0.10808613896369934\n","step: 1760, loss: 0.1129460334777832\n","step: 1770, loss: 0.05494779720902443\n","step: 1780, loss: 0.1266663372516632\n","step: 1790, loss: 0.140161395072937\n","step: 1800, loss: 0.08382495492696762\n","step: 1810, loss: 0.10995554178953171\n","step: 1820, loss: 0.03258910030126572\n","step: 1830, loss: 0.08706793189048767\n","step: 1840, loss: 0.19761762022972107\n","step: 1850, loss: 0.2400672286748886\n","step: 1860, loss: 0.06738600134849548\n","step: 1870, loss: 0.1894245594739914\n","step: 1880, loss: 0.13759326934814453\n","step: 1890, loss: 0.07737427204847336\n","step: 1900, loss: 0.057323455810546875\n","step: 1910, loss: 0.05414947122335434\n","step: 1920, loss: 0.09623577445745468\n","step: 1930, loss: 0.06121128797531128\n","step: 1940, loss: 0.07095080614089966\n","step: 1950, loss: 0.05777274817228317\n","step: 1960, loss: 0.19998933374881744\n","step: 1970, loss: 0.07947684079408646\n","step: 1980, loss: 0.08494725823402405\n","step: 1990, loss: 0.10776258260011673\n","step: 2000, loss: 0.047684378921985626\n","step: 2010, loss: 0.13167068362236023\n","step: 2020, loss: 0.15486714243888855\n","step: 2030, loss: 0.0700686126947403\n","step: 2040, loss: 0.04711436107754707\n","step: 2050, loss: 0.08683856576681137\n","step: 2060, loss: 0.07848283648490906\n","step: 2070, loss: 0.044651567935943604\n","step: 2080, loss: 0.04506213217973709\n","step: 2090, loss: 0.05846407637000084\n","step: 2100, loss: 0.06838862597942352\n","step: 2110, loss: 0.2703757882118225\n","step: 2120, loss: 0.014208357781171799\n","step: 2130, loss: 0.04148409515619278\n","step: 2140, loss: 0.045438770204782486\n","step: 2150, loss: 0.06935808807611465\n","step: 2160, loss: 0.10009042918682098\n","step: 2170, loss: 0.09575293958187103\n","step: 2180, loss: 0.07098851352930069\n","step: 2190, loss: 0.07085657119750977\n","step: 2200, loss: 0.08251489698886871\n","step: 2210, loss: 0.16577740013599396\n","step: 2220, loss: 0.06672284007072449\n","step: 2230, loss: 0.08960362523794174\n","step: 2240, loss: 0.055779583752155304\n","step: 2250, loss: 0.030557584017515182\n","step: 2260, loss: 0.1060376837849617\n","step: 2270, loss: 0.07913633435964584\n","step: 2280, loss: 0.04477778822183609\n","step: 2290, loss: 0.1099420115351677\n","step: 2300, loss: 0.106710284948349\n","step: 2310, loss: 0.07133235782384872\n","step: 2320, loss: 0.08180580288171768\n","step: 2330, loss: 0.140952467918396\n","step: 2340, loss: 0.12646213173866272\n","step: 2350, loss: 0.09308163821697235\n","step: 2360, loss: 0.13951648771762848\n","step: 2370, loss: 0.1960134655237198\n","step: 2380, loss: 0.15920862555503845\n","step: 2390, loss: 0.1001199334859848\n","step: 2400, loss: 0.11475931853055954\n","step: 2410, loss: 0.04393778368830681\n","step: 2420, loss: 0.0701494812965393\n","step: 2430, loss: 0.2921270430088043\n","step: 2440, loss: 0.08545844256877899\n","step: 2450, loss: 0.14061082899570465\n","step: 2460, loss: 0.11813729256391525\n","step: 2470, loss: 0.09409263730049133\n","step: 2480, loss: 0.053966496139764786\n","step: 2490, loss: 0.06051715463399887\n","step: 2500, loss: 0.039373014122247696\n","step: 2510, loss: 0.05432978272438049\n","step: 2520, loss: 0.02262258343398571\n","step: 2530, loss: 0.08242753148078918\n","step: 2540, loss: 0.08640693873167038\n","step: 2550, loss: 0.06458932906389236\n","step: 2560, loss: 0.08051903545856476\n","step: 2570, loss: 0.08510659635066986\n","step: 2580, loss: 0.050750959664583206\n","step: 2590, loss: 0.05893154814839363\n","step: 2600, loss: 0.10171116143465042\n","step: 2610, loss: 0.09983820468187332\n","step: 2620, loss: 0.045820508152246475\n","step: 2630, loss: 0.06742732971906662\n","step: 2640, loss: 0.14770378172397614\n","step: 2650, loss: 0.02756941318511963\n","step: 2660, loss: 0.09906743466854095\n","step: 2670, loss: 0.06700263917446136\n","step: 2680, loss: 0.1252080202102661\n","step: 2690, loss: 0.12896227836608887\n","step: 2700, loss: 0.10034149140119553\n","step: 2710, loss: 0.08010747283697128\n","step: 2720, loss: 0.1160053089261055\n","step: 2730, loss: 0.08373522758483887\n","step: 2740, loss: 0.089523546397686\n","step: 2750, loss: 0.07233355939388275\n","step: 2760, loss: 0.1825246661901474\n","step: 2770, loss: 0.036686185747385025\n","step: 2780, loss: 0.08703381568193436\n","step: 2790, loss: 0.10431438684463501\n","step: 2800, loss: 0.09580889344215393\n","step: 2810, loss: 0.03126014769077301\n","step: 2820, loss: 0.04953258857131004\n","step: 2830, loss: 0.0722641721367836\n","step: 2840, loss: 0.025653468444943428\n","step: 2850, loss: 0.1440693736076355\n","step: 2860, loss: 0.05783744528889656\n","step: 2870, loss: 0.06245052441954613\n","step: 2880, loss: 0.051575470715761185\n","step: 2890, loss: 0.01848480850458145\n","step: 2900, loss: 0.14457038044929504\n","step: 2910, loss: 0.09215017408132553\n","step: 2920, loss: 0.0801628902554512\n","step: 2930, loss: 0.0862501785159111\n","step: 2940, loss: 0.08160314708948135\n","step: 2950, loss: 0.1874714493751526\n","step: 2960, loss: 0.12407690286636353\n","step: 2970, loss: 0.09050044417381287\n","step: 2980, loss: 0.14843159914016724\n","step: 2990, loss: 0.05465085431933403\n","step: 3000, loss: 0.04320184886455536\n","step: 3010, loss: 0.062251586467027664\n","step: 3020, loss: 0.0782155990600586\n","step: 3030, loss: 0.11803174018859863\n","step: 3040, loss: 0.10569248348474503\n","step: 3050, loss: 0.1450018435716629\n","step: 3060, loss: 0.08465678244829178\n","step: 3070, loss: 0.19059914350509644\n","step: 3080, loss: 0.06474240869283676\n","step: 3090, loss: 0.1372906118631363\n","step: 3100, loss: 0.043815720826387405\n","step: 3110, loss: 0.07617243379354477\n","step: 3120, loss: 0.0738116055727005\n","step: 3130, loss: 0.06435633450746536\n","step: 3140, loss: 0.05423281341791153\n","step: 3150, loss: 0.05389706417918205\n","step: 3160, loss: 0.09286251664161682\n","step: 3170, loss: 0.10239764302968979\n","step: 3180, loss: 0.0987391397356987\n","step: 3190, loss: 0.03182331100106239\n","step: 3200, loss: 0.0872235894203186\n","step: 3210, loss: 0.0648694634437561\n","step: 3220, loss: 0.07341420650482178\n","step: 3230, loss: 0.06865298002958298\n","step: 3240, loss: 0.09871731698513031\n","step: 3250, loss: 0.047718361020088196\n","step: 3260, loss: 0.07315563410520554\n","step: 3270, loss: 0.11130978912115097\n","step: 3280, loss: 0.08213609457015991\n","step: 3290, loss: 0.13978148996829987\n","step: 3300, loss: 0.05491252616047859\n","step: 3310, loss: 0.08529075235128403\n","step: 3320, loss: 0.016436278820037842\n","step: 3330, loss: 0.0915159359574318\n","step: 3340, loss: 0.16942098736763\n","step: 3350, loss: 0.1352165937423706\n","step: 3360, loss: 0.05979877710342407\n","step: 3370, loss: 0.07795175909996033\n","step: 3380, loss: 0.09410259872674942\n","step: 3390, loss: 0.07099978625774384\n","step: 3400, loss: 0.11633983999490738\n","step: 3410, loss: 0.07982436567544937\n","step: 3420, loss: 0.028961699455976486\n","step: 3430, loss: 0.11728726327419281\n","step: 3440, loss: 0.16064834594726562\n","step: 3450, loss: 0.09234244376420975\n","step: 3460, loss: 0.06500361114740372\n","step: 3470, loss: 0.08963155001401901\n","step: 3480, loss: 0.04834293946623802\n","step: 3490, loss: 0.1164868026971817\n","step: 3500, loss: 0.068773053586483\n","step: 3510, loss: 0.049217481166124344\n","step: 3520, loss: 0.10112125426530838\n","step: 3530, loss: 0.11965207010507584\n","step: 3540, loss: 0.0346006378531456\n","step: 3550, loss: 0.09684770554304123\n","step: 3560, loss: 0.054093651473522186\n","step: 3570, loss: 0.07233881950378418\n","step: 3580, loss: 0.03452659398317337\n","step: 3590, loss: 0.12352604418992996\n","step: 3600, loss: 0.0853591039776802\n","step: 3610, loss: 0.040394123643636703\n","step: 3620, loss: 0.0709768608212471\n","step: 3630, loss: 0.04423424229025841\n","step: 3640, loss: 0.03295409306883812\n","step: 3650, loss: 0.02487184852361679\n","step: 3660, loss: 0.05857207626104355\n","step: 3670, loss: 0.054578643292188644\n","step: 3680, loss: 0.09687263518571854\n","step: 3690, loss: 0.10192198306322098\n","step: 3700, loss: 0.09051993489265442\n","step: 3710, loss: 0.055007658898830414\n","step: 3720, loss: 0.12433306872844696\n","step: 3730, loss: 0.08672229200601578\n","step: 3740, loss: 0.0766039490699768\n","step: 3750, loss: 0.0768384039402008\n","step: 3760, loss: 0.12069196254014969\n","step: 3770, loss: 0.024149535223841667\n","step: 3780, loss: 0.03734884783625603\n","step: 3790, loss: 0.03759688884019852\n","step: 3800, loss: 0.04143260791897774\n","step: 3810, loss: 0.06928981840610504\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.67      0.16      0.25        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.72      0.86      0.78       291\n","           5       0.97      0.82      0.89       294\n","           6       0.99      0.97      0.98      1570\n","           7       0.39      0.94      0.55       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.96      0.92      0.94       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.35      0.54      0.42        13\n","          14       0.61      1.00      0.76        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.90      0.83      0.86      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.91      0.97      0.94        32\n","          19       0.72      0.82      0.77        40\n","          20       1.00      1.00      1.00       584\n","          21       0.09      0.04      0.05        52\n","          22       0.97      0.72      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.37      0.41      0.39        44\n","          25       0.86      0.93      0.89       888\n","          26       0.88      0.78      0.82         9\n","          27       0.97      0.97      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.88      0.92      0.90      1136\n","          31       0.67      0.53      0.59        19\n","          32       0.83      0.62      0.71         8\n","          33       0.82      0.73      0.77        86\n","          34       0.23      0.56      0.32        32\n","          35       0.97      0.99      0.98       474\n","          36       0.67      0.16      0.26       182\n","          37       0.86      0.96      0.91      1592\n","          38       0.98      0.97      0.97       404\n","          39       0.98      0.94      0.96       485\n","          40       0.91      0.95      0.93       573\n","          41       0.92      0.95      0.93       841\n","          42       0.98      0.99      0.99       575\n","          43       0.96      0.77      0.85       152\n","          44       0.91      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.17      0.03      0.04        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.79      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 443\n","\n","Loop 40\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.864757776260376\n","step: 10, loss: 1.7239850759506226\n","step: 20, loss: 0.8288119435310364\n","step: 30, loss: 0.3564641773700714\n","step: 40, loss: 0.21333026885986328\n","step: 50, loss: 0.22471864521503448\n","step: 60, loss: 0.25397607684135437\n","step: 70, loss: 0.17759199440479279\n","step: 80, loss: 0.3097107708454132\n","step: 90, loss: 0.20673254132270813\n","step: 100, loss: 0.266372948884964\n","step: 110, loss: 0.14085261523723602\n","step: 120, loss: 0.3687836229801178\n","step: 130, loss: 0.14101256430149078\n","step: 140, loss: 0.09956920892000198\n","step: 150, loss: 0.14494085311889648\n","step: 160, loss: 0.1496981531381607\n","step: 170, loss: 0.1388646960258484\n","step: 180, loss: 0.06745640188455582\n","step: 190, loss: 0.14588893949985504\n","step: 200, loss: 0.15627913177013397\n","step: 210, loss: 0.09728321433067322\n","step: 220, loss: 0.05335378274321556\n","step: 230, loss: 0.09887602180242538\n","step: 240, loss: 0.21320024132728577\n","step: 250, loss: 0.11062014102935791\n","step: 260, loss: 0.10534782707691193\n","step: 270, loss: 0.15827098488807678\n","step: 280, loss: 0.08946504443883896\n","step: 290, loss: 0.05886203795671463\n","step: 300, loss: 0.11931709200143814\n","step: 310, loss: 0.06644094735383987\n","step: 320, loss: 0.21851542592048645\n","step: 330, loss: 0.107271209359169\n","step: 340, loss: 0.13888101279735565\n","step: 350, loss: 0.0723797157406807\n","step: 360, loss: 0.05163884907960892\n","step: 370, loss: 0.16050398349761963\n","step: 380, loss: 0.1370660364627838\n","step: 390, loss: 0.11321859806776047\n","step: 400, loss: 0.22221745550632477\n","step: 410, loss: 0.08101261407136917\n","step: 420, loss: 0.02787632681429386\n","step: 430, loss: 0.1907976120710373\n","step: 440, loss: 0.12229053676128387\n","step: 450, loss: 0.08297745138406754\n","step: 460, loss: 0.1296839714050293\n","step: 470, loss: 0.09999187290668488\n","step: 480, loss: 0.17273524403572083\n","step: 490, loss: 0.13302084803581238\n","step: 500, loss: 0.08595533668994904\n","step: 510, loss: 0.194318488240242\n","step: 520, loss: 0.14494897425174713\n","step: 530, loss: 0.15027475357055664\n","step: 540, loss: 0.22535064816474915\n","step: 550, loss: 0.03629656508564949\n","step: 560, loss: 0.07558322697877884\n","step: 570, loss: 0.01855352707207203\n","step: 580, loss: 0.07486449927091599\n","step: 590, loss: 0.243764266371727\n","step: 600, loss: 0.05213840305805206\n","step: 610, loss: 0.18463268876075745\n","step: 620, loss: 0.11838290840387344\n","step: 630, loss: 0.06292117387056351\n","step: 640, loss: 0.07052478194236755\n","step: 650, loss: 0.05990024656057358\n","step: 660, loss: 0.1214686781167984\n","step: 670, loss: 0.15779708325862885\n","step: 680, loss: 0.09527398645877838\n","step: 690, loss: 0.08856679499149323\n","step: 700, loss: 0.15390074253082275\n","step: 710, loss: 0.11287172138690948\n","step: 720, loss: 0.1544577181339264\n","step: 730, loss: 0.10240932554006577\n","step: 740, loss: 0.1928662210702896\n","step: 750, loss: 0.040926314890384674\n","step: 760, loss: 0.06112521514296532\n","step: 770, loss: 0.13433073461055756\n","step: 780, loss: 0.20251400768756866\n","step: 790, loss: 0.130941241979599\n","step: 800, loss: 0.16892702877521515\n","step: 810, loss: 0.10101881623268127\n","step: 820, loss: 0.11823233962059021\n","step: 830, loss: 0.22922469675540924\n","step: 840, loss: 0.036755483597517014\n","step: 850, loss: 0.18506042659282684\n","step: 860, loss: 0.05580025166273117\n","step: 870, loss: 0.17466340959072113\n","step: 880, loss: 0.047260530292987823\n","step: 890, loss: 0.08045298606157303\n","step: 900, loss: 0.12878064811229706\n","step: 910, loss: 0.05060090124607086\n","step: 920, loss: 0.029843460768461227\n","step: 930, loss: 0.0671376883983612\n","step: 940, loss: 0.0736900344491005\n","step: 950, loss: 0.17773203551769257\n","step: 960, loss: 0.08639629185199738\n","step: 970, loss: 0.09728355705738068\n","step: 980, loss: 0.07902109622955322\n","step: 990, loss: 0.10508248955011368\n","step: 1000, loss: 0.12118080258369446\n","step: 1010, loss: 0.16658532619476318\n","step: 1020, loss: 0.10686014592647552\n","step: 1030, loss: 0.21614298224449158\n","step: 1040, loss: 0.2034418135881424\n","step: 1050, loss: 0.023651206865906715\n","step: 1060, loss: 0.08053069561719894\n","step: 1070, loss: 0.14807359874248505\n","step: 1080, loss: 0.1271798461675644\n","step: 1090, loss: 0.10170825570821762\n","step: 1100, loss: 0.22320127487182617\n","step: 1110, loss: 0.17000684142112732\n","step: 1120, loss: 0.08506789803504944\n","step: 1130, loss: 0.14793485403060913\n","step: 1140, loss: 0.07988977432250977\n","step: 1150, loss: 0.07326873391866684\n","step: 1160, loss: 0.09913434833288193\n","step: 1170, loss: 0.10922569036483765\n","step: 1180, loss: 0.125660702586174\n","step: 1190, loss: 0.07532200217247009\n","step: 1200, loss: 0.14937996864318848\n","step: 1210, loss: 0.09820876270532608\n","step: 1220, loss: 0.04480079188942909\n","step: 1230, loss: 0.08734694868326187\n","step: 1240, loss: 0.09986060857772827\n","step: 1250, loss: 0.10319578647613525\n","step: 1260, loss: 0.023736439645290375\n","step: 1270, loss: 0.12056397646665573\n","step: 1280, loss: 0.1271086186170578\n","step: 1290, loss: 0.04985140264034271\n","step: 1300, loss: 0.06986535340547562\n","step: 1310, loss: 0.05802285298705101\n","step: 1320, loss: 0.10424722731113434\n","step: 1330, loss: 0.15396521985530853\n","step: 1340, loss: 0.13309995830059052\n","step: 1350, loss: 0.08517920225858688\n","step: 1360, loss: 0.09081447869539261\n","step: 1370, loss: 0.1085362359881401\n","step: 1380, loss: 0.08843803405761719\n","step: 1390, loss: 0.11666024476289749\n","step: 1400, loss: 0.11829672753810883\n","step: 1410, loss: 0.05989263951778412\n","step: 1420, loss: 0.2020687758922577\n","step: 1430, loss: 0.138392373919487\n","step: 1440, loss: 0.12564174830913544\n","step: 1450, loss: 0.06754645705223083\n","step: 1460, loss: 0.0994957983493805\n","step: 1470, loss: 0.13378535211086273\n","step: 1480, loss: 0.2132413685321808\n","step: 1490, loss: 0.09070327132940292\n","step: 1500, loss: 0.12849409878253937\n","step: 1510, loss: 0.14107069373130798\n","step: 1520, loss: 0.009921922348439693\n","step: 1530, loss: 0.09101971983909607\n","step: 1540, loss: 0.4647286832332611\n","step: 1550, loss: 0.09401122480630875\n","step: 1560, loss: 0.10558974742889404\n","step: 1570, loss: 0.07386872172355652\n","step: 1580, loss: 0.041873037815093994\n","step: 1590, loss: 0.07919580489397049\n","step: 1600, loss: 0.12695902585983276\n","step: 1610, loss: 0.10989826172590256\n","step: 1620, loss: 0.11396439373493195\n","step: 1630, loss: 0.10454237461090088\n","step: 1640, loss: 0.07079287618398666\n","step: 1650, loss: 0.1464020311832428\n","step: 1660, loss: 0.10245097428560257\n","step: 1670, loss: 0.07359641790390015\n","step: 1680, loss: 0.0558495968580246\n","step: 1690, loss: 0.08787498623132706\n","step: 1700, loss: 0.12608885765075684\n","step: 1710, loss: 0.1257326900959015\n","step: 1720, loss: 0.1739821434020996\n","step: 1730, loss: 0.13241063058376312\n","step: 1740, loss: 0.11430834233760834\n","step: 1750, loss: 0.11168719828128815\n","step: 1760, loss: 0.17379119992256165\n","step: 1770, loss: 0.20290431380271912\n","step: 1780, loss: 0.07858064770698547\n","step: 1790, loss: 0.07276014983654022\n","step: 1800, loss: 0.16227591037750244\n","step: 1810, loss: 0.1475803703069687\n","step: 1820, loss: 0.08891469240188599\n","step: 1830, loss: 0.08873357623815536\n","step: 1840, loss: 0.09166603535413742\n","step: 1850, loss: 0.060991544276475906\n","step: 1860, loss: 0.2298305183649063\n","step: 1870, loss: 0.06996448338031769\n","step: 1880, loss: 0.1001405343413353\n","step: 1890, loss: 0.04785160347819328\n","step: 1900, loss: 0.07543282210826874\n","step: 1910, loss: 0.13583126664161682\n","step: 1920, loss: 0.10237611085176468\n","step: 1930, loss: 0.14516852796077728\n","step: 1940, loss: 0.06473687291145325\n","step: 1950, loss: 0.041678640991449356\n","step: 1960, loss: 0.04279147461056709\n","step: 1970, loss: 0.07153378427028656\n","step: 1980, loss: 0.11776944994926453\n","step: 1990, loss: 0.037389758974313736\n","step: 2000, loss: 0.07917627692222595\n","step: 2010, loss: 0.21539056301116943\n","step: 2020, loss: 0.09688939154148102\n","step: 2030, loss: 0.1733056753873825\n","step: 2040, loss: 0.11995861679315567\n","step: 2050, loss: 0.05632400885224342\n","step: 2060, loss: 0.15894508361816406\n","step: 2070, loss: 0.1068212017416954\n","step: 2080, loss: 0.1511591374874115\n","step: 2090, loss: 0.27215757966041565\n","step: 2100, loss: 0.1059126928448677\n","step: 2110, loss: 0.20925958454608917\n","step: 2120, loss: 0.09631669521331787\n","step: 2130, loss: 0.07371462136507034\n","step: 2140, loss: 0.13546329736709595\n","step: 2150, loss: 0.2022031545639038\n","step: 2160, loss: 0.25534841418266296\n","step: 2170, loss: 0.06585200130939484\n","step: 2180, loss: 0.05696482211351395\n","step: 2190, loss: 0.16505396366119385\n","step: 2200, loss: 0.11658822000026703\n","step: 2210, loss: 0.0903494656085968\n","step: 2220, loss: 0.061103809624910355\n","step: 2230, loss: 0.0776413232088089\n","step: 2240, loss: 0.10866717249155045\n","step: 2250, loss: 0.0811295285820961\n","step: 2260, loss: 0.07027756422758102\n","step: 2270, loss: 0.09363333135843277\n","step: 2280, loss: 0.03365476801991463\n","step: 2290, loss: 0.08312992006540298\n","step: 2300, loss: 0.050481945276260376\n","step: 2310, loss: 0.13813062012195587\n","step: 2320, loss: 0.08016717433929443\n","step: 2330, loss: 0.04918792098760605\n","step: 2340, loss: 0.1266152411699295\n","step: 2350, loss: 0.04596148431301117\n","step: 2360, loss: 0.0934087261557579\n","step: 2370, loss: 0.05852794274687767\n","step: 2380, loss: 0.13003642857074738\n","step: 2390, loss: 0.09537091106176376\n","step: 2400, loss: 0.13546046614646912\n","step: 2410, loss: 0.15113811194896698\n","step: 2420, loss: 0.09703201800584793\n","step: 2430, loss: 0.043672967702150345\n","step: 2440, loss: 0.04136018082499504\n","step: 2450, loss: 0.06995861232280731\n","step: 2460, loss: 0.15606433153152466\n","step: 2470, loss: 0.09471305459737778\n","step: 2480, loss: 0.07053679972887039\n","step: 2490, loss: 0.06290639191865921\n","step: 2500, loss: 0.05566093698143959\n","step: 2510, loss: 0.0732848197221756\n","step: 2520, loss: 0.0986948013305664\n","step: 2530, loss: 0.08493215590715408\n","step: 2540, loss: 0.12162584811449051\n","step: 2550, loss: 0.15714168548583984\n","step: 2560, loss: 0.03614702820777893\n","step: 2570, loss: 0.052453137934207916\n","step: 2580, loss: 0.0722353458404541\n","step: 2590, loss: 0.08106528222560883\n","step: 2600, loss: 0.14281795918941498\n","step: 2610, loss: 0.04223177209496498\n","step: 2620, loss: 0.12313421070575714\n","step: 2630, loss: 0.08125809580087662\n","step: 2640, loss: 0.1362886279821396\n","step: 2650, loss: 0.18523146212100983\n","step: 2660, loss: 0.06685750186443329\n","step: 2670, loss: 0.1505247801542282\n","step: 2680, loss: 0.0439319871366024\n","step: 2690, loss: 0.12955276668071747\n","step: 2700, loss: 0.12054628133773804\n","step: 2710, loss: 0.1196606382727623\n","step: 2720, loss: 0.04139503464102745\n","step: 2730, loss: 0.048577696084976196\n","step: 2740, loss: 0.1067865639925003\n","step: 2750, loss: 0.17874675989151\n","step: 2760, loss: 0.10088741779327393\n","step: 2770, loss: 0.06261057406663895\n","step: 2780, loss: 0.042818401008844376\n","step: 2790, loss: 0.07385461777448654\n","step: 2800, loss: 0.05335986986756325\n","step: 2810, loss: 0.10734176635742188\n","step: 2820, loss: 0.06127762794494629\n","step: 2830, loss: 0.02921566553413868\n","step: 2840, loss: 0.12583966553211212\n","step: 2850, loss: 0.061550743877887726\n","step: 2860, loss: 0.07562082260847092\n","step: 2870, loss: 0.04460776597261429\n","step: 2880, loss: 0.04680107533931732\n","step: 2890, loss: 0.08013035356998444\n","step: 2900, loss: 0.08480802178382874\n","step: 2910, loss: 0.0816040113568306\n","step: 2920, loss: 0.043992772698402405\n","step: 2930, loss: 0.04039808362722397\n","step: 2940, loss: 0.1503409743309021\n","step: 2950, loss: 0.102210134267807\n","step: 2960, loss: 0.13829930126667023\n","step: 2970, loss: 0.04575734585523605\n","step: 2980, loss: 0.058669861406087875\n","step: 2990, loss: 0.041662730276584625\n","step: 3000, loss: 0.07743623107671738\n","step: 3010, loss: 0.051996950060129166\n","step: 3020, loss: 0.09332647174596786\n","step: 3030, loss: 0.02871849201619625\n","step: 3040, loss: 0.013906091451644897\n","step: 3050, loss: 0.04322832077741623\n","step: 3060, loss: 0.05608686804771423\n","step: 3070, loss: 0.05313130095601082\n","step: 3080, loss: 0.13358014822006226\n","step: 3090, loss: 0.03176935762166977\n","step: 3100, loss: 0.07893072068691254\n","step: 3110, loss: 0.05522837117314339\n","step: 3120, loss: 0.07188211381435394\n","step: 3130, loss: 0.10864807665348053\n","step: 3140, loss: 0.09873145073652267\n","step: 3150, loss: 0.05444516986608505\n","step: 3160, loss: 0.0931609496474266\n","step: 3170, loss: 0.11987023055553436\n","step: 3180, loss: 0.08115054666996002\n","step: 3190, loss: 0.0921143963932991\n","step: 3200, loss: 0.07159436494112015\n","step: 3210, loss: 0.033839475363492966\n","step: 3220, loss: 0.09887537360191345\n","step: 3230, loss: 0.06291487067937851\n","step: 3240, loss: 0.062283311039209366\n","step: 3250, loss: 0.10298825800418854\n","step: 3260, loss: 0.06535572558641434\n","step: 3270, loss: 0.01139120850712061\n","step: 3280, loss: 0.06189938634634018\n","step: 3290, loss: 0.08374436944723129\n","step: 3300, loss: 0.09246938675642014\n","step: 3310, loss: 0.08353755623102188\n","step: 3320, loss: 0.06490694731473923\n","step: 3330, loss: 0.08058065921068192\n","step: 3340, loss: 0.0994216725230217\n","step: 3350, loss: 0.02840554341673851\n","step: 3360, loss: 0.0814543068408966\n","step: 3370, loss: 0.05088336020708084\n","step: 3380, loss: 0.027187028899788857\n","step: 3390, loss: 0.11816452443599701\n","step: 3400, loss: 0.042579829692840576\n","step: 3410, loss: 0.07828091084957123\n","step: 3420, loss: 0.09336595237255096\n","step: 3430, loss: 0.07339534908533096\n","step: 3440, loss: 0.05320335179567337\n","step: 3450, loss: 0.08628426492214203\n","step: 3460, loss: 0.07442408800125122\n","step: 3470, loss: 0.13453906774520874\n","step: 3480, loss: 0.1402970403432846\n","step: 3490, loss: 0.03291890025138855\n","step: 3500, loss: 0.09391269087791443\n","step: 3510, loss: 0.11252380162477493\n","step: 3520, loss: 0.07095958292484283\n","step: 3530, loss: 0.03125981241464615\n","step: 3540, loss: 0.0994793176651001\n","step: 3550, loss: 0.15117229521274567\n","step: 3560, loss: 0.10636842995882034\n","step: 3570, loss: 0.094361811876297\n","step: 3580, loss: 0.09577441215515137\n","step: 3590, loss: 0.11409249156713486\n","step: 3600, loss: 0.09808392822742462\n","step: 3610, loss: 0.10609638690948486\n","step: 3620, loss: 0.10377932339906693\n","step: 3630, loss: 0.042475081980228424\n","step: 3640, loss: 0.06394588947296143\n","step: 3650, loss: 0.08404170721769333\n","step: 3660, loss: 0.11717549711465836\n","step: 3670, loss: 0.10908597707748413\n","step: 3680, loss: 0.054400619119405746\n","step: 3690, loss: 0.06916482001543045\n","step: 3700, loss: 0.11175537109375\n","step: 3710, loss: 0.06833023577928543\n","step: 3720, loss: 0.0804070457816124\n","step: 3730, loss: 0.04381779581308365\n","step: 3740, loss: 0.0972866490483284\n","step: 3750, loss: 0.19583186507225037\n","step: 3760, loss: 0.08174988627433777\n","step: 3770, loss: 0.05269557982683182\n","step: 3780, loss: 0.09905320405960083\n","step: 3790, loss: 0.1261713206768036\n","step: 3800, loss: 0.06400379538536072\n","step: 3810, loss: 0.10822531580924988\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.36      0.48      0.41        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.95      0.86      0.90       291\n","           5       0.94      0.84      0.88       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.44      0.95      0.60       186\n","           8       0.00      0.00      0.00        11\n","           9       0.98      0.98      0.98       689\n","          10       0.97      0.96      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.40      0.15      0.22        13\n","          14       0.43      1.00      0.61        43\n","          15       0.96      0.97      0.97      2778\n","          16       0.81      0.87      0.84      1151\n","          17       0.95      0.98      0.96        41\n","          18       0.86      1.00      0.93        32\n","          19       0.45      0.72      0.55        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.96      0.73      0.83      4175\n","          23       0.70      0.97      0.81      2253\n","          24       0.32      0.70      0.44        44\n","          25       0.87      0.88      0.88       888\n","          26       1.00      0.89      0.94         9\n","          27       0.91      0.99      0.94        69\n","          28       1.00      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.96      0.85      0.90      1136\n","          31       0.53      0.84      0.65        19\n","          32       1.00      0.38      0.55         8\n","          33       0.50      0.99      0.66        86\n","          34       0.14      0.25      0.18        32\n","          35       0.96      0.99      0.98       474\n","          36       0.94      0.19      0.31       182\n","          37       0.85      0.95      0.89      1592\n","          38       0.90      0.99      0.94       404\n","          39       0.95      0.94      0.94       485\n","          40       0.92      0.86      0.89       573\n","          41       0.92      0.94      0.93       841\n","          42       1.00      0.99      0.99       575\n","          43       0.96      0.76      0.85       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.57      0.05      0.09        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.79      0.76     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 442\n","\n","Loop 41\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9358298778533936\n","step: 10, loss: 2.2466273307800293\n","step: 20, loss: 0.8041014671325684\n","step: 30, loss: 0.43872424960136414\n","step: 40, loss: 0.2484079897403717\n","step: 50, loss: 0.2658046782016754\n","step: 60, loss: 0.18988747894763947\n","step: 70, loss: 0.1539314240217209\n","step: 80, loss: 0.0902363732457161\n","step: 90, loss: 0.1545673906803131\n","step: 100, loss: 0.24288064241409302\n","step: 110, loss: 0.21379302442073822\n","step: 120, loss: 0.22533059120178223\n","step: 130, loss: 0.1229313537478447\n","step: 140, loss: 0.15117251873016357\n","step: 150, loss: 0.08084571361541748\n","step: 160, loss: 0.1884625256061554\n","step: 170, loss: 0.04344213753938675\n","step: 180, loss: 0.19340765476226807\n","step: 190, loss: 0.1742456704378128\n","step: 200, loss: 0.13018785417079926\n","step: 210, loss: 0.1554989516735077\n","step: 220, loss: 0.12866194546222687\n","step: 230, loss: 0.23414544761180878\n","step: 240, loss: 0.11854326725006104\n","step: 250, loss: 0.06546331942081451\n","step: 260, loss: 0.06903573870658875\n","step: 270, loss: 0.07925732433795929\n","step: 280, loss: 0.12613444030284882\n","step: 290, loss: 0.16644257307052612\n","step: 300, loss: 0.06769022345542908\n","step: 310, loss: 0.07207047194242477\n","step: 320, loss: 0.13754203915596008\n","step: 330, loss: 0.15631763637065887\n","step: 340, loss: 0.06747829169034958\n","step: 350, loss: 0.08403313159942627\n","step: 360, loss: 0.09771397709846497\n","step: 370, loss: 0.11420352011919022\n","step: 380, loss: 0.11681485176086426\n","step: 390, loss: 0.09574802219867706\n","step: 400, loss: 0.0682196319103241\n","step: 410, loss: 0.11362884938716888\n","step: 420, loss: 0.11933694034814835\n","step: 430, loss: 0.16860854625701904\n","step: 440, loss: 0.0744454488158226\n","step: 450, loss: 0.038981836289167404\n","step: 460, loss: 0.03733813762664795\n","step: 470, loss: 0.1341458112001419\n","step: 480, loss: 0.09469261765480042\n","step: 490, loss: 0.08199887722730637\n","step: 500, loss: 0.07004185765981674\n","step: 510, loss: 0.22752203047275543\n","step: 520, loss: 0.10936267673969269\n","step: 530, loss: 0.13486632704734802\n","step: 540, loss: 0.22860920429229736\n","step: 550, loss: 0.09589455276727676\n","step: 560, loss: 0.08558444678783417\n","step: 570, loss: 0.08076798915863037\n","step: 580, loss: 0.17527294158935547\n","step: 590, loss: 0.14514656364917755\n","step: 600, loss: 0.04986990615725517\n","step: 610, loss: 0.05731691047549248\n","step: 620, loss: 0.14374729990959167\n","step: 630, loss: 0.08897081762552261\n","step: 640, loss: 0.08937240391969681\n","step: 650, loss: 0.2715981602668762\n","step: 660, loss: 0.16465447843074799\n","step: 670, loss: 0.15514586865901947\n","step: 680, loss: 0.04539304971694946\n","step: 690, loss: 0.10103379935026169\n","step: 700, loss: 0.06283651292324066\n","step: 710, loss: 0.08331433683633804\n","step: 720, loss: 0.0434206984937191\n","step: 730, loss: 0.08445096760988235\n","step: 740, loss: 0.12945511937141418\n","step: 750, loss: 0.07965663820505142\n","step: 760, loss: 0.11391386389732361\n","step: 770, loss: 0.13357438147068024\n","step: 780, loss: 0.10675884038209915\n","step: 790, loss: 0.10975416004657745\n","step: 800, loss: 0.08855824917554855\n","step: 810, loss: 0.10439319908618927\n","step: 820, loss: 0.07784418761730194\n","step: 830, loss: 0.1544768512248993\n","step: 840, loss: 0.06371520459651947\n","step: 850, loss: 0.07375773787498474\n","step: 860, loss: 0.13907966017723083\n","step: 870, loss: 0.08305065333843231\n","step: 880, loss: 0.05471065640449524\n","step: 890, loss: 0.12296773493289948\n","step: 900, loss: 0.16124874353408813\n","step: 910, loss: 0.10291752964258194\n","step: 920, loss: 0.12199448049068451\n","step: 930, loss: 0.093777135014534\n","step: 940, loss: 0.17877067625522614\n","step: 950, loss: 0.03111284412443638\n","step: 960, loss: 0.11782298237085342\n","step: 970, loss: 0.049018777906894684\n","step: 980, loss: 0.13257873058319092\n","step: 990, loss: 0.0897158607840538\n","step: 1000, loss: 0.06870151311159134\n","step: 1010, loss: 0.18447794020175934\n","step: 1020, loss: 0.133057102560997\n","step: 1030, loss: 0.13122084736824036\n","step: 1040, loss: 0.11389791965484619\n","step: 1050, loss: 0.16413965821266174\n","step: 1060, loss: 0.09141545742750168\n","step: 1070, loss: 0.04938386008143425\n","step: 1080, loss: 0.09588953107595444\n","step: 1090, loss: 0.057075098156929016\n","step: 1100, loss: 0.10047776252031326\n","step: 1110, loss: 0.07840816676616669\n","step: 1120, loss: 0.04355480521917343\n","step: 1130, loss: 0.12209291756153107\n","step: 1140, loss: 0.08821600675582886\n","step: 1150, loss: 0.05311853066086769\n","step: 1160, loss: 0.060282815247774124\n","step: 1170, loss: 0.14959004521369934\n","step: 1180, loss: 0.1330563724040985\n","step: 1190, loss: 0.08890906721353531\n","step: 1200, loss: 0.16620421409606934\n","step: 1210, loss: 0.0556800439953804\n","step: 1220, loss: 0.08228089660406113\n","step: 1230, loss: 0.09056999534368515\n","step: 1240, loss: 0.06855738908052444\n","step: 1250, loss: 0.05019930750131607\n","step: 1260, loss: 0.05633559823036194\n","step: 1270, loss: 0.08795754611492157\n","step: 1280, loss: 0.0660862997174263\n","step: 1290, loss: 0.08081267029047012\n","step: 1300, loss: 0.11056414246559143\n","step: 1310, loss: 0.14750628173351288\n","step: 1320, loss: 0.09843552112579346\n","step: 1330, loss: 0.10167400538921356\n","step: 1340, loss: 0.11657040566205978\n","step: 1350, loss: 0.14970122277736664\n","step: 1360, loss: 0.2105853259563446\n","step: 1370, loss: 0.0343049019575119\n","step: 1380, loss: 0.08359040319919586\n","step: 1390, loss: 0.035229433327913284\n","step: 1400, loss: 0.13244396448135376\n","step: 1410, loss: 0.07955148071050644\n","step: 1420, loss: 0.13695333898067474\n","step: 1430, loss: 0.11027000844478607\n","step: 1440, loss: 0.06796486675739288\n","step: 1450, loss: 0.050126850605010986\n","step: 1460, loss: 0.09785522520542145\n","step: 1470, loss: 0.07462994754314423\n","step: 1480, loss: 0.11384593695402145\n","step: 1490, loss: 0.07379039376974106\n","step: 1500, loss: 0.06885476410388947\n","step: 1510, loss: 0.11699210107326508\n","step: 1520, loss: 0.08584591746330261\n","step: 1530, loss: 0.058323465287685394\n","step: 1540, loss: 0.08879421651363373\n","step: 1550, loss: 0.12330377101898193\n","step: 1560, loss: 0.06570075452327728\n","step: 1570, loss: 0.0672866627573967\n","step: 1580, loss: 0.10805666446685791\n","step: 1590, loss: 0.12082652002573013\n","step: 1600, loss: 0.05768918991088867\n","step: 1610, loss: 0.08071132004261017\n","step: 1620, loss: 0.12732741236686707\n","step: 1630, loss: 0.10104650259017944\n","step: 1640, loss: 0.1337999403476715\n","step: 1650, loss: 0.08163193613290787\n","step: 1660, loss: 0.22289936244487762\n","step: 1670, loss: 0.186207115650177\n","step: 1680, loss: 0.26237475872039795\n","step: 1690, loss: 0.11132409423589706\n","step: 1700, loss: 0.044309381395578384\n","step: 1710, loss: 0.04326397925615311\n","step: 1720, loss: 0.053525760769844055\n","step: 1730, loss: 0.07104431092739105\n","step: 1740, loss: 0.0200986098498106\n","step: 1750, loss: 0.0862915888428688\n","step: 1760, loss: 0.1421876698732376\n","step: 1770, loss: 0.08040110766887665\n","step: 1780, loss: 0.07011298090219498\n","step: 1790, loss: 0.12171973288059235\n","step: 1800, loss: 0.06481838971376419\n","step: 1810, loss: 0.13153715431690216\n","step: 1820, loss: 0.18392027914524078\n","step: 1830, loss: 0.02502199076116085\n","step: 1840, loss: 0.11606331169605255\n","step: 1850, loss: 0.0804726853966713\n","step: 1860, loss: 0.09592962265014648\n","step: 1870, loss: 0.03344337269663811\n","step: 1880, loss: 0.04714895412325859\n","step: 1890, loss: 0.06689712405204773\n","step: 1900, loss: 0.08184608072042465\n","step: 1910, loss: 0.08712345361709595\n","step: 1920, loss: 0.03992839902639389\n","step: 1930, loss: 0.10466684401035309\n","step: 1940, loss: 0.046284180134534836\n","step: 1950, loss: 0.13092245161533356\n","step: 1960, loss: 0.04135582223534584\n","step: 1970, loss: 0.2928071618080139\n","step: 1980, loss: 0.09248478710651398\n","step: 1990, loss: 0.057086240500211716\n","step: 2000, loss: 0.034823402762413025\n","step: 2010, loss: 0.15307262539863586\n","step: 2020, loss: 0.04644761607050896\n","step: 2030, loss: 0.10911259800195694\n","step: 2040, loss: 0.10206721723079681\n","step: 2050, loss: 0.06653425097465515\n","step: 2060, loss: 0.10290727764368057\n","step: 2070, loss: 0.10284357517957687\n","step: 2080, loss: 0.09196162223815918\n","step: 2090, loss: 0.08179271221160889\n","step: 2100, loss: 0.16002140939235687\n","step: 2110, loss: 0.11307364702224731\n","step: 2120, loss: 0.05006355792284012\n","step: 2130, loss: 0.21530047059059143\n","step: 2140, loss: 0.0767696276307106\n","step: 2150, loss: 0.08066233992576599\n","step: 2160, loss: 0.08448969572782516\n","step: 2170, loss: 0.060157518833875656\n","step: 2180, loss: 0.03836122155189514\n","step: 2190, loss: 0.11241550743579865\n","step: 2200, loss: 0.09560731053352356\n","step: 2210, loss: 0.038981299847364426\n","step: 2220, loss: 0.22300606966018677\n","step: 2230, loss: 0.13355356454849243\n","step: 2240, loss: 0.07021243870258331\n","step: 2250, loss: 0.031992487609386444\n","step: 2260, loss: 0.06432745605707169\n","step: 2270, loss: 0.06646982580423355\n","step: 2280, loss: 0.08715597540140152\n","step: 2290, loss: 0.11969651281833649\n","step: 2300, loss: 0.04395328089594841\n","step: 2310, loss: 0.17124629020690918\n","step: 2320, loss: 0.05499996989965439\n","step: 2330, loss: 0.055469002574682236\n","step: 2340, loss: 0.06329764425754547\n","step: 2350, loss: 0.20034748315811157\n","step: 2360, loss: 0.07451285421848297\n","step: 2370, loss: 0.09672078490257263\n","step: 2380, loss: 0.03147248178720474\n","step: 2390, loss: 0.05669711157679558\n","step: 2400, loss: 0.02290816232562065\n","step: 2410, loss: 0.13083790242671967\n","step: 2420, loss: 0.07880089432001114\n","step: 2430, loss: 0.14756304025650024\n","step: 2440, loss: 0.11973082274198532\n","step: 2450, loss: 0.06647192686796188\n","step: 2460, loss: 0.09085274487733841\n","step: 2470, loss: 0.04278971627354622\n","step: 2480, loss: 0.06134280189871788\n","step: 2490, loss: 0.0981488823890686\n","step: 2500, loss: 0.12028193473815918\n","step: 2510, loss: 0.09779055416584015\n","step: 2520, loss: 0.11016906797885895\n","step: 2530, loss: 0.047615423798561096\n","step: 2540, loss: 0.21345771849155426\n","step: 2550, loss: 0.09787845611572266\n","step: 2560, loss: 0.04917311295866966\n","step: 2570, loss: 0.13653934001922607\n","step: 2580, loss: 0.05911600589752197\n","step: 2590, loss: 0.10584184527397156\n","step: 2600, loss: 0.21340519189834595\n","step: 2610, loss: 0.07979579269886017\n","step: 2620, loss: 0.1068144291639328\n","step: 2630, loss: 0.05206844583153725\n","step: 2640, loss: 0.09748043119907379\n","step: 2650, loss: 0.0684606060385704\n","step: 2660, loss: 0.02827427349984646\n","step: 2670, loss: 0.08176970481872559\n","step: 2680, loss: 0.14401653409004211\n","step: 2690, loss: 0.10005169361829758\n","step: 2700, loss: 0.01732868142426014\n","step: 2710, loss: 0.06350067257881165\n","step: 2720, loss: 0.11654270440340042\n","step: 2730, loss: 0.09155018627643585\n","step: 2740, loss: 0.09991013258695602\n","step: 2750, loss: 0.05450060963630676\n","step: 2760, loss: 0.14829961955547333\n","step: 2770, loss: 0.10768014192581177\n","step: 2780, loss: 0.0695425420999527\n","step: 2790, loss: 0.0639086365699768\n","step: 2800, loss: 0.0797986164689064\n","step: 2810, loss: 0.2435680627822876\n","step: 2820, loss: 0.10197862237691879\n","step: 2830, loss: 0.11223104596138\n","step: 2840, loss: 0.07950884848833084\n","step: 2850, loss: 0.0759805366396904\n","step: 2860, loss: 0.040758166462183\n","step: 2870, loss: 0.0725061371922493\n","step: 2880, loss: 0.08867917954921722\n","step: 2890, loss: 0.08091691881418228\n","step: 2900, loss: 0.03654097393155098\n","step: 2910, loss: 0.06277221441268921\n","step: 2920, loss: 0.07831207662820816\n","step: 2930, loss: 0.20600247383117676\n","step: 2940, loss: 0.14270511269569397\n","step: 2950, loss: 0.10567108541727066\n","step: 2960, loss: 0.11922448128461838\n","step: 2970, loss: 0.055609576404094696\n","step: 2980, loss: 0.06439341604709625\n","step: 2990, loss: 0.03652036562561989\n","step: 3000, loss: 0.041539810597896576\n","step: 3010, loss: 0.07108920812606812\n","step: 3020, loss: 0.13680946826934814\n","step: 3030, loss: 0.11065427958965302\n","step: 3040, loss: 0.05163883417844772\n","step: 3050, loss: 0.035964250564575195\n","step: 3060, loss: 0.08462168276309967\n","step: 3070, loss: 0.23444204032421112\n","step: 3080, loss: 0.024218862876296043\n","step: 3090, loss: 0.06869785487651825\n","step: 3100, loss: 0.06475861370563507\n","step: 3110, loss: 0.11987323313951492\n","step: 3120, loss: 0.060006722807884216\n","step: 3130, loss: 0.05252455547451973\n","step: 3140, loss: 0.08082982152700424\n","step: 3150, loss: 0.14046400785446167\n","step: 3160, loss: 0.15423913300037384\n","step: 3170, loss: 0.02253883332014084\n","step: 3180, loss: 0.09771779179573059\n","step: 3190, loss: 0.09978469461202621\n","step: 3200, loss: 0.10060575604438782\n","step: 3210, loss: 0.08154919743537903\n","step: 3220, loss: 0.10506925731897354\n","step: 3230, loss: 0.16947919130325317\n","step: 3240, loss: 0.09754413366317749\n","step: 3250, loss: 0.09757252037525177\n","step: 3260, loss: 0.07347612828016281\n","step: 3270, loss: 0.13685135543346405\n","step: 3280, loss: 0.048042479902505875\n","step: 3290, loss: 0.009168594144284725\n","step: 3300, loss: 0.11527649313211441\n","step: 3310, loss: 0.08737422525882721\n","step: 3320, loss: 0.12596853077411652\n","step: 3330, loss: 0.03663664683699608\n","step: 3340, loss: 0.05016279220581055\n","step: 3350, loss: 0.12529003620147705\n","step: 3360, loss: 0.09163165837526321\n","step: 3370, loss: 0.029420197010040283\n","step: 3380, loss: 0.0416838638484478\n","step: 3390, loss: 0.036539070308208466\n","step: 3400, loss: 0.10284927487373352\n","step: 3410, loss: 0.10147791355848312\n","step: 3420, loss: 0.04115863889455795\n","step: 3430, loss: 0.044708576053380966\n","step: 3440, loss: 0.16307853162288666\n","step: 3450, loss: 0.13889066874980927\n","step: 3460, loss: 0.1892729103565216\n","step: 3470, loss: 0.23217082023620605\n","step: 3480, loss: 0.043413519859313965\n","step: 3490, loss: 0.11604756116867065\n","step: 3500, loss: 0.10856958478689194\n","step: 3510, loss: 0.06602247804403305\n","step: 3520, loss: 0.09034346044063568\n","step: 3530, loss: 0.07766834646463394\n","step: 3540, loss: 0.1466171145439148\n","step: 3550, loss: 0.14394181966781616\n","step: 3560, loss: 0.14274686574935913\n","step: 3570, loss: 0.07205729186534882\n","step: 3580, loss: 0.11884840577840805\n","step: 3590, loss: 0.1165342852473259\n","step: 3600, loss: 0.0712597593665123\n","step: 3610, loss: 0.10088704526424408\n","step: 3620, loss: 0.10208477079868317\n","step: 3630, loss: 0.09056587517261505\n","step: 3640, loss: 0.12372630834579468\n","step: 3650, loss: 0.12742967903614044\n","step: 3660, loss: 0.16086389124393463\n","step: 3670, loss: 0.21306443214416504\n","step: 3680, loss: 0.023450953885912895\n","step: 3690, loss: 0.12092749029397964\n","step: 3700, loss: 0.0768829807639122\n","step: 3710, loss: 0.10943928360939026\n","step: 3720, loss: 0.18949493765830994\n","step: 3730, loss: 0.12109624594449997\n","step: 3740, loss: 0.07289265096187592\n","step: 3750, loss: 0.08795803040266037\n","step: 3760, loss: 0.09510834515094757\n","step: 3770, loss: 0.08887583762407303\n","step: 3780, loss: 0.06989200413227081\n","step: 3790, loss: 0.0924600213766098\n","step: 3800, loss: 0.0639653354883194\n","step: 3810, loss: 0.06868518143892288\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.47      1.00      0.64        35\n","           2       0.59      0.92      0.72        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.64      0.85      0.73       291\n","           5       0.91      0.82      0.86       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.59      0.70      0.64       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.93      0.97      0.95       901\n","          11       0.97      1.00      0.99      2111\n","          12       0.88      0.98      0.93        47\n","          13       0.75      0.69      0.72        13\n","          14       0.36      1.00      0.53        43\n","          15       0.97      0.97      0.97      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.91      0.97      0.94        32\n","          19       0.00      0.00      0.00        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.75      0.83      4175\n","          23       0.72      0.96      0.82      2253\n","          24       0.36      0.59      0.45        44\n","          25       0.86      0.92      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       0.87      1.00      0.93        69\n","          28       1.00      0.98      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.96      0.84      0.90      1136\n","          31       0.55      0.63      0.59        19\n","          32       1.00      0.62      0.77         8\n","          33       0.72      0.91      0.80        86\n","          34       0.19      0.47      0.27        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.09      0.16       182\n","          37       0.88      0.94      0.91      1592\n","          38       0.96      0.98      0.97       404\n","          39       0.93      0.98      0.95       485\n","          40       0.94      0.88      0.91       573\n","          41       0.85      0.95      0.89       841\n","          42       0.99      0.99      0.99       575\n","          43       0.92      0.96      0.94       152\n","          44       0.96      0.92      0.94        75\n","          46       1.00      0.96      0.98        82\n","          48       0.04      0.06      0.05        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.77      0.80      0.76     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 449\n","\n","Loop 42\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9555978775024414\n","step: 10, loss: 1.6326723098754883\n","step: 20, loss: 1.0784960985183716\n","step: 30, loss: 0.5778010487556458\n","step: 40, loss: 0.2593838572502136\n","step: 50, loss: 0.183100625872612\n","step: 60, loss: 0.13610757887363434\n","step: 70, loss: 0.12334715574979782\n","step: 80, loss: 0.10100452601909637\n","step: 90, loss: 0.18718692660331726\n","step: 100, loss: 0.12110275775194168\n","step: 110, loss: 0.17203551530838013\n","step: 120, loss: 0.18723468482494354\n","step: 130, loss: 0.13622857630252838\n","step: 140, loss: 0.23485156893730164\n","step: 150, loss: 0.17897473275661469\n","step: 160, loss: 0.12936393916606903\n","step: 170, loss: 0.16715066134929657\n","step: 180, loss: 0.09094057977199554\n","step: 190, loss: 0.1655702143907547\n","step: 200, loss: 0.09673520922660828\n","step: 210, loss: 0.10353197157382965\n","step: 220, loss: 0.12251419574022293\n","step: 230, loss: 0.030579598620533943\n","step: 240, loss: 0.10840312391519547\n","step: 250, loss: 0.08367463201284409\n","step: 260, loss: 0.07935643941164017\n","step: 270, loss: 0.1683817356824875\n","step: 280, loss: 0.2726198136806488\n","step: 290, loss: 0.16289837658405304\n","step: 300, loss: 0.0672491267323494\n","step: 310, loss: 0.1302124708890915\n","step: 320, loss: 0.1814640909433365\n","step: 330, loss: 0.10881885886192322\n","step: 340, loss: 0.06389390677213669\n","step: 350, loss: 0.12296898663043976\n","step: 360, loss: 0.11798343062400818\n","step: 370, loss: 0.06346909701824188\n","step: 380, loss: 0.06698038429021835\n","step: 390, loss: 0.08586882054805756\n","step: 400, loss: 0.13689012825489044\n","step: 410, loss: 0.14633026719093323\n","step: 420, loss: 0.11856039613485336\n","step: 430, loss: 0.11893007904291153\n","step: 440, loss: 0.09759360551834106\n","step: 450, loss: 0.07203386723995209\n","step: 460, loss: 0.03605975955724716\n","step: 470, loss: 0.06075635552406311\n","step: 480, loss: 0.09201844781637192\n","step: 490, loss: 0.1268286257982254\n","step: 500, loss: 0.05541197583079338\n","step: 510, loss: 0.01710747368633747\n","step: 520, loss: 0.10192127525806427\n","step: 530, loss: 0.09906932711601257\n","step: 540, loss: 0.07827350497245789\n","step: 550, loss: 0.13955062627792358\n","step: 560, loss: 0.09575667232275009\n","step: 570, loss: 0.12013959139585495\n","step: 580, loss: 0.13561661541461945\n","step: 590, loss: 0.07215949892997742\n","step: 600, loss: 0.04827181622385979\n","step: 610, loss: 0.11038810759782791\n","step: 620, loss: 0.0741550549864769\n","step: 630, loss: 0.039250150322914124\n","step: 640, loss: 0.08793029189109802\n","step: 650, loss: 0.1752227246761322\n","step: 660, loss: 0.18415650725364685\n","step: 670, loss: 0.11750083416700363\n","step: 680, loss: 0.09062457084655762\n","step: 690, loss: 0.06596783548593521\n","step: 700, loss: 0.11677062511444092\n","step: 710, loss: 0.13914313912391663\n","step: 720, loss: 0.1018424928188324\n","step: 730, loss: 0.09322717785835266\n","step: 740, loss: 0.04036504030227661\n","step: 750, loss: 0.1633838713169098\n","step: 760, loss: 0.2595987319946289\n","step: 770, loss: 0.10339614748954773\n","step: 780, loss: 0.14104855060577393\n","step: 790, loss: 0.09294217824935913\n","step: 800, loss: 0.10338811576366425\n","step: 810, loss: 0.14444951713085175\n","step: 820, loss: 0.07385724037885666\n","step: 830, loss: 0.06879393756389618\n","step: 840, loss: 0.16742482781410217\n","step: 850, loss: 0.030133729800581932\n","step: 860, loss: 0.08354327827692032\n","step: 870, loss: 0.1907515972852707\n","step: 880, loss: 0.1312844306230545\n","step: 890, loss: 0.10219050943851471\n","step: 900, loss: 0.09581056237220764\n","step: 910, loss: 0.0949031263589859\n","step: 920, loss: 0.0655113160610199\n","step: 930, loss: 0.18843331933021545\n","step: 940, loss: 0.15219545364379883\n","step: 950, loss: 0.04028123989701271\n","step: 960, loss: 0.09521738439798355\n","step: 970, loss: 0.11278780549764633\n","step: 980, loss: 0.09892306476831436\n","step: 990, loss: 0.13293670117855072\n","step: 1000, loss: 0.05636390671133995\n","step: 1010, loss: 0.13037745654582977\n","step: 1020, loss: 0.041332680732011795\n","step: 1030, loss: 0.05362391844391823\n","step: 1040, loss: 0.058470066636800766\n","step: 1050, loss: 0.10271593928337097\n","step: 1060, loss: 0.04715775325894356\n","step: 1070, loss: 0.08942042291164398\n","step: 1080, loss: 0.09690438210964203\n","step: 1090, loss: 0.06353038549423218\n","step: 1100, loss: 0.09586255997419357\n","step: 1110, loss: 0.08267060667276382\n","step: 1120, loss: 0.15622296929359436\n","step: 1130, loss: 0.07073600590229034\n","step: 1140, loss: 0.09544197469949722\n","step: 1150, loss: 0.07083849608898163\n","step: 1160, loss: 0.08757204562425613\n","step: 1170, loss: 0.09485656023025513\n","step: 1180, loss: 0.046990834176540375\n","step: 1190, loss: 0.11237603425979614\n","step: 1200, loss: 0.13110771775245667\n","step: 1210, loss: 0.09804315865039825\n","step: 1220, loss: 0.06303923577070236\n","step: 1230, loss: 0.044118206948041916\n","step: 1240, loss: 0.0993918627500534\n","step: 1250, loss: 0.1435096114873886\n","step: 1260, loss: 0.044152453541755676\n","step: 1270, loss: 0.09979086369276047\n","step: 1280, loss: 0.0722988098859787\n","step: 1290, loss: 0.0838172659277916\n","step: 1300, loss: 0.11727990955114365\n","step: 1310, loss: 0.18172316253185272\n","step: 1320, loss: 0.1101444661617279\n","step: 1330, loss: 0.10030539333820343\n","step: 1340, loss: 0.13064226508140564\n","step: 1350, loss: 0.03702623397111893\n","step: 1360, loss: 0.08278385549783707\n","step: 1370, loss: 0.13536928594112396\n","step: 1380, loss: 0.046484995633363724\n","step: 1390, loss: 0.09246817231178284\n","step: 1400, loss: 0.03692968562245369\n","step: 1410, loss: 0.05292343348264694\n","step: 1420, loss: 0.15158432722091675\n","step: 1430, loss: 0.15565282106399536\n","step: 1440, loss: 0.0630514919757843\n","step: 1450, loss: 0.12258397042751312\n","step: 1460, loss: 0.04290631040930748\n","step: 1470, loss: 0.08633794635534286\n","step: 1480, loss: 0.020636966452002525\n","step: 1490, loss: 0.06056885048747063\n","step: 1500, loss: 0.0485616959631443\n","step: 1510, loss: 0.04329083114862442\n","step: 1520, loss: 0.1566544473171234\n","step: 1530, loss: 0.05249973013997078\n","step: 1540, loss: 0.18130341172218323\n","step: 1550, loss: 0.05853349715471268\n","step: 1560, loss: 0.09131638705730438\n","step: 1570, loss: 0.054107461124658585\n","step: 1580, loss: 0.09120859950780869\n","step: 1590, loss: 0.09860891848802567\n","step: 1600, loss: 0.07340098172426224\n","step: 1610, loss: 0.04563458636403084\n","step: 1620, loss: 0.06498551368713379\n","step: 1630, loss: 0.15738116204738617\n","step: 1640, loss: 0.17098656296730042\n","step: 1650, loss: 0.10147266834974289\n","step: 1660, loss: 0.0674976035952568\n","step: 1670, loss: 0.14040032029151917\n","step: 1680, loss: 0.07910968363285065\n","step: 1690, loss: 0.17870384454727173\n","step: 1700, loss: 0.0877942144870758\n","step: 1710, loss: 0.19111062586307526\n","step: 1720, loss: 0.05093856155872345\n","step: 1730, loss: 0.08537568151950836\n","step: 1740, loss: 0.11328262090682983\n","step: 1750, loss: 0.09200469404459\n","step: 1760, loss: 0.07026306539773941\n","step: 1770, loss: 0.13366591930389404\n","step: 1780, loss: 0.06371957063674927\n","step: 1790, loss: 0.05259350314736366\n","step: 1800, loss: 0.08269109576940536\n","step: 1810, loss: 0.15269458293914795\n","step: 1820, loss: 0.11190986633300781\n","step: 1830, loss: 0.09855952858924866\n","step: 1840, loss: 0.0812699943780899\n","step: 1850, loss: 0.14450408518314362\n","step: 1860, loss: 0.17408978939056396\n","step: 1870, loss: 0.10932304710149765\n","step: 1880, loss: 0.10214877873659134\n","step: 1890, loss: 0.06273811310529709\n","step: 1900, loss: 0.08770425617694855\n","step: 1910, loss: 0.09598620980978012\n","step: 1920, loss: 0.10678106546401978\n","step: 1930, loss: 0.04987851530313492\n","step: 1940, loss: 0.09315940737724304\n","step: 1950, loss: 0.12745119631290436\n","step: 1960, loss: 0.19375237822532654\n","step: 1970, loss: 0.06364138424396515\n","step: 1980, loss: 0.08902876824140549\n","step: 1990, loss: 0.1176440566778183\n","step: 2000, loss: 0.10877051949501038\n","step: 2010, loss: 0.08744696527719498\n","step: 2020, loss: 0.08693598210811615\n","step: 2030, loss: 0.12541134655475616\n","step: 2040, loss: 0.12506529688835144\n","step: 2050, loss: 0.035724665969610214\n","step: 2060, loss: 0.009542187675833702\n","step: 2070, loss: 0.07466820627450943\n","step: 2080, loss: 0.09483547508716583\n","step: 2090, loss: 0.12093494832515717\n","step: 2100, loss: 0.13740995526313782\n","step: 2110, loss: 0.10406205803155899\n","step: 2120, loss: 0.05238298699259758\n","step: 2130, loss: 0.09475208818912506\n","step: 2140, loss: 0.045112110674381256\n","step: 2150, loss: 0.09142252802848816\n","step: 2160, loss: 0.06699641048908234\n","step: 2170, loss: 0.07396405190229416\n","step: 2180, loss: 0.10196828097105026\n","step: 2190, loss: 0.018439779058098793\n","step: 2200, loss: 0.059010330587625504\n","step: 2210, loss: 0.07671409845352173\n","step: 2220, loss: 0.037960559129714966\n","step: 2230, loss: 0.067721888422966\n","step: 2240, loss: 0.16530892252922058\n","step: 2250, loss: 0.14870311319828033\n","step: 2260, loss: 0.029935961589217186\n","step: 2270, loss: 0.14862065017223358\n","step: 2280, loss: 0.16435876488685608\n","step: 2290, loss: 0.06536990404129028\n","step: 2300, loss: 0.16452255845069885\n","step: 2310, loss: 0.05003388971090317\n","step: 2320, loss: 0.09123994410037994\n","step: 2330, loss: 0.1553441882133484\n","step: 2340, loss: 0.08284614980220795\n","step: 2350, loss: 0.09516328573226929\n","step: 2360, loss: 0.07638924568891525\n","step: 2370, loss: 0.08611515909433365\n","step: 2380, loss: 0.10141310095787048\n","step: 2390, loss: 0.06510993093252182\n","step: 2400, loss: 0.052467167377471924\n","step: 2410, loss: 0.1024375781416893\n","step: 2420, loss: 0.09866417944431305\n","step: 2430, loss: 0.05922824144363403\n","step: 2440, loss: 0.03468224033713341\n","step: 2450, loss: 0.059710245579481125\n","step: 2460, loss: 0.11249422281980515\n","step: 2470, loss: 0.04980633780360222\n","step: 2480, loss: 0.05287153646349907\n","step: 2490, loss: 0.08250997960567474\n","step: 2500, loss: 0.1510755568742752\n","step: 2510, loss: 0.13366755843162537\n","step: 2520, loss: 0.06257764250040054\n","step: 2530, loss: 0.03157481551170349\n","step: 2540, loss: 0.09572825580835342\n","step: 2550, loss: 0.043795470148324966\n","step: 2560, loss: 0.05432318150997162\n","step: 2570, loss: 0.090923011302948\n","step: 2580, loss: 0.06195656955242157\n","step: 2590, loss: 0.039618119597435\n","step: 2600, loss: 0.11663251370191574\n","step: 2610, loss: 0.16121821105480194\n","step: 2620, loss: 0.04621099680662155\n","step: 2630, loss: 0.02861189655959606\n","step: 2640, loss: 0.07243144512176514\n","step: 2650, loss: 0.04635227471590042\n","step: 2660, loss: 0.035562820732593536\n","step: 2670, loss: 0.05463060364127159\n","step: 2680, loss: 0.11960820108652115\n","step: 2690, loss: 0.07830505818128586\n","step: 2700, loss: 0.056418295949697495\n","step: 2710, loss: 0.21068042516708374\n","step: 2720, loss: 0.052434153854846954\n","step: 2730, loss: 0.07597831636667252\n","step: 2740, loss: 0.1277710646390915\n","step: 2750, loss: 0.07558463513851166\n","step: 2760, loss: 0.11714194715023041\n","step: 2770, loss: 0.05443890392780304\n","step: 2780, loss: 0.10654228180646896\n","step: 2790, loss: 0.043288540095090866\n","step: 2800, loss: 0.10066623985767365\n","step: 2810, loss: 0.11189667135477066\n","step: 2820, loss: 0.0917639210820198\n","step: 2830, loss: 0.046187736093997955\n","step: 2840, loss: 0.08173626661300659\n","step: 2850, loss: 0.10152261704206467\n","step: 2860, loss: 0.06740827113389969\n","step: 2870, loss: 0.152181938290596\n","step: 2880, loss: 0.047867149114608765\n","step: 2890, loss: 0.0054390220902860165\n","step: 2900, loss: 0.1848718672990799\n","step: 2910, loss: 0.10189453512430191\n","step: 2920, loss: 0.16146469116210938\n","step: 2930, loss: 0.11524715274572372\n","step: 2940, loss: 0.051697228103876114\n","step: 2950, loss: 0.1341354101896286\n","step: 2960, loss: 0.04577987641096115\n","step: 2970, loss: 0.09120391309261322\n","step: 2980, loss: 0.04696778953075409\n","step: 2990, loss: 0.03631165251135826\n","step: 3000, loss: 0.08060573041439056\n","step: 3010, loss: 0.11809796094894409\n","step: 3020, loss: 0.05795930698513985\n","step: 3030, loss: 0.08579754084348679\n","step: 3040, loss: 0.08599449694156647\n","step: 3050, loss: 0.044263456016778946\n","step: 3060, loss: 0.0799802914261818\n","step: 3070, loss: 0.1072479635477066\n","step: 3080, loss: 0.0923999473452568\n","step: 3090, loss: 0.032220639288425446\n","step: 3100, loss: 0.06260011345148087\n","step: 3110, loss: 0.14095008373260498\n","step: 3120, loss: 0.07287266105413437\n","step: 3130, loss: 0.185861274600029\n","step: 3140, loss: 0.050109054893255234\n","step: 3150, loss: 0.09527940303087234\n","step: 3160, loss: 0.024695225059986115\n","step: 3170, loss: 0.1150834858417511\n","step: 3180, loss: 0.12938997149467468\n","step: 3190, loss: 0.08386093378067017\n","step: 3200, loss: 0.039340317249298096\n","step: 3210, loss: 0.023533400148153305\n","step: 3220, loss: 0.09982969611883163\n","step: 3230, loss: 0.04370429739356041\n","step: 3240, loss: 0.06169350817799568\n","step: 3250, loss: 0.05922388285398483\n","step: 3260, loss: 0.13677425682544708\n","step: 3270, loss: 0.05788329243659973\n","step: 3280, loss: 0.02513887919485569\n","step: 3290, loss: 0.04775840789079666\n","step: 3300, loss: 0.10108942538499832\n","step: 3310, loss: 0.06193790212273598\n","step: 3320, loss: 0.08324439823627472\n","step: 3330, loss: 0.10380759835243225\n","step: 3340, loss: 0.09521206468343735\n","step: 3350, loss: 0.12716498970985413\n","step: 3360, loss: 0.07189138978719711\n","step: 3370, loss: 0.05777205154299736\n","step: 3380, loss: 0.07739286124706268\n","step: 3390, loss: 0.08745196461677551\n","step: 3400, loss: 0.03514023870229721\n","step: 3410, loss: 0.10175397992134094\n","step: 3420, loss: 0.03011607564985752\n","step: 3430, loss: 0.16414491832256317\n","step: 3440, loss: 0.11092925816774368\n","step: 3450, loss: 0.09705401957035065\n","step: 3460, loss: 0.1340358853340149\n","step: 3470, loss: 0.2812873423099518\n","step: 3480, loss: 0.04160945862531662\n","step: 3490, loss: 0.08031833171844482\n","step: 3500, loss: 0.07121630758047104\n","step: 3510, loss: 0.06497543305158615\n","step: 3520, loss: 0.2247403860092163\n","step: 3530, loss: 0.07331006973981857\n","step: 3540, loss: 0.04576873406767845\n","step: 3550, loss: 0.02897990681231022\n","step: 3560, loss: 0.07827014476060867\n","step: 3570, loss: 0.1111125573515892\n","step: 3580, loss: 0.06837903708219528\n","step: 3590, loss: 0.06880700588226318\n","step: 3600, loss: 0.11829822510480881\n","step: 3610, loss: 0.08666519820690155\n","step: 3620, loss: 0.07048557698726654\n","step: 3630, loss: 0.13296595215797424\n","step: 3640, loss: 0.10059528052806854\n","step: 3650, loss: 0.05152172967791557\n","step: 3660, loss: 0.11915317177772522\n","step: 3670, loss: 0.10553910583257675\n","step: 3680, loss: 0.03996840864419937\n","step: 3690, loss: 0.06066003814339638\n","step: 3700, loss: 0.03200609236955643\n","step: 3710, loss: 0.04222764074802399\n","step: 3720, loss: 0.038645293563604355\n","step: 3730, loss: 0.06635858118534088\n","step: 3740, loss: 0.1340617537498474\n","step: 3750, loss: 0.0269489549100399\n","step: 3760, loss: 0.06404800713062286\n","step: 3770, loss: 0.04493669420480728\n","step: 3780, loss: 0.13458596169948578\n","step: 3790, loss: 0.056174665689468384\n","step: 3800, loss: 0.15527893602848053\n","step: 3810, loss: 0.056963950395584106\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.81      1.00      0.90        35\n","           2       0.56      0.90      0.69        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.84      0.90       291\n","           5       0.97      0.84      0.90       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.55      0.94      0.69       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.96      0.98      0.97       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       1.00      0.15      0.27        13\n","          14       0.34      1.00      0.51        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.90      0.81      0.85      1151\n","          17       0.85      0.98      0.91        41\n","          18       1.00      0.91      0.95        32\n","          19       0.70      0.78      0.74        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.97      0.72      0.83      4175\n","          23       0.70      0.97      0.81      2253\n","          24       0.31      0.82      0.45        44\n","          25       0.87      0.89      0.88       888\n","          26       0.82      1.00      0.90         9\n","          27       0.85      1.00      0.92        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.89      0.85      0.87      1136\n","          31       0.59      0.68      0.63        19\n","          32       0.70      0.88      0.78         8\n","          33       0.60      0.99      0.75        86\n","          34       0.24      0.62      0.35        32\n","          35       0.98      0.99      0.98       474\n","          36       0.93      0.15      0.26       182\n","          37       0.85      0.97      0.91      1592\n","          38       0.97      0.96      0.96       404\n","          39       0.90      0.99      0.95       485\n","          40       0.89      0.95      0.92       573\n","          41       0.94      0.91      0.92       841\n","          42       0.98      0.99      0.98       575\n","          43       0.95      0.89      0.92       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.82      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 453\n","\n","Loop 43\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9681103229522705\n","step: 10, loss: 2.0285375118255615\n","step: 20, loss: 0.9123215675354004\n","step: 30, loss: 0.4078019857406616\n","step: 40, loss: 0.3636032044887543\n","step: 50, loss: 0.396932989358902\n","step: 60, loss: 0.2721887230873108\n","step: 70, loss: 0.15963265299797058\n","step: 80, loss: 0.1352388560771942\n","step: 90, loss: 0.09224873036146164\n","step: 100, loss: 0.0836498811841011\n","step: 110, loss: 0.13483165204524994\n","step: 120, loss: 0.07264231145381927\n","step: 130, loss: 0.14103305339813232\n","step: 140, loss: 0.07327605783939362\n","step: 150, loss: 0.06277741491794586\n","step: 160, loss: 0.08289482444524765\n","step: 170, loss: 0.15018902719020844\n","step: 180, loss: 0.2114550918340683\n","step: 190, loss: 0.11842069774866104\n","step: 200, loss: 0.1719295084476471\n","step: 210, loss: 0.21058127284049988\n","step: 220, loss: 0.12019719928503036\n","step: 230, loss: 0.14349696040153503\n","step: 240, loss: 0.14067187905311584\n","step: 250, loss: 0.11646910011768341\n","step: 260, loss: 0.10840488225221634\n","step: 270, loss: 0.18920280039310455\n","step: 280, loss: 0.08063097298145294\n","step: 290, loss: 0.1175987720489502\n","step: 300, loss: 0.09971009939908981\n","step: 310, loss: 0.09475310146808624\n","step: 320, loss: 0.15846888720989227\n","step: 330, loss: 0.16336201131343842\n","step: 340, loss: 0.07686695456504822\n","step: 350, loss: 0.07685612142086029\n","step: 360, loss: 0.14066363871097565\n","step: 370, loss: 0.13452672958374023\n","step: 380, loss: 0.0825454443693161\n","step: 390, loss: 0.08240567892789841\n","step: 400, loss: 0.10735447704792023\n","step: 410, loss: 0.1051708534359932\n","step: 420, loss: 0.15383516252040863\n","step: 430, loss: 0.06497814506292343\n","step: 440, loss: 0.13348236680030823\n","step: 450, loss: 0.1271573305130005\n","step: 460, loss: 0.14975391328334808\n","step: 470, loss: 0.16415275633335114\n","step: 480, loss: 0.15828828513622284\n","step: 490, loss: 0.06226992979645729\n","step: 500, loss: 0.11228612810373306\n","step: 510, loss: 0.034412629902362823\n","step: 520, loss: 0.08278494328260422\n","step: 530, loss: 0.1462971419095993\n","step: 540, loss: 0.09894881397485733\n","step: 550, loss: 0.0553128682076931\n","step: 560, loss: 0.14476725459098816\n","step: 570, loss: 0.07551225274801254\n","step: 580, loss: 0.11952577531337738\n","step: 590, loss: 0.08066301047801971\n","step: 600, loss: 0.11002752929925919\n","step: 610, loss: 0.07713828235864639\n","step: 620, loss: 0.08087021857500076\n","step: 630, loss: 0.05359320715069771\n","step: 640, loss: 0.07363751530647278\n","step: 650, loss: 0.04098087176680565\n","step: 660, loss: 0.027083076536655426\n","step: 670, loss: 0.10640392452478409\n","step: 680, loss: 0.1304040253162384\n","step: 690, loss: 0.06842450052499771\n","step: 700, loss: 0.10459504276514053\n","step: 710, loss: 0.07410121709108353\n","step: 720, loss: 0.14548814296722412\n","step: 730, loss: 0.13333964347839355\n","step: 740, loss: 0.041258055716753006\n","step: 750, loss: 0.21137027442455292\n","step: 760, loss: 0.1371154487133026\n","step: 770, loss: 0.10844875127077103\n","step: 780, loss: 0.1686469465494156\n","step: 790, loss: 0.21897180378437042\n","step: 800, loss: 0.12950041890144348\n","step: 810, loss: 0.1249731108546257\n","step: 820, loss: 0.1285923719406128\n","step: 830, loss: 0.1340702623128891\n","step: 840, loss: 0.13899558782577515\n","step: 850, loss: 0.10497497767210007\n","step: 860, loss: 0.09406823664903641\n","step: 870, loss: 0.05898166075348854\n","step: 880, loss: 0.09244232624769211\n","step: 890, loss: 0.09661693125963211\n","step: 900, loss: 0.09583879262208939\n","step: 910, loss: 0.0872841477394104\n","step: 920, loss: 0.09217462688684464\n","step: 930, loss: 0.12575188279151917\n","step: 940, loss: 0.10374284535646439\n","step: 950, loss: 0.08377885073423386\n","step: 960, loss: 0.030481070280075073\n","step: 970, loss: 0.09511135518550873\n","step: 980, loss: 0.050625745207071304\n","step: 990, loss: 0.09661884605884552\n","step: 1000, loss: 0.12577494978904724\n","step: 1010, loss: 0.07822536677122116\n","step: 1020, loss: 0.10581117123365402\n","step: 1030, loss: 0.12153863906860352\n","step: 1040, loss: 0.06914030015468597\n","step: 1050, loss: 0.09516076743602753\n","step: 1060, loss: 0.0669020265340805\n","step: 1070, loss: 0.05686759948730469\n","step: 1080, loss: 0.06340350955724716\n","step: 1090, loss: 0.09089420735836029\n","step: 1100, loss: 0.12989076972007751\n","step: 1110, loss: 0.0414164774119854\n","step: 1120, loss: 0.03483039140701294\n","step: 1130, loss: 0.08000573515892029\n","step: 1140, loss: 0.14158788323402405\n","step: 1150, loss: 0.03309071809053421\n","step: 1160, loss: 0.04904148727655411\n","step: 1170, loss: 0.050158947706222534\n","step: 1180, loss: 0.11261806637048721\n","step: 1190, loss: 0.11034888029098511\n","step: 1200, loss: 0.13660727441310883\n","step: 1210, loss: 0.11829197406768799\n","step: 1220, loss: 0.06034678965806961\n","step: 1230, loss: 0.07824599742889404\n","step: 1240, loss: 0.08084175735712051\n","step: 1250, loss: 0.04104849323630333\n","step: 1260, loss: 0.058542896062135696\n","step: 1270, loss: 0.06535752862691879\n","step: 1280, loss: 0.10968904197216034\n","step: 1290, loss: 0.08208323270082474\n","step: 1300, loss: 0.09664864093065262\n","step: 1310, loss: 0.056840475648641586\n","step: 1320, loss: 0.07637863606214523\n","step: 1330, loss: 0.10862496495246887\n","step: 1340, loss: 0.10255613178014755\n","step: 1350, loss: 0.06232057884335518\n","step: 1360, loss: 0.1568058729171753\n","step: 1370, loss: 0.06670750677585602\n","step: 1380, loss: 0.029804475605487823\n","step: 1390, loss: 0.10066058486700058\n","step: 1400, loss: 0.03814540430903435\n","step: 1410, loss: 0.15700729191303253\n","step: 1420, loss: 0.07964517921209335\n","step: 1430, loss: 0.11870357394218445\n","step: 1440, loss: 0.10824629664421082\n","step: 1450, loss: 0.09254255890846252\n","step: 1460, loss: 0.05872360244393349\n","step: 1470, loss: 0.23322345316410065\n","step: 1480, loss: 0.13276422023773193\n","step: 1490, loss: 0.06741607189178467\n","step: 1500, loss: 0.0787462666630745\n","step: 1510, loss: 0.18473570048809052\n","step: 1520, loss: 0.07978273928165436\n","step: 1530, loss: 0.1293148398399353\n","step: 1540, loss: 0.076956607401371\n","step: 1550, loss: 0.08670146018266678\n","step: 1560, loss: 0.11538822948932648\n","step: 1570, loss: 0.053410060703754425\n","step: 1580, loss: 0.21623948216438293\n","step: 1590, loss: 0.08771680295467377\n","step: 1600, loss: 0.10319613665342331\n","step: 1610, loss: 0.09666910022497177\n","step: 1620, loss: 0.02238849550485611\n","step: 1630, loss: 0.02936248481273651\n","step: 1640, loss: 0.0823095515370369\n","step: 1650, loss: 0.1364268660545349\n","step: 1660, loss: 0.044669270515441895\n","step: 1670, loss: 0.08821915090084076\n","step: 1680, loss: 0.11687146872282028\n","step: 1690, loss: 0.087188720703125\n","step: 1700, loss: 0.12107347697019577\n","step: 1710, loss: 0.03973078355193138\n","step: 1720, loss: 0.04891567677259445\n","step: 1730, loss: 0.08888506889343262\n","step: 1740, loss: 0.02957536280155182\n","step: 1750, loss: 0.04067576304078102\n","step: 1760, loss: 0.17128075659275055\n","step: 1770, loss: 0.05609709024429321\n","step: 1780, loss: 0.1338953673839569\n","step: 1790, loss: 0.08103203028440475\n","step: 1800, loss: 0.07954692095518112\n","step: 1810, loss: 0.09977903962135315\n","step: 1820, loss: 0.08708235621452332\n","step: 1830, loss: 0.11332093924283981\n","step: 1840, loss: 0.13716813921928406\n","step: 1850, loss: 0.11449094116687775\n","step: 1860, loss: 0.132075235247612\n","step: 1870, loss: 0.061250951141119\n","step: 1880, loss: 0.060373954474925995\n","step: 1890, loss: 0.07817473262548447\n","step: 1900, loss: 0.10053267329931259\n","step: 1910, loss: 0.061820875853300095\n","step: 1920, loss: 0.03861632198095322\n","step: 1930, loss: 0.04545839875936508\n","step: 1940, loss: 0.04078329727053642\n","step: 1950, loss: 0.06001800671219826\n","step: 1960, loss: 0.1413375735282898\n","step: 1970, loss: 0.05431680008769035\n","step: 1980, loss: 0.09566840529441833\n","step: 1990, loss: 0.06437953561544418\n","step: 2000, loss: 0.07267162203788757\n","step: 2010, loss: 0.18077324330806732\n","step: 2020, loss: 0.07034780085086823\n","step: 2030, loss: 0.10348346084356308\n","step: 2040, loss: 0.10119302570819855\n","step: 2050, loss: 0.05186181515455246\n","step: 2060, loss: 0.05101139843463898\n","step: 2070, loss: 0.14917150139808655\n","step: 2080, loss: 0.03261089697480202\n","step: 2090, loss: 0.05575696751475334\n","step: 2100, loss: 0.12523531913757324\n","step: 2110, loss: 0.10290412604808807\n","step: 2120, loss: 0.10120560973882675\n","step: 2130, loss: 0.12160439044237137\n","step: 2140, loss: 0.01846645213663578\n","step: 2150, loss: 0.06118303909897804\n","step: 2160, loss: 0.09783778339624405\n","step: 2170, loss: 0.13527874648571014\n","step: 2180, loss: 0.17554615437984467\n","step: 2190, loss: 0.15794110298156738\n","step: 2200, loss: 0.12051853537559509\n","step: 2210, loss: 0.08966813236474991\n","step: 2220, loss: 0.056198880076408386\n","step: 2230, loss: 0.029825611039996147\n","step: 2240, loss: 0.09336057305335999\n","step: 2250, loss: 0.08274343609809875\n","step: 2260, loss: 0.06841441988945007\n","step: 2270, loss: 0.1255078911781311\n","step: 2280, loss: 0.16976962983608246\n","step: 2290, loss: 0.07796556502580643\n","step: 2300, loss: 0.028788702562451363\n","step: 2310, loss: 0.061681706458330154\n","step: 2320, loss: 0.18676774203777313\n","step: 2330, loss: 0.049720194190740585\n","step: 2340, loss: 0.14066477119922638\n","step: 2350, loss: 0.12871696054935455\n","step: 2360, loss: 0.033770833164453506\n","step: 2370, loss: 0.11203381419181824\n","step: 2380, loss: 0.14808207750320435\n","step: 2390, loss: 0.16601134836673737\n","step: 2400, loss: 0.1313924491405487\n","step: 2410, loss: 0.07528909295797348\n","step: 2420, loss: 0.08961102366447449\n","step: 2430, loss: 0.07613427191972733\n","step: 2440, loss: 0.07844255864620209\n","step: 2450, loss: 0.12375161051750183\n","step: 2460, loss: 0.22524285316467285\n","step: 2470, loss: 0.0826973244547844\n","step: 2480, loss: 0.05344386771321297\n","step: 2490, loss: 0.13352668285369873\n","step: 2500, loss: 0.07744091749191284\n","step: 2510, loss: 0.07875595986843109\n","step: 2520, loss: 0.13915935158729553\n","step: 2530, loss: 0.07042603194713593\n","step: 2540, loss: 0.0999993085861206\n","step: 2550, loss: 0.10773599147796631\n","step: 2560, loss: 0.14188940823078156\n","step: 2570, loss: 0.09349816292524338\n","step: 2580, loss: 0.029293514788150787\n","step: 2590, loss: 0.054375600069761276\n","step: 2600, loss: 0.04872489720582962\n","step: 2610, loss: 0.10046101361513138\n","step: 2620, loss: 0.13959503173828125\n","step: 2630, loss: 0.08866623789072037\n","step: 2640, loss: 0.08261501044034958\n","step: 2650, loss: 0.10155831277370453\n","step: 2660, loss: 0.09107161313295364\n","step: 2670, loss: 0.10439541935920715\n","step: 2680, loss: 0.08294861018657684\n","step: 2690, loss: 0.09745889902114868\n","step: 2700, loss: 0.06571026891469955\n","step: 2710, loss: 0.0832873061299324\n","step: 2720, loss: 0.05680657923221588\n","step: 2730, loss: 0.23328398168087006\n","step: 2740, loss: 0.14035676419734955\n","step: 2750, loss: 0.056310009211301804\n","step: 2760, loss: 0.12880171835422516\n","step: 2770, loss: 0.04086953401565552\n","step: 2780, loss: 0.09769733250141144\n","step: 2790, loss: 0.12383846193552017\n","step: 2800, loss: 0.1516781449317932\n","step: 2810, loss: 0.09499756991863251\n","step: 2820, loss: 0.06710918247699738\n","step: 2830, loss: 0.08638772368431091\n","step: 2840, loss: 0.05090470239520073\n","step: 2850, loss: 0.13684001564979553\n","step: 2860, loss: 0.09003812074661255\n","step: 2870, loss: 0.14310865104198456\n","step: 2880, loss: 0.11572069674730301\n","step: 2890, loss: 0.0648195669054985\n","step: 2900, loss: 0.033459074795246124\n","step: 2910, loss: 0.0901188775897026\n","step: 2920, loss: 0.09962301701307297\n","step: 2930, loss: 0.11596067994832993\n","step: 2940, loss: 0.0509798526763916\n","step: 2950, loss: 0.02969055250287056\n","step: 2960, loss: 0.07998114079236984\n","step: 2970, loss: 0.1372002363204956\n","step: 2980, loss: 0.09680170565843582\n","step: 2990, loss: 0.24196161329746246\n","step: 3000, loss: 0.04228900372982025\n","step: 3010, loss: 0.15183061361312866\n","step: 3020, loss: 0.06648151576519012\n","step: 3030, loss: 0.06258148699998856\n","step: 3040, loss: 0.056067660450935364\n","step: 3050, loss: 0.06841619312763214\n","step: 3060, loss: 0.1787085235118866\n","step: 3070, loss: 0.047379542142152786\n","step: 3080, loss: 0.15040276944637299\n","step: 3090, loss: 0.06621153652667999\n","step: 3100, loss: 0.05553915351629257\n","step: 3110, loss: 0.05256729573011398\n","step: 3120, loss: 0.046028610318899155\n","step: 3130, loss: 0.046169962733983994\n","step: 3140, loss: 0.039610978215932846\n","step: 3150, loss: 0.11479461193084717\n","step: 3160, loss: 0.04184366762638092\n","step: 3170, loss: 0.07785440981388092\n","step: 3180, loss: 0.11953313648700714\n","step: 3190, loss: 0.14318740367889404\n","step: 3200, loss: 0.09249845892190933\n","step: 3210, loss: 0.08787430077791214\n","step: 3220, loss: 0.02761830762028694\n","step: 3230, loss: 0.08985917270183563\n","step: 3240, loss: 0.1422618180513382\n","step: 3250, loss: 0.07308782637119293\n","step: 3260, loss: 0.09205847978591919\n","step: 3270, loss: 0.15012814104557037\n","step: 3280, loss: 0.12224680930376053\n","step: 3290, loss: 0.11225011944770813\n","step: 3300, loss: 0.12389055639505386\n","step: 3310, loss: 0.04058052599430084\n","step: 3320, loss: 0.20624589920043945\n","step: 3330, loss: 0.06928414106369019\n","step: 3340, loss: 0.05301963537931442\n","step: 3350, loss: 0.14264266192913055\n","step: 3360, loss: 0.08820338547229767\n","step: 3370, loss: 0.10809823870658875\n","step: 3380, loss: 0.06720518320798874\n","step: 3390, loss: 0.05713750422000885\n","step: 3400, loss: 0.04590727761387825\n","step: 3410, loss: 0.10793483257293701\n","step: 3420, loss: 0.1880812644958496\n","step: 3430, loss: 0.0495956651866436\n","step: 3440, loss: 0.1965557336807251\n","step: 3450, loss: 0.04310772567987442\n","step: 3460, loss: 0.11318027973175049\n","step: 3470, loss: 0.05516446381807327\n","step: 3480, loss: 0.0493340790271759\n","step: 3490, loss: 0.057579156011343\n","step: 3500, loss: 0.11418775469064713\n","step: 3510, loss: 0.0680227056145668\n","step: 3520, loss: 0.011629543267190456\n","step: 3530, loss: 0.06407685577869415\n","step: 3540, loss: 0.07081085443496704\n","step: 3550, loss: 0.04449623078107834\n","step: 3560, loss: 0.12241074442863464\n","step: 3570, loss: 0.14158014953136444\n","step: 3580, loss: 0.08448340743780136\n","step: 3590, loss: 0.0831010639667511\n","step: 3600, loss: 0.07405947893857956\n","step: 3610, loss: 0.1015707477927208\n","step: 3620, loss: 0.07052847743034363\n","step: 3630, loss: 0.05327963829040527\n","step: 3640, loss: 0.05963987484574318\n","step: 3650, loss: 0.09937799721956253\n","step: 3660, loss: 0.09922850877046585\n","step: 3670, loss: 0.11616084724664688\n","step: 3680, loss: 0.10312747210264206\n","step: 3690, loss: 0.03338354453444481\n","step: 3700, loss: 0.06593425571918488\n","step: 3710, loss: 0.1828223317861557\n","step: 3720, loss: 0.07246723026037216\n","step: 3730, loss: 0.027465922757983208\n","step: 3740, loss: 0.1314729005098343\n","step: 3750, loss: 0.05312733352184296\n","step: 3760, loss: 0.04891239479184151\n","step: 3770, loss: 0.09898608922958374\n","step: 3780, loss: 0.07045462727546692\n","step: 3790, loss: 0.09156566858291626\n","step: 3800, loss: 0.07864215224981308\n","step: 3810, loss: 0.05917550250887871\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      1.00      0.92        35\n","           2       0.77      0.44      0.56        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.88      0.83      0.85       291\n","           5       0.93      0.84      0.88       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.58      0.94      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.93      0.97      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.83      0.77      0.80        13\n","          14       0.32      1.00      0.48        43\n","          15       0.95      0.98      0.97      2778\n","          16       0.85      0.84      0.85      1151\n","          17       0.95      0.95      0.95        41\n","          18       0.97      0.91      0.94        32\n","          19       0.00      0.00      0.00        40\n","          20       0.97      1.00      0.98       584\n","          21       0.01      0.02      0.02        52\n","          22       0.95      0.72      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.35      0.48      0.40        44\n","          25       0.87      0.90      0.88       888\n","          26       0.64      1.00      0.78         9\n","          27       0.99      1.00      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.84      0.89      1136\n","          31       0.58      0.74      0.65        19\n","          32       0.70      0.88      0.78         8\n","          33       0.79      0.90      0.84        86\n","          34       0.25      0.69      0.36        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.12      0.22       182\n","          37       0.87      0.97      0.92      1592\n","          38       0.94      0.98      0.96       404\n","          39       0.96      0.95      0.96       485\n","          40       0.91      0.91      0.91       573\n","          41       0.97      0.94      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.93      0.95       152\n","          44       0.91      0.92      0.91        75\n","          46       1.00      0.99      0.99        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.80      0.77     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 443\n","\n","Loop 44\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9623427391052246\n","step: 10, loss: 1.8966691493988037\n","step: 20, loss: 0.7803052067756653\n","step: 30, loss: 0.2601768970489502\n","step: 40, loss: 0.3431254029273987\n","step: 50, loss: 0.1712368130683899\n","step: 60, loss: 0.27732759714126587\n","step: 70, loss: 0.25007152557373047\n","step: 80, loss: 0.16980856657028198\n","step: 90, loss: 0.17424534261226654\n","step: 100, loss: 0.21397767961025238\n","step: 110, loss: 0.13506202399730682\n","step: 120, loss: 0.13949212431907654\n","step: 130, loss: 0.10206527262926102\n","step: 140, loss: 0.16542178392410278\n","step: 150, loss: 0.14843830466270447\n","step: 160, loss: 0.09021269530057907\n","step: 170, loss: 0.16290336847305298\n","step: 180, loss: 0.23917558789253235\n","step: 190, loss: 0.21353398263454437\n","step: 200, loss: 0.09355674684047699\n","step: 210, loss: 0.1399371325969696\n","step: 220, loss: 0.15627558529376984\n","step: 230, loss: 0.10328761488199234\n","step: 240, loss: 0.1915159672498703\n","step: 250, loss: 0.12056891620159149\n","step: 260, loss: 0.11911474913358688\n","step: 270, loss: 0.23602944612503052\n","step: 280, loss: 0.1933237910270691\n","step: 290, loss: 0.10662230104207993\n","step: 300, loss: 0.11119350045919418\n","step: 310, loss: 0.0899607241153717\n","step: 320, loss: 0.10207881033420563\n","step: 330, loss: 0.19553281366825104\n","step: 340, loss: 0.12113867700099945\n","step: 350, loss: 0.15992073714733124\n","step: 360, loss: 0.07915682345628738\n","step: 370, loss: 0.18442903459072113\n","step: 380, loss: 0.14577104151248932\n","step: 390, loss: 0.10007965564727783\n","step: 400, loss: 0.10017964243888855\n","step: 410, loss: 0.16049401462078094\n","step: 420, loss: 0.1116773709654808\n","step: 430, loss: 0.06379145383834839\n","step: 440, loss: 0.06886372715234756\n","step: 450, loss: 0.09943442046642303\n","step: 460, loss: 0.13763362169265747\n","step: 470, loss: 0.07798667252063751\n","step: 480, loss: 0.16682478785514832\n","step: 490, loss: 0.07780583947896957\n","step: 500, loss: 0.20221976935863495\n","step: 510, loss: 0.13418063521385193\n","step: 520, loss: 0.08972512930631638\n","step: 530, loss: 0.2501433193683624\n","step: 540, loss: 0.106629379093647\n","step: 550, loss: 0.13068987429141998\n","step: 560, loss: 0.1319672018289566\n","step: 570, loss: 0.12179526686668396\n","step: 580, loss: 0.12611517310142517\n","step: 590, loss: 0.034625642001628876\n","step: 600, loss: 0.13860952854156494\n","step: 610, loss: 0.11236505210399628\n","step: 620, loss: 0.11244246363639832\n","step: 630, loss: 0.09087289869785309\n","step: 640, loss: 0.07459791004657745\n","step: 650, loss: 0.057079948484897614\n","step: 660, loss: 0.10091665387153625\n","step: 670, loss: 0.10811799019575119\n","step: 680, loss: 0.10036575049161911\n","step: 690, loss: 0.0919278934597969\n","step: 700, loss: 0.07310324162244797\n","step: 710, loss: 0.09078287333250046\n","step: 720, loss: 0.2554360330104828\n","step: 730, loss: 0.04485011845827103\n","step: 740, loss: 0.1682530641555786\n","step: 750, loss: 0.04407969489693642\n","step: 760, loss: 0.0671057403087616\n","step: 770, loss: 0.06450290977954865\n","step: 780, loss: 0.1259898543357849\n","step: 790, loss: 0.09638365358114243\n","step: 800, loss: 0.1070842444896698\n","step: 810, loss: 0.07194411754608154\n","step: 820, loss: 0.157106414437294\n","step: 830, loss: 0.1581735610961914\n","step: 840, loss: 0.1381479650735855\n","step: 850, loss: 0.18444156646728516\n","step: 860, loss: 0.12420159578323364\n","step: 870, loss: 0.08903662860393524\n","step: 880, loss: 0.062215957790613174\n","step: 890, loss: 0.04952076077461243\n","step: 900, loss: 0.16798290610313416\n","step: 910, loss: 0.03437699005007744\n","step: 920, loss: 0.07930731773376465\n","step: 930, loss: 0.1348932832479477\n","step: 940, loss: 0.06812518835067749\n","step: 950, loss: 0.1599562019109726\n","step: 960, loss: 0.22481821477413177\n","step: 970, loss: 0.07654305547475815\n","step: 980, loss: 0.13686154782772064\n","step: 990, loss: 0.11504532396793365\n","step: 1000, loss: 0.1631801575422287\n","step: 1010, loss: 0.16486598551273346\n","step: 1020, loss: 0.22135251760482788\n","step: 1030, loss: 0.12382511049509048\n","step: 1040, loss: 0.15775181353092194\n","step: 1050, loss: 0.07759363949298859\n","step: 1060, loss: 0.06717903912067413\n","step: 1070, loss: 0.1100134626030922\n","step: 1080, loss: 0.03845808655023575\n","step: 1090, loss: 0.09512293338775635\n","step: 1100, loss: 0.13417421281337738\n","step: 1110, loss: 0.05701030418276787\n","step: 1120, loss: 0.03143531456589699\n","step: 1130, loss: 0.0660652369260788\n","step: 1140, loss: 0.03310000151395798\n","step: 1150, loss: 0.06522318720817566\n","step: 1160, loss: 0.09650717675685883\n","step: 1170, loss: 0.05468662455677986\n","step: 1180, loss: 0.054540980607271194\n","step: 1190, loss: 0.0697673037648201\n","step: 1200, loss: 0.19051337242126465\n","step: 1210, loss: 0.10440132766962051\n","step: 1220, loss: 0.0725473165512085\n","step: 1230, loss: 0.04914984852075577\n","step: 1240, loss: 0.12739798426628113\n","step: 1250, loss: 0.08138003945350647\n","step: 1260, loss: 0.20978398621082306\n","step: 1270, loss: 0.05520861595869064\n","step: 1280, loss: 0.13275682926177979\n","step: 1290, loss: 0.17499689757823944\n","step: 1300, loss: 0.08234517276287079\n","step: 1310, loss: 0.04486314579844475\n","step: 1320, loss: 0.128879576921463\n","step: 1330, loss: 0.1388530433177948\n","step: 1340, loss: 0.1165350005030632\n","step: 1350, loss: 0.05468512699007988\n","step: 1360, loss: 0.04389459639787674\n","step: 1370, loss: 0.12589432299137115\n","step: 1380, loss: 0.030122673138976097\n","step: 1390, loss: 0.11370684951543808\n","step: 1400, loss: 0.04554750397801399\n","step: 1410, loss: 0.0681253969669342\n","step: 1420, loss: 0.08365939557552338\n","step: 1430, loss: 0.15173779428005219\n","step: 1440, loss: 0.05979183688759804\n","step: 1450, loss: 0.0594131238758564\n","step: 1460, loss: 0.09044911712408066\n","step: 1470, loss: 0.1151946634054184\n","step: 1480, loss: 0.05575846880674362\n","step: 1490, loss: 0.10521573573350906\n","step: 1500, loss: 0.07092887163162231\n","step: 1510, loss: 0.13972844183444977\n","step: 1520, loss: 0.029562750831246376\n","step: 1530, loss: 0.07646168768405914\n","step: 1540, loss: 0.07360377907752991\n","step: 1550, loss: 0.08171555399894714\n","step: 1560, loss: 0.10740292817354202\n","step: 1570, loss: 0.06790342926979065\n","step: 1580, loss: 0.0873384177684784\n","step: 1590, loss: 0.02774449810385704\n","step: 1600, loss: 0.05607159063220024\n","step: 1610, loss: 0.08438853919506073\n","step: 1620, loss: 0.17078067362308502\n","step: 1630, loss: 0.09038899838924408\n","step: 1640, loss: 0.1333397477865219\n","step: 1650, loss: 0.07631746679544449\n","step: 1660, loss: 0.14467549324035645\n","step: 1670, loss: 0.0592476949095726\n","step: 1680, loss: 0.07742729038000107\n","step: 1690, loss: 0.1515219360589981\n","step: 1700, loss: 0.03308773785829544\n","step: 1710, loss: 0.13625892996788025\n","step: 1720, loss: 0.13722504675388336\n","step: 1730, loss: 0.09801297634840012\n","step: 1740, loss: 0.04676678031682968\n","step: 1750, loss: 0.17862269282341003\n","step: 1760, loss: 0.12593962252140045\n","step: 1770, loss: 0.04619094729423523\n","step: 1780, loss: 0.09014073759317398\n","step: 1790, loss: 0.05756153166294098\n","step: 1800, loss: 0.11637838184833527\n","step: 1810, loss: 0.09293296933174133\n","step: 1820, loss: 0.07448035478591919\n","step: 1830, loss: 0.0445430763065815\n","step: 1840, loss: 0.12633086740970612\n","step: 1850, loss: 0.06782737374305725\n","step: 1860, loss: 0.09491670876741409\n","step: 1870, loss: 0.07623901963233948\n","step: 1880, loss: 0.15672257542610168\n","step: 1890, loss: 0.08529096096754074\n","step: 1900, loss: 0.07952401041984558\n","step: 1910, loss: 0.0566437728703022\n","step: 1920, loss: 0.1255030333995819\n","step: 1930, loss: 0.08145274966955185\n","step: 1940, loss: 0.061495378613471985\n","step: 1950, loss: 0.12110546231269836\n","step: 1960, loss: 0.03849763795733452\n","step: 1970, loss: 0.08151919394731522\n","step: 1980, loss: 0.09236197918653488\n","step: 1990, loss: 0.053220901638269424\n","step: 2000, loss: 0.0683492049574852\n","step: 2010, loss: 0.04559732601046562\n","step: 2020, loss: 0.18494951725006104\n","step: 2030, loss: 0.10002312064170837\n","step: 2040, loss: 0.12064876407384872\n","step: 2050, loss: 0.07298047840595245\n","step: 2060, loss: 0.043437957763671875\n","step: 2070, loss: 0.05239493399858475\n","step: 2080, loss: 0.08707623183727264\n","step: 2090, loss: 0.10674680769443512\n","step: 2100, loss: 0.09653851389884949\n","step: 2110, loss: 0.05747929960489273\n","step: 2120, loss: 0.02893630973994732\n","step: 2130, loss: 0.17333106696605682\n","step: 2140, loss: 0.06356044858694077\n","step: 2150, loss: 0.07515430450439453\n","step: 2160, loss: 0.04811936616897583\n","step: 2170, loss: 0.07896937429904938\n","step: 2180, loss: 0.05175565183162689\n","step: 2190, loss: 0.04415331408381462\n","step: 2200, loss: 0.10714970529079437\n","step: 2210, loss: 0.13183103501796722\n","step: 2220, loss: 0.051854901015758514\n","step: 2230, loss: 0.059799496084451675\n","step: 2240, loss: 0.11173208057880402\n","step: 2250, loss: 0.02415499836206436\n","step: 2260, loss: 0.06852985173463821\n","step: 2270, loss: 0.07242457568645477\n","step: 2280, loss: 0.10237132012844086\n","step: 2290, loss: 0.07739394158124924\n","step: 2300, loss: 0.04895133152604103\n","step: 2310, loss: 0.09461397677659988\n","step: 2320, loss: 0.08481480181217194\n","step: 2330, loss: 0.028889760375022888\n","step: 2340, loss: 0.06370317935943604\n","step: 2350, loss: 0.043000392615795135\n","step: 2360, loss: 0.11085452884435654\n","step: 2370, loss: 0.15121902525424957\n","step: 2380, loss: 0.0661223754286766\n","step: 2390, loss: 0.09846463799476624\n","step: 2400, loss: 0.048345897346735\n","step: 2410, loss: 0.10820108652114868\n","step: 2420, loss: 0.04084806144237518\n","step: 2430, loss: 0.044442068785429\n","step: 2440, loss: 0.10651537775993347\n","step: 2450, loss: 0.03436703979969025\n","step: 2460, loss: 0.06970079988241196\n","step: 2470, loss: 0.07358051091432571\n","step: 2480, loss: 0.019432619214057922\n","step: 2490, loss: 0.0420273020863533\n","step: 2500, loss: 0.06084858998656273\n","step: 2510, loss: 0.027992095798254013\n","step: 2520, loss: 0.07208871841430664\n","step: 2530, loss: 0.26044830679893494\n","step: 2540, loss: 0.12452729046344757\n","step: 2550, loss: 0.08205538988113403\n","step: 2560, loss: 0.09463618695735931\n","step: 2570, loss: 0.0482303760945797\n","step: 2580, loss: 0.06275542080402374\n","step: 2590, loss: 0.15085969865322113\n","step: 2600, loss: 0.06586164981126785\n","step: 2610, loss: 0.1689324527978897\n","step: 2620, loss: 0.08162818104028702\n","step: 2630, loss: 0.15592791140079498\n","step: 2640, loss: 0.0830877274274826\n","step: 2650, loss: 0.06988144665956497\n","step: 2660, loss: 0.14335890114307404\n","step: 2670, loss: 0.11223147809505463\n","step: 2680, loss: 0.05767485126852989\n","step: 2690, loss: 0.047896549105644226\n","step: 2700, loss: 0.22931301593780518\n","step: 2710, loss: 0.01995358243584633\n","step: 2720, loss: 0.09820796549320221\n","step: 2730, loss: 0.09118320047855377\n","step: 2740, loss: 0.06849226355552673\n","step: 2750, loss: 0.056093402206897736\n","step: 2760, loss: 0.1359836608171463\n","step: 2770, loss: 0.11416872590780258\n","step: 2780, loss: 0.07736095786094666\n","step: 2790, loss: 0.10080703347921371\n","step: 2800, loss: 0.09233848750591278\n","step: 2810, loss: 0.09929187595844269\n","step: 2820, loss: 0.05389174073934555\n","step: 2830, loss: 0.0410768985748291\n","step: 2840, loss: 0.02576143853366375\n","step: 2850, loss: 0.02245473489165306\n","step: 2860, loss: 0.048302896320819855\n","step: 2870, loss: 0.06845089048147202\n","step: 2880, loss: 0.14312773942947388\n","step: 2890, loss: 0.07962242513895035\n","step: 2900, loss: 0.08168116956949234\n","step: 2910, loss: 0.1180868148803711\n","step: 2920, loss: 0.12384951859712601\n","step: 2930, loss: 0.15091226994991302\n","step: 2940, loss: 0.05615419149398804\n","step: 2950, loss: 0.03804736211895943\n","step: 2960, loss: 0.16193199157714844\n","step: 2970, loss: 0.059858277440071106\n","step: 2980, loss: 0.05023866891860962\n","step: 2990, loss: 0.06644546240568161\n","step: 3000, loss: 0.09550905972719193\n","step: 3010, loss: 0.08228275179862976\n","step: 3020, loss: 0.026450615376234055\n","step: 3030, loss: 0.12708614766597748\n","step: 3040, loss: 0.17667695879936218\n","step: 3050, loss: 0.1261284351348877\n","step: 3060, loss: 0.061470214277505875\n","step: 3070, loss: 0.015572383999824524\n","step: 3080, loss: 0.10576777160167694\n","step: 3090, loss: 0.02510087750852108\n","step: 3100, loss: 0.05635233223438263\n","step: 3110, loss: 0.031340863555669785\n","step: 3120, loss: 0.11034466326236725\n","step: 3130, loss: 0.04739683121442795\n","step: 3140, loss: 0.0898498147726059\n","step: 3150, loss: 0.11598151177167892\n","step: 3160, loss: 0.05652175098657608\n","step: 3170, loss: 0.09427149593830109\n","step: 3180, loss: 0.02694861777126789\n","step: 3190, loss: 0.11440543830394745\n","step: 3200, loss: 0.07216895371675491\n","step: 3210, loss: 0.06856627017259598\n","step: 3220, loss: 0.15798364579677582\n","step: 3230, loss: 0.09327339380979538\n","step: 3240, loss: 0.07528695464134216\n","step: 3250, loss: 0.07043924927711487\n","step: 3260, loss: 0.051196590065956116\n","step: 3270, loss: 0.04086935147643089\n","step: 3280, loss: 0.11839106678962708\n","step: 3290, loss: 0.0749792531132698\n","step: 3300, loss: 0.07412464171648026\n","step: 3310, loss: 0.16551773250102997\n","step: 3320, loss: 0.11648348718881607\n","step: 3330, loss: 0.15330535173416138\n","step: 3340, loss: 0.08705934137105942\n","step: 3350, loss: 0.03642374277114868\n","step: 3360, loss: 0.1191474199295044\n","step: 3370, loss: 0.0746898502111435\n","step: 3380, loss: 0.06251857429742813\n","step: 3390, loss: 0.1360093057155609\n","step: 3400, loss: 0.052420735359191895\n","step: 3410, loss: 0.08396807312965393\n","step: 3420, loss: 0.06339344382286072\n","step: 3430, loss: 0.05233063921332359\n","step: 3440, loss: 0.08775912970304489\n","step: 3450, loss: 0.05683302879333496\n","step: 3460, loss: 0.07793579250574112\n","step: 3470, loss: 0.07028863579034805\n","step: 3480, loss: 0.06003857031464577\n","step: 3490, loss: 0.09311066567897797\n","step: 3500, loss: 0.09223223477602005\n","step: 3510, loss: 0.22375209629535675\n","step: 3520, loss: 0.048257455229759216\n","step: 3530, loss: 0.10542145371437073\n","step: 3540, loss: 0.09318086504936218\n","step: 3550, loss: 0.0643993616104126\n","step: 3560, loss: 0.14022782444953918\n","step: 3570, loss: 0.07193649560213089\n","step: 3580, loss: 0.0752754807472229\n","step: 3590, loss: 0.058684222400188446\n","step: 3600, loss: 0.1746731549501419\n","step: 3610, loss: 0.04481904208660126\n","step: 3620, loss: 0.1943769007921219\n","step: 3630, loss: 0.09955384582281113\n","step: 3640, loss: 0.06124584749341011\n","step: 3650, loss: 0.05678596347570419\n","step: 3660, loss: 0.13088358938694\n","step: 3670, loss: 0.09865166991949081\n","step: 3680, loss: 0.0789954736828804\n","step: 3690, loss: 0.12376775592565536\n","step: 3700, loss: 0.039167825132608414\n","step: 3710, loss: 0.022275155410170555\n","step: 3720, loss: 0.021610641852021217\n","step: 3730, loss: 0.10412853956222534\n","step: 3740, loss: 0.04899924248456955\n","step: 3750, loss: 0.05434869974851608\n","step: 3760, loss: 0.06923835724592209\n","step: 3770, loss: 0.10232093185186386\n","step: 3780, loss: 0.05348668992519379\n","step: 3790, loss: 0.08770573884248734\n","step: 3800, loss: 0.21868443489074707\n","step: 3810, loss: 0.04755527526140213\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.81      1.00      0.90        35\n","           2       0.72      0.86      0.78        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.84      0.90       291\n","           5       0.97      0.82      0.89       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.55      0.94      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.97      0.98      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.20      0.92      0.32        13\n","          14       0.34      1.00      0.50        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.86      0.87      0.87      1151\n","          17       0.97      0.95      0.96        41\n","          18       0.91      0.97      0.94        32\n","          19       0.75      0.68      0.71        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.97      0.76      0.85      4175\n","          23       0.73      0.96      0.83      2253\n","          24       0.45      0.43      0.44        44\n","          25       0.85      0.95      0.89       888\n","          26       0.89      0.89      0.89         9\n","          27       0.96      0.99      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.92      0.87      0.89      1136\n","          31       0.48      0.74      0.58        19\n","          32       0.83      0.62      0.71         8\n","          33       0.68      0.97      0.80        86\n","          34       0.24      0.69      0.36        32\n","          35       1.00      0.98      0.99       474\n","          36       1.00      0.12      0.22       182\n","          37       0.89      0.93      0.91      1592\n","          38       0.97      0.97      0.97       404\n","          39       0.94      0.97      0.96       485\n","          40       0.91      0.96      0.94       573\n","          41       0.88      0.95      0.91       841\n","          42       0.98      0.99      0.99       575\n","          43       0.96      0.86      0.91       152\n","          44       0.85      0.92      0.88        75\n","          46       1.00      0.96      0.98        82\n","          48       0.97      0.80      0.88        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.84      0.80     28417\n","weighted avg       0.93      0.91      0.91     28417\n","\n","Difference 466\n","\n","Loop 45\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.008667945861816\n","step: 10, loss: 2.123138189315796\n","step: 20, loss: 0.6585496664047241\n","step: 30, loss: 0.3760696351528168\n","step: 40, loss: 0.2852446734905243\n","step: 50, loss: 0.35284724831581116\n","step: 60, loss: 0.3372957110404968\n","step: 70, loss: 0.1853935420513153\n","step: 80, loss: 0.16300082206726074\n","step: 90, loss: 0.09872939437627792\n","step: 100, loss: 0.10356563329696655\n","step: 110, loss: 0.10304224491119385\n","step: 120, loss: 0.17673034965991974\n","step: 130, loss: 0.10655201971530914\n","step: 140, loss: 0.17311321198940277\n","step: 150, loss: 0.10110468417406082\n","step: 160, loss: 0.055706124752759933\n","step: 170, loss: 0.10649729520082474\n","step: 180, loss: 0.21981896460056305\n","step: 190, loss: 0.20212829113006592\n","step: 200, loss: 0.04057512432336807\n","step: 210, loss: 0.1143881231546402\n","step: 220, loss: 0.17347072064876556\n","step: 230, loss: 0.12349089980125427\n","step: 240, loss: 0.17459352314472198\n","step: 250, loss: 0.04905720427632332\n","step: 260, loss: 0.11257674545049667\n","step: 270, loss: 0.11621619760990143\n","step: 280, loss: 0.11319434642791748\n","step: 290, loss: 0.1045905277132988\n","step: 300, loss: 0.13889239728450775\n","step: 310, loss: 0.12689408659934998\n","step: 320, loss: 0.08107417821884155\n","step: 330, loss: 0.1590745598077774\n","step: 340, loss: 0.13310544192790985\n","step: 350, loss: 0.126778706908226\n","step: 360, loss: 0.03775567561388016\n","step: 370, loss: 0.09960143268108368\n","step: 380, loss: 0.09093709290027618\n","step: 390, loss: 0.08972742408514023\n","step: 400, loss: 0.17150036990642548\n","step: 410, loss: 0.10360243171453476\n","step: 420, loss: 0.08344371616840363\n","step: 430, loss: 0.0822446197271347\n","step: 440, loss: 0.11674397438764572\n","step: 450, loss: 0.04076264426112175\n","step: 460, loss: 0.21867795288562775\n","step: 470, loss: 0.11636296659708023\n","step: 480, loss: 0.1548749804496765\n","step: 490, loss: 0.064356230199337\n","step: 500, loss: 0.07967778295278549\n","step: 510, loss: 0.06572086364030838\n","step: 520, loss: 0.02642480470240116\n","step: 530, loss: 0.07124481350183487\n","step: 540, loss: 0.12757061421871185\n","step: 550, loss: 0.14339719712734222\n","step: 560, loss: 0.10477825254201889\n","step: 570, loss: 0.18112346529960632\n","step: 580, loss: 0.10643065720796585\n","step: 590, loss: 0.10419921576976776\n","step: 600, loss: 0.13579896092414856\n","step: 610, loss: 0.030381543561816216\n","step: 620, loss: 0.06125117838382721\n","step: 630, loss: 0.12968219816684723\n","step: 640, loss: 0.07871335744857788\n","step: 650, loss: 0.10029823333024979\n","step: 660, loss: 0.0568353496491909\n","step: 670, loss: 0.1023549810051918\n","step: 680, loss: 0.023860756307840347\n","step: 690, loss: 0.08635400980710983\n","step: 700, loss: 0.04131609573960304\n","step: 710, loss: 0.17840954661369324\n","step: 720, loss: 0.1410541832447052\n","step: 730, loss: 0.049052249640226364\n","step: 740, loss: 0.1169188991189003\n","step: 750, loss: 0.20702072978019714\n","step: 760, loss: 0.031634896993637085\n","step: 770, loss: 0.17832718789577484\n","step: 780, loss: 0.09659325331449509\n","step: 790, loss: 0.06725974380970001\n","step: 800, loss: 0.07889895886182785\n","step: 810, loss: 0.17619584500789642\n","step: 820, loss: 0.13265866041183472\n","step: 830, loss: 0.09951917827129364\n","step: 840, loss: 0.16001006960868835\n","step: 850, loss: 0.08636659383773804\n","step: 860, loss: 0.0689530298113823\n","step: 870, loss: 0.1435394585132599\n","step: 880, loss: 0.027041444554924965\n","step: 890, loss: 0.10211608558893204\n","step: 900, loss: 0.12096099555492401\n","step: 910, loss: 0.1524454951286316\n","step: 920, loss: 0.10882469266653061\n","step: 930, loss: 0.12077639997005463\n","step: 940, loss: 0.12289662659168243\n","step: 950, loss: 0.06005322188138962\n","step: 960, loss: 0.04261813685297966\n","step: 970, loss: 0.049407921731472015\n","step: 980, loss: 0.09217002987861633\n","step: 990, loss: 0.15404975414276123\n","step: 1000, loss: 0.04708675295114517\n","step: 1010, loss: 0.1120762974023819\n","step: 1020, loss: 0.10475613921880722\n","step: 1030, loss: 0.014436125755310059\n","step: 1040, loss: 0.088905930519104\n","step: 1050, loss: 0.12320122867822647\n","step: 1060, loss: 0.12438404560089111\n","step: 1070, loss: 0.14843986928462982\n","step: 1080, loss: 0.10162992775440216\n","step: 1090, loss: 0.07386548817157745\n","step: 1100, loss: 0.13492019474506378\n","step: 1110, loss: 0.14914405345916748\n","step: 1120, loss: 0.1339782178401947\n","step: 1130, loss: 0.11310716718435287\n","step: 1140, loss: 0.062133245170116425\n","step: 1150, loss: 0.08542762696743011\n","step: 1160, loss: 0.08005119860172272\n","step: 1170, loss: 0.16956384479999542\n","step: 1180, loss: 0.09560170769691467\n","step: 1190, loss: 0.07483041286468506\n","step: 1200, loss: 0.11285454034805298\n","step: 1210, loss: 0.088276706635952\n","step: 1220, loss: 0.09748987853527069\n","step: 1230, loss: 0.041072532534599304\n","step: 1240, loss: 0.06850704550743103\n","step: 1250, loss: 0.08002402633428574\n","step: 1260, loss: 0.18597710132598877\n","step: 1270, loss: 0.13694967329502106\n","step: 1280, loss: 0.19449752569198608\n","step: 1290, loss: 0.13749061524868011\n","step: 1300, loss: 0.09556400030851364\n","step: 1310, loss: 0.041785627603530884\n","step: 1320, loss: 0.0698319524526596\n","step: 1330, loss: 0.0447121262550354\n","step: 1340, loss: 0.1535961925983429\n","step: 1350, loss: 0.053115542978048325\n","step: 1360, loss: 0.0726848766207695\n","step: 1370, loss: 0.13009768724441528\n","step: 1380, loss: 0.0698956772685051\n","step: 1390, loss: 0.05300644785165787\n","step: 1400, loss: 0.07352767884731293\n","step: 1410, loss: 0.10529033839702606\n","step: 1420, loss: 0.08684711903333664\n","step: 1430, loss: 0.1140563040971756\n","step: 1440, loss: 0.1343679428100586\n","step: 1450, loss: 0.17833928763866425\n","step: 1460, loss: 0.08269497752189636\n","step: 1470, loss: 0.09353192150592804\n","step: 1480, loss: 0.06103936582803726\n","step: 1490, loss: 0.10582026094198227\n","step: 1500, loss: 0.12492144852876663\n","step: 1510, loss: 0.06040089577436447\n","step: 1520, loss: 0.06304804980754852\n","step: 1530, loss: 0.07873034477233887\n","step: 1540, loss: 0.1704535335302353\n","step: 1550, loss: 0.06487508863210678\n","step: 1560, loss: 0.10803253203630447\n","step: 1570, loss: 0.08783844858407974\n","step: 1580, loss: 0.09155212342739105\n","step: 1590, loss: 0.08034366369247437\n","step: 1600, loss: 0.11274795234203339\n","step: 1610, loss: 0.05924150347709656\n","step: 1620, loss: 0.08913745731115341\n","step: 1630, loss: 0.056544385850429535\n","step: 1640, loss: 0.1208617091178894\n","step: 1650, loss: 0.03277789056301117\n","step: 1660, loss: 0.03363978862762451\n","step: 1670, loss: 0.09262484312057495\n","step: 1680, loss: 0.04927706718444824\n","step: 1690, loss: 0.22422249615192413\n","step: 1700, loss: 0.08771120011806488\n","step: 1710, loss: 0.09967818111181259\n","step: 1720, loss: 0.10283254086971283\n","step: 1730, loss: 0.17029671370983124\n","step: 1740, loss: 0.172286257147789\n","step: 1750, loss: 0.15235450863838196\n","step: 1760, loss: 0.14346197247505188\n","step: 1770, loss: 0.13828013837337494\n","step: 1780, loss: 0.1163468137383461\n","step: 1790, loss: 0.15600676834583282\n","step: 1800, loss: 0.11799301952123642\n","step: 1810, loss: 0.10088244080543518\n","step: 1820, loss: 0.0729813352227211\n","step: 1830, loss: 0.12418980151414871\n","step: 1840, loss: 0.15284593403339386\n","step: 1850, loss: 0.08133433014154434\n","step: 1860, loss: 0.08673214167356491\n","step: 1870, loss: 0.12264764308929443\n","step: 1880, loss: 0.12041129171848297\n","step: 1890, loss: 0.028669999912381172\n","step: 1900, loss: 0.06179427728056908\n","step: 1910, loss: 0.06558399647474289\n","step: 1920, loss: 0.11439160257577896\n","step: 1930, loss: 0.04750046133995056\n","step: 1940, loss: 0.10507410019636154\n","step: 1950, loss: 0.03802819922566414\n","step: 1960, loss: 0.057076841592788696\n","step: 1970, loss: 0.11255847662687302\n","step: 1980, loss: 0.14259150624275208\n","step: 1990, loss: 0.14983414113521576\n","step: 2000, loss: 0.06582995504140854\n","step: 2010, loss: 0.125504270195961\n","step: 2020, loss: 0.148503378033638\n","step: 2030, loss: 0.021172551438212395\n","step: 2040, loss: 0.15972432494163513\n","step: 2050, loss: 0.058629006147384644\n","step: 2060, loss: 0.04647546634078026\n","step: 2070, loss: 0.06035841256380081\n","step: 2080, loss: 0.15833912789821625\n","step: 2090, loss: 0.12750862538814545\n","step: 2100, loss: 0.09347042441368103\n","step: 2110, loss: 0.12511888146400452\n","step: 2120, loss: 0.07013023644685745\n","step: 2130, loss: 0.1270390748977661\n","step: 2140, loss: 0.1589769721031189\n","step: 2150, loss: 0.09493782371282578\n","step: 2160, loss: 0.20428107678890228\n","step: 2170, loss: 0.016201341524720192\n","step: 2180, loss: 0.11032705754041672\n","step: 2190, loss: 0.1357460916042328\n","step: 2200, loss: 0.0847637802362442\n","step: 2210, loss: 0.06061871722340584\n","step: 2220, loss: 0.0871000662446022\n","step: 2230, loss: 0.17073196172714233\n","step: 2240, loss: 0.061501529067754745\n","step: 2250, loss: 0.06704692542552948\n","step: 2260, loss: 0.06278213113546371\n","step: 2270, loss: 0.11539863795042038\n","step: 2280, loss: 0.13038946688175201\n","step: 2290, loss: 0.03812438249588013\n","step: 2300, loss: 0.12330746650695801\n","step: 2310, loss: 0.10646900534629822\n","step: 2320, loss: 0.05383017286658287\n","step: 2330, loss: 0.13990378379821777\n","step: 2340, loss: 0.0318329893052578\n","step: 2350, loss: 0.05582622066140175\n","step: 2360, loss: 0.10857018828392029\n","step: 2370, loss: 0.04649290814995766\n","step: 2380, loss: 0.03486992418766022\n","step: 2390, loss: 0.057372547686100006\n","step: 2400, loss: 0.057118456810712814\n","step: 2410, loss: 0.13464504480361938\n","step: 2420, loss: 0.06924377381801605\n","step: 2430, loss: 0.06467463821172714\n","step: 2440, loss: 0.13714894652366638\n","step: 2450, loss: 0.07425568252801895\n","step: 2460, loss: 0.1440276950597763\n","step: 2470, loss: 0.12486179172992706\n","step: 2480, loss: 0.16794148087501526\n","step: 2490, loss: 0.09416025131940842\n","step: 2500, loss: 0.0569293349981308\n","step: 2510, loss: 0.04444113001227379\n","step: 2520, loss: 0.06343034654855728\n","step: 2530, loss: 0.05312527343630791\n","step: 2540, loss: 0.1662786304950714\n","step: 2550, loss: 0.11069373786449432\n","step: 2560, loss: 0.17501342296600342\n","step: 2570, loss: 0.02930731140077114\n","step: 2580, loss: 0.10882972925901413\n","step: 2590, loss: 0.20737223327159882\n","step: 2600, loss: 0.11390458047389984\n","step: 2610, loss: 0.10305831581354141\n","step: 2620, loss: 0.11231289058923721\n","step: 2630, loss: 0.10196401923894882\n","step: 2640, loss: 0.11170360445976257\n","step: 2650, loss: 0.07510646432638168\n","step: 2660, loss: 0.07133331894874573\n","step: 2670, loss: 0.1382182091474533\n","step: 2680, loss: 0.08313604444265366\n","step: 2690, loss: 0.05923432111740112\n","step: 2700, loss: 0.09671477228403091\n","step: 2710, loss: 0.05687132105231285\n","step: 2720, loss: 0.11339646577835083\n","step: 2730, loss: 0.06211356073617935\n","step: 2740, loss: 0.09960654377937317\n","step: 2750, loss: 0.06856051832437515\n","step: 2760, loss: 0.10598672181367874\n","step: 2770, loss: 0.03908242657780647\n","step: 2780, loss: 0.09861863404512405\n","step: 2790, loss: 0.0629953071475029\n","step: 2800, loss: 0.06859003752470016\n","step: 2810, loss: 0.08521972596645355\n","step: 2820, loss: 0.082351453602314\n","step: 2830, loss: 0.05143113061785698\n","step: 2840, loss: 0.09237195551395416\n","step: 2850, loss: 0.12360743433237076\n","step: 2860, loss: 0.07471414655447006\n","step: 2870, loss: 0.05724363029003143\n","step: 2880, loss: 0.04323677346110344\n","step: 2890, loss: 0.02249212935566902\n","step: 2900, loss: 0.03681836649775505\n","step: 2910, loss: 0.17007482051849365\n","step: 2920, loss: 0.03412715345621109\n","step: 2930, loss: 0.06703341007232666\n","step: 2940, loss: 0.046181149780750275\n","step: 2950, loss: 0.1437062919139862\n","step: 2960, loss: 0.06846196204423904\n","step: 2970, loss: 0.03777240961790085\n","step: 2980, loss: 0.11305724829435349\n","step: 2990, loss: 0.022105107083916664\n","step: 3000, loss: 0.061013076454401016\n","step: 3010, loss: 0.14136727154254913\n","step: 3020, loss: 0.14504081010818481\n","step: 3030, loss: 0.4123145639896393\n","step: 3040, loss: 0.06644740700721741\n","step: 3050, loss: 0.07205811142921448\n","step: 3060, loss: 0.06393396854400635\n","step: 3070, loss: 0.11146122217178345\n","step: 3080, loss: 0.060672152787446976\n","step: 3090, loss: 0.10136343538761139\n","step: 3100, loss: 0.08264540135860443\n","step: 3110, loss: 0.09636716544628143\n","step: 3120, loss: 0.08798514306545258\n","step: 3130, loss: 0.04693180322647095\n","step: 3140, loss: 0.14715693891048431\n","step: 3150, loss: 0.06715571135282516\n","step: 3160, loss: 0.09386502206325531\n","step: 3170, loss: 0.11991976201534271\n","step: 3180, loss: 0.13083967566490173\n","step: 3190, loss: 0.07165408879518509\n","step: 3200, loss: 0.07102935016155243\n","step: 3210, loss: 0.006390804424881935\n","step: 3220, loss: 0.054664626717567444\n","step: 3230, loss: 0.10299926996231079\n","step: 3240, loss: 0.0484633669257164\n","step: 3250, loss: 0.07071671634912491\n","step: 3260, loss: 0.03646880015730858\n","step: 3270, loss: 0.11993397027254105\n","step: 3280, loss: 0.06190396845340729\n","step: 3290, loss: 0.12188409268856049\n","step: 3300, loss: 0.1219613179564476\n","step: 3310, loss: 0.03627437353134155\n","step: 3320, loss: 0.061589498072862625\n","step: 3330, loss: 0.09011100977659225\n","step: 3340, loss: 0.08276758342981339\n","step: 3350, loss: 0.1434597223997116\n","step: 3360, loss: 0.09260521084070206\n","step: 3370, loss: 0.045250292867422104\n","step: 3380, loss: 0.0766088217496872\n","step: 3390, loss: 0.12235493212938309\n","step: 3400, loss: 0.05113448202610016\n","step: 3410, loss: 0.0684589296579361\n","step: 3420, loss: 0.02077599987387657\n","step: 3430, loss: 0.04569287598133087\n","step: 3440, loss: 0.11669380962848663\n","step: 3450, loss: 0.1603366583585739\n","step: 3460, loss: 0.059296563267707825\n","step: 3470, loss: 0.02293367125093937\n","step: 3480, loss: 0.09586115926504135\n","step: 3490, loss: 0.050088733434677124\n","step: 3500, loss: 0.2032804638147354\n","step: 3510, loss: 0.1378524899482727\n","step: 3520, loss: 0.06502197682857513\n","step: 3530, loss: 0.03865848481655121\n","step: 3540, loss: 0.08178089559078217\n","step: 3550, loss: 0.03545790910720825\n","step: 3560, loss: 0.04509579762816429\n","step: 3570, loss: 0.05829184502363205\n","step: 3580, loss: 0.13458499312400818\n","step: 3590, loss: 0.17756392061710358\n","step: 3600, loss: 0.09903282672166824\n","step: 3610, loss: 0.08168699592351913\n","step: 3620, loss: 0.04874616116285324\n","step: 3630, loss: 0.07480931282043457\n","step: 3640, loss: 0.09629801660776138\n","step: 3650, loss: 0.16122174263000488\n","step: 3660, loss: 0.1827325075864792\n","step: 3670, loss: 0.03413671255111694\n","step: 3680, loss: 0.07313545048236847\n","step: 3690, loss: 0.21164554357528687\n","step: 3700, loss: 0.05189726874232292\n","step: 3710, loss: 0.12906570732593536\n","step: 3720, loss: 0.052517473697662354\n","step: 3730, loss: 0.09310884028673172\n","step: 3740, loss: 0.1354326754808426\n","step: 3750, loss: 0.06931459903717041\n","step: 3760, loss: 0.04343508929014206\n","step: 3770, loss: 0.04506310075521469\n","step: 3780, loss: 0.06092355027794838\n","step: 3790, loss: 0.09414476901292801\n","step: 3800, loss: 0.13798847794532776\n","step: 3810, loss: 0.1553383618593216\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.52      0.52      0.52        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.98      0.82      0.89       291\n","           5       0.86      0.84      0.85       294\n","           6       0.95      1.00      0.97      1570\n","           7       0.70      0.94      0.80       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.98      0.97       901\n","          11       0.97      1.00      0.98      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.75      0.92      0.83        13\n","          14       0.37      1.00      0.54        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.82      0.86      0.84      1151\n","          17       0.78      0.98      0.87        41\n","          18       0.97      0.91      0.94        32\n","          19       0.66      0.68      0.67        40\n","          20       0.98      1.00      0.99       584\n","          21       0.10      0.02      0.03        52\n","          22       0.96      0.71      0.81      4175\n","          23       0.65      0.97      0.78      2253\n","          24       0.38      0.25      0.30        44\n","          25       0.86      0.93      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       1.00      0.99      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.92      0.86      0.89      1136\n","          31       0.67      0.53      0.59        19\n","          32       0.78      0.88      0.82         8\n","          33       0.69      0.98      0.81        86\n","          34       0.29      0.75      0.42        32\n","          35       0.97      0.99      0.98       474\n","          36       0.76      0.18      0.29       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.97      0.98      0.97       404\n","          39       0.98      0.92      0.95       485\n","          40       0.92      0.83      0.87       573\n","          41       0.93      0.96      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.96      0.78      0.86       152\n","          44       0.94      0.91      0.93        75\n","          46       0.95      0.96      0.96        82\n","          48       0.38      0.04      0.07        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.79     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 442\n","\n","Loop 46\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8868348598480225\n","step: 10, loss: 1.795323133468628\n","step: 20, loss: 0.6906288266181946\n","step: 30, loss: 0.5298102498054504\n","step: 40, loss: 0.377384215593338\n","step: 50, loss: 0.2771925926208496\n","step: 60, loss: 0.13009336590766907\n","step: 70, loss: 0.22708934545516968\n","step: 80, loss: 0.20638911426067352\n","step: 90, loss: 0.14473019540309906\n","step: 100, loss: 0.09677142649888992\n","step: 110, loss: 0.21193721890449524\n","step: 120, loss: 0.21287600696086884\n","step: 130, loss: 0.19535289704799652\n","step: 140, loss: 0.15549778938293457\n","step: 150, loss: 0.19682380557060242\n","step: 160, loss: 0.1784082055091858\n","step: 170, loss: 0.13034148514270782\n","step: 180, loss: 0.2464001625776291\n","step: 190, loss: 0.11796712130308151\n","step: 200, loss: 0.06220724806189537\n","step: 210, loss: 0.20384465157985687\n","step: 220, loss: 0.2516816258430481\n","step: 230, loss: 0.20446638762950897\n","step: 240, loss: 0.16015049815177917\n","step: 250, loss: 0.2163160741329193\n","step: 260, loss: 0.1748257279396057\n","step: 270, loss: 0.026313496753573418\n","step: 280, loss: 0.11890070885419846\n","step: 290, loss: 0.04205939918756485\n","step: 300, loss: 0.1164124384522438\n","step: 310, loss: 0.12068116664886475\n","step: 320, loss: 0.08317291736602783\n","step: 330, loss: 0.17527303099632263\n","step: 340, loss: 0.13690805435180664\n","step: 350, loss: 0.11447224766016006\n","step: 360, loss: 0.1753239929676056\n","step: 370, loss: 0.12527009844779968\n","step: 380, loss: 0.15514138340950012\n","step: 390, loss: 0.179579496383667\n","step: 400, loss: 0.11969523131847382\n","step: 410, loss: 0.15119269490242004\n","step: 420, loss: 0.24902941286563873\n","step: 430, loss: 0.07192569971084595\n","step: 440, loss: 0.15781717002391815\n","step: 450, loss: 0.23181597888469696\n","step: 460, loss: 0.36501404643058777\n","step: 470, loss: 0.03813707455992699\n","step: 480, loss: 0.08208321034908295\n","step: 490, loss: 0.07924407720565796\n","step: 500, loss: 0.05057063698768616\n","step: 510, loss: 0.08841465413570404\n","step: 520, loss: 0.05050212889909744\n","step: 530, loss: 0.09549588710069656\n","step: 540, loss: 0.19740690290927887\n","step: 550, loss: 0.15194039046764374\n","step: 560, loss: 0.04234979674220085\n","step: 570, loss: 0.04834410175681114\n","step: 580, loss: 0.06094035878777504\n","step: 590, loss: 0.1506892293691635\n","step: 600, loss: 0.09264413267374039\n","step: 610, loss: 0.09613777697086334\n","step: 620, loss: 0.10905597358942032\n","step: 630, loss: 0.17031605541706085\n","step: 640, loss: 0.07839030027389526\n","step: 650, loss: 0.10309826582670212\n","step: 660, loss: 0.05929037183523178\n","step: 670, loss: 0.1138414815068245\n","step: 680, loss: 0.07626362890005112\n","step: 690, loss: 0.06944630295038223\n","step: 700, loss: 0.09940709173679352\n","step: 710, loss: 0.07340787351131439\n","step: 720, loss: 0.09541888535022736\n","step: 730, loss: 0.06813506036996841\n","step: 740, loss: 0.030023645609617233\n","step: 750, loss: 0.07777190208435059\n","step: 760, loss: 0.22748710215091705\n","step: 770, loss: 0.07241485267877579\n","step: 780, loss: 0.12251083552837372\n","step: 790, loss: 0.13283033668994904\n","step: 800, loss: 0.025916432961821556\n","step: 810, loss: 0.10207296907901764\n","step: 820, loss: 0.03453692048788071\n","step: 830, loss: 0.09850726276636124\n","step: 840, loss: 0.08431927859783173\n","step: 850, loss: 0.19954755902290344\n","step: 860, loss: 0.07975742965936661\n","step: 870, loss: 0.10132085531949997\n","step: 880, loss: 0.12057339400053024\n","step: 890, loss: 0.09567465633153915\n","step: 900, loss: 0.0766042098402977\n","step: 910, loss: 0.19421036541461945\n","step: 920, loss: 0.10208989679813385\n","step: 930, loss: 0.1181541234254837\n","step: 940, loss: 0.11264948546886444\n","step: 950, loss: 0.1160687655210495\n","step: 960, loss: 0.020523790270090103\n","step: 970, loss: 0.14829188585281372\n","step: 980, loss: 0.13629868626594543\n","step: 990, loss: 0.3009273111820221\n","step: 1000, loss: 0.13152267038822174\n","step: 1010, loss: 0.09651925414800644\n","step: 1020, loss: 0.07321038842201233\n","step: 1030, loss: 0.12284115701913834\n","step: 1040, loss: 0.06313499063253403\n","step: 1050, loss: 0.02815425954759121\n","step: 1060, loss: 0.03121509961783886\n","step: 1070, loss: 0.04063699394464493\n","step: 1080, loss: 0.16181696951389313\n","step: 1090, loss: 0.07464578747749329\n","step: 1100, loss: 0.1199665367603302\n","step: 1110, loss: 0.10305146127939224\n","step: 1120, loss: 0.07566487044095993\n","step: 1130, loss: 0.06876543164253235\n","step: 1140, loss: 0.10674381256103516\n","step: 1150, loss: 0.18766991794109344\n","step: 1160, loss: 0.055800728499889374\n","step: 1170, loss: 0.11868980526924133\n","step: 1180, loss: 0.08898405730724335\n","step: 1190, loss: 0.06996509432792664\n","step: 1200, loss: 0.04531952366232872\n","step: 1210, loss: 0.08078768104314804\n","step: 1220, loss: 0.07611695677042007\n","step: 1230, loss: 0.24256211519241333\n","step: 1240, loss: 0.11377812922000885\n","step: 1250, loss: 0.19306647777557373\n","step: 1260, loss: 0.17816802859306335\n","step: 1270, loss: 0.1394272893667221\n","step: 1280, loss: 0.1333332657814026\n","step: 1290, loss: 0.12577393651008606\n","step: 1300, loss: 0.07768634706735611\n","step: 1310, loss: 0.10096759349107742\n","step: 1320, loss: 0.19991528987884521\n","step: 1330, loss: 0.15063244104385376\n","step: 1340, loss: 0.11573842912912369\n","step: 1350, loss: 0.1581002026796341\n","step: 1360, loss: 0.12253589928150177\n","step: 1370, loss: 0.09434399008750916\n","step: 1380, loss: 0.07433828711509705\n","step: 1390, loss: 0.13706327974796295\n","step: 1400, loss: 0.11067096889019012\n","step: 1410, loss: 0.023566199466586113\n","step: 1420, loss: 0.15190578997135162\n","step: 1430, loss: 0.13075383007526398\n","step: 1440, loss: 0.17710274457931519\n","step: 1450, loss: 0.13917171955108643\n","step: 1460, loss: 0.07259444892406464\n","step: 1470, loss: 0.15186530351638794\n","step: 1480, loss: 0.13433551788330078\n","step: 1490, loss: 0.08818407356739044\n","step: 1500, loss: 0.10584153980016708\n","step: 1510, loss: 0.06039349362254143\n","step: 1520, loss: 0.06208733096718788\n","step: 1530, loss: 0.08599189668893814\n","step: 1540, loss: 0.04187722131609917\n","step: 1550, loss: 0.1423841118812561\n","step: 1560, loss: 0.07102367281913757\n","step: 1570, loss: 0.10113438963890076\n","step: 1580, loss: 0.09946326911449432\n","step: 1590, loss: 0.07959424704313278\n","step: 1600, loss: 0.17349106073379517\n","step: 1610, loss: 0.05987714231014252\n","step: 1620, loss: 0.17531372606754303\n","step: 1630, loss: 0.08881811052560806\n","step: 1640, loss: 0.19021816551685333\n","step: 1650, loss: 0.21400924026966095\n","step: 1660, loss: 0.11432600766420364\n","step: 1670, loss: 0.10469771921634674\n","step: 1680, loss: 0.11461770534515381\n","step: 1690, loss: 0.08315830677747726\n","step: 1700, loss: 0.07159147411584854\n","step: 1710, loss: 0.05663811415433884\n","step: 1720, loss: 0.0713290423154831\n","step: 1730, loss: 0.1810377687215805\n","step: 1740, loss: 0.13072198629379272\n","step: 1750, loss: 0.16234198212623596\n","step: 1760, loss: 0.04856162145733833\n","step: 1770, loss: 0.0893094465136528\n","step: 1780, loss: 0.05979501083493233\n","step: 1790, loss: 0.04997435584664345\n","step: 1800, loss: 0.1650390326976776\n","step: 1810, loss: 0.021401282399892807\n","step: 1820, loss: 0.050197068601846695\n","step: 1830, loss: 0.1794486790895462\n","step: 1840, loss: 0.06412234902381897\n","step: 1850, loss: 0.1048421859741211\n","step: 1860, loss: 0.12910278141498566\n","step: 1870, loss: 0.1188117042183876\n","step: 1880, loss: 0.08533066511154175\n","step: 1890, loss: 0.05741589888930321\n","step: 1900, loss: 0.09487023949623108\n","step: 1910, loss: 0.14679652452468872\n","step: 1920, loss: 0.045157838612794876\n","step: 1930, loss: 0.033984169363975525\n","step: 1940, loss: 0.08246272802352905\n","step: 1950, loss: 0.053711265325546265\n","step: 1960, loss: 0.18462592363357544\n","step: 1970, loss: 0.12210448831319809\n","step: 1980, loss: 0.01764672063291073\n","step: 1990, loss: 0.09914430975914001\n","step: 2000, loss: 0.03223356977105141\n","step: 2010, loss: 0.042199812829494476\n","step: 2020, loss: 0.14653031527996063\n","step: 2030, loss: 0.07527270913124084\n","step: 2040, loss: 0.08175183087587357\n","step: 2050, loss: 0.022163616493344307\n","step: 2060, loss: 0.1877673715353012\n","step: 2070, loss: 0.05522434413433075\n","step: 2080, loss: 0.11285727471113205\n","step: 2090, loss: 0.11775874346494675\n","step: 2100, loss: 0.22130493819713593\n","step: 2110, loss: 0.02102554216980934\n","step: 2120, loss: 0.09166761487722397\n","step: 2130, loss: 0.1511499285697937\n","step: 2140, loss: 0.07250826060771942\n","step: 2150, loss: 0.06847380846738815\n","step: 2160, loss: 0.04595385864377022\n","step: 2170, loss: 0.03297466039657593\n","step: 2180, loss: 0.13719575107097626\n","step: 2190, loss: 0.06958098709583282\n","step: 2200, loss: 0.05021532252430916\n","step: 2210, loss: 0.09638182073831558\n","step: 2220, loss: 0.054800890386104584\n","step: 2230, loss: 0.02799364924430847\n","step: 2240, loss: 0.07980998605489731\n","step: 2250, loss: 0.051567815244197845\n","step: 2260, loss: 0.09595336019992828\n","step: 2270, loss: 0.15764211118221283\n","step: 2280, loss: 0.0897183045744896\n","step: 2290, loss: 0.03848118335008621\n","step: 2300, loss: 0.15658138692378998\n","step: 2310, loss: 0.14479988813400269\n","step: 2320, loss: 0.02471577562391758\n","step: 2330, loss: 0.04245385900139809\n","step: 2340, loss: 0.06574945151805878\n","step: 2350, loss: 0.0999126210808754\n","step: 2360, loss: 0.16515891253948212\n","step: 2370, loss: 0.11627858132123947\n","step: 2380, loss: 0.058610644191503525\n","step: 2390, loss: 0.09788581728935242\n","step: 2400, loss: 0.06752406805753708\n","step: 2410, loss: 0.18342791497707367\n","step: 2420, loss: 0.03688182681798935\n","step: 2430, loss: 0.11307715624570847\n","step: 2440, loss: 0.03901297226548195\n","step: 2450, loss: 0.06599240750074387\n","step: 2460, loss: 0.030952608212828636\n","step: 2470, loss: 0.06686113029718399\n","step: 2480, loss: 0.07570555806159973\n","step: 2490, loss: 0.07193320989608765\n","step: 2500, loss: 0.06179836392402649\n","step: 2510, loss: 0.09861299395561218\n","step: 2520, loss: 0.08782581984996796\n","step: 2530, loss: 0.10568941384553909\n","step: 2540, loss: 0.1005934551358223\n","step: 2550, loss: 0.0373009517788887\n","step: 2560, loss: 0.047886818647384644\n","step: 2570, loss: 0.060651667416095734\n","step: 2580, loss: 0.12496382743120193\n","step: 2590, loss: 0.09522958099842072\n","step: 2600, loss: 0.08437000215053558\n","step: 2610, loss: 0.0367150753736496\n","step: 2620, loss: 0.05112592503428459\n","step: 2630, loss: 0.08130612969398499\n","step: 2640, loss: 0.09792746603488922\n","step: 2650, loss: 0.11614632606506348\n","step: 2660, loss: 0.11253076046705246\n","step: 2670, loss: 0.06467518955469131\n","step: 2680, loss: 0.1460578739643097\n","step: 2690, loss: 0.09816649556159973\n","step: 2700, loss: 0.05621043220162392\n","step: 2710, loss: 0.11528141796588898\n","step: 2720, loss: 0.0970679372549057\n","step: 2730, loss: 0.06536035239696503\n","step: 2740, loss: 0.17035643756389618\n","step: 2750, loss: 0.09129633009433746\n","step: 2760, loss: 0.09121261537075043\n","step: 2770, loss: 0.062394220381975174\n","step: 2780, loss: 0.15212543308734894\n","step: 2790, loss: 0.04410253092646599\n","step: 2800, loss: 0.05515152961015701\n","step: 2810, loss: 0.05361194163560867\n","step: 2820, loss: 0.06685776263475418\n","step: 2830, loss: 0.12306267023086548\n","step: 2840, loss: 0.11110024899244308\n","step: 2850, loss: 0.0611049123108387\n","step: 2860, loss: 0.16455091536045074\n","step: 2870, loss: 0.09818153828382492\n","step: 2880, loss: 0.07549804449081421\n","step: 2890, loss: 0.07565772533416748\n","step: 2900, loss: 0.03061039000749588\n","step: 2910, loss: 0.040149811655282974\n","step: 2920, loss: 0.09715355932712555\n","step: 2930, loss: 0.046656083315610886\n","step: 2940, loss: 0.11238416284322739\n","step: 2950, loss: 0.07354974001646042\n","step: 2960, loss: 0.15202970802783966\n","step: 2970, loss: 0.106338731944561\n","step: 2980, loss: 0.0998743399977684\n","step: 2990, loss: 0.1350359469652176\n","step: 3000, loss: 0.05377595126628876\n","step: 3010, loss: 0.09185169637203217\n","step: 3020, loss: 0.09735081344842911\n","step: 3030, loss: 0.024074198678135872\n","step: 3040, loss: 0.10514836758375168\n","step: 3050, loss: 0.12886066734790802\n","step: 3060, loss: 0.051847074180841446\n","step: 3070, loss: 0.036415763199329376\n","step: 3080, loss: 0.03429466485977173\n","step: 3090, loss: 0.09992257505655289\n","step: 3100, loss: 0.05093083530664444\n","step: 3110, loss: 0.10159915685653687\n","step: 3120, loss: 0.0889417752623558\n","step: 3130, loss: 0.17163170874118805\n","step: 3140, loss: 0.054626669734716415\n","step: 3150, loss: 0.09240397065877914\n","step: 3160, loss: 0.04110950231552124\n","step: 3170, loss: 0.1078084409236908\n","step: 3180, loss: 0.13282832503318787\n","step: 3190, loss: 0.0681423619389534\n","step: 3200, loss: 0.2658424973487854\n","step: 3210, loss: 0.1281033605337143\n","step: 3220, loss: 0.1812935620546341\n","step: 3230, loss: 0.1644214391708374\n","step: 3240, loss: 0.09934181720018387\n","step: 3250, loss: 0.1268821358680725\n","step: 3260, loss: 0.05171133205294609\n","step: 3270, loss: 0.09973278641700745\n","step: 3280, loss: 0.21547551453113556\n","step: 3290, loss: 0.1640976220369339\n","step: 3300, loss: 0.04839245229959488\n","step: 3310, loss: 0.09667414426803589\n","step: 3320, loss: 0.11126664280891418\n","step: 3330, loss: 0.213328018784523\n","step: 3340, loss: 0.1311899572610855\n","step: 3350, loss: 0.056697994470596313\n","step: 3360, loss: 0.09105238318443298\n","step: 3370, loss: 0.08304055035114288\n","step: 3380, loss: 0.1002117320895195\n","step: 3390, loss: 0.07433344423770905\n","step: 3400, loss: 0.08716165274381638\n","step: 3410, loss: 0.050685662776231766\n","step: 3420, loss: 0.13418449461460114\n","step: 3430, loss: 0.06584323942661285\n","step: 3440, loss: 0.08194208890199661\n","step: 3450, loss: 0.19843104481697083\n","step: 3460, loss: 0.06688855588436127\n","step: 3470, loss: 0.07967770844697952\n","step: 3480, loss: 0.07349926233291626\n","step: 3490, loss: 0.019916359335184097\n","step: 3500, loss: 0.08765557408332825\n","step: 3510, loss: 0.070346899330616\n","step: 3520, loss: 0.04584130272269249\n","step: 3530, loss: 0.06959245353937149\n","step: 3540, loss: 0.027414631098508835\n","step: 3550, loss: 0.17033416032791138\n","step: 3560, loss: 0.06005244329571724\n","step: 3570, loss: 0.07108668237924576\n","step: 3580, loss: 0.156484454870224\n","step: 3590, loss: 0.06861963868141174\n","step: 3600, loss: 0.08150890469551086\n","step: 3610, loss: 0.09083318710327148\n","step: 3620, loss: 0.06275991350412369\n","step: 3630, loss: 0.07103274017572403\n","step: 3640, loss: 0.1361146867275238\n","step: 3650, loss: 0.08913783729076385\n","step: 3660, loss: 0.09648732095956802\n","step: 3670, loss: 0.08001796156167984\n","step: 3680, loss: 0.1374504566192627\n","step: 3690, loss: 0.054850175976753235\n","step: 3700, loss: 0.11419715732336044\n","step: 3710, loss: 0.08708573132753372\n","step: 3720, loss: 0.08867913484573364\n","step: 3730, loss: 0.059380609542131424\n","step: 3740, loss: 0.0903598815202713\n","step: 3750, loss: 0.14754272997379303\n","step: 3760, loss: 0.042430706322193146\n","step: 3770, loss: 0.16965556144714355\n","step: 3780, loss: 0.04525751993060112\n","step: 3790, loss: 0.05722738057374954\n","step: 3800, loss: 0.06095757707953453\n","step: 3810, loss: 0.09128990769386292\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.81      0.97      0.88        35\n","           2       0.83      0.39      0.53        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.94      0.85      0.89       291\n","           5       0.98      0.84      0.90       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.59      0.94      0.73       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.92      0.98      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.41      0.92      0.57        13\n","          14       0.25      1.00      0.39        43\n","          15       0.93      0.98      0.96      2778\n","          16       0.88      0.82      0.85      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.91      1.00      0.96        32\n","          19       1.00      0.03      0.05        40\n","          20       0.99      1.00      1.00       584\n","          21       0.20      0.04      0.06        52\n","          22       0.93      0.78      0.85      4175\n","          23       0.73      0.95      0.82      2253\n","          24       0.46      0.57      0.51        44\n","          25       0.83      0.95      0.89       888\n","          26       0.50      0.11      0.18         9\n","          27       1.00      0.99      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.93      0.83      0.88      1136\n","          31       0.43      0.79      0.56        19\n","          32       1.00      0.62      0.77         8\n","          33       0.66      0.98      0.79        86\n","          34       0.05      0.09      0.07        32\n","          35       0.98      0.99      0.98       474\n","          36       0.93      0.15      0.26       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.98      0.93      0.95       404\n","          39       0.94      0.97      0.95       485\n","          40       0.88      0.97      0.93       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.94      0.89      0.91       152\n","          44       0.89      0.89      0.89        75\n","          46       0.99      0.98      0.98        82\n","          48       1.00      0.48      0.65        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.79      0.76     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 457\n","\n","Loop 47\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.959040880203247\n","step: 10, loss: 2.0310254096984863\n","step: 20, loss: 0.9113648533821106\n","step: 30, loss: 0.443359911441803\n","step: 40, loss: 0.501987636089325\n","step: 50, loss: 0.20850533246994019\n","step: 60, loss: 0.29904690384864807\n","step: 70, loss: 0.2047886997461319\n","step: 80, loss: 0.23451143503189087\n","step: 90, loss: 0.24451617896556854\n","step: 100, loss: 0.13573628664016724\n","step: 110, loss: 0.0750793069601059\n","step: 120, loss: 0.23212003707885742\n","step: 130, loss: 0.1023145392537117\n","step: 140, loss: 0.15250645577907562\n","step: 150, loss: 0.18496663868427277\n","step: 160, loss: 0.13079003989696503\n","step: 170, loss: 0.0729406401515007\n","step: 180, loss: 0.15721918642520905\n","step: 190, loss: 0.19918182492256165\n","step: 200, loss: 0.23211678862571716\n","step: 210, loss: 0.1096203401684761\n","step: 220, loss: 0.1401509791612625\n","step: 230, loss: 0.053563930094242096\n","step: 240, loss: 0.13158375024795532\n","step: 250, loss: 0.1378379613161087\n","step: 260, loss: 0.19387951493263245\n","step: 270, loss: 0.09369819611310959\n","step: 280, loss: 0.13219177722930908\n","step: 290, loss: 0.15187017619609833\n","step: 300, loss: 0.11551564186811447\n","step: 310, loss: 0.10148396342992783\n","step: 320, loss: 0.09217128157615662\n","step: 330, loss: 0.09523393958806992\n","step: 340, loss: 0.11403264850378036\n","step: 350, loss: 0.10836256295442581\n","step: 360, loss: 0.05583542585372925\n","step: 370, loss: 0.14186528325080872\n","step: 380, loss: 0.11877252906560898\n","step: 390, loss: 0.20601189136505127\n","step: 400, loss: 0.03513273596763611\n","step: 410, loss: 0.15308091044425964\n","step: 420, loss: 0.09343285858631134\n","step: 430, loss: 0.08304257690906525\n","step: 440, loss: 0.09059905260801315\n","step: 450, loss: 0.09787625819444656\n","step: 460, loss: 0.09257698804140091\n","step: 470, loss: 0.09830096364021301\n","step: 480, loss: 0.06498205661773682\n","step: 490, loss: 0.07698480039834976\n","step: 500, loss: 0.051963549107313156\n","step: 510, loss: 0.15423350036144257\n","step: 520, loss: 0.14442959427833557\n","step: 530, loss: 0.2094983160495758\n","step: 540, loss: 0.1283658742904663\n","step: 550, loss: 0.08814380317926407\n","step: 560, loss: 0.16537156701087952\n","step: 570, loss: 0.22260336577892303\n","step: 580, loss: 0.07546599954366684\n","step: 590, loss: 0.13208751380443573\n","step: 600, loss: 0.13435444235801697\n","step: 610, loss: 0.11938634514808655\n","step: 620, loss: 0.10009725391864777\n","step: 630, loss: 0.12449956685304642\n","step: 640, loss: 0.0687728226184845\n","step: 650, loss: 0.12723514437675476\n","step: 660, loss: 0.10495328158140182\n","step: 670, loss: 0.08735094964504242\n","step: 680, loss: 0.1473708301782608\n","step: 690, loss: 0.07486323267221451\n","step: 700, loss: 0.033151108771562576\n","step: 710, loss: 0.06821384280920029\n","step: 720, loss: 0.08359917998313904\n","step: 730, loss: 0.11651746183633804\n","step: 740, loss: 0.1383574903011322\n","step: 750, loss: 0.08889109641313553\n","step: 760, loss: 0.07235810160636902\n","step: 770, loss: 0.1537955105304718\n","step: 780, loss: 0.09621262550354004\n","step: 790, loss: 0.11413514614105225\n","step: 800, loss: 0.22368839383125305\n","step: 810, loss: 0.04737001284956932\n","step: 820, loss: 0.041441842913627625\n","step: 830, loss: 0.12540234625339508\n","step: 840, loss: 0.1497579663991928\n","step: 850, loss: 0.06657777726650238\n","step: 860, loss: 0.12042119354009628\n","step: 870, loss: 0.07020904123783112\n","step: 880, loss: 0.08520888537168503\n","step: 890, loss: 0.05585349723696709\n","step: 900, loss: 0.0731266587972641\n","step: 910, loss: 0.05407434329390526\n","step: 920, loss: 0.10123402625322342\n","step: 930, loss: 0.08069171011447906\n","step: 940, loss: 0.11306849867105484\n","step: 950, loss: 0.06028716266155243\n","step: 960, loss: 0.09140138328075409\n","step: 970, loss: 0.12945301830768585\n","step: 980, loss: 0.09965305030345917\n","step: 990, loss: 0.16098250448703766\n","step: 1000, loss: 0.11486711353063583\n","step: 1010, loss: 0.18484359979629517\n","step: 1020, loss: 0.10502928495407104\n","step: 1030, loss: 0.15650220215320587\n","step: 1040, loss: 0.03521471843123436\n","step: 1050, loss: 0.06526389718055725\n","step: 1060, loss: 0.1464543491601944\n","step: 1070, loss: 0.021079307422041893\n","step: 1080, loss: 0.0572781041264534\n","step: 1090, loss: 0.08207016438245773\n","step: 1100, loss: 0.1056165024638176\n","step: 1110, loss: 0.06410829722881317\n","step: 1120, loss: 0.09436012804508209\n","step: 1130, loss: 0.028063248842954636\n","step: 1140, loss: 0.062222130596637726\n","step: 1150, loss: 0.08630220592021942\n","step: 1160, loss: 0.05220072343945503\n","step: 1170, loss: 0.16621972620487213\n","step: 1180, loss: 0.11982864141464233\n","step: 1190, loss: 0.09032969176769257\n","step: 1200, loss: 0.21259024739265442\n","step: 1210, loss: 0.0902668684720993\n","step: 1220, loss: 0.1044483408331871\n","step: 1230, loss: 0.18370509147644043\n","step: 1240, loss: 0.15052326023578644\n","step: 1250, loss: 0.07482045888900757\n","step: 1260, loss: 0.0899219959974289\n","step: 1270, loss: 0.03201911225914955\n","step: 1280, loss: 0.12191102653741837\n","step: 1290, loss: 0.07422255724668503\n","step: 1300, loss: 0.03033040091395378\n","step: 1310, loss: 0.12431745231151581\n","step: 1320, loss: 0.0886298194527626\n","step: 1330, loss: 0.17949433624744415\n","step: 1340, loss: 0.03070606105029583\n","step: 1350, loss: 0.13032488524913788\n","step: 1360, loss: 0.05623859539628029\n","step: 1370, loss: 0.10946435481309891\n","step: 1380, loss: 0.10347206890583038\n","step: 1390, loss: 0.07098209857940674\n","step: 1400, loss: 0.2345731109380722\n","step: 1410, loss: 0.04366393759846687\n","step: 1420, loss: 0.09969686716794968\n","step: 1430, loss: 0.07123994827270508\n","step: 1440, loss: 0.061239439994096756\n","step: 1450, loss: 0.10457468032836914\n","step: 1460, loss: 0.10002803057432175\n","step: 1470, loss: 0.08401509374380112\n","step: 1480, loss: 0.04331072419881821\n","step: 1490, loss: 0.0435771644115448\n","step: 1500, loss: 0.07879775017499924\n","step: 1510, loss: 0.05998200550675392\n","step: 1520, loss: 0.16936448216438293\n","step: 1530, loss: 0.06614178419113159\n","step: 1540, loss: 0.121623694896698\n","step: 1550, loss: 0.12533482909202576\n","step: 1560, loss: 0.1120009496808052\n","step: 1570, loss: 0.10358353704214096\n","step: 1580, loss: 0.14412640035152435\n","step: 1590, loss: 0.11277920752763748\n","step: 1600, loss: 0.2015434056520462\n","step: 1610, loss: 0.17765794694423676\n","step: 1620, loss: 0.06206642463803291\n","step: 1630, loss: 0.09520667791366577\n","step: 1640, loss: 0.082233726978302\n","step: 1650, loss: 0.0792793333530426\n","step: 1660, loss: 0.07087355107069016\n","step: 1670, loss: 0.048872094601392746\n","step: 1680, loss: 0.14261458814144135\n","step: 1690, loss: 0.07075019925832748\n","step: 1700, loss: 0.06769484281539917\n","step: 1710, loss: 0.10127303004264832\n","step: 1720, loss: 0.09853506088256836\n","step: 1730, loss: 0.10496459901332855\n","step: 1740, loss: 0.0912909135222435\n","step: 1750, loss: 0.0800250843167305\n","step: 1760, loss: 0.04108894243836403\n","step: 1770, loss: 0.06083505600690842\n","step: 1780, loss: 0.1255420744419098\n","step: 1790, loss: 0.053022805601358414\n","step: 1800, loss: 0.0398273803293705\n","step: 1810, loss: 0.09327762573957443\n","step: 1820, loss: 0.10604557394981384\n","step: 1830, loss: 0.054203178733587265\n","step: 1840, loss: 0.051808495074510574\n","step: 1850, loss: 0.11337767541408539\n","step: 1860, loss: 0.0997212752699852\n","step: 1870, loss: 0.11128750443458557\n","step: 1880, loss: 0.07895743101835251\n","step: 1890, loss: 0.23957769572734833\n","step: 1900, loss: 0.1500261127948761\n","step: 1910, loss: 0.06674452871084213\n","step: 1920, loss: 0.11371657997369766\n","step: 1930, loss: 0.038181763142347336\n","step: 1940, loss: 0.08729221671819687\n","step: 1950, loss: 0.08269795775413513\n","step: 1960, loss: 0.0323898084461689\n","step: 1970, loss: 0.03636891767382622\n","step: 1980, loss: 0.050070520490407944\n","step: 1990, loss: 0.09074694663286209\n","step: 2000, loss: 0.09068313986063004\n","step: 2010, loss: 0.06928282231092453\n","step: 2020, loss: 0.14428867399692535\n","step: 2030, loss: 0.045393072068691254\n","step: 2040, loss: 0.12631377577781677\n","step: 2050, loss: 0.04596525803208351\n","step: 2060, loss: 0.06139466166496277\n","step: 2070, loss: 0.014622949063777924\n","step: 2080, loss: 0.06941598653793335\n","step: 2090, loss: 0.13347408175468445\n","step: 2100, loss: 0.09552218019962311\n","step: 2110, loss: 0.12936413288116455\n","step: 2120, loss: 0.17118118703365326\n","step: 2130, loss: 0.08012478053569794\n","step: 2140, loss: 0.08928219228982925\n","step: 2150, loss: 0.12450285255908966\n","step: 2160, loss: 0.030421292409300804\n","step: 2170, loss: 0.163020059466362\n","step: 2180, loss: 0.06428156793117523\n","step: 2190, loss: 0.08885093033313751\n","step: 2200, loss: 0.05424666777253151\n","step: 2210, loss: 0.1114768534898758\n","step: 2220, loss: 0.054481085389852524\n","step: 2230, loss: 0.1072712317109108\n","step: 2240, loss: 0.12911809980869293\n","step: 2250, loss: 0.03412637114524841\n","step: 2260, loss: 0.13161244988441467\n","step: 2270, loss: 0.037266433238983154\n","step: 2280, loss: 0.09883035719394684\n","step: 2290, loss: 0.06700905412435532\n","step: 2300, loss: 0.12099746614694595\n","step: 2310, loss: 0.09472420811653137\n","step: 2320, loss: 0.15771542489528656\n","step: 2330, loss: 0.11906154453754425\n","step: 2340, loss: 0.09851094335317612\n","step: 2350, loss: 0.13533487915992737\n","step: 2360, loss: 0.08128924667835236\n","step: 2370, loss: 0.08668924868106842\n","step: 2380, loss: 0.16784028708934784\n","step: 2390, loss: 0.15695540606975555\n","step: 2400, loss: 0.05882731080055237\n","step: 2410, loss: 0.10103088617324829\n","step: 2420, loss: 0.1218840554356575\n","step: 2430, loss: 0.038145795464515686\n","step: 2440, loss: 0.05635914206504822\n","step: 2450, loss: 0.05574161559343338\n","step: 2460, loss: 0.10408292710781097\n","step: 2470, loss: 0.10547082126140594\n","step: 2480, loss: 0.08644174039363861\n","step: 2490, loss: 0.10009293258190155\n","step: 2500, loss: 0.17067283391952515\n","step: 2510, loss: 0.14514009654521942\n","step: 2520, loss: 0.12058628350496292\n","step: 2530, loss: 0.09194444119930267\n","step: 2540, loss: 0.04878586530685425\n","step: 2550, loss: 0.08601551502943039\n","step: 2560, loss: 0.11021360754966736\n","step: 2570, loss: 0.08268610388040543\n","step: 2580, loss: 0.10378097742795944\n","step: 2590, loss: 0.0771331638097763\n","step: 2600, loss: 0.1613965928554535\n","step: 2610, loss: 0.07331159710884094\n","step: 2620, loss: 0.04946913570165634\n","step: 2630, loss: 0.042699769139289856\n","step: 2640, loss: 0.10272674262523651\n","step: 2650, loss: 0.07033604383468628\n","step: 2660, loss: 0.08335340768098831\n","step: 2670, loss: 0.02599176950752735\n","step: 2680, loss: 0.06718633323907852\n","step: 2690, loss: 0.08067470788955688\n","step: 2700, loss: 0.042726095765829086\n","step: 2710, loss: 0.0879121795296669\n","step: 2720, loss: 0.023307733237743378\n","step: 2730, loss: 0.14257651567459106\n","step: 2740, loss: 0.05166766792535782\n","step: 2750, loss: 0.12795718014240265\n","step: 2760, loss: 0.05363134667277336\n","step: 2770, loss: 0.07155555486679077\n","step: 2780, loss: 0.0657164454460144\n","step: 2790, loss: 0.16616205871105194\n","step: 2800, loss: 0.06205794960260391\n","step: 2810, loss: 0.11346286535263062\n","step: 2820, loss: 0.07898417115211487\n","step: 2830, loss: 0.043948907405138016\n","step: 2840, loss: 0.09892112016677856\n","step: 2850, loss: 0.0816422700881958\n","step: 2860, loss: 0.05630270764231682\n","step: 2870, loss: 0.03852352872490883\n","step: 2880, loss: 0.0321982279419899\n","step: 2890, loss: 0.056721124798059464\n","step: 2900, loss: 0.12725140154361725\n","step: 2910, loss: 0.09169097244739532\n","step: 2920, loss: 0.16073255240917206\n","step: 2930, loss: 0.07051797211170197\n","step: 2940, loss: 0.08202492445707321\n","step: 2950, loss: 0.21169915795326233\n","step: 2960, loss: 0.07800237089395523\n","step: 2970, loss: 0.0651659443974495\n","step: 2980, loss: 0.0947745218873024\n","step: 2990, loss: 0.078114815056324\n","step: 3000, loss: 0.023216193541884422\n","step: 3010, loss: 0.020578883588314056\n","step: 3020, loss: 0.05376221984624863\n","step: 3030, loss: 0.06791484355926514\n","step: 3040, loss: 0.03766149654984474\n","step: 3050, loss: 0.05950092151761055\n","step: 3060, loss: 0.2436850666999817\n","step: 3070, loss: 0.0729852095246315\n","step: 3080, loss: 0.036277998238801956\n","step: 3090, loss: 0.14175206422805786\n","step: 3100, loss: 0.1630118042230606\n","step: 3110, loss: 0.07689327746629715\n","step: 3120, loss: 0.10544843226671219\n","step: 3130, loss: 0.056368011981248856\n","step: 3140, loss: 0.0868823230266571\n","step: 3150, loss: 0.05336810275912285\n","step: 3160, loss: 0.06016040965914726\n","step: 3170, loss: 0.10253981500864029\n","step: 3180, loss: 0.02782415598630905\n","step: 3190, loss: 0.08789967745542526\n","step: 3200, loss: 0.12249734252691269\n","step: 3210, loss: 0.08156297355890274\n","step: 3220, loss: 0.07462751120328903\n","step: 3230, loss: 0.07454220950603485\n","step: 3240, loss: 0.04081554710865021\n","step: 3250, loss: 0.10814210027456284\n","step: 3260, loss: 0.11362160742282867\n","step: 3270, loss: 0.02794257551431656\n","step: 3280, loss: 0.05729492008686066\n","step: 3290, loss: 0.09126436710357666\n","step: 3300, loss: 0.1400749236345291\n","step: 3310, loss: 0.04206662252545357\n","step: 3320, loss: 0.056152477860450745\n","step: 3330, loss: 0.03276367112994194\n","step: 3340, loss: 0.07136103510856628\n","step: 3350, loss: 0.12657593190670013\n","step: 3360, loss: 0.1320791095495224\n","step: 3370, loss: 0.13405463099479675\n","step: 3380, loss: 0.12799368798732758\n","step: 3390, loss: 0.05871640890836716\n","step: 3400, loss: 0.19413244724273682\n","step: 3410, loss: 0.10884305089712143\n","step: 3420, loss: 0.08362223207950592\n","step: 3430, loss: 0.12807221710681915\n","step: 3440, loss: 0.08894649893045425\n","step: 3450, loss: 0.029041266068816185\n","step: 3460, loss: 0.04251304641366005\n","step: 3470, loss: 0.10102485120296478\n","step: 3480, loss: 0.11663466691970825\n","step: 3490, loss: 0.09261274337768555\n","step: 3500, loss: 0.10261855274438858\n","step: 3510, loss: 0.10615658015012741\n","step: 3520, loss: 0.06529385596513748\n","step: 3530, loss: 0.10780219733715057\n","step: 3540, loss: 0.13102729618549347\n","step: 3550, loss: 0.11736621707677841\n","step: 3560, loss: 0.08387637138366699\n","step: 3570, loss: 0.08826196193695068\n","step: 3580, loss: 0.04293513298034668\n","step: 3590, loss: 0.07074588537216187\n","step: 3600, loss: 0.060671884566545486\n","step: 3610, loss: 0.0642494335770607\n","step: 3620, loss: 0.10732054710388184\n","step: 3630, loss: 0.0744914636015892\n","step: 3640, loss: 0.0838669091463089\n","step: 3650, loss: 0.03345188498497009\n","step: 3660, loss: 0.016283703967928886\n","step: 3670, loss: 0.06251829117536545\n","step: 3680, loss: 0.05408565327525139\n","step: 3690, loss: 0.0769079253077507\n","step: 3700, loss: 0.07036513090133667\n","step: 3710, loss: 0.1020578145980835\n","step: 3720, loss: 0.10287230461835861\n","step: 3730, loss: 0.14019159972667694\n","step: 3740, loss: 0.05795636400580406\n","step: 3750, loss: 0.05673682689666748\n","step: 3760, loss: 0.05762999504804611\n","step: 3770, loss: 0.11876830458641052\n","step: 3780, loss: 0.09161802381277084\n","step: 3790, loss: 0.15123985707759857\n","step: 3800, loss: 0.0497807078063488\n","step: 3810, loss: 0.096637062728405\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.74      0.40      0.52        77\n","           3       0.99      0.79      0.88      1030\n","           4       0.94      0.84      0.89       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.93      0.98      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.50      0.77      0.61        13\n","          14       0.28      1.00      0.44        43\n","          15       0.98      0.96      0.97      2778\n","          16       0.86      0.84      0.85      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.97      0.91      0.94        32\n","          19       0.60      0.07      0.13        40\n","          20       0.99      1.00      1.00       584\n","          21       0.39      0.13      0.20        52\n","          22       0.92      0.74      0.82      4175\n","          23       0.68      0.96      0.79      2253\n","          24       0.33      0.61      0.43        44\n","          25       0.86      0.91      0.88       888\n","          26       0.82      1.00      0.90         9\n","          27       1.00      0.97      0.99        69\n","          28       0.99      0.99      0.99      1864\n","          29       1.00      0.97      0.99       344\n","          30       0.92      0.87      0.90      1136\n","          31       0.65      0.58      0.61        19\n","          32       0.78      0.88      0.82         8\n","          33       0.53      0.99      0.69        86\n","          34       0.21      0.53      0.30        32\n","          35       0.99      0.99      0.99       474\n","          36       1.00      0.12      0.21       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.97      0.97      0.97       404\n","          39       0.97      0.94      0.95       485\n","          40       0.91      0.95      0.93       573\n","          41       0.96      0.93      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.98      0.90      0.94       152\n","          44       0.88      0.95      0.91        75\n","          46       1.00      0.93      0.96        82\n","          48       0.91      0.13      0.22        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.81      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 450\n","\n","Loop 48\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.960095167160034\n","step: 10, loss: 1.831671118736267\n","step: 20, loss: 0.7679698467254639\n","step: 30, loss: 0.4193817675113678\n","step: 40, loss: 0.18262892961502075\n","step: 50, loss: 0.3013148903846741\n","step: 60, loss: 0.21076831221580505\n","step: 70, loss: 0.1683776080608368\n","step: 80, loss: 0.1768563985824585\n","step: 90, loss: 0.13118433952331543\n","step: 100, loss: 0.13871490955352783\n","step: 110, loss: 0.08509650826454163\n","step: 120, loss: 0.1894870400428772\n","step: 130, loss: 0.17446064949035645\n","step: 140, loss: 0.16969835758209229\n","step: 150, loss: 0.16423174738883972\n","step: 160, loss: 0.13391868770122528\n","step: 170, loss: 0.12463785707950592\n","step: 180, loss: 0.14621633291244507\n","step: 190, loss: 0.16642646491527557\n","step: 200, loss: 0.14482979476451874\n","step: 210, loss: 0.19682151079177856\n","step: 220, loss: 0.1453271508216858\n","step: 230, loss: 0.1303224265575409\n","step: 240, loss: 0.09301222860813141\n","step: 250, loss: 0.08455400913953781\n","step: 260, loss: 0.07143307477235794\n","step: 270, loss: 0.2003258466720581\n","step: 280, loss: 0.10906342417001724\n","step: 290, loss: 0.06196511164307594\n","step: 300, loss: 0.2164078652858734\n","step: 310, loss: 0.10577495396137238\n","step: 320, loss: 0.07441282272338867\n","step: 330, loss: 0.10807447880506516\n","step: 340, loss: 0.1580505520105362\n","step: 350, loss: 0.1518995761871338\n","step: 360, loss: 0.13953393697738647\n","step: 370, loss: 0.0526939332485199\n","step: 380, loss: 0.13376252353191376\n","step: 390, loss: 0.10413290560245514\n","step: 400, loss: 0.16391143202781677\n","step: 410, loss: 0.09618407487869263\n","step: 420, loss: 0.13055436313152313\n","step: 430, loss: 0.08195995539426804\n","step: 440, loss: 0.17187032103538513\n","step: 450, loss: 0.09435372799634933\n","step: 460, loss: 0.11828125268220901\n","step: 470, loss: 0.22139717638492584\n","step: 480, loss: 0.06477027386426926\n","step: 490, loss: 0.12165791541337967\n","step: 500, loss: 0.11276297271251678\n","step: 510, loss: 0.05899488553404808\n","step: 520, loss: 0.15054520964622498\n","step: 530, loss: 0.17849428951740265\n","step: 540, loss: 0.2322169542312622\n","step: 550, loss: 0.13933542370796204\n","step: 560, loss: 0.08794686198234558\n","step: 570, loss: 0.07042640447616577\n","step: 580, loss: 0.06371204555034637\n","step: 590, loss: 0.169131338596344\n","step: 600, loss: 0.14276330173015594\n","step: 610, loss: 0.059688109904527664\n","step: 620, loss: 0.15606072545051575\n","step: 630, loss: 0.08688835799694061\n","step: 640, loss: 0.06930909305810928\n","step: 650, loss: 0.0679391473531723\n","step: 660, loss: 0.11680947989225388\n","step: 670, loss: 0.07597453892230988\n","step: 680, loss: 0.1717177778482437\n","step: 690, loss: 0.015641914680600166\n","step: 700, loss: 0.10269401967525482\n","step: 710, loss: 0.04709047079086304\n","step: 720, loss: 0.0555235855281353\n","step: 730, loss: 0.195978045463562\n","step: 740, loss: 0.03980621322989464\n","step: 750, loss: 0.3979687988758087\n","step: 760, loss: 0.032114021480083466\n","step: 770, loss: 0.13156729936599731\n","step: 780, loss: 0.21810099482536316\n","step: 790, loss: 0.1517597883939743\n","step: 800, loss: 0.08031942695379257\n","step: 810, loss: 0.09746427088975906\n","step: 820, loss: 0.12491331994533539\n","step: 830, loss: 0.0946032926440239\n","step: 840, loss: 0.0572206974029541\n","step: 850, loss: 0.0781455785036087\n","step: 860, loss: 0.04672715440392494\n","step: 870, loss: 0.0843142718076706\n","step: 880, loss: 0.15786071121692657\n","step: 890, loss: 0.09267949312925339\n","step: 900, loss: 0.17520606517791748\n","step: 910, loss: 0.08332231640815735\n","step: 920, loss: 0.13326506316661835\n","step: 930, loss: 0.09163785725831985\n","step: 940, loss: 0.09937126934528351\n","step: 950, loss: 0.0645330548286438\n","step: 960, loss: 0.07702261209487915\n","step: 970, loss: 0.12990425527095795\n","step: 980, loss: 0.06667885184288025\n","step: 990, loss: 0.09323333203792572\n","step: 1000, loss: 0.0536041222512722\n","step: 1010, loss: 0.07838741689920425\n","step: 1020, loss: 0.14690543711185455\n","step: 1030, loss: 0.08042646944522858\n","step: 1040, loss: 0.10269684344530106\n","step: 1050, loss: 0.09205274283885956\n","step: 1060, loss: 0.03270488232374191\n","step: 1070, loss: 0.05659239739179611\n","step: 1080, loss: 0.07467798888683319\n","step: 1090, loss: 0.14967410266399384\n","step: 1100, loss: 0.16908104717731476\n","step: 1110, loss: 0.11594868451356888\n","step: 1120, loss: 0.25888514518737793\n","step: 1130, loss: 0.03827068954706192\n","step: 1140, loss: 0.15165840089321136\n","step: 1150, loss: 0.1253170520067215\n","step: 1160, loss: 0.13837003707885742\n","step: 1170, loss: 0.1201971247792244\n","step: 1180, loss: 0.20121002197265625\n","step: 1190, loss: 0.06675737351179123\n","step: 1200, loss: 0.08730239421129227\n","step: 1210, loss: 0.014101440086960793\n","step: 1220, loss: 0.08996511995792389\n","step: 1230, loss: 0.0349937342107296\n","step: 1240, loss: 0.04796319827437401\n","step: 1250, loss: 0.11025743931531906\n","step: 1260, loss: 0.04538324475288391\n","step: 1270, loss: 0.09071517735719681\n","step: 1280, loss: 0.11903540790081024\n","step: 1290, loss: 0.12947292625904083\n","step: 1300, loss: 0.1101398766040802\n","step: 1310, loss: 0.06231790408492088\n","step: 1320, loss: 0.11267128586769104\n","step: 1330, loss: 0.0894269123673439\n","step: 1340, loss: 0.04387793317437172\n","step: 1350, loss: 0.033850159496068954\n","step: 1360, loss: 0.12886548042297363\n","step: 1370, loss: 0.05195755138993263\n","step: 1380, loss: 0.1742783635854721\n","step: 1390, loss: 0.06202702224254608\n","step: 1400, loss: 0.0703180581331253\n","step: 1410, loss: 0.027653666213154793\n","step: 1420, loss: 0.011713891290128231\n","step: 1430, loss: 0.03646673634648323\n","step: 1440, loss: 0.13722766935825348\n","step: 1450, loss: 0.10575637966394424\n","step: 1460, loss: 0.11039968580007553\n","step: 1470, loss: 0.03965942934155464\n","step: 1480, loss: 0.12320587784051895\n","step: 1490, loss: 0.19826765358448029\n","step: 1500, loss: 0.18060162663459778\n","step: 1510, loss: 0.07301585376262665\n","step: 1520, loss: 0.07612684369087219\n","step: 1530, loss: 0.16101139783859253\n","step: 1540, loss: 0.08810976147651672\n","step: 1550, loss: 0.08566702157258987\n","step: 1560, loss: 0.06865152716636658\n","step: 1570, loss: 0.07462147623300552\n","step: 1580, loss: 0.11631253361701965\n","step: 1590, loss: 0.09888807684183121\n","step: 1600, loss: 0.11724015325307846\n","step: 1610, loss: 0.06880446523427963\n","step: 1620, loss: 0.032266877591609955\n","step: 1630, loss: 0.06502287089824677\n","step: 1640, loss: 0.10099968314170837\n","step: 1650, loss: 0.0572405681014061\n","step: 1660, loss: 0.09095171838998795\n","step: 1670, loss: 0.09659411013126373\n","step: 1680, loss: 0.11483542621135712\n","step: 1690, loss: 0.09140387922525406\n","step: 1700, loss: 0.09155517816543579\n","step: 1710, loss: 0.02699892222881317\n","step: 1720, loss: 0.12471280246973038\n","step: 1730, loss: 0.13330033421516418\n","step: 1740, loss: 0.12933249771595\n","step: 1750, loss: 0.07456420361995697\n","step: 1760, loss: 0.13199208676815033\n","step: 1770, loss: 0.023377979174256325\n","step: 1780, loss: 0.17998194694519043\n","step: 1790, loss: 0.04222903028130531\n","step: 1800, loss: 0.08109433948993683\n","step: 1810, loss: 0.055287305265665054\n","step: 1820, loss: 0.08617641031742096\n","step: 1830, loss: 0.08332566171884537\n","step: 1840, loss: 0.13331077992916107\n","step: 1850, loss: 0.19487018883228302\n","step: 1860, loss: 0.10219451785087585\n","step: 1870, loss: 0.07952047884464264\n","step: 1880, loss: 0.07476219534873962\n","step: 1890, loss: 0.08826902508735657\n","step: 1900, loss: 0.08501835912466049\n","step: 1910, loss: 0.10523352026939392\n","step: 1920, loss: 0.0628020316362381\n","step: 1930, loss: 0.09279913455247879\n","step: 1940, loss: 0.0645148977637291\n","step: 1950, loss: 0.0792243555188179\n","step: 1960, loss: 0.1661963015794754\n","step: 1970, loss: 0.13474974036216736\n","step: 1980, loss: 0.07470665872097015\n","step: 1990, loss: 0.16296935081481934\n","step: 2000, loss: 0.06489921361207962\n","step: 2010, loss: 0.06772562861442566\n","step: 2020, loss: 0.015771731734275818\n","step: 2030, loss: 0.0608537383377552\n","step: 2040, loss: 0.09054673463106155\n","step: 2050, loss: 0.07228939235210419\n","step: 2060, loss: 0.04211922362446785\n","step: 2070, loss: 0.10148193687200546\n","step: 2080, loss: 0.150575190782547\n","step: 2090, loss: 0.08125767111778259\n","step: 2100, loss: 0.12359600514173508\n","step: 2110, loss: 0.14336742460727692\n","step: 2120, loss: 0.1325620412826538\n","step: 2130, loss: 0.1696675717830658\n","step: 2140, loss: 0.10241234302520752\n","step: 2150, loss: 0.0662994310259819\n","step: 2160, loss: 0.06633353978395462\n","step: 2170, loss: 0.019531184807419777\n","step: 2180, loss: 0.09448623657226562\n","step: 2190, loss: 0.03425660356879234\n","step: 2200, loss: 0.07216925919055939\n","step: 2210, loss: 0.04474753141403198\n","step: 2220, loss: 0.14552980661392212\n","step: 2230, loss: 0.03905228152871132\n","step: 2240, loss: 0.17566414177417755\n","step: 2250, loss: 0.061270833015441895\n","step: 2260, loss: 0.07920566201210022\n","step: 2270, loss: 0.10433322191238403\n","step: 2280, loss: 0.04318925738334656\n","step: 2290, loss: 0.02581440471112728\n","step: 2300, loss: 0.1418922394514084\n","step: 2310, loss: 0.06785927712917328\n","step: 2320, loss: 0.03487151116132736\n","step: 2330, loss: 0.06359636783599854\n","step: 2340, loss: 0.2103356570005417\n","step: 2350, loss: 0.07996159791946411\n","step: 2360, loss: 0.10504555702209473\n","step: 2370, loss: 0.04761967062950134\n","step: 2380, loss: 0.06550173461437225\n","step: 2390, loss: 0.08276762068271637\n","step: 2400, loss: 0.07338911294937134\n","step: 2410, loss: 0.1701076775789261\n","step: 2420, loss: 0.14528760313987732\n","step: 2430, loss: 0.11108734458684921\n","step: 2440, loss: 0.04713457077741623\n","step: 2450, loss: 0.06633812189102173\n","step: 2460, loss: 0.15507113933563232\n","step: 2470, loss: 0.05185853689908981\n","step: 2480, loss: 0.10083817690610886\n","step: 2490, loss: 0.14487113058567047\n","step: 2500, loss: 0.034696802496910095\n","step: 2510, loss: 0.10037477314472198\n","step: 2520, loss: 0.18014147877693176\n","step: 2530, loss: 0.019037365913391113\n","step: 2540, loss: 0.1144057884812355\n","step: 2550, loss: 0.1033257469534874\n","step: 2560, loss: 0.0724375918507576\n","step: 2570, loss: 0.09854473173618317\n","step: 2580, loss: 0.08112845569849014\n","step: 2590, loss: 0.06704644858837128\n","step: 2600, loss: 0.07333530485630035\n","step: 2610, loss: 0.14597424864768982\n","step: 2620, loss: 0.030741209164261818\n","step: 2630, loss: 0.05454449728131294\n","step: 2640, loss: 0.08139101415872574\n","step: 2650, loss: 0.06426888704299927\n","step: 2660, loss: 0.06470263004302979\n","step: 2670, loss: 0.051142651587724686\n","step: 2680, loss: 0.03911948576569557\n","step: 2690, loss: 0.1696096807718277\n","step: 2700, loss: 0.09076977521181107\n","step: 2710, loss: 0.04505544528365135\n","step: 2720, loss: 0.043333280831575394\n","step: 2730, loss: 0.07227252423763275\n","step: 2740, loss: 0.027032170444726944\n","step: 2750, loss: 0.04220465570688248\n","step: 2760, loss: 0.0760723203420639\n","step: 2770, loss: 0.11140000820159912\n","step: 2780, loss: 0.06585688143968582\n","step: 2790, loss: 0.052133575081825256\n","step: 2800, loss: 0.11853598803281784\n","step: 2810, loss: 0.08507231622934341\n","step: 2820, loss: 0.07430531829595566\n","step: 2830, loss: 0.1418832540512085\n","step: 2840, loss: 0.10371486842632294\n","step: 2850, loss: 0.07137110829353333\n","step: 2860, loss: 0.10914178192615509\n","step: 2870, loss: 0.09262324124574661\n","step: 2880, loss: 0.1249585896730423\n","step: 2890, loss: 0.09838345646858215\n","step: 2900, loss: 0.0932847335934639\n","step: 2910, loss: 0.05190190300345421\n","step: 2920, loss: 0.179586261510849\n","step: 2930, loss: 0.12223789840936661\n","step: 2940, loss: 0.07705260813236237\n","step: 2950, loss: 0.07996152341365814\n","step: 2960, loss: 0.054065726697444916\n","step: 2970, loss: 0.08660407364368439\n","step: 2980, loss: 0.07508350163698196\n","step: 2990, loss: 0.10618235915899277\n","step: 3000, loss: 0.08493628352880478\n","step: 3010, loss: 0.08895814418792725\n","step: 3020, loss: 0.11706891655921936\n","step: 3030, loss: 0.04784683510661125\n","step: 3040, loss: 0.07106556743383408\n","step: 3050, loss: 0.052238114178180695\n","step: 3060, loss: 0.05465098097920418\n","step: 3070, loss: 0.09094507992267609\n","step: 3080, loss: 0.09510070085525513\n","step: 3090, loss: 0.11090853065252304\n","step: 3100, loss: 0.0708494707942009\n","step: 3110, loss: 0.04366796091198921\n","step: 3120, loss: 0.11888647079467773\n","step: 3130, loss: 0.09485707432031631\n","step: 3140, loss: 0.03909063711762428\n","step: 3150, loss: 0.03292817249894142\n","step: 3160, loss: 0.06161369010806084\n","step: 3170, loss: 0.08148626983165741\n","step: 3180, loss: 0.1128753274679184\n","step: 3190, loss: 0.1607612818479538\n","step: 3200, loss: 0.08483103662729263\n","step: 3210, loss: 0.0581255704164505\n","step: 3220, loss: 0.045673247426748276\n","step: 3230, loss: 0.06340374052524567\n","step: 3240, loss: 0.08168087154626846\n","step: 3250, loss: 0.05445907264947891\n","step: 3260, loss: 0.07626774162054062\n","step: 3270, loss: 0.027608124539256096\n","step: 3280, loss: 0.12678349018096924\n","step: 3290, loss: 0.05428513512015343\n","step: 3300, loss: 0.07087019085884094\n","step: 3310, loss: 0.045957647264003754\n","step: 3320, loss: 0.03938891738653183\n","step: 3330, loss: 0.08967456966638565\n","step: 3340, loss: 0.0950901061296463\n","step: 3350, loss: 0.02466866932809353\n","step: 3360, loss: 0.12206907570362091\n","step: 3370, loss: 0.09169600903987885\n","step: 3380, loss: 0.10380584001541138\n","step: 3390, loss: 0.1208469569683075\n","step: 3400, loss: 0.08893897384405136\n","step: 3410, loss: 0.07528197020292282\n","step: 3420, loss: 0.023635579273104668\n","step: 3430, loss: 0.07955910265445709\n","step: 3440, loss: 0.051268890500068665\n","step: 3450, loss: 0.09394368529319763\n","step: 3460, loss: 0.07941178977489471\n","step: 3470, loss: 0.028952226042747498\n","step: 3480, loss: 0.16479098796844482\n","step: 3490, loss: 0.06519974023103714\n","step: 3500, loss: 0.11560268700122833\n","step: 3510, loss: 0.07435885071754456\n","step: 3520, loss: 0.13012388348579407\n","step: 3530, loss: 0.07675769925117493\n","step: 3540, loss: 0.10759656131267548\n","step: 3550, loss: 0.08898791670799255\n","step: 3560, loss: 0.14357981085777283\n","step: 3570, loss: 0.048582129180431366\n","step: 3580, loss: 0.06886325031518936\n","step: 3590, loss: 0.03936542570590973\n","step: 3600, loss: 0.04831787198781967\n","step: 3610, loss: 0.01741536147892475\n","step: 3620, loss: 0.12489888817071915\n","step: 3630, loss: 0.06851371377706528\n","step: 3640, loss: 0.05430714413523674\n","step: 3650, loss: 0.1236632913351059\n","step: 3660, loss: 0.05994138494133949\n","step: 3670, loss: 0.07123322039842606\n","step: 3680, loss: 0.15835444629192352\n","step: 3690, loss: 0.06919067353010178\n","step: 3700, loss: 0.06079009175300598\n","step: 3710, loss: 0.10587741434574127\n","step: 3720, loss: 0.09591102600097656\n","step: 3730, loss: 0.068636454641819\n","step: 3740, loss: 0.06232234835624695\n","step: 3750, loss: 0.07689421623945236\n","step: 3760, loss: 0.09429474920034409\n","step: 3770, loss: 0.06555142998695374\n","step: 3780, loss: 0.1441202014684677\n","step: 3790, loss: 0.06695839017629623\n","step: 3800, loss: 0.049776315689086914\n","step: 3810, loss: 0.040886592119932175\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.83      1.00      0.91        35\n","           2       0.71      0.47      0.56        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.80      0.88       291\n","           5       0.99      0.82      0.90       294\n","           6       0.98      0.98      0.98      1570\n","           7       0.56      0.94      0.70       186\n","           8       0.00      0.00      0.00        11\n","           9       0.97      0.99      0.98       689\n","          10       0.91      0.97      0.94       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.25      0.92      0.39        13\n","          14       0.31      1.00      0.48        43\n","          15       0.97      0.98      0.97      2778\n","          16       0.89      0.82      0.85      1151\n","          17       0.93      0.93      0.93        41\n","          18       1.00      0.97      0.98        32\n","          19       0.39      0.65      0.49        40\n","          20       1.00      1.00      1.00       584\n","          21       0.36      0.17      0.23        52\n","          22       0.95      0.72      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.24      0.55      0.34        44\n","          25       0.85      0.90      0.87       888\n","          26       0.90      1.00      0.95         9\n","          27       0.94      0.97      0.96        69\n","          28       1.00      0.97      0.99      1864\n","          29       0.97      0.99      0.98       344\n","          30       0.89      0.89      0.89      1136\n","          31       0.61      0.74      0.67        19\n","          32       1.00      1.00      1.00         8\n","          33       0.70      0.97      0.81        86\n","          34       0.24      0.78      0.37        32\n","          35       0.98      0.99      0.98       474\n","          36       0.91      0.16      0.27       182\n","          37       0.89      0.97      0.93      1592\n","          38       0.93      0.98      0.95       404\n","          39       0.97      0.95      0.96       485\n","          40       0.92      0.90      0.91       573\n","          41       0.97      0.94      0.95       841\n","          42       0.98      0.99      0.99       575\n","          43       0.96      0.82      0.88       152\n","          44       0.88      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.83      0.79     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 443\n","\n","Loop 49\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.916170835494995\n","step: 10, loss: 1.982944130897522\n","step: 20, loss: 0.7716990113258362\n","step: 30, loss: 0.626810610294342\n","step: 40, loss: 0.291324645280838\n","step: 50, loss: 0.19179773330688477\n","step: 60, loss: 0.1934954971075058\n","step: 70, loss: 0.2348651885986328\n","step: 80, loss: 0.20660999417304993\n","step: 90, loss: 0.1661296933889389\n","step: 100, loss: 0.1051936075091362\n","step: 110, loss: 0.17230038344860077\n","step: 120, loss: 0.14863477647304535\n","step: 130, loss: 0.23250268399715424\n","step: 140, loss: 0.10126712173223495\n","step: 150, loss: 0.15330666303634644\n","step: 160, loss: 0.3045642077922821\n","step: 170, loss: 0.15333598852157593\n","step: 180, loss: 0.1344618946313858\n","step: 190, loss: 0.18856285512447357\n","step: 200, loss: 0.11375031620264053\n","step: 210, loss: 0.13073758780956268\n","step: 220, loss: 0.08021168410778046\n","step: 230, loss: 0.0971570760011673\n","step: 240, loss: 0.10998137295246124\n","step: 250, loss: 0.18573929369449615\n","step: 260, loss: 0.12979677319526672\n","step: 270, loss: 0.17929816246032715\n","step: 280, loss: 0.22484350204467773\n","step: 290, loss: 0.14178869128227234\n","step: 300, loss: 0.14422646164894104\n","step: 310, loss: 0.09601227939128876\n","step: 320, loss: 0.06928035616874695\n","step: 330, loss: 0.18051587045192719\n","step: 340, loss: 0.05512697622179985\n","step: 350, loss: 0.169828400015831\n","step: 360, loss: 0.11337314546108246\n","step: 370, loss: 0.1357256919145584\n","step: 380, loss: 0.12844793498516083\n","step: 390, loss: 0.13499167561531067\n","step: 400, loss: 0.033649396151304245\n","step: 410, loss: 0.062173012644052505\n","step: 420, loss: 0.1720656007528305\n","step: 430, loss: 0.09367766231298447\n","step: 440, loss: 0.09876377135515213\n","step: 450, loss: 0.1276043802499771\n","step: 460, loss: 0.04765608161687851\n","step: 470, loss: 0.053764019161462784\n","step: 480, loss: 0.12414445728063583\n","step: 490, loss: 0.09421098232269287\n","step: 500, loss: 0.11346732825040817\n","step: 510, loss: 0.11573513597249985\n","step: 520, loss: 0.06010740250349045\n","step: 530, loss: 0.16001635789871216\n","step: 540, loss: 0.09440456330776215\n","step: 550, loss: 0.07137051224708557\n","step: 560, loss: 0.09898729622364044\n","step: 570, loss: 0.2244287133216858\n","step: 580, loss: 0.10393764078617096\n","step: 590, loss: 0.08459266275167465\n","step: 600, loss: 0.07401367276906967\n","step: 610, loss: 0.08294760435819626\n","step: 620, loss: 0.08330640941858292\n","step: 630, loss: 0.11079869419336319\n","step: 640, loss: 0.09980413317680359\n","step: 650, loss: 0.07788251340389252\n","step: 660, loss: 0.14755386114120483\n","step: 670, loss: 0.06882327049970627\n","step: 680, loss: 0.048321329057216644\n","step: 690, loss: 0.15783263742923737\n","step: 700, loss: 0.15660522878170013\n","step: 710, loss: 0.10075874626636505\n","step: 720, loss: 0.12526050209999084\n","step: 730, loss: 0.20887605845928192\n","step: 740, loss: 0.09008316695690155\n","step: 750, loss: 0.12967118620872498\n","step: 760, loss: 0.18441326916217804\n","step: 770, loss: 0.14022000133991241\n","step: 780, loss: 0.044860731810331345\n","step: 790, loss: 0.14223939180374146\n","step: 800, loss: 0.12757164239883423\n","step: 810, loss: 0.13887634873390198\n","step: 820, loss: 0.10183685272932053\n","step: 830, loss: 0.06987326592206955\n","step: 840, loss: 0.07259918004274368\n","step: 850, loss: 0.10955461114645004\n","step: 860, loss: 0.1406589150428772\n","step: 870, loss: 0.04481315240263939\n","step: 880, loss: 0.07345129549503326\n","step: 890, loss: 0.08629700541496277\n","step: 900, loss: 0.12297549098730087\n","step: 910, loss: 0.06521324068307877\n","step: 920, loss: 0.13459531962871552\n","step: 930, loss: 0.08788810670375824\n","step: 940, loss: 0.0680052787065506\n","step: 950, loss: 0.08334839344024658\n","step: 960, loss: 0.036544714123010635\n","step: 970, loss: 0.05602991580963135\n","step: 980, loss: 0.23149369657039642\n","step: 990, loss: 0.17115050554275513\n","step: 1000, loss: 0.10536357760429382\n","step: 1010, loss: 0.09575877338647842\n","step: 1020, loss: 0.12768353521823883\n","step: 1030, loss: 0.10939811915159225\n","step: 1040, loss: 0.05651714280247688\n","step: 1050, loss: 0.08720259368419647\n","step: 1060, loss: 0.15354159474372864\n","step: 1070, loss: 0.07912082970142365\n","step: 1080, loss: 0.10943540930747986\n","step: 1090, loss: 0.04023691639304161\n","step: 1100, loss: 0.030412564054131508\n","step: 1110, loss: 0.23529277741909027\n","step: 1120, loss: 0.08149126172065735\n","step: 1130, loss: 0.05447478964924812\n","step: 1140, loss: 0.058792319148778915\n","step: 1150, loss: 0.07662104070186615\n","step: 1160, loss: 0.12990237772464752\n","step: 1170, loss: 0.07334070652723312\n","step: 1180, loss: 0.15849438309669495\n","step: 1190, loss: 0.09545283764600754\n","step: 1200, loss: 0.07335090637207031\n","step: 1210, loss: 0.1512269824743271\n","step: 1220, loss: 0.10262774676084518\n","step: 1230, loss: 0.10252636671066284\n","step: 1240, loss: 0.13412807881832123\n","step: 1250, loss: 0.025387099012732506\n","step: 1260, loss: 0.06506500393152237\n","step: 1270, loss: 0.11025223881006241\n","step: 1280, loss: 0.15639176964759827\n","step: 1290, loss: 0.11362787336111069\n","step: 1300, loss: 0.1425943225622177\n","step: 1310, loss: 0.10605604201555252\n","step: 1320, loss: 0.03621770441532135\n","step: 1330, loss: 0.24782222509384155\n","step: 1340, loss: 0.11695358902215958\n","step: 1350, loss: 0.04332445189356804\n","step: 1360, loss: 0.04217997565865517\n","step: 1370, loss: 0.07999329268932343\n","step: 1380, loss: 0.1031714603304863\n","step: 1390, loss: 0.11028005182743073\n","step: 1400, loss: 0.11204037815332413\n","step: 1410, loss: 0.03726629912853241\n","step: 1420, loss: 0.03716553747653961\n","step: 1430, loss: 0.20555122196674347\n","step: 1440, loss: 0.09226483851671219\n","step: 1450, loss: 0.08547621220350266\n","step: 1460, loss: 0.1390247493982315\n","step: 1470, loss: 0.0572221614420414\n","step: 1480, loss: 0.12730474770069122\n","step: 1490, loss: 0.1974659115076065\n","step: 1500, loss: 0.09747811406850815\n","step: 1510, loss: 0.11041499674320221\n","step: 1520, loss: 0.10609890520572662\n","step: 1530, loss: 0.06321742385625839\n","step: 1540, loss: 0.08623099327087402\n","step: 1550, loss: 0.16835902631282806\n","step: 1560, loss: 0.1634959876537323\n","step: 1570, loss: 0.033007338643074036\n","step: 1580, loss: 0.06396558880805969\n","step: 1590, loss: 0.12211106717586517\n","step: 1600, loss: 0.15474830567836761\n","step: 1610, loss: 0.06851909309625626\n","step: 1620, loss: 0.1227339580655098\n","step: 1630, loss: 0.09018893539905548\n","step: 1640, loss: 0.1632615327835083\n","step: 1650, loss: 0.08383359014987946\n","step: 1660, loss: 0.06096022203564644\n","step: 1670, loss: 0.053149934858083725\n","step: 1680, loss: 0.050586406141519547\n","step: 1690, loss: 0.12220644950866699\n","step: 1700, loss: 0.13529756665229797\n","step: 1710, loss: 0.06585866212844849\n","step: 1720, loss: 0.09276837110519409\n","step: 1730, loss: 0.08944052457809448\n","step: 1740, loss: 0.08052074164152145\n","step: 1750, loss: 0.0838320329785347\n","step: 1760, loss: 0.04778492823243141\n","step: 1770, loss: 0.16805270314216614\n","step: 1780, loss: 0.07850813865661621\n","step: 1790, loss: 0.11964015662670135\n","step: 1800, loss: 0.067054882645607\n","step: 1810, loss: 0.058171533048152924\n","step: 1820, loss: 0.08590982109308243\n","step: 1830, loss: 0.11881835013628006\n","step: 1840, loss: 0.14554442465305328\n","step: 1850, loss: 0.1101856380701065\n","step: 1860, loss: 0.06481873244047165\n","step: 1870, loss: 0.24629952013492584\n","step: 1880, loss: 0.16867288947105408\n","step: 1890, loss: 0.059379663318395615\n","step: 1900, loss: 0.10231092572212219\n","step: 1910, loss: 0.06042539328336716\n","step: 1920, loss: 0.08996660262346268\n","step: 1930, loss: 0.08210229128599167\n","step: 1940, loss: 0.06799601763486862\n","step: 1950, loss: 0.1594419777393341\n","step: 1960, loss: 0.07593069225549698\n","step: 1970, loss: 0.04759049415588379\n","step: 1980, loss: 0.11993572860956192\n","step: 1990, loss: 0.042600031942129135\n","step: 2000, loss: 0.04607028514146805\n","step: 2010, loss: 0.05781370773911476\n","step: 2020, loss: 0.12561151385307312\n","step: 2030, loss: 0.07998139411211014\n","step: 2040, loss: 0.18328049778938293\n","step: 2050, loss: 0.0871613398194313\n","step: 2060, loss: 0.08389110863208771\n","step: 2070, loss: 0.06167538836598396\n","step: 2080, loss: 0.1100148856639862\n","step: 2090, loss: 0.15302471816539764\n","step: 2100, loss: 0.08835047483444214\n","step: 2110, loss: 0.1024002954363823\n","step: 2120, loss: 0.04401044175028801\n","step: 2130, loss: 0.04345012828707695\n","step: 2140, loss: 0.08405409753322601\n","step: 2150, loss: 0.06153399124741554\n","step: 2160, loss: 0.07575006037950516\n","step: 2170, loss: 0.06855728477239609\n","step: 2180, loss: 0.033817458897829056\n","step: 2190, loss: 0.08942040055990219\n","step: 2200, loss: 0.11122006922960281\n","step: 2210, loss: 0.06249047443270683\n","step: 2220, loss: 0.02224627695977688\n","step: 2230, loss: 0.06230238080024719\n","step: 2240, loss: 0.06525848805904388\n","step: 2250, loss: 0.09991779923439026\n","step: 2260, loss: 0.11088018864393234\n","step: 2270, loss: 0.04003560543060303\n","step: 2280, loss: 0.02086220495402813\n","step: 2290, loss: 0.041829999536275864\n","step: 2300, loss: 0.07578511536121368\n","step: 2310, loss: 0.10204357653856277\n","step: 2320, loss: 0.17944587767124176\n","step: 2330, loss: 0.05458197742700577\n","step: 2340, loss: 0.06005026400089264\n","step: 2350, loss: 0.1475270688533783\n","step: 2360, loss: 0.09935861080884933\n","step: 2370, loss: 0.04504438862204552\n","step: 2380, loss: 0.057608895003795624\n","step: 2390, loss: 0.028894327580928802\n","step: 2400, loss: 0.049000345170497894\n","step: 2410, loss: 0.06582089513540268\n","step: 2420, loss: 0.04442574456334114\n","step: 2430, loss: 0.08790546655654907\n","step: 2440, loss: 0.05994442477822304\n","step: 2450, loss: 0.051418643444776535\n","step: 2460, loss: 0.09422340989112854\n","step: 2470, loss: 0.07846151292324066\n","step: 2480, loss: 0.07947029918432236\n","step: 2490, loss: 0.07864028215408325\n","step: 2500, loss: 0.13279859721660614\n","step: 2510, loss: 0.060398802161216736\n","step: 2520, loss: 0.09918031096458435\n","step: 2530, loss: 0.05434046685695648\n","step: 2540, loss: 0.057073626667261124\n","step: 2550, loss: 0.06979533284902573\n","step: 2560, loss: 0.09566287696361542\n","step: 2570, loss: 0.10065397620201111\n","step: 2580, loss: 0.05762364715337753\n","step: 2590, loss: 0.07231546938419342\n","step: 2600, loss: 0.1642128825187683\n","step: 2610, loss: 0.11250923573970795\n","step: 2620, loss: 0.0747014656662941\n","step: 2630, loss: 0.04351535812020302\n","step: 2640, loss: 0.0618150532245636\n","step: 2650, loss: 0.20187413692474365\n","step: 2660, loss: 0.11009477823972702\n","step: 2670, loss: 0.18969093263149261\n","step: 2680, loss: 0.05880004167556763\n","step: 2690, loss: 0.08477874845266342\n","step: 2700, loss: 0.03370046988129616\n","step: 2710, loss: 0.07835160940885544\n","step: 2720, loss: 0.11276587098836899\n","step: 2730, loss: 0.0741812065243721\n","step: 2740, loss: 0.10537827759981155\n","step: 2750, loss: 0.11018162220716476\n","step: 2760, loss: 0.19220592081546783\n","step: 2770, loss: 0.16758126020431519\n","step: 2780, loss: 0.06605225801467896\n","step: 2790, loss: 0.18507516384124756\n","step: 2800, loss: 0.057812150567770004\n","step: 2810, loss: 0.04533081874251366\n","step: 2820, loss: 0.2318568229675293\n","step: 2830, loss: 0.11572353541851044\n","step: 2840, loss: 0.04435545578598976\n","step: 2850, loss: 0.11383279412984848\n","step: 2860, loss: 0.07569503039121628\n","step: 2870, loss: 0.08341841399669647\n","step: 2880, loss: 0.10295666754245758\n","step: 2890, loss: 0.04865460470318794\n","step: 2900, loss: 0.08060934394598007\n","step: 2910, loss: 0.09943509101867676\n","step: 2920, loss: 0.07795741409063339\n","step: 2930, loss: 0.06946231424808502\n","step: 2940, loss: 0.09806320071220398\n","step: 2950, loss: 0.08442036062479019\n","step: 2960, loss: 0.15905795991420746\n","step: 2970, loss: 0.09392370283603668\n","step: 2980, loss: 0.022023340687155724\n","step: 2990, loss: 0.1268024444580078\n","step: 3000, loss: 0.12190224975347519\n","step: 3010, loss: 0.08665164560079575\n","step: 3020, loss: 0.04961639270186424\n","step: 3030, loss: 0.0886135846376419\n","step: 3040, loss: 0.13321469724178314\n","step: 3050, loss: 0.029135115444660187\n","step: 3060, loss: 0.04263847693800926\n","step: 3070, loss: 0.04089012369513512\n","step: 3080, loss: 0.08215761184692383\n","step: 3090, loss: 0.1281462013721466\n","step: 3100, loss: 0.12522777915000916\n","step: 3110, loss: 0.08068283647298813\n","step: 3120, loss: 0.07396495342254639\n","step: 3130, loss: 0.11882565915584564\n","step: 3140, loss: 0.031031865626573563\n","step: 3150, loss: 0.08159070461988449\n","step: 3160, loss: 0.05635563284158707\n","step: 3170, loss: 0.15177853405475616\n","step: 3180, loss: 0.1025649681687355\n","step: 3190, loss: 0.02531750127673149\n","step: 3200, loss: 0.15063321590423584\n","step: 3210, loss: 0.11266814917325974\n","step: 3220, loss: 0.09708386659622192\n","step: 3230, loss: 0.05288684368133545\n","step: 3240, loss: 0.0793546587228775\n","step: 3250, loss: 0.09990119934082031\n","step: 3260, loss: 0.052276600152254105\n","step: 3270, loss: 0.09774661064147949\n","step: 3280, loss: 0.07450395822525024\n","step: 3290, loss: 0.12114530056715012\n","step: 3300, loss: 0.12883678078651428\n","step: 3310, loss: 0.1156434416770935\n","step: 3320, loss: 0.10968434065580368\n","step: 3330, loss: 0.22514396905899048\n","step: 3340, loss: 0.10677470266819\n","step: 3350, loss: 0.045877546072006226\n","step: 3360, loss: 0.09528062492609024\n","step: 3370, loss: 0.07302512228488922\n","step: 3380, loss: 0.17007209360599518\n","step: 3390, loss: 0.03677989915013313\n","step: 3400, loss: 0.1408464014530182\n","step: 3410, loss: 0.1006285548210144\n","step: 3420, loss: 0.08993913978338242\n","step: 3430, loss: 0.0976310595870018\n","step: 3440, loss: 0.06593350321054459\n","step: 3450, loss: 0.29426223039627075\n","step: 3460, loss: 0.07354827225208282\n","step: 3470, loss: 0.06458000093698502\n","step: 3480, loss: 0.08659014850854874\n","step: 3490, loss: 0.05634626746177673\n","step: 3500, loss: 0.08664105832576752\n","step: 3510, loss: 0.019629284739494324\n","step: 3520, loss: 0.15444211661815643\n","step: 3530, loss: 0.04272238537669182\n","step: 3540, loss: 0.12265793234109879\n","step: 3550, loss: 0.08908211439847946\n","step: 3560, loss: 0.17348766326904297\n","step: 3570, loss: 0.06756975501775742\n","step: 3580, loss: 0.07502558082342148\n","step: 3590, loss: 0.04428117349743843\n","step: 3600, loss: 0.0633314847946167\n","step: 3610, loss: 0.025833724066615105\n","step: 3620, loss: 0.11082103103399277\n","step: 3630, loss: 0.03472301736474037\n","step: 3640, loss: 0.16050587594509125\n","step: 3650, loss: 0.0627317875623703\n","step: 3660, loss: 0.09576188772916794\n","step: 3670, loss: 0.06527374684810638\n","step: 3680, loss: 0.2188747376203537\n","step: 3690, loss: 0.12192528694868088\n","step: 3700, loss: 0.053940679877996445\n","step: 3710, loss: 0.08276747912168503\n","step: 3720, loss: 0.11190613359212875\n","step: 3730, loss: 0.017591191455721855\n","step: 3740, loss: 0.015631547197699547\n","step: 3750, loss: 0.12759168446063995\n","step: 3760, loss: 0.045629289001226425\n","step: 3770, loss: 0.09550363570451736\n","step: 3780, loss: 0.1587747037410736\n","step: 3790, loss: 0.09447865933179855\n","step: 3800, loss: 0.03749343752861023\n","step: 3810, loss: 0.09348795562982559\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.77      0.94      0.85        35\n","           2       0.44      0.49      0.46        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.99      0.84      0.91       291\n","           5       0.98      0.84      0.90       294\n","           6       0.97      0.98      0.97      1570\n","           7       0.49      0.94      0.65       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.96      0.96      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.89      0.62      0.73        13\n","          14       0.37      1.00      0.54        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.80      0.88      0.84      1151\n","          17       0.98      0.98      0.98        41\n","          18       0.91      1.00      0.96        32\n","          19       0.55      0.88      0.67        40\n","          20       1.00      1.00      1.00       584\n","          21       0.13      0.10      0.11        52\n","          22       0.94      0.75      0.83      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.23      0.25      0.24        44\n","          25       0.86      0.91      0.89       888\n","          26       1.00      1.00      1.00         9\n","          27       0.97      0.99      0.98        69\n","          28       0.99      1.00      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.84      0.89      1136\n","          31       0.60      0.79      0.68        19\n","          32       1.00      0.62      0.77         8\n","          33       0.70      0.95      0.81        86\n","          34       0.21      0.53      0.30        32\n","          35       0.99      0.99      0.99       474\n","          36       0.93      0.14      0.25       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.95      0.99      0.97       404\n","          39       0.98      0.93      0.95       485\n","          40       0.90      0.81      0.85       573\n","          41       0.97      0.93      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.95      0.80      0.87       152\n","          44       0.88      0.93      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.79     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 434\n","\n","Loop 50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9276421070098877\n","step: 10, loss: 2.0107600688934326\n","step: 20, loss: 0.6412672996520996\n","step: 30, loss: 0.38036203384399414\n","step: 40, loss: 0.3293927311897278\n","step: 50, loss: 0.2412095069885254\n","step: 60, loss: 0.24165615439414978\n","step: 70, loss: 0.16257010400295258\n","step: 80, loss: 0.13483195006847382\n","step: 90, loss: 0.14514292776584625\n","step: 100, loss: 0.1812020093202591\n","step: 110, loss: 0.1305546909570694\n","step: 120, loss: 0.14527179300785065\n","step: 130, loss: 0.1642572581768036\n","step: 140, loss: 0.3017669916152954\n","step: 150, loss: 0.08671228587627411\n","step: 160, loss: 0.24213802814483643\n","step: 170, loss: 0.0745009258389473\n","step: 180, loss: 0.08250083029270172\n","step: 190, loss: 0.11874690651893616\n","step: 200, loss: 0.14494819939136505\n","step: 210, loss: 0.10813605785369873\n","step: 220, loss: 0.23726940155029297\n","step: 230, loss: 0.1603071540594101\n","step: 240, loss: 0.15998908877372742\n","step: 250, loss: 0.06342843174934387\n","step: 260, loss: 0.1077989786863327\n","step: 270, loss: 0.1153617724776268\n","step: 280, loss: 0.20830997824668884\n","step: 290, loss: 0.20096969604492188\n","step: 300, loss: 0.08526858687400818\n","step: 310, loss: 0.11283980309963226\n","step: 320, loss: 0.05898677557706833\n","step: 330, loss: 0.2337360978126526\n","step: 340, loss: 0.06923574209213257\n","step: 350, loss: 0.1210624948143959\n","step: 360, loss: 0.0936286523938179\n","step: 370, loss: 0.12484247982501984\n","step: 380, loss: 0.15325844287872314\n","step: 390, loss: 0.05762995406985283\n","step: 400, loss: 0.11861389875411987\n","step: 410, loss: 0.16046452522277832\n","step: 420, loss: 0.09636618942022324\n","step: 430, loss: 0.13017718493938446\n","step: 440, loss: 0.05127917230129242\n","step: 450, loss: 0.176258847117424\n","step: 460, loss: 0.05692015588283539\n","step: 470, loss: 0.08990851044654846\n","step: 480, loss: 0.0860944390296936\n","step: 490, loss: 0.07259848713874817\n","step: 500, loss: 0.08902429044246674\n","step: 510, loss: 0.09438236057758331\n","step: 520, loss: 0.07003815472126007\n","step: 530, loss: 0.05246588587760925\n","step: 540, loss: 0.2083962857723236\n","step: 550, loss: 0.0622858926653862\n","step: 560, loss: 0.10831785202026367\n","step: 570, loss: 0.09062334895133972\n","step: 580, loss: 0.0915619283914566\n","step: 590, loss: 0.09502334147691727\n","step: 600, loss: 0.16281747817993164\n","step: 610, loss: 0.12228702753782272\n","step: 620, loss: 0.1631622314453125\n","step: 630, loss: 0.047044601291418076\n","step: 640, loss: 0.16124635934829712\n","step: 650, loss: 0.05578434467315674\n","step: 660, loss: 0.08039873838424683\n","step: 670, loss: 0.051411356776952744\n","step: 680, loss: 0.08372753858566284\n","step: 690, loss: 0.26928505301475525\n","step: 700, loss: 0.19175979495048523\n","step: 710, loss: 0.0717117115855217\n","step: 720, loss: 0.041786886751651764\n","step: 730, loss: 0.08629598468542099\n","step: 740, loss: 0.06874345242977142\n","step: 750, loss: 0.0780988559126854\n","step: 760, loss: 0.0413302481174469\n","step: 770, loss: 0.12042491883039474\n","step: 780, loss: 0.05237600952386856\n","step: 790, loss: 0.07233858108520508\n","step: 800, loss: 0.09387350082397461\n","step: 810, loss: 0.05114636570215225\n","step: 820, loss: 0.10624141246080399\n","step: 830, loss: 0.10951227694749832\n","step: 840, loss: 0.11805512756109238\n","step: 850, loss: 0.06842458248138428\n","step: 860, loss: 0.06142576411366463\n","step: 870, loss: 0.11651505529880524\n","step: 880, loss: 0.11320341378450394\n","step: 890, loss: 0.08404428511857986\n","step: 900, loss: 0.05719325318932533\n","step: 910, loss: 0.2287476360797882\n","step: 920, loss: 0.12516257166862488\n","step: 930, loss: 0.07150586694478989\n","step: 940, loss: 0.19944770634174347\n","step: 950, loss: 0.09714816510677338\n","step: 960, loss: 0.174569234251976\n","step: 970, loss: 0.08074510097503662\n","step: 980, loss: 0.06786089390516281\n","step: 990, loss: 0.07737629860639572\n","step: 1000, loss: 0.16494104266166687\n","step: 1010, loss: 0.07871261984109879\n","step: 1020, loss: 0.06817534565925598\n","step: 1030, loss: 0.1429568976163864\n","step: 1040, loss: 0.07651627063751221\n","step: 1050, loss: 0.11238893121480942\n","step: 1060, loss: 0.1331675499677658\n","step: 1070, loss: 0.1433795839548111\n","step: 1080, loss: 0.09807244688272476\n","step: 1090, loss: 0.06916560977697372\n","step: 1100, loss: 0.13207292556762695\n","step: 1110, loss: 0.19102293252944946\n","step: 1120, loss: 0.16734705865383148\n","step: 1130, loss: 0.08673114329576492\n","step: 1140, loss: 0.07067567110061646\n","step: 1150, loss: 0.045956943184137344\n","step: 1160, loss: 0.1441575437784195\n","step: 1170, loss: 0.0998915284872055\n","step: 1180, loss: 0.08112428337335587\n","step: 1190, loss: 0.14485101401805878\n","step: 1200, loss: 0.0872122198343277\n","step: 1210, loss: 0.18378788232803345\n","step: 1220, loss: 0.1180330365896225\n","step: 1230, loss: 0.14921332895755768\n","step: 1240, loss: 0.05166736617684364\n","step: 1250, loss: 0.07895772159099579\n","step: 1260, loss: 0.07628966867923737\n","step: 1270, loss: 0.07740380614995956\n","step: 1280, loss: 0.05623943358659744\n","step: 1290, loss: 0.11847563087940216\n","step: 1300, loss: 0.17009374499320984\n","step: 1310, loss: 0.202989861369133\n","step: 1320, loss: 0.08899302780628204\n","step: 1330, loss: 0.16423095762729645\n","step: 1340, loss: 0.100600965321064\n","step: 1350, loss: 0.14371730387210846\n","step: 1360, loss: 0.09924180060625076\n","step: 1370, loss: 0.12461526691913605\n","step: 1380, loss: 0.06956987828016281\n","step: 1390, loss: 0.14905457198619843\n","step: 1400, loss: 0.050017740577459335\n","step: 1410, loss: 0.03478088229894638\n","step: 1420, loss: 0.08022391051054001\n","step: 1430, loss: 0.07047906517982483\n","step: 1440, loss: 0.1161067932844162\n","step: 1450, loss: 0.12917523086071014\n","step: 1460, loss: 0.04334495589137077\n","step: 1470, loss: 0.06558319926261902\n","step: 1480, loss: 0.10441549122333527\n","step: 1490, loss: 0.11342775821685791\n","step: 1500, loss: 0.0931445062160492\n","step: 1510, loss: 0.1555369347333908\n","step: 1520, loss: 0.05128202214837074\n","step: 1530, loss: 0.08964979648590088\n","step: 1540, loss: 0.04917608201503754\n","step: 1550, loss: 0.09638633579015732\n","step: 1560, loss: 0.09590227156877518\n","step: 1570, loss: 0.17674246430397034\n","step: 1580, loss: 0.07128410786390305\n","step: 1590, loss: 0.12120571732521057\n","step: 1600, loss: 0.14523103833198547\n","step: 1610, loss: 0.07943295687437057\n","step: 1620, loss: 0.048424385488033295\n","step: 1630, loss: 0.01583153009414673\n","step: 1640, loss: 0.04577303305268288\n","step: 1650, loss: 0.07965091615915298\n","step: 1660, loss: 0.13789330422878265\n","step: 1670, loss: 0.11570864170789719\n","step: 1680, loss: 0.07411588728427887\n","step: 1690, loss: 0.05973856523633003\n","step: 1700, loss: 0.04560139402747154\n","step: 1710, loss: 0.07303319871425629\n","step: 1720, loss: 0.1760525405406952\n","step: 1730, loss: 0.07509120553731918\n","step: 1740, loss: 0.08170334994792938\n","step: 1750, loss: 0.05351654067635536\n","step: 1760, loss: 0.04743195325136185\n","step: 1770, loss: 0.10338921844959259\n","step: 1780, loss: 0.11629179865121841\n","step: 1790, loss: 0.09227187931537628\n","step: 1800, loss: 0.02160673961043358\n","step: 1810, loss: 0.04624798148870468\n","step: 1820, loss: 0.13858629763126373\n","step: 1830, loss: 0.04448304697871208\n","step: 1840, loss: 0.08350532501935959\n","step: 1850, loss: 0.16342218220233917\n","step: 1860, loss: 0.06847627460956573\n","step: 1870, loss: 0.14389729499816895\n","step: 1880, loss: 0.14426997303962708\n","step: 1890, loss: 0.061256472021341324\n","step: 1900, loss: 0.15854527056217194\n","step: 1910, loss: 0.11180134117603302\n","step: 1920, loss: 0.08030609786510468\n","step: 1930, loss: 0.15427733957767487\n","step: 1940, loss: 0.042118966579437256\n","step: 1950, loss: 0.0923936665058136\n","step: 1960, loss: 0.0615307055413723\n","step: 1970, loss: 0.09857350587844849\n","step: 1980, loss: 0.18262425065040588\n","step: 1990, loss: 0.08529088646173477\n","step: 2000, loss: 0.04848163574934006\n","step: 2010, loss: 0.0879572331905365\n","step: 2020, loss: 0.036103297024965286\n","step: 2030, loss: 0.09881695359945297\n","step: 2040, loss: 0.0792260617017746\n","step: 2050, loss: 0.04416996240615845\n","step: 2060, loss: 0.04828968271613121\n","step: 2070, loss: 0.07808724045753479\n","step: 2080, loss: 0.11052585393190384\n","step: 2090, loss: 0.2397116869688034\n","step: 2100, loss: 0.059082671999931335\n","step: 2110, loss: 0.06797172129154205\n","step: 2120, loss: 0.08256153762340546\n","step: 2130, loss: 0.09073107689619064\n","step: 2140, loss: 0.16055072844028473\n","step: 2150, loss: 0.08675805479288101\n","step: 2160, loss: 0.03407486155629158\n","step: 2170, loss: 0.09788327664136887\n","step: 2180, loss: 0.1692839413881302\n","step: 2190, loss: 0.09348689019680023\n","step: 2200, loss: 0.07418262213468552\n","step: 2210, loss: 0.11433722823858261\n","step: 2220, loss: 0.051128823310136795\n","step: 2230, loss: 0.16251224279403687\n","step: 2240, loss: 0.056563183665275574\n","step: 2250, loss: 0.06620700657367706\n","step: 2260, loss: 0.07689155638217926\n","step: 2270, loss: 0.06963653862476349\n","step: 2280, loss: 0.05222107842564583\n","step: 2290, loss: 0.06736394017934799\n","step: 2300, loss: 0.06559957563877106\n","step: 2310, loss: 0.0851319432258606\n","step: 2320, loss: 0.02579043060541153\n","step: 2330, loss: 0.0464489720761776\n","step: 2340, loss: 0.08116142451763153\n","step: 2350, loss: 0.05913371220231056\n","step: 2360, loss: 0.08732039481401443\n","step: 2370, loss: 0.10949664562940598\n","step: 2380, loss: 0.15957507491111755\n","step: 2390, loss: 0.16651996970176697\n","step: 2400, loss: 0.14049990475177765\n","step: 2410, loss: 0.07366010546684265\n","step: 2420, loss: 0.05965891480445862\n","step: 2430, loss: 0.09007176011800766\n","step: 2440, loss: 0.08243291825056076\n","step: 2450, loss: 0.07154404371976852\n","step: 2460, loss: 0.0657443180680275\n","step: 2470, loss: 0.017724832519888878\n","step: 2480, loss: 0.08271539211273193\n","step: 2490, loss: 0.10418201982975006\n","step: 2500, loss: 0.11400515586137772\n","step: 2510, loss: 0.070644810795784\n","step: 2520, loss: 0.10523633658885956\n","step: 2530, loss: 0.04221923649311066\n","step: 2540, loss: 0.09260337799787521\n","step: 2550, loss: 0.06133651360869408\n","step: 2560, loss: 0.11753254383802414\n","step: 2570, loss: 0.06736990064382553\n","step: 2580, loss: 0.1494392603635788\n","step: 2590, loss: 0.11007893830537796\n","step: 2600, loss: 0.14749360084533691\n","step: 2610, loss: 0.12890170514583588\n","step: 2620, loss: 0.09280933439731598\n","step: 2630, loss: 0.07473817467689514\n","step: 2640, loss: 0.3029302656650543\n","step: 2650, loss: 0.06092901900410652\n","step: 2660, loss: 0.1530146449804306\n","step: 2670, loss: 0.1139393001794815\n","step: 2680, loss: 0.17030178010463715\n","step: 2690, loss: 0.13735747337341309\n","step: 2700, loss: 0.15431059896945953\n","step: 2710, loss: 0.13603653013706207\n","step: 2720, loss: 0.05202697217464447\n","step: 2730, loss: 0.08709604293107986\n","step: 2740, loss: 0.11821166425943375\n","step: 2750, loss: 0.06042833253741264\n","step: 2760, loss: 0.10446704179048538\n","step: 2770, loss: 0.0334031768143177\n","step: 2780, loss: 0.14491213858127594\n","step: 2790, loss: 0.09540238976478577\n","step: 2800, loss: 0.041907768696546555\n","step: 2810, loss: 0.09095337241888046\n","step: 2820, loss: 0.055874310433864594\n","step: 2830, loss: 0.1182856634259224\n","step: 2840, loss: 0.05298997461795807\n","step: 2850, loss: 0.07669346034526825\n","step: 2860, loss: 0.013539343141019344\n","step: 2870, loss: 0.0474872887134552\n","step: 2880, loss: 0.07883629202842712\n","step: 2890, loss: 0.05528402701020241\n","step: 2900, loss: 0.0841536894440651\n","step: 2910, loss: 0.08366374671459198\n","step: 2920, loss: 0.05927566811442375\n","step: 2930, loss: 0.13695299625396729\n","step: 2940, loss: 0.12381149083375931\n","step: 2950, loss: 0.029649222269654274\n","step: 2960, loss: 0.11388228088617325\n","step: 2970, loss: 0.09601456671953201\n","step: 2980, loss: 0.0739843100309372\n","step: 2990, loss: 0.06498269736766815\n","step: 3000, loss: 0.07722224295139313\n","step: 3010, loss: 0.10965283960103989\n","step: 3020, loss: 0.1193443238735199\n","step: 3030, loss: 0.05033514276146889\n","step: 3040, loss: 0.10082994401454926\n","step: 3050, loss: 0.05750902742147446\n","step: 3060, loss: 0.07160738855600357\n","step: 3070, loss: 0.07001356035470963\n","step: 3080, loss: 0.01961701177060604\n","step: 3090, loss: 0.0906745046377182\n","step: 3100, loss: 0.07585161924362183\n","step: 3110, loss: 0.05572554096579552\n","step: 3120, loss: 0.06478774547576904\n","step: 3130, loss: 0.08796985447406769\n","step: 3140, loss: 0.1323913335800171\n","step: 3150, loss: 0.07530868053436279\n","step: 3160, loss: 0.05450413376092911\n","step: 3170, loss: 0.1643974632024765\n","step: 3180, loss: 0.11092653125524521\n","step: 3190, loss: 0.17127685248851776\n","step: 3200, loss: 0.07486774772405624\n","step: 3210, loss: 0.13501952588558197\n","step: 3220, loss: 0.08074004203081131\n","step: 3230, loss: 0.062267325818538666\n","step: 3240, loss: 0.031129887327551842\n","step: 3250, loss: 0.10566096752882004\n","step: 3260, loss: 0.08987188339233398\n","step: 3270, loss: 0.0565163753926754\n","step: 3280, loss: 0.07014558464288712\n","step: 3290, loss: 0.1564474254846573\n","step: 3300, loss: 0.055434394627809525\n","step: 3310, loss: 0.0764472559094429\n","step: 3320, loss: 0.12409762293100357\n","step: 3330, loss: 0.05799653381109238\n","step: 3340, loss: 0.13698990643024445\n","step: 3350, loss: 0.08260509371757507\n","step: 3360, loss: 0.07475299388170242\n","step: 3370, loss: 0.15625935792922974\n","step: 3380, loss: 0.08565260469913483\n","step: 3390, loss: 0.06244228407740593\n","step: 3400, loss: 0.06452237814664841\n","step: 3410, loss: 0.06388813257217407\n","step: 3420, loss: 0.048922378569841385\n","step: 3430, loss: 0.08794955909252167\n","step: 3440, loss: 0.13649828732013702\n","step: 3450, loss: 0.03506632149219513\n","step: 3460, loss: 0.13444069027900696\n","step: 3470, loss: 0.10076282918453217\n","step: 3480, loss: 0.0988055169582367\n","step: 3490, loss: 0.10193587094545364\n","step: 3500, loss: 0.054180778563022614\n","step: 3510, loss: 0.05532054603099823\n","step: 3520, loss: 0.06084572151303291\n","step: 3530, loss: 0.10630034655332565\n","step: 3540, loss: 0.037355877459049225\n","step: 3550, loss: 0.12422788143157959\n","step: 3560, loss: 0.12451625615358353\n","step: 3570, loss: 0.033499862998723984\n","step: 3580, loss: 0.08944813162088394\n","step: 3590, loss: 0.14943000674247742\n","step: 3600, loss: 0.08031774312257767\n","step: 3610, loss: 0.12358559668064117\n","step: 3620, loss: 0.13321912288665771\n","step: 3630, loss: 0.12380252033472061\n","step: 3640, loss: 0.0835600271821022\n","step: 3650, loss: 0.048326365649700165\n","step: 3660, loss: 0.04746464267373085\n","step: 3670, loss: 0.057861220091581345\n","step: 3680, loss: 0.07182496041059494\n","step: 3690, loss: 0.09831307828426361\n","step: 3700, loss: 0.03820605203509331\n","step: 3710, loss: 0.04311228170990944\n","step: 3720, loss: 0.13861429691314697\n","step: 3730, loss: 0.05165010690689087\n","step: 3740, loss: 0.037418924272060394\n","step: 3750, loss: 0.20484977960586548\n","step: 3760, loss: 0.03699669986963272\n","step: 3770, loss: 0.05214638262987137\n","step: 3780, loss: 0.14992505311965942\n","step: 3790, loss: 0.16929291188716888\n","step: 3800, loss: 0.25767114758491516\n","step: 3810, loss: 0.0576876699924469\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      1.00      0.89        35\n","           2       0.90      0.91      0.90        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.94      0.84      0.89       291\n","           5       0.95      0.84      0.89       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.45      0.95      0.61       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.93      0.99      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.78      0.98      0.87        47\n","          13       0.33      0.08      0.12        13\n","          14       0.54      1.00      0.70        43\n","          15       0.95      0.99      0.97      2778\n","          16       0.88      0.83      0.86      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.91      0.97      0.94        32\n","          19       0.88      0.35      0.50        40\n","          20       1.00      1.00      1.00       584\n","          21       0.03      0.04      0.04        52\n","          22       0.96      0.70      0.81      4175\n","          23       0.66      0.98      0.79      2253\n","          24       0.29      0.61      0.39        44\n","          25       0.87      0.90      0.88       888\n","          26       1.00      0.33      0.50         9\n","          27       0.95      1.00      0.97        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.83      0.88      1136\n","          31       0.52      0.63      0.57        19\n","          32       1.00      0.62      0.77         8\n","          33       0.75      0.90      0.82        86\n","          34       0.18      0.47      0.26        32\n","          35       0.99      0.98      0.99       474\n","          36       0.81      0.14      0.24       182\n","          37       0.86      0.98      0.92      1592\n","          38       0.95      0.99      0.97       404\n","          39       0.94      0.96      0.95       485\n","          40       0.90      0.92      0.91       573\n","          41       0.96      0.90      0.93       841\n","          42       0.99      0.99      0.99       575\n","          43       0.95      0.74      0.83       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.98      0.99        82\n","          48       0.90      0.70      0.79        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 445\n","\n","Loop 51\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8749887943267822\n","step: 10, loss: 1.7468065023422241\n","step: 20, loss: 0.7857472896575928\n","step: 30, loss: 0.34477221965789795\n","step: 40, loss: 0.2092415988445282\n","step: 50, loss: 0.28058570623397827\n","step: 60, loss: 0.33698585629463196\n","step: 70, loss: 0.2531597912311554\n","step: 80, loss: 0.15061794221401215\n","step: 90, loss: 0.26120725274086\n","step: 100, loss: 0.13932137191295624\n","step: 110, loss: 0.2091992199420929\n","step: 120, loss: 0.06363054364919662\n","step: 130, loss: 0.20826083421707153\n","step: 140, loss: 0.11957211792469025\n","step: 150, loss: 0.11137189716100693\n","step: 160, loss: 0.188290074467659\n","step: 170, loss: 0.12046846002340317\n","step: 180, loss: 0.11150538921356201\n","step: 190, loss: 0.07822588831186295\n","step: 200, loss: 0.0844736248254776\n","step: 210, loss: 0.1260073035955429\n","step: 220, loss: 0.03734308481216431\n","step: 230, loss: 0.12362390756607056\n","step: 240, loss: 0.2047717571258545\n","step: 250, loss: 0.12191155552864075\n","step: 260, loss: 0.22699132561683655\n","step: 270, loss: 0.0781998559832573\n","step: 280, loss: 0.057660434395074844\n","step: 290, loss: 0.14435583353042603\n","step: 300, loss: 0.07820886373519897\n","step: 310, loss: 0.07017365097999573\n","step: 320, loss: 0.08730234950780869\n","step: 330, loss: 0.16435638070106506\n","step: 340, loss: 0.1403423696756363\n","step: 350, loss: 0.23970046639442444\n","step: 360, loss: 0.13759636878967285\n","step: 370, loss: 0.16035519540309906\n","step: 380, loss: 0.18443332612514496\n","step: 390, loss: 0.14098899066448212\n","step: 400, loss: 0.1728939414024353\n","step: 410, loss: 0.06256241351366043\n","step: 420, loss: 0.11347955465316772\n","step: 430, loss: 0.11049424856901169\n","step: 440, loss: 0.08693929761648178\n","step: 450, loss: 0.19302919507026672\n","step: 460, loss: 0.03167546167969704\n","step: 470, loss: 0.05273473262786865\n","step: 480, loss: 0.16339489817619324\n","step: 490, loss: 0.10815656930208206\n","step: 500, loss: 0.12668010592460632\n","step: 510, loss: 0.09439176321029663\n","step: 520, loss: 0.08753646910190582\n","step: 530, loss: 0.13085442781448364\n","step: 540, loss: 0.13761118054389954\n","step: 550, loss: 0.06986423581838608\n","step: 560, loss: 0.11015649884939194\n","step: 570, loss: 0.09779218584299088\n","step: 580, loss: 0.10903427749872208\n","step: 590, loss: 0.12125308066606522\n","step: 600, loss: 0.1117815151810646\n","step: 610, loss: 0.08739594370126724\n","step: 620, loss: 0.15453678369522095\n","step: 630, loss: 0.1021336242556572\n","step: 640, loss: 0.08169689774513245\n","step: 650, loss: 0.11395557224750519\n","step: 660, loss: 0.05129852890968323\n","step: 670, loss: 0.07122122496366501\n","step: 680, loss: 0.166658416390419\n","step: 690, loss: 0.16125799715518951\n","step: 700, loss: 0.10363656282424927\n","step: 710, loss: 0.07927726209163666\n","step: 720, loss: 0.10344833135604858\n","step: 730, loss: 0.14291734993457794\n","step: 740, loss: 0.02811356447637081\n","step: 750, loss: 0.11822635680437088\n","step: 760, loss: 0.10192845016717911\n","step: 770, loss: 0.1268809735774994\n","step: 780, loss: 0.07050160318613052\n","step: 790, loss: 0.061007481068372726\n","step: 800, loss: 0.15900248289108276\n","step: 810, loss: 0.07818832993507385\n","step: 820, loss: 0.11755502969026566\n","step: 830, loss: 0.04043712466955185\n","step: 840, loss: 0.05081568658351898\n","step: 850, loss: 0.03708454594016075\n","step: 860, loss: 0.07222489267587662\n","step: 870, loss: 0.08343486487865448\n","step: 880, loss: 0.12551839649677277\n","step: 890, loss: 0.11524562537670135\n","step: 900, loss: 0.0941917896270752\n","step: 910, loss: 0.06739544123411179\n","step: 920, loss: 0.09735940396785736\n","step: 930, loss: 0.09857480227947235\n","step: 940, loss: 0.15524700284004211\n","step: 950, loss: 0.09523632377386093\n","step: 960, loss: 0.10336854308843613\n","step: 970, loss: 0.15761996805667877\n","step: 980, loss: 0.06858309358358383\n","step: 990, loss: 0.12875375151634216\n","step: 1000, loss: 0.08718971163034439\n","step: 1010, loss: 0.10488034784793854\n","step: 1020, loss: 0.06312612444162369\n","step: 1030, loss: 0.057796817272901535\n","step: 1040, loss: 0.07100936025381088\n","step: 1050, loss: 0.20246237516403198\n","step: 1060, loss: 0.07069691270589828\n","step: 1070, loss: 0.17099274694919586\n","step: 1080, loss: 0.11107918620109558\n","step: 1090, loss: 0.07531099021434784\n","step: 1100, loss: 0.09623724222183228\n","step: 1110, loss: 0.10819906741380692\n","step: 1120, loss: 0.11144202947616577\n","step: 1130, loss: 0.0557975210249424\n","step: 1140, loss: 0.07088227570056915\n","step: 1150, loss: 0.03747283294796944\n","step: 1160, loss: 0.16460222005844116\n","step: 1170, loss: 0.05535260587930679\n","step: 1180, loss: 0.07618843019008636\n","step: 1190, loss: 0.0633668452501297\n","step: 1200, loss: 0.06915026903152466\n","step: 1210, loss: 0.12702889740467072\n","step: 1220, loss: 0.08268365263938904\n","step: 1230, loss: 0.032735731452703476\n","step: 1240, loss: 0.11135032773017883\n","step: 1250, loss: 0.09578641504049301\n","step: 1260, loss: 0.03700966015458107\n","step: 1270, loss: 0.08227933943271637\n","step: 1280, loss: 0.17506328225135803\n","step: 1290, loss: 0.1083887368440628\n","step: 1300, loss: 0.07375238835811615\n","step: 1310, loss: 0.11352510005235672\n","step: 1320, loss: 0.2229389101266861\n","step: 1330, loss: 0.10537383705377579\n","step: 1340, loss: 0.10237982124090195\n","step: 1350, loss: 0.044078461825847626\n","step: 1360, loss: 0.10073015093803406\n","step: 1370, loss: 0.028807681053876877\n","step: 1380, loss: 0.12051498144865036\n","step: 1390, loss: 0.147403746843338\n","step: 1400, loss: 0.15778611600399017\n","step: 1410, loss: 0.07699589431285858\n","step: 1420, loss: 0.1308557242155075\n","step: 1430, loss: 0.03892118111252785\n","step: 1440, loss: 0.07296248525381088\n","step: 1450, loss: 0.15292057394981384\n","step: 1460, loss: 0.08900981396436691\n","step: 1470, loss: 0.02762698195874691\n","step: 1480, loss: 0.09082069247961044\n","step: 1490, loss: 0.06702964752912521\n","step: 1500, loss: 0.08275162428617477\n","step: 1510, loss: 0.16660453379154205\n","step: 1520, loss: 0.04711303859949112\n","step: 1530, loss: 0.09270262718200684\n","step: 1540, loss: 0.13226613402366638\n","step: 1550, loss: 0.06137819588184357\n","step: 1560, loss: 0.10682917386293411\n","step: 1570, loss: 0.06347183138132095\n","step: 1580, loss: 0.06911475956439972\n","step: 1590, loss: 0.06000613048672676\n","step: 1600, loss: 0.14502836763858795\n","step: 1610, loss: 0.05055085942149162\n","step: 1620, loss: 0.06443150341510773\n","step: 1630, loss: 0.1713852882385254\n","step: 1640, loss: 0.127736896276474\n","step: 1650, loss: 0.058204762637615204\n","step: 1660, loss: 0.08055770397186279\n","step: 1670, loss: 0.11419987678527832\n","step: 1680, loss: 0.07253536581993103\n","step: 1690, loss: 0.016391567885875702\n","step: 1700, loss: 0.14483007788658142\n","step: 1710, loss: 0.06700728088617325\n","step: 1720, loss: 0.17613515257835388\n","step: 1730, loss: 0.07766376435756683\n","step: 1740, loss: 0.043892815709114075\n","step: 1750, loss: 0.1107427179813385\n","step: 1760, loss: 0.0869852602481842\n","step: 1770, loss: 0.07066725939512253\n","step: 1780, loss: 0.15276533365249634\n","step: 1790, loss: 0.049032166600227356\n","step: 1800, loss: 0.0306992344558239\n","step: 1810, loss: 0.13383662700653076\n","step: 1820, loss: 0.126609668135643\n","step: 1830, loss: 0.11774647235870361\n","step: 1840, loss: 0.048309437930583954\n","step: 1850, loss: 0.06546428799629211\n","step: 1860, loss: 0.14315888285636902\n","step: 1870, loss: 0.07063720375299454\n","step: 1880, loss: 0.08765046298503876\n","step: 1890, loss: 0.22421789169311523\n","step: 1900, loss: 0.044380564242601395\n","step: 1910, loss: 0.05534723401069641\n","step: 1920, loss: 0.049781229346990585\n","step: 1930, loss: 0.08897078782320023\n","step: 1940, loss: 0.05852784961462021\n","step: 1950, loss: 0.07830679416656494\n","step: 1960, loss: 0.04125512018799782\n","step: 1970, loss: 0.03246978297829628\n","step: 1980, loss: 0.14194783568382263\n","step: 1990, loss: 0.07194618135690689\n","step: 2000, loss: 0.05643226206302643\n","step: 2010, loss: 0.2091761976480484\n","step: 2020, loss: 0.06431017071008682\n","step: 2030, loss: 0.1149761900305748\n","step: 2040, loss: 0.07216406613588333\n","step: 2050, loss: 0.17809225618839264\n","step: 2060, loss: 0.07294469326734543\n","step: 2070, loss: 0.09355343133211136\n","step: 2080, loss: 0.06636858731508255\n","step: 2090, loss: 0.08984680473804474\n","step: 2100, loss: 0.027904635295271873\n","step: 2110, loss: 0.1708543747663498\n","step: 2120, loss: 0.07012242078781128\n","step: 2130, loss: 0.07322025299072266\n","step: 2140, loss: 0.17917773127555847\n","step: 2150, loss: 0.09951402992010117\n","step: 2160, loss: 0.08037903159856796\n","step: 2170, loss: 0.1482657492160797\n","step: 2180, loss: 0.09128810465335846\n","step: 2190, loss: 0.08460304886102676\n","step: 2200, loss: 0.04965344816446304\n","step: 2210, loss: 0.2280278354883194\n","step: 2220, loss: 0.08140856772661209\n","step: 2230, loss: 0.06783539056777954\n","step: 2240, loss: 0.12781427800655365\n","step: 2250, loss: 0.09004469960927963\n","step: 2260, loss: 0.034074582159519196\n","step: 2270, loss: 0.0363272987306118\n","step: 2280, loss: 0.05582074448466301\n","step: 2290, loss: 0.06968190521001816\n","step: 2300, loss: 0.10177480429410934\n","step: 2310, loss: 0.13742543756961823\n","step: 2320, loss: 0.0530843511223793\n","step: 2330, loss: 0.06198993697762489\n","step: 2340, loss: 0.11469422280788422\n","step: 2350, loss: 0.04072025045752525\n","step: 2360, loss: 0.1194511353969574\n","step: 2370, loss: 0.038003623485565186\n","step: 2380, loss: 0.06872999668121338\n","step: 2390, loss: 0.0625373125076294\n","step: 2400, loss: 0.06590598821640015\n","step: 2410, loss: 0.140069380402565\n","step: 2420, loss: 0.048421312123537064\n","step: 2430, loss: 0.10042700171470642\n","step: 2440, loss: 0.040393173694610596\n","step: 2450, loss: 0.11527112871408463\n","step: 2460, loss: 0.04759763181209564\n","step: 2470, loss: 0.05112607404589653\n","step: 2480, loss: 0.06419302523136139\n","step: 2490, loss: 0.15064279735088348\n","step: 2500, loss: 0.059756528586149216\n","step: 2510, loss: 0.023852692916989326\n","step: 2520, loss: 0.05705222114920616\n","step: 2530, loss: 0.06293127685785294\n","step: 2540, loss: 0.11156699806451797\n","step: 2550, loss: 0.08622199296951294\n","step: 2560, loss: 0.04670655354857445\n","step: 2570, loss: 0.12302188575267792\n","step: 2580, loss: 0.09958749264478683\n","step: 2590, loss: 0.0925716683268547\n","step: 2600, loss: 0.06005692854523659\n","step: 2610, loss: 0.0653456449508667\n","step: 2620, loss: 0.10085982084274292\n","step: 2630, loss: 0.041468847543001175\n","step: 2640, loss: 0.09685489535331726\n","step: 2650, loss: 0.0953826978802681\n","step: 2660, loss: 0.04262122884392738\n","step: 2670, loss: 0.07944060862064362\n","step: 2680, loss: 0.15191777050495148\n","step: 2690, loss: 0.0513276644051075\n","step: 2700, loss: 0.05514802411198616\n","step: 2710, loss: 0.06305117905139923\n","step: 2720, loss: 0.18832086026668549\n","step: 2730, loss: 0.11428260803222656\n","step: 2740, loss: 0.07602731138467789\n","step: 2750, loss: 0.03735388070344925\n","step: 2760, loss: 0.13825123012065887\n","step: 2770, loss: 0.23462003469467163\n","step: 2780, loss: 0.1018943041563034\n","step: 2790, loss: 0.06369158625602722\n","step: 2800, loss: 0.14954860508441925\n","step: 2810, loss: 0.06756114214658737\n","step: 2820, loss: 0.11353698372840881\n","step: 2830, loss: 0.2215607762336731\n","step: 2840, loss: 0.10031343251466751\n","step: 2850, loss: 0.059037648141384125\n","step: 2860, loss: 0.09911521524190903\n","step: 2870, loss: 0.0581703782081604\n","step: 2880, loss: 0.12276361882686615\n","step: 2890, loss: 0.08164536207914352\n","step: 2900, loss: 0.04093972593545914\n","step: 2910, loss: 0.08204410970211029\n","step: 2920, loss: 0.11809751391410828\n","step: 2930, loss: 0.08140464127063751\n","step: 2940, loss: 0.08472375571727753\n","step: 2950, loss: 0.061183538287878036\n","step: 2960, loss: 0.16157473623752594\n","step: 2970, loss: 0.1277725100517273\n","step: 2980, loss: 0.07173669338226318\n","step: 2990, loss: 0.07366298139095306\n","step: 3000, loss: 0.13811786472797394\n","step: 3010, loss: 0.09391665458679199\n","step: 3020, loss: 0.08553482592105865\n","step: 3030, loss: 0.07674602419137955\n","step: 3040, loss: 0.11874701827764511\n","step: 3050, loss: 0.08270452171564102\n","step: 3060, loss: 0.11158543825149536\n","step: 3070, loss: 0.05973190814256668\n","step: 3080, loss: 0.14328166842460632\n","step: 3090, loss: 0.08885559439659119\n","step: 3100, loss: 0.11395179480314255\n","step: 3110, loss: 0.027136651799082756\n","step: 3120, loss: 0.15558098256587982\n","step: 3130, loss: 0.059518247842788696\n","step: 3140, loss: 0.16907155513763428\n","step: 3150, loss: 0.08380208164453506\n","step: 3160, loss: 0.12217452377080917\n","step: 3170, loss: 0.056640561670064926\n","step: 3180, loss: 0.03660953417420387\n","step: 3190, loss: 0.08697881549596786\n","step: 3200, loss: 0.11685881018638611\n","step: 3210, loss: 0.014036031439900398\n","step: 3220, loss: 0.10723008215427399\n","step: 3230, loss: 0.0628216415643692\n","step: 3240, loss: 0.09029687941074371\n","step: 3250, loss: 0.05937139689922333\n","step: 3260, loss: 0.03318954259157181\n","step: 3270, loss: 0.06796856969594955\n","step: 3280, loss: 0.13638819754123688\n","step: 3290, loss: 0.06395416706800461\n","step: 3300, loss: 0.1102677583694458\n","step: 3310, loss: 0.07792669534683228\n","step: 3320, loss: 0.09010903537273407\n","step: 3330, loss: 0.09474943578243256\n","step: 3340, loss: 0.03214297443628311\n","step: 3350, loss: 0.17497804760932922\n","step: 3360, loss: 0.0849776342511177\n","step: 3370, loss: 0.09999459981918335\n","step: 3380, loss: 0.0923779159784317\n","step: 3390, loss: 0.05384382605552673\n","step: 3400, loss: 0.044519294053316116\n","step: 3410, loss: 0.12918312847614288\n","step: 3420, loss: 0.051582932472229004\n","step: 3430, loss: 0.17030782997608185\n","step: 3440, loss: 0.10919032990932465\n","step: 3450, loss: 0.14591039717197418\n","step: 3460, loss: 0.04132692888379097\n","step: 3470, loss: 0.037811022251844406\n","step: 3480, loss: 0.0703616589307785\n","step: 3490, loss: 0.12183500826358795\n","step: 3500, loss: 0.14251890778541565\n","step: 3510, loss: 0.08506632596254349\n","step: 3520, loss: 0.157270148396492\n","step: 3530, loss: 0.05068689212203026\n","step: 3540, loss: 0.07215695083141327\n","step: 3550, loss: 0.07139713317155838\n","step: 3560, loss: 0.15278714895248413\n","step: 3570, loss: 0.03249306231737137\n","step: 3580, loss: 0.11650308966636658\n","step: 3590, loss: 0.061336107552051544\n","step: 3600, loss: 0.06498555094003677\n","step: 3610, loss: 0.0430445596575737\n","step: 3620, loss: 0.08062838762998581\n","step: 3630, loss: 0.09411629289388657\n","step: 3640, loss: 0.08005041629076004\n","step: 3650, loss: 0.05879957973957062\n","step: 3660, loss: 0.07776954770088196\n","step: 3670, loss: 0.08786223083734512\n","step: 3680, loss: 0.11030516028404236\n","step: 3690, loss: 0.06656906753778458\n","step: 3700, loss: 0.1423938274383545\n","step: 3710, loss: 0.08464761078357697\n","step: 3720, loss: 0.05289167910814285\n","step: 3730, loss: 0.04287487268447876\n","step: 3740, loss: 0.06606951355934143\n","step: 3750, loss: 0.03818012773990631\n","step: 3760, loss: 0.11429192125797272\n","step: 3770, loss: 0.08701666444540024\n","step: 3780, loss: 0.0832672119140625\n","step: 3790, loss: 0.05766509473323822\n","step: 3800, loss: 0.3683988153934479\n","step: 3810, loss: 0.10941460728645325\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      0.94      0.89        35\n","           2       0.53      0.70      0.61        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.84      0.90       291\n","           5       0.94      0.79      0.86       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.50      0.94      0.66       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.97      0.97      0.97       901\n","          11       0.99      0.99      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.36      1.00      0.53        13\n","          14       0.33      1.00      0.50        43\n","          15       0.98      0.97      0.97      2778\n","          16       0.87      0.85      0.86      1151\n","          17       0.98      0.98      0.98        41\n","          18       0.94      1.00      0.97        32\n","          19       0.49      0.82      0.62        40\n","          20       1.00      1.00      1.00       584\n","          21       0.14      0.08      0.10        52\n","          22       0.95      0.73      0.83      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.34      0.77      0.47        44\n","          25       0.87      0.90      0.89       888\n","          26       1.00      0.78      0.88         9\n","          27       0.95      1.00      0.97        69\n","          28       0.99      0.99      0.99      1864\n","          29       0.99      0.97      0.98       344\n","          30       0.87      0.89      0.88      1136\n","          31       0.67      0.74      0.70        19\n","          32       1.00      0.62      0.77         8\n","          33       0.65      0.97      0.78        86\n","          34       0.24      0.62      0.34        32\n","          35       0.98      0.99      0.99       474\n","          36       0.76      0.12      0.21       182\n","          37       0.89      0.91      0.90      1592\n","          38       0.98      0.98      0.98       404\n","          39       0.96      0.92      0.94       485\n","          40       0.93      0.97      0.95       573\n","          41       0.88      0.94      0.91       841\n","          42       0.98      0.99      0.98       575\n","          43       0.94      0.83      0.88       152\n","          44       0.88      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.83      0.79     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 434\n","\n","Loop 52\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9826526641845703\n","step: 10, loss: 2.0843541622161865\n","step: 20, loss: 0.5950069427490234\n","step: 30, loss: 0.5080621838569641\n","step: 40, loss: 0.5369895100593567\n","step: 50, loss: 0.20122016966342926\n","step: 60, loss: 0.15766313672065735\n","step: 70, loss: 0.3583151400089264\n","step: 80, loss: 0.13639657199382782\n","step: 90, loss: 0.1384068876504898\n","step: 100, loss: 0.1984596848487854\n","step: 110, loss: 0.10337593406438828\n","step: 120, loss: 0.16928090155124664\n","step: 130, loss: 0.12109661847352982\n","step: 140, loss: 0.06675271689891815\n","step: 150, loss: 0.12777762115001678\n","step: 160, loss: 0.08939576148986816\n","step: 170, loss: 0.0870320275425911\n","step: 180, loss: 0.09719623625278473\n","step: 190, loss: 0.14430908858776093\n","step: 200, loss: 0.13102532923221588\n","step: 210, loss: 0.10081414878368378\n","step: 220, loss: 0.14174515008926392\n","step: 230, loss: 0.05947151407599449\n","step: 240, loss: 0.04016437754034996\n","step: 250, loss: 0.06868080794811249\n","step: 260, loss: 0.17887797951698303\n","step: 270, loss: 0.14850395917892456\n","step: 280, loss: 0.2092236429452896\n","step: 290, loss: 0.11927245557308197\n","step: 300, loss: 0.03667355701327324\n","step: 310, loss: 0.05747571215033531\n","step: 320, loss: 0.16890287399291992\n","step: 330, loss: 0.09012100845575333\n","step: 340, loss: 0.06400889903306961\n","step: 350, loss: 0.05967463180422783\n","step: 360, loss: 0.1064193919301033\n","step: 370, loss: 0.09658854454755783\n","step: 380, loss: 0.11852133274078369\n","step: 390, loss: 0.15519380569458008\n","step: 400, loss: 0.09498931467533112\n","step: 410, loss: 0.22646832466125488\n","step: 420, loss: 0.1517772525548935\n","step: 430, loss: 0.02595006301999092\n","step: 440, loss: 0.07522143423557281\n","step: 450, loss: 0.16877830028533936\n","step: 460, loss: 0.17953425645828247\n","step: 470, loss: 0.093989297747612\n","step: 480, loss: 0.1191163957118988\n","step: 490, loss: 0.15534526109695435\n","step: 500, loss: 0.07499173283576965\n","step: 510, loss: 0.10945010930299759\n","step: 520, loss: 0.0817459374666214\n","step: 530, loss: 0.049574293196201324\n","step: 540, loss: 0.09830337762832642\n","step: 550, loss: 0.05803542211651802\n","step: 560, loss: 0.12252859771251678\n","step: 570, loss: 0.10574764758348465\n","step: 580, loss: 0.13609778881072998\n","step: 590, loss: 0.055042628198862076\n","step: 600, loss: 0.07920672744512558\n","step: 610, loss: 0.18094684183597565\n","step: 620, loss: 0.17987771332263947\n","step: 630, loss: 0.05505760386586189\n","step: 640, loss: 0.04249243438243866\n","step: 650, loss: 0.19856101274490356\n","step: 660, loss: 0.0947742909193039\n","step: 670, loss: 0.08443491160869598\n","step: 680, loss: 0.1135464459657669\n","step: 690, loss: 0.18683095276355743\n","step: 700, loss: 0.07519596815109253\n","step: 710, loss: 0.06329554319381714\n","step: 720, loss: 0.12960542738437653\n","step: 730, loss: 0.15400823950767517\n","step: 740, loss: 0.03767301142215729\n","step: 750, loss: 0.1315540075302124\n","step: 760, loss: 0.11539697647094727\n","step: 770, loss: 0.07967444509267807\n","step: 780, loss: 0.2142544388771057\n","step: 790, loss: 0.3506859838962555\n","step: 800, loss: 0.12708374857902527\n","step: 810, loss: 0.10081113874912262\n","step: 820, loss: 0.07864142954349518\n","step: 830, loss: 0.04459336772561073\n","step: 840, loss: 0.08500377088785172\n","step: 850, loss: 0.21801172196865082\n","step: 860, loss: 0.10036693513393402\n","step: 870, loss: 0.05325658246874809\n","step: 880, loss: 0.18179981410503387\n","step: 890, loss: 0.05634399875998497\n","step: 900, loss: 0.19339203834533691\n","step: 910, loss: 0.056809909641742706\n","step: 920, loss: 0.12194253504276276\n","step: 930, loss: 0.1409134864807129\n","step: 940, loss: 0.03082728013396263\n","step: 950, loss: 0.07128550112247467\n","step: 960, loss: 0.057312432676553726\n","step: 970, loss: 0.12248631566762924\n","step: 980, loss: 0.041501324623823166\n","step: 990, loss: 0.13311229646205902\n","step: 1000, loss: 0.06470687687397003\n","step: 1010, loss: 0.07427114248275757\n","step: 1020, loss: 0.032388027757406235\n","step: 1030, loss: 0.083663210272789\n","step: 1040, loss: 0.09943000227212906\n","step: 1050, loss: 0.11043834686279297\n","step: 1060, loss: 0.05999217927455902\n","step: 1070, loss: 0.07258152216672897\n","step: 1080, loss: 0.13137026131153107\n","step: 1090, loss: 0.0656592920422554\n","step: 1100, loss: 0.06279058009386063\n","step: 1110, loss: 0.13161629438400269\n","step: 1120, loss: 0.07653414458036423\n","step: 1130, loss: 0.07777731120586395\n","step: 1140, loss: 0.06510987132787704\n","step: 1150, loss: 0.09765759110450745\n","step: 1160, loss: 0.05004632845520973\n","step: 1170, loss: 0.08943531662225723\n","step: 1180, loss: 0.07873380184173584\n","step: 1190, loss: 0.11491077393293381\n","step: 1200, loss: 0.05564875528216362\n","step: 1210, loss: 0.041473280638456345\n","step: 1220, loss: 0.18941418826580048\n","step: 1230, loss: 0.09588763862848282\n","step: 1240, loss: 0.1962423175573349\n","step: 1250, loss: 0.11183535307645798\n","step: 1260, loss: 0.04020196944475174\n","step: 1270, loss: 0.2494065910577774\n","step: 1280, loss: 0.07196338474750519\n","step: 1290, loss: 0.1431596875190735\n","step: 1300, loss: 0.0767180547118187\n","step: 1310, loss: 0.2060420960187912\n","step: 1320, loss: 0.04876342788338661\n","step: 1330, loss: 0.03208369389176369\n","step: 1340, loss: 0.02430686354637146\n","step: 1350, loss: 0.1086825281381607\n","step: 1360, loss: 0.0695619210600853\n","step: 1370, loss: 0.0427730455994606\n","step: 1380, loss: 0.10836876183748245\n","step: 1390, loss: 0.08015497773885727\n","step: 1400, loss: 0.026765158399939537\n","step: 1410, loss: 0.04653603956103325\n","step: 1420, loss: 0.11959461122751236\n","step: 1430, loss: 0.14693540334701538\n","step: 1440, loss: 0.05135189741849899\n","step: 1450, loss: 0.20736055076122284\n","step: 1460, loss: 0.03747539594769478\n","step: 1470, loss: 0.11565449088811874\n","step: 1480, loss: 0.20868733525276184\n","step: 1490, loss: 0.057691626250743866\n","step: 1500, loss: 0.05533932149410248\n","step: 1510, loss: 0.04375014454126358\n","step: 1520, loss: 0.13121312856674194\n","step: 1530, loss: 0.053156111389398575\n","step: 1540, loss: 0.03294103965163231\n","step: 1550, loss: 0.030327467247843742\n","step: 1560, loss: 0.07366872578859329\n","step: 1570, loss: 0.12150382995605469\n","step: 1580, loss: 0.1642039716243744\n","step: 1590, loss: 0.11853080242872238\n","step: 1600, loss: 0.13076268136501312\n","step: 1610, loss: 0.04130196571350098\n","step: 1620, loss: 0.15890507400035858\n","step: 1630, loss: 0.07523774355649948\n","step: 1640, loss: 0.16447202861309052\n","step: 1650, loss: 0.0854448452591896\n","step: 1660, loss: 0.02586253546178341\n","step: 1670, loss: 0.06066484376788139\n","step: 1680, loss: 0.049058739095926285\n","step: 1690, loss: 0.1343342512845993\n","step: 1700, loss: 0.1367136836051941\n","step: 1710, loss: 0.11245990544557571\n","step: 1720, loss: 0.08372752368450165\n","step: 1730, loss: 0.07968149334192276\n","step: 1740, loss: 0.07161431759595871\n","step: 1750, loss: 0.14015161991119385\n","step: 1760, loss: 0.1588601917028427\n","step: 1770, loss: 0.1521337777376175\n","step: 1780, loss: 0.09144782274961472\n","step: 1790, loss: 0.1734483540058136\n","step: 1800, loss: 0.11671896278858185\n","step: 1810, loss: 0.09122548252344131\n","step: 1820, loss: 0.08291880041360855\n","step: 1830, loss: 0.17665961384773254\n","step: 1840, loss: 0.0555938221514225\n","step: 1850, loss: 0.10002204775810242\n","step: 1860, loss: 0.0913938358426094\n","step: 1870, loss: 0.03436962142586708\n","step: 1880, loss: 0.03156503662467003\n","step: 1890, loss: 0.1583707332611084\n","step: 1900, loss: 0.04906279221177101\n","step: 1910, loss: 0.07858971506357193\n","step: 1920, loss: 0.1950656622648239\n","step: 1930, loss: 0.07207994908094406\n","step: 1940, loss: 0.1869501769542694\n","step: 1950, loss: 0.09647496044635773\n","step: 1960, loss: 0.12117883563041687\n","step: 1970, loss: 0.036244072020053864\n","step: 1980, loss: 0.05782775580883026\n","step: 1990, loss: 0.10913605988025665\n","step: 2000, loss: 0.11108968406915665\n","step: 2010, loss: 0.0411909855902195\n","step: 2020, loss: 0.06441259384155273\n","step: 2030, loss: 0.09437216073274612\n","step: 2040, loss: 0.04364897683262825\n","step: 2050, loss: 0.044050928205251694\n","step: 2060, loss: 0.0470082126557827\n","step: 2070, loss: 0.07964819669723511\n","step: 2080, loss: 0.04375668242573738\n","step: 2090, loss: 0.14897994697093964\n","step: 2100, loss: 0.13041485846042633\n","step: 2110, loss: 0.06772452592849731\n","step: 2120, loss: 0.0856388509273529\n","step: 2130, loss: 0.06469826400279999\n","step: 2140, loss: 0.09782061725854874\n","step: 2150, loss: 0.07685744017362595\n","step: 2160, loss: 0.07297464460134506\n","step: 2170, loss: 0.04089638963341713\n","step: 2180, loss: 0.09832970798015594\n","step: 2190, loss: 0.09091359376907349\n","step: 2200, loss: 0.17412053048610687\n","step: 2210, loss: 0.07602022588253021\n","step: 2220, loss: 0.048904795199632645\n","step: 2230, loss: 0.04533357545733452\n","step: 2240, loss: 0.07658269256353378\n","step: 2250, loss: 0.023782121017575264\n","step: 2260, loss: 0.07177329808473587\n","step: 2270, loss: 0.07691065222024918\n","step: 2280, loss: 0.021226217970252037\n","step: 2290, loss: 0.08869405835866928\n","step: 2300, loss: 0.09018216282129288\n","step: 2310, loss: 0.11205635219812393\n","step: 2320, loss: 0.10051620006561279\n","step: 2330, loss: 0.1314813792705536\n","step: 2340, loss: 0.035075489431619644\n","step: 2350, loss: 0.12608371675014496\n","step: 2360, loss: 0.087319016456604\n","step: 2370, loss: 0.02560410648584366\n","step: 2380, loss: 0.08239612728357315\n","step: 2390, loss: 0.16794313490390778\n","step: 2400, loss: 0.21859464049339294\n","step: 2410, loss: 0.05627884343266487\n","step: 2420, loss: 0.07316994667053223\n","step: 2430, loss: 0.14982227981090546\n","step: 2440, loss: 0.06380469352006912\n","step: 2450, loss: 0.11533211171627045\n","step: 2460, loss: 0.12415625900030136\n","step: 2470, loss: 0.07153195142745972\n","step: 2480, loss: 0.10518931597471237\n","step: 2490, loss: 0.15920394659042358\n","step: 2500, loss: 0.1781051605939865\n","step: 2510, loss: 0.16205522418022156\n","step: 2520, loss: 0.11688993126153946\n","step: 2530, loss: 0.09548480808734894\n","step: 2540, loss: 0.09374348819255829\n","step: 2550, loss: 0.07751430571079254\n","step: 2560, loss: 0.11778854578733444\n","step: 2570, loss: 0.09163934737443924\n","step: 2580, loss: 0.11633935570716858\n","step: 2590, loss: 0.10171365737915039\n","step: 2600, loss: 0.07926351577043533\n","step: 2610, loss: 0.15362054109573364\n","step: 2620, loss: 0.21045827865600586\n","step: 2630, loss: 0.09740094095468521\n","step: 2640, loss: 0.12472283840179443\n","step: 2650, loss: 0.05688781291246414\n","step: 2660, loss: 0.10144227743148804\n","step: 2670, loss: 0.15574099123477936\n","step: 2680, loss: 0.13634857535362244\n","step: 2690, loss: 0.10599976778030396\n","step: 2700, loss: 0.11456000059843063\n","step: 2710, loss: 0.09636735171079636\n","step: 2720, loss: 0.07957132905721664\n","step: 2730, loss: 0.06735223531723022\n","step: 2740, loss: 0.08099109679460526\n","step: 2750, loss: 0.07932525873184204\n","step: 2760, loss: 0.12320511788129807\n","step: 2770, loss: 0.11000778526067734\n","step: 2780, loss: 0.0933789610862732\n","step: 2790, loss: 0.11096806824207306\n","step: 2800, loss: 0.05029039829969406\n","step: 2810, loss: 0.11858731508255005\n","step: 2820, loss: 0.11049358546733856\n","step: 2830, loss: 0.11144208908081055\n","step: 2840, loss: 0.05929435417056084\n","step: 2850, loss: 0.160831481218338\n","step: 2860, loss: 0.0904369056224823\n","step: 2870, loss: 0.09575815498828888\n","step: 2880, loss: 0.14832477271556854\n","step: 2890, loss: 0.08416662365198135\n","step: 2900, loss: 0.11305475980043411\n","step: 2910, loss: 0.21239455044269562\n","step: 2920, loss: 0.019158003851771355\n","step: 2930, loss: 0.1326536238193512\n","step: 2940, loss: 0.17355380952358246\n","step: 2950, loss: 0.10691910982131958\n","step: 2960, loss: 0.05284520983695984\n","step: 2970, loss: 0.12381931394338608\n","step: 2980, loss: 0.09268681704998016\n","step: 2990, loss: 0.03944224491715431\n","step: 3000, loss: 0.07043245434761047\n","step: 3010, loss: 0.09933991730213165\n","step: 3020, loss: 0.046135369688272476\n","step: 3030, loss: 0.04335515573620796\n","step: 3040, loss: 0.09977836906909943\n","step: 3050, loss: 0.09018543362617493\n","step: 3060, loss: 0.05629981681704521\n","step: 3070, loss: 0.11801813542842865\n","step: 3080, loss: 0.05542147904634476\n","step: 3090, loss: 0.03660886362195015\n","step: 3100, loss: 0.12124338001012802\n","step: 3110, loss: 0.16544418036937714\n","step: 3120, loss: 0.10662929713726044\n","step: 3130, loss: 0.06038324162364006\n","step: 3140, loss: 0.1719636619091034\n","step: 3150, loss: 0.04172484949231148\n","step: 3160, loss: 0.16319403052330017\n","step: 3170, loss: 0.20172636210918427\n","step: 3180, loss: 0.1794213205575943\n","step: 3190, loss: 0.07251003384590149\n","step: 3200, loss: 0.20047397911548615\n","step: 3210, loss: 0.05021785944700241\n","step: 3220, loss: 0.13494814932346344\n","step: 3230, loss: 0.11980775743722916\n","step: 3240, loss: 0.05194421857595444\n","step: 3250, loss: 0.04228382185101509\n","step: 3260, loss: 0.04360554739832878\n","step: 3270, loss: 0.03759828582406044\n","step: 3280, loss: 0.11743539571762085\n","step: 3290, loss: 0.08048185706138611\n","step: 3300, loss: 0.03702251613140106\n","step: 3310, loss: 0.0729634016752243\n","step: 3320, loss: 0.08278128504753113\n","step: 3330, loss: 0.06095258146524429\n","step: 3340, loss: 0.07246191054582596\n","step: 3350, loss: 0.18246036767959595\n","step: 3360, loss: 0.1762428730726242\n","step: 3370, loss: 0.06067030876874924\n","step: 3380, loss: 0.06853069365024567\n","step: 3390, loss: 0.1108592078089714\n","step: 3400, loss: 0.04654114693403244\n","step: 3410, loss: 0.13304129242897034\n","step: 3420, loss: 0.10609828680753708\n","step: 3430, loss: 0.1872376948595047\n","step: 3440, loss: 0.1011795699596405\n","step: 3450, loss: 0.07666368037462234\n","step: 3460, loss: 0.04789309576153755\n","step: 3470, loss: 0.018688606098294258\n","step: 3480, loss: 0.06696061789989471\n","step: 3490, loss: 0.09810091555118561\n","step: 3500, loss: 0.09815483540296555\n","step: 3510, loss: 0.0812760666012764\n","step: 3520, loss: 0.1022004559636116\n","step: 3530, loss: 0.1009712964296341\n","step: 3540, loss: 0.07276104390621185\n","step: 3550, loss: 0.0799347311258316\n","step: 3560, loss: 0.04209713265299797\n","step: 3570, loss: 0.1463518589735031\n","step: 3580, loss: 0.12740904092788696\n","step: 3590, loss: 0.028780607506632805\n","step: 3600, loss: 0.03713131323456764\n","step: 3610, loss: 0.02553720213472843\n","step: 3620, loss: 0.03134756535291672\n","step: 3630, loss: 0.12529005110263824\n","step: 3640, loss: 0.11609411984682083\n","step: 3650, loss: 0.1310175508260727\n","step: 3660, loss: 0.08172314614057541\n","step: 3670, loss: 0.049010515213012695\n","step: 3680, loss: 0.05297533795237541\n","step: 3690, loss: 0.07998853176832199\n","step: 3700, loss: 0.09710799902677536\n","step: 3710, loss: 0.0318397581577301\n","step: 3720, loss: 0.06116290017962456\n","step: 3730, loss: 0.038826655596494675\n","step: 3740, loss: 0.03983727842569351\n","step: 3750, loss: 0.10258336365222931\n","step: 3760, loss: 0.09144557267427444\n","step: 3770, loss: 0.118161641061306\n","step: 3780, loss: 0.08609624207019806\n","step: 3790, loss: 0.13772588968276978\n","step: 3800, loss: 0.09076277911663055\n","step: 3810, loss: 0.10661258548498154\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      1.00      0.89        35\n","           2       0.63      0.87      0.73        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.80      0.84      0.82       291\n","           5       0.90      0.84      0.87       294\n","           6       0.97      0.99      0.98      1570\n","           7       0.66      0.94      0.77       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.97      0.98      0.97       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.44      0.92      0.60        13\n","          14       0.37      1.00      0.54        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.82      0.86      0.84      1151\n","          17       0.88      0.93      0.90        41\n","          18       0.82      0.88      0.85        32\n","          19       0.67      0.78      0.72        40\n","          20       0.99      1.00      0.99       584\n","          21       0.16      0.21      0.18        52\n","          22       0.96      0.72      0.82      4175\n","          23       0.69      0.96      0.80      2253\n","          24       0.30      0.68      0.41        44\n","          25       0.85      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       1.00      0.97      0.99        69\n","          28       0.99      0.99      0.99      1864\n","          29       0.99      0.97      0.98       344\n","          30       0.81      0.88      0.84      1136\n","          31       0.65      0.68      0.67        19\n","          32       0.64      0.88      0.74         8\n","          33       0.81      0.94      0.87        86\n","          34       0.25      0.69      0.37        32\n","          35       0.95      0.99      0.97       474\n","          36       0.68      0.21      0.33       182\n","          37       0.95      0.93      0.94      1592\n","          38       0.97      0.96      0.97       404\n","          39       0.97      0.95      0.96       485\n","          40       0.93      0.82      0.87       573\n","          41       0.90      0.96      0.93       841\n","          42       0.96      0.99      0.98       575\n","          43       0.93      0.91      0.92       152\n","          44       0.86      0.92      0.89        75\n","          46       0.99      0.99      0.99        82\n","          48       0.44      0.05      0.09        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.84      0.80     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 438\n","\n","Loop 53\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.05072546005249\n","step: 10, loss: 2.079935312271118\n","step: 20, loss: 1.1515299081802368\n","step: 30, loss: 0.5111343264579773\n","step: 40, loss: 0.34140393137931824\n","step: 50, loss: 0.24725016951560974\n","step: 60, loss: 0.25193795561790466\n","step: 70, loss: 0.1720106601715088\n","step: 80, loss: 0.26305127143859863\n","step: 90, loss: 0.17262904345989227\n","step: 100, loss: 0.24584728479385376\n","step: 110, loss: 0.20003953576087952\n","step: 120, loss: 0.1853145956993103\n","step: 130, loss: 0.07296455651521683\n","step: 140, loss: 0.08162181079387665\n","step: 150, loss: 0.1814326047897339\n","step: 160, loss: 0.13427726924419403\n","step: 170, loss: 0.07466243207454681\n","step: 180, loss: 0.19029615819454193\n","step: 190, loss: 0.16631749272346497\n","step: 200, loss: 0.17219825088977814\n","step: 210, loss: 0.2518295347690582\n","step: 220, loss: 0.07000984251499176\n","step: 230, loss: 0.15591128170490265\n","step: 240, loss: 0.1454756110906601\n","step: 250, loss: 0.10654862970113754\n","step: 260, loss: 0.10162365436553955\n","step: 270, loss: 0.0440516360104084\n","step: 280, loss: 0.125188946723938\n","step: 290, loss: 0.1336802840232849\n","step: 300, loss: 0.10293243080377579\n","step: 310, loss: 0.061583012342453\n","step: 320, loss: 0.06407708674669266\n","step: 330, loss: 0.10279820114374161\n","step: 340, loss: 0.1280059814453125\n","step: 350, loss: 0.02817417122423649\n","step: 360, loss: 0.11455985903739929\n","step: 370, loss: 0.02154463343322277\n","step: 380, loss: 0.08997637778520584\n","step: 390, loss: 0.2640627324581146\n","step: 400, loss: 0.03947337716817856\n","step: 410, loss: 0.17146898806095123\n","step: 420, loss: 0.20845963060855865\n","step: 430, loss: 0.05827309191226959\n","step: 440, loss: 0.11714579910039902\n","step: 450, loss: 0.08677477389574051\n","step: 460, loss: 0.057844746857881546\n","step: 470, loss: 0.12787936627864838\n","step: 480, loss: 0.0715240091085434\n","step: 490, loss: 0.15985701978206635\n","step: 500, loss: 0.10766284912824631\n","step: 510, loss: 0.11351200938224792\n","step: 520, loss: 0.05563698709011078\n","step: 530, loss: 0.1720803827047348\n","step: 540, loss: 0.1420002430677414\n","step: 550, loss: 0.12401998043060303\n","step: 560, loss: 0.08486279100179672\n","step: 570, loss: 0.08124450594186783\n","step: 580, loss: 0.05045118182897568\n","step: 590, loss: 0.15963074564933777\n","step: 600, loss: 0.09919136017560959\n","step: 610, loss: 0.13583672046661377\n","step: 620, loss: 0.05310698598623276\n","step: 630, loss: 0.10117895901203156\n","step: 640, loss: 0.21057434380054474\n","step: 650, loss: 0.062555231153965\n","step: 660, loss: 0.10757845640182495\n","step: 670, loss: 0.07775535434484482\n","step: 680, loss: 0.07225801795721054\n","step: 690, loss: 0.1006244346499443\n","step: 700, loss: 0.07397687435150146\n","step: 710, loss: 0.08203520625829697\n","step: 720, loss: 0.08028354495763779\n","step: 730, loss: 0.07115021347999573\n","step: 740, loss: 0.14577646553516388\n","step: 750, loss: 0.08414983749389648\n","step: 760, loss: 0.06541343033313751\n","step: 770, loss: 0.12841859459877014\n","step: 780, loss: 0.12326538562774658\n","step: 790, loss: 0.07315127551555634\n","step: 800, loss: 0.09399174898862839\n","step: 810, loss: 0.17493684589862823\n","step: 820, loss: 0.128767192363739\n","step: 830, loss: 0.04185090959072113\n","step: 840, loss: 0.13809141516685486\n","step: 850, loss: 0.09201963245868683\n","step: 860, loss: 0.08131934702396393\n","step: 870, loss: 0.07780051231384277\n","step: 880, loss: 0.1739199459552765\n","step: 890, loss: 0.07538747042417526\n","step: 900, loss: 0.10108646005392075\n","step: 910, loss: 0.07563845813274384\n","step: 920, loss: 0.059039659798145294\n","step: 930, loss: 0.05442181974649429\n","step: 940, loss: 0.06229403242468834\n","step: 950, loss: 0.06892374902963638\n","step: 960, loss: 0.10688408464193344\n","step: 970, loss: 0.05608842894434929\n","step: 980, loss: 0.11082082241773605\n","step: 990, loss: 0.10584351420402527\n","step: 1000, loss: 0.08099261671304703\n","step: 1010, loss: 0.10551658272743225\n","step: 1020, loss: 0.12390518188476562\n","step: 1030, loss: 0.13656245172023773\n","step: 1040, loss: 0.06911444664001465\n","step: 1050, loss: 0.09853234142065048\n","step: 1060, loss: 0.07570767402648926\n","step: 1070, loss: 0.1875874251127243\n","step: 1080, loss: 0.051347989588975906\n","step: 1090, loss: 0.03686746954917908\n","step: 1100, loss: 0.08611750602722168\n","step: 1110, loss: 0.07544156163930893\n","step: 1120, loss: 0.10842856019735336\n","step: 1130, loss: 0.03869662061333656\n","step: 1140, loss: 0.2089870572090149\n","step: 1150, loss: 0.10913075506687164\n","step: 1160, loss: 0.13424398005008698\n","step: 1170, loss: 0.06327049434185028\n","step: 1180, loss: 0.06516588479280472\n","step: 1190, loss: 0.14414732158184052\n","step: 1200, loss: 0.07565047591924667\n","step: 1210, loss: 0.10284869372844696\n","step: 1220, loss: 0.10737236589193344\n","step: 1230, loss: 0.12333069741725922\n","step: 1240, loss: 0.12375594675540924\n","step: 1250, loss: 0.028017861768603325\n","step: 1260, loss: 0.05350364372134209\n","step: 1270, loss: 0.11616041511297226\n","step: 1280, loss: 0.11117205023765564\n","step: 1290, loss: 0.04458252340555191\n","step: 1300, loss: 0.058758728206157684\n","step: 1310, loss: 0.0630372166633606\n","step: 1320, loss: 0.07351285219192505\n","step: 1330, loss: 0.1834224909543991\n","step: 1340, loss: 0.03295379877090454\n","step: 1350, loss: 0.1463763415813446\n","step: 1360, loss: 0.08346288651227951\n","step: 1370, loss: 0.06695941835641861\n","step: 1380, loss: 0.1536959856748581\n","step: 1390, loss: 0.10145796090364456\n","step: 1400, loss: 0.05663830414414406\n","step: 1410, loss: 0.16526387631893158\n","step: 1420, loss: 0.052639275789260864\n","step: 1430, loss: 0.11216618865728378\n","step: 1440, loss: 0.04712069034576416\n","step: 1450, loss: 0.05547793209552765\n","step: 1460, loss: 0.1600378304719925\n","step: 1470, loss: 0.0448482409119606\n","step: 1480, loss: 0.10823022574186325\n","step: 1490, loss: 0.047744449228048325\n","step: 1500, loss: 0.08012732118368149\n","step: 1510, loss: 0.03514689579606056\n","step: 1520, loss: 0.056287046521902084\n","step: 1530, loss: 0.14233596622943878\n","step: 1540, loss: 0.03996649011969566\n","step: 1550, loss: 0.08901175111532211\n","step: 1560, loss: 0.08131742477416992\n","step: 1570, loss: 0.09430092573165894\n","step: 1580, loss: 0.07875389605760574\n","step: 1590, loss: 0.06249811127781868\n","step: 1600, loss: 0.13686063885688782\n","step: 1610, loss: 0.061920423060655594\n","step: 1620, loss: 0.043965164572000504\n","step: 1630, loss: 0.1747289001941681\n","step: 1640, loss: 0.14319390058517456\n","step: 1650, loss: 0.0805746391415596\n","step: 1660, loss: 0.19726473093032837\n","step: 1670, loss: 0.08089748024940491\n","step: 1680, loss: 0.15368440747261047\n","step: 1690, loss: 0.1752559095621109\n","step: 1700, loss: 0.07462111860513687\n","step: 1710, loss: 0.11381427943706512\n","step: 1720, loss: 0.061000097543001175\n","step: 1730, loss: 0.13594909012317657\n","step: 1740, loss: 0.02852444164454937\n","step: 1750, loss: 0.1126059740781784\n","step: 1760, loss: 0.05830840393900871\n","step: 1770, loss: 0.012402242980897427\n","step: 1780, loss: 0.0615248829126358\n","step: 1790, loss: 0.15395598113536835\n","step: 1800, loss: 0.05986505746841431\n","step: 1810, loss: 0.03474606201052666\n","step: 1820, loss: 0.14864425361156464\n","step: 1830, loss: 0.07566006481647491\n","step: 1840, loss: 0.048366911709308624\n","step: 1850, loss: 0.11614745110273361\n","step: 1860, loss: 0.10193189233541489\n","step: 1870, loss: 0.09404899179935455\n","step: 1880, loss: 0.10173860192298889\n","step: 1890, loss: 0.1029340997338295\n","step: 1900, loss: 0.038716208189725876\n","step: 1910, loss: 0.10215851664543152\n","step: 1920, loss: 0.057403940707445145\n","step: 1930, loss: 0.13385464251041412\n","step: 1940, loss: 0.04551851376891136\n","step: 1950, loss: 0.11293648928403854\n","step: 1960, loss: 0.14788784086704254\n","step: 1970, loss: 0.16849391162395477\n","step: 1980, loss: 0.08921495079994202\n","step: 1990, loss: 0.08106985688209534\n","step: 2000, loss: 0.08703326433897018\n","step: 2010, loss: 0.16578926146030426\n","step: 2020, loss: 0.0197361521422863\n","step: 2030, loss: 0.1366146206855774\n","step: 2040, loss: 0.09561341255903244\n","step: 2050, loss: 0.09364920109510422\n","step: 2060, loss: 0.057813484221696854\n","step: 2070, loss: 0.07269418984651566\n","step: 2080, loss: 0.11275053769350052\n","step: 2090, loss: 0.14620251953601837\n","step: 2100, loss: 0.06627794355154037\n","step: 2110, loss: 0.015278543345630169\n","step: 2120, loss: 0.0221538245677948\n","step: 2130, loss: 0.05284421145915985\n","step: 2140, loss: 0.06524384766817093\n","step: 2150, loss: 0.11638602614402771\n","step: 2160, loss: 0.04488267004489899\n","step: 2170, loss: 0.09321792423725128\n","step: 2180, loss: 0.13301794230937958\n","step: 2190, loss: 0.1792396754026413\n","step: 2200, loss: 0.04042106121778488\n","step: 2210, loss: 0.17125600576400757\n","step: 2220, loss: 0.09719589352607727\n","step: 2230, loss: 0.07335866242647171\n","step: 2240, loss: 0.14586301147937775\n","step: 2250, loss: 0.1506025642156601\n","step: 2260, loss: 0.08321123570203781\n","step: 2270, loss: 0.011328578926622868\n","step: 2280, loss: 0.01671646535396576\n","step: 2290, loss: 0.12240144610404968\n","step: 2300, loss: 0.0762181207537651\n","step: 2310, loss: 0.07302270084619522\n","step: 2320, loss: 0.15093228220939636\n","step: 2330, loss: 0.0853801965713501\n","step: 2340, loss: 0.11301659047603607\n","step: 2350, loss: 0.12292107194662094\n","step: 2360, loss: 0.05907363072037697\n","step: 2370, loss: 0.14813655614852905\n","step: 2380, loss: 0.1020168736577034\n","step: 2390, loss: 0.06620597094297409\n","step: 2400, loss: 0.07162504643201828\n","step: 2410, loss: 0.08277153223752975\n","step: 2420, loss: 0.06410857290029526\n","step: 2430, loss: 0.08093038946390152\n","step: 2440, loss: 0.1407151222229004\n","step: 2450, loss: 0.09089275449514389\n","step: 2460, loss: 0.05997731536626816\n","step: 2470, loss: 0.06970257312059402\n","step: 2480, loss: 0.1178855150938034\n","step: 2490, loss: 0.04360784962773323\n","step: 2500, loss: 0.09485935419797897\n","step: 2510, loss: 0.049309246242046356\n","step: 2520, loss: 0.0694180577993393\n","step: 2530, loss: 0.04153116047382355\n","step: 2540, loss: 0.08737348765134811\n","step: 2550, loss: 0.02221810072660446\n","step: 2560, loss: 0.05081741511821747\n","step: 2570, loss: 0.08473628014326096\n","step: 2580, loss: 0.04715132713317871\n","step: 2590, loss: 0.08703874796628952\n","step: 2600, loss: 0.13099659979343414\n","step: 2610, loss: 0.07859338819980621\n","step: 2620, loss: 0.0664755254983902\n","step: 2630, loss: 0.0911756083369255\n","step: 2640, loss: 0.057197488844394684\n","step: 2650, loss: 0.08834096044301987\n","step: 2660, loss: 0.03932744637131691\n","step: 2670, loss: 0.019011197611689568\n","step: 2680, loss: 0.2321978062391281\n","step: 2690, loss: 0.07371537387371063\n","step: 2700, loss: 0.05696394667029381\n","step: 2710, loss: 0.15345452725887299\n","step: 2720, loss: 0.044490743428468704\n","step: 2730, loss: 0.08462159335613251\n","step: 2740, loss: 0.09111739695072174\n","step: 2750, loss: 0.09455474466085434\n","step: 2760, loss: 0.04110970348119736\n","step: 2770, loss: 0.12200066447257996\n","step: 2780, loss: 0.07863181084394455\n","step: 2790, loss: 0.1292451024055481\n","step: 2800, loss: 0.21197636425495148\n","step: 2810, loss: 0.08162009716033936\n","step: 2820, loss: 0.08849804848432541\n","step: 2830, loss: 0.08038051426410675\n","step: 2840, loss: 0.06363785266876221\n","step: 2850, loss: 0.08045651018619537\n","step: 2860, loss: 0.1105087623000145\n","step: 2870, loss: 0.05140003189444542\n","step: 2880, loss: 0.09350179135799408\n","step: 2890, loss: 0.24083612859249115\n","step: 2900, loss: 0.11048958450555801\n","step: 2910, loss: 0.1291758418083191\n","step: 2920, loss: 0.11772630363702774\n","step: 2930, loss: 0.07967749983072281\n","step: 2940, loss: 0.02436881512403488\n","step: 2950, loss: 0.02939995564520359\n","step: 2960, loss: 0.13120311498641968\n","step: 2970, loss: 0.09716643393039703\n","step: 2980, loss: 0.08064360171556473\n","step: 2990, loss: 0.046987660229206085\n","step: 3000, loss: 0.08548925817012787\n","step: 3010, loss: 0.06539281457662582\n","step: 3020, loss: 0.06315027177333832\n","step: 3030, loss: 0.056668102741241455\n","step: 3040, loss: 0.10120663791894913\n","step: 3050, loss: 0.15075966715812683\n","step: 3060, loss: 0.061639294028282166\n","step: 3070, loss: 0.04032925143837929\n","step: 3080, loss: 0.14912158250808716\n","step: 3090, loss: 0.08849623799324036\n","step: 3100, loss: 0.1173621267080307\n","step: 3110, loss: 0.18068154156208038\n","step: 3120, loss: 0.04384566470980644\n","step: 3130, loss: 0.05326838046312332\n","step: 3140, loss: 0.12862814962863922\n","step: 3150, loss: 0.06371407955884933\n","step: 3160, loss: 0.10248127579689026\n","step: 3170, loss: 0.07258037477731705\n","step: 3180, loss: 0.04796186462044716\n","step: 3190, loss: 0.12479408085346222\n","step: 3200, loss: 0.024542564526200294\n","step: 3210, loss: 0.06812784075737\n","step: 3220, loss: 0.05709338188171387\n","step: 3230, loss: 0.08083033561706543\n","step: 3240, loss: 0.04910364747047424\n","step: 3250, loss: 0.08767200261354446\n","step: 3260, loss: 0.05412536859512329\n","step: 3270, loss: 0.04762951284646988\n","step: 3280, loss: 0.08277003467082977\n","step: 3290, loss: 0.04765420779585838\n","step: 3300, loss: 0.10602826625108719\n","step: 3310, loss: 0.08236526697874069\n","step: 3320, loss: 0.09953001141548157\n","step: 3330, loss: 0.061688341200351715\n","step: 3340, loss: 0.06497641652822495\n","step: 3350, loss: 0.0389900878071785\n","step: 3360, loss: 0.09578602761030197\n","step: 3370, loss: 0.06358367949724197\n","step: 3380, loss: 0.0847686305642128\n","step: 3390, loss: 0.06448420137166977\n","step: 3400, loss: 0.15027101337909698\n","step: 3410, loss: 0.17276600003242493\n","step: 3420, loss: 0.16019372642040253\n","step: 3430, loss: 0.05834130197763443\n","step: 3440, loss: 0.08825737237930298\n","step: 3450, loss: 0.05489007756114006\n","step: 3460, loss: 0.23383627831935883\n","step: 3470, loss: 0.030158353969454765\n","step: 3480, loss: 0.19521772861480713\n","step: 3490, loss: 0.10127749294042587\n","step: 3500, loss: 0.09015309810638428\n","step: 3510, loss: 0.08716338127851486\n","step: 3520, loss: 0.037521153688430786\n","step: 3530, loss: 0.11045485734939575\n","step: 3540, loss: 0.12560610473155975\n","step: 3550, loss: 0.04858561232686043\n","step: 3560, loss: 0.1373092532157898\n","step: 3570, loss: 0.11878523230552673\n","step: 3580, loss: 0.08894576877355576\n","step: 3590, loss: 0.12085928022861481\n","step: 3600, loss: 0.07513505965471268\n","step: 3610, loss: 0.0783015713095665\n","step: 3620, loss: 0.08383607864379883\n","step: 3630, loss: 0.041061192750930786\n","step: 3640, loss: 0.04992583394050598\n","step: 3650, loss: 0.0817735567688942\n","step: 3660, loss: 0.09592843800783157\n","step: 3670, loss: 0.08357986807823181\n","step: 3680, loss: 0.03448335453867912\n","step: 3690, loss: 0.07976462692022324\n","step: 3700, loss: 0.13893075287342072\n","step: 3710, loss: 0.0558290034532547\n","step: 3720, loss: 0.11564359068870544\n","step: 3730, loss: 0.0920594334602356\n","step: 3740, loss: 0.10093545913696289\n","step: 3750, loss: 0.04826194792985916\n","step: 3760, loss: 0.1337609887123108\n","step: 3770, loss: 0.0579778216779232\n","step: 3780, loss: 0.11328694969415665\n","step: 3790, loss: 0.13176260888576508\n","step: 3800, loss: 0.10589540749788284\n","step: 3810, loss: 0.06369908899068832\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      0.94      0.87        35\n","           2       0.56      0.70      0.62        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.89      0.85      0.87       291\n","           5       0.99      0.84      0.91       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.51      0.96      0.67       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.97      0.98      0.97       901\n","          11       0.99      0.99      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.75      0.92      0.83        13\n","          14       0.45      1.00      0.62        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.89      0.82      0.86      1151\n","          17       0.97      0.95      0.96        41\n","          18       0.96      0.75      0.84        32\n","          19       0.71      0.85      0.77        40\n","          20       1.00      1.00      1.00       584\n","          21       0.28      0.21      0.24        52\n","          22       0.94      0.72      0.82      4175\n","          23       0.68      0.95      0.79      2253\n","          24       0.40      0.68      0.50        44\n","          25       0.82      0.93      0.87       888\n","          26       0.90      1.00      0.95         9\n","          27       1.00      0.99      0.99        69\n","          28       0.97      1.00      0.98      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.92      0.84      0.88      1136\n","          31       0.59      0.84      0.70        19\n","          32       0.43      0.75      0.55         8\n","          33       0.66      0.99      0.79        86\n","          34       0.26      0.66      0.38        32\n","          35       0.99      0.98      0.98       474\n","          36       0.69      0.37      0.48       182\n","          37       0.93      0.96      0.95      1592\n","          38       0.97      0.98      0.98       404\n","          39       0.94      0.98      0.96       485\n","          40       0.92      0.89      0.90       573\n","          41       0.93      0.94      0.94       841\n","          42       0.97      0.99      0.98       575\n","          43       0.96      0.89      0.93       152\n","          44       0.92      0.93      0.93        75\n","          46       1.00      0.96      0.98        82\n","          48       0.73      0.24      0.36        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.85      0.81     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 437\n","\n","Loop 54\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9816367626190186\n","step: 10, loss: 1.9828946590423584\n","step: 20, loss: 0.6430389881134033\n","step: 30, loss: 0.41326093673706055\n","step: 40, loss: 0.33333176374435425\n","step: 50, loss: 0.2781907916069031\n","step: 60, loss: 0.1692461222410202\n","step: 70, loss: 0.11948822438716888\n","step: 80, loss: 0.17902271449565887\n","step: 90, loss: 0.2281421571969986\n","step: 100, loss: 0.2235051542520523\n","step: 110, loss: 0.16443374752998352\n","step: 120, loss: 0.1635790467262268\n","step: 130, loss: 0.05207251012325287\n","step: 140, loss: 0.27641353011131287\n","step: 150, loss: 0.17572878301143646\n","step: 160, loss: 0.11132755130529404\n","step: 170, loss: 0.18299554288387299\n","step: 180, loss: 0.11782286316156387\n","step: 190, loss: 0.1314026415348053\n","step: 200, loss: 0.10674039274454117\n","step: 210, loss: 0.2372926026582718\n","step: 220, loss: 0.12283238768577576\n","step: 230, loss: 0.12082339078187943\n","step: 240, loss: 0.09162910282611847\n","step: 250, loss: 0.06934843212366104\n","step: 260, loss: 0.06759125739336014\n","step: 270, loss: 0.11365526914596558\n","step: 280, loss: 0.13674642145633698\n","step: 290, loss: 0.1371309608221054\n","step: 300, loss: 0.11822966486215591\n","step: 310, loss: 0.15384645760059357\n","step: 320, loss: 0.14003503322601318\n","step: 330, loss: 0.15095685422420502\n","step: 340, loss: 0.17599311470985413\n","step: 350, loss: 0.07395870238542557\n","step: 360, loss: 0.07621784508228302\n","step: 370, loss: 0.09451010823249817\n","step: 380, loss: 0.043238770216703415\n","step: 390, loss: 0.10422441363334656\n","step: 400, loss: 0.17093321681022644\n","step: 410, loss: 0.04782785102725029\n","step: 420, loss: 0.08503810316324234\n","step: 430, loss: 0.17689603567123413\n","step: 440, loss: 0.17070215940475464\n","step: 450, loss: 0.2876613140106201\n","step: 460, loss: 0.17475132644176483\n","step: 470, loss: 0.0816255733370781\n","step: 480, loss: 0.10959180444478989\n","step: 490, loss: 0.06376499682664871\n","step: 500, loss: 0.15869803726673126\n","step: 510, loss: 0.08197019249200821\n","step: 520, loss: 0.1774732768535614\n","step: 530, loss: 0.11635855585336685\n","step: 540, loss: 0.10924969613552094\n","step: 550, loss: 0.12392964214086533\n","step: 560, loss: 0.06296476721763611\n","step: 570, loss: 0.1256864070892334\n","step: 580, loss: 0.06485213339328766\n","step: 590, loss: 0.15487688779830933\n","step: 600, loss: 0.04507501795887947\n","step: 610, loss: 0.09003683179616928\n","step: 620, loss: 0.10784538835287094\n","step: 630, loss: 0.08706636726856232\n","step: 640, loss: 0.09655746817588806\n","step: 650, loss: 0.17197273671627045\n","step: 660, loss: 0.0630139485001564\n","step: 670, loss: 0.06420841813087463\n","step: 680, loss: 0.15624278783798218\n","step: 690, loss: 0.11529652029275894\n","step: 700, loss: 0.1907200664281845\n","step: 710, loss: 0.04709552228450775\n","step: 720, loss: 0.06241516396403313\n","step: 730, loss: 0.1446336805820465\n","step: 740, loss: 0.1501254141330719\n","step: 750, loss: 0.0880826860666275\n","step: 760, loss: 0.06420072168111801\n","step: 770, loss: 0.09478351473808289\n","step: 780, loss: 0.089552141726017\n","step: 790, loss: 0.1187320202589035\n","step: 800, loss: 0.10257848352193832\n","step: 810, loss: 0.09067144244909286\n","step: 820, loss: 0.18387480080127716\n","step: 830, loss: 0.10675543546676636\n","step: 840, loss: 0.12465240061283112\n","step: 850, loss: 0.038417402654886246\n","step: 860, loss: 0.30011868476867676\n","step: 870, loss: 0.09811687469482422\n","step: 880, loss: 0.14172251522541046\n","step: 890, loss: 0.09534749388694763\n","step: 900, loss: 0.09501907974481583\n","step: 910, loss: 0.05521263927221298\n","step: 920, loss: 0.10838091373443604\n","step: 930, loss: 0.07135995477437973\n","step: 940, loss: 0.09025202691555023\n","step: 950, loss: 0.10150600969791412\n","step: 960, loss: 0.07899713516235352\n","step: 970, loss: 0.0763491690158844\n","step: 980, loss: 0.08836524188518524\n","step: 990, loss: 0.04246807470917702\n","step: 1000, loss: 0.0984918624162674\n","step: 1010, loss: 0.0662095695734024\n","step: 1020, loss: 0.06128079816699028\n","step: 1030, loss: 0.04065128043293953\n","step: 1040, loss: 0.10219580680131912\n","step: 1050, loss: 0.039454251527786255\n","step: 1060, loss: 0.03781301900744438\n","step: 1070, loss: 0.2045542150735855\n","step: 1080, loss: 0.16396430134773254\n","step: 1090, loss: 0.11205312609672546\n","step: 1100, loss: 0.12517866492271423\n","step: 1110, loss: 0.1558862328529358\n","step: 1120, loss: 0.13058067858219147\n","step: 1130, loss: 0.20466746389865875\n","step: 1140, loss: 0.16592472791671753\n","step: 1150, loss: 0.16929925978183746\n","step: 1160, loss: 0.0331122949719429\n","step: 1170, loss: 0.09618602693080902\n","step: 1180, loss: 0.06296325474977493\n","step: 1190, loss: 0.063491091132164\n","step: 1200, loss: 0.055518005043268204\n","step: 1210, loss: 0.11238915473222733\n","step: 1220, loss: 0.16265803575515747\n","step: 1230, loss: 0.1253003627061844\n","step: 1240, loss: 0.2412254512310028\n","step: 1250, loss: 0.10493050515651703\n","step: 1260, loss: 0.07630674540996552\n","step: 1270, loss: 0.1685575544834137\n","step: 1280, loss: 0.08203968405723572\n","step: 1290, loss: 0.05481104180216789\n","step: 1300, loss: 0.057945262640714645\n","step: 1310, loss: 0.045848485082387924\n","step: 1320, loss: 0.08924882113933563\n","step: 1330, loss: 0.13899976015090942\n","step: 1340, loss: 0.054038871079683304\n","step: 1350, loss: 0.037757717072963715\n","step: 1360, loss: 0.08085904270410538\n","step: 1370, loss: 0.03752277418971062\n","step: 1380, loss: 0.11781463772058487\n","step: 1390, loss: 0.10060824453830719\n","step: 1400, loss: 0.1131025180220604\n","step: 1410, loss: 0.08200035989284515\n","step: 1420, loss: 0.13606931269168854\n","step: 1430, loss: 0.1087547093629837\n","step: 1440, loss: 0.06980560719966888\n","step: 1450, loss: 0.06367633491754532\n","step: 1460, loss: 0.11904365569353104\n","step: 1470, loss: 0.08959963917732239\n","step: 1480, loss: 0.13170790672302246\n","step: 1490, loss: 0.07053244113922119\n","step: 1500, loss: 0.15899838507175446\n","step: 1510, loss: 0.10216721892356873\n","step: 1520, loss: 0.061207473278045654\n","step: 1530, loss: 0.074086032807827\n","step: 1540, loss: 0.05969034880399704\n","step: 1550, loss: 0.05431457981467247\n","step: 1560, loss: 0.11041513085365295\n","step: 1570, loss: 0.10090351104736328\n","step: 1580, loss: 0.135501429438591\n","step: 1590, loss: 0.06738314777612686\n","step: 1600, loss: 0.21701021492481232\n","step: 1610, loss: 0.19271455705165863\n","step: 1620, loss: 0.10815908014774323\n","step: 1630, loss: 0.11972925066947937\n","step: 1640, loss: 0.023590058088302612\n","step: 1650, loss: 0.15436844527721405\n","step: 1660, loss: 0.08982766419649124\n","step: 1670, loss: 0.14431850612163544\n","step: 1680, loss: 0.06079970300197601\n","step: 1690, loss: 0.06526894867420197\n","step: 1700, loss: 0.07152450084686279\n","step: 1710, loss: 0.0773492157459259\n","step: 1720, loss: 0.0954003781080246\n","step: 1730, loss: 0.13113157451152802\n","step: 1740, loss: 0.1267455369234085\n","step: 1750, loss: 0.06857670098543167\n","step: 1760, loss: 0.10869082063436508\n","step: 1770, loss: 0.13279935717582703\n","step: 1780, loss: 0.0503520667552948\n","step: 1790, loss: 0.07658844441175461\n","step: 1800, loss: 0.19465352594852448\n","step: 1810, loss: 0.01969013549387455\n","step: 1820, loss: 0.0741611197590828\n","step: 1830, loss: 0.035739995539188385\n","step: 1840, loss: 0.17219661176204681\n","step: 1850, loss: 0.03469519689679146\n","step: 1860, loss: 0.07453381270170212\n","step: 1870, loss: 0.14545504748821259\n","step: 1880, loss: 0.10664577782154083\n","step: 1890, loss: 0.09037542343139648\n","step: 1900, loss: 0.13252030313014984\n","step: 1910, loss: 0.12873969972133636\n","step: 1920, loss: 0.09263274073600769\n","step: 1930, loss: 0.1556471586227417\n","step: 1940, loss: 0.15468519926071167\n","step: 1950, loss: 0.10914468765258789\n","step: 1960, loss: 0.13540594279766083\n","step: 1970, loss: 0.06718592345714569\n","step: 1980, loss: 0.06820224970579147\n","step: 1990, loss: 0.06603201478719711\n","step: 2000, loss: 0.03633502125740051\n","step: 2010, loss: 0.05444999039173126\n","step: 2020, loss: 0.08686898648738861\n","step: 2030, loss: 0.05526053532958031\n","step: 2040, loss: 0.18867583572864532\n","step: 2050, loss: 0.06290927529335022\n","step: 2060, loss: 0.11074833571910858\n","step: 2070, loss: 0.09327372908592224\n","step: 2080, loss: 0.0819975808262825\n","step: 2090, loss: 0.051406219601631165\n","step: 2100, loss: 0.057166457176208496\n","step: 2110, loss: 0.03462595492601395\n","step: 2120, loss: 0.10171524435281754\n","step: 2130, loss: 0.06850256770849228\n","step: 2140, loss: 0.07215408980846405\n","step: 2150, loss: 0.1416083127260208\n","step: 2160, loss: 0.03884924575686455\n","step: 2170, loss: 0.06481915712356567\n","step: 2180, loss: 0.12111102789640427\n","step: 2190, loss: 0.0854158028960228\n","step: 2200, loss: 0.02684568427503109\n","step: 2210, loss: 0.09348109364509583\n","step: 2220, loss: 0.10120517760515213\n","step: 2230, loss: 0.17018502950668335\n","step: 2240, loss: 0.07051790505647659\n","step: 2250, loss: 0.0709536224603653\n","step: 2260, loss: 0.06112170219421387\n","step: 2270, loss: 0.06719519197940826\n","step: 2280, loss: 0.07295221090316772\n","step: 2290, loss: 0.18628397583961487\n","step: 2300, loss: 0.0623733215034008\n","step: 2310, loss: 0.08437441289424896\n","step: 2320, loss: 0.057295721024274826\n","step: 2330, loss: 0.06990528851747513\n","step: 2340, loss: 0.04416007548570633\n","step: 2350, loss: 0.08965978026390076\n","step: 2360, loss: 0.03926727548241615\n","step: 2370, loss: 0.10049401968717575\n","step: 2380, loss: 0.11972954124212265\n","step: 2390, loss: 0.06753693521022797\n","step: 2400, loss: 0.13553334772586823\n","step: 2410, loss: 0.18420708179473877\n","step: 2420, loss: 0.04610282927751541\n","step: 2430, loss: 0.04449469596147537\n","step: 2440, loss: 0.17455200850963593\n","step: 2450, loss: 0.13446395099163055\n","step: 2460, loss: 0.13845856487751007\n","step: 2470, loss: 0.03437385335564613\n","step: 2480, loss: 0.05524427816271782\n","step: 2490, loss: 0.0765497237443924\n","step: 2500, loss: 0.08559346944093704\n","step: 2510, loss: 0.08819687366485596\n","step: 2520, loss: 0.05911029502749443\n","step: 2530, loss: 0.09984514862298965\n","step: 2540, loss: 0.05713137611746788\n","step: 2550, loss: 0.1933552324771881\n","step: 2560, loss: 0.02296276018023491\n","step: 2570, loss: 0.027425963431596756\n","step: 2580, loss: 0.0575820729136467\n","step: 2590, loss: 0.05413294956088066\n","step: 2600, loss: 0.10421086847782135\n","step: 2610, loss: 0.12902331352233887\n","step: 2620, loss: 0.08025868982076645\n","step: 2630, loss: 0.06163846328854561\n","step: 2640, loss: 0.08229736983776093\n","step: 2650, loss: 0.13954852521419525\n","step: 2660, loss: 0.09457682818174362\n","step: 2670, loss: 0.06329060345888138\n","step: 2680, loss: 0.0714893639087677\n","step: 2690, loss: 0.07943407446146011\n","step: 2700, loss: 0.08839362114667892\n","step: 2710, loss: 0.013833921402692795\n","step: 2720, loss: 0.0998891070485115\n","step: 2730, loss: 0.1291298121213913\n","step: 2740, loss: 0.05581462383270264\n","step: 2750, loss: 0.09486006945371628\n","step: 2760, loss: 0.121256522834301\n","step: 2770, loss: 0.15599195659160614\n","step: 2780, loss: 0.09244359284639359\n","step: 2790, loss: 0.0670008584856987\n","step: 2800, loss: 0.043644629418849945\n","step: 2810, loss: 0.05112110450863838\n","step: 2820, loss: 0.02887670136988163\n","step: 2830, loss: 0.07601860910654068\n","step: 2840, loss: 0.10502582043409348\n","step: 2850, loss: 0.11270727217197418\n","step: 2860, loss: 0.06505509465932846\n","step: 2870, loss: 0.1398991197347641\n","step: 2880, loss: 0.06838405132293701\n","step: 2890, loss: 0.10552705824375153\n","step: 2900, loss: 0.10687457770109177\n","step: 2910, loss: 0.07432738691568375\n","step: 2920, loss: 0.05639796704053879\n","step: 2930, loss: 0.044645603746175766\n","step: 2940, loss: 0.09354158490896225\n","step: 2950, loss: 0.09553217887878418\n","step: 2960, loss: 0.15092070400714874\n","step: 2970, loss: 0.06196289882063866\n","step: 2980, loss: 0.11512605100870132\n","step: 2990, loss: 0.03576910123229027\n","step: 3000, loss: 0.0772244855761528\n","step: 3010, loss: 0.07971984893083572\n","step: 3020, loss: 0.07947981357574463\n","step: 3030, loss: 0.06253337860107422\n","step: 3040, loss: 0.07159459590911865\n","step: 3050, loss: 0.019330279901623726\n","step: 3060, loss: 0.13284240663051605\n","step: 3070, loss: 0.07560417056083679\n","step: 3080, loss: 0.06653806567192078\n","step: 3090, loss: 0.12008952349424362\n","step: 3100, loss: 0.03965571150183678\n","step: 3110, loss: 0.043739188462495804\n","step: 3120, loss: 0.059166401624679565\n","step: 3130, loss: 0.05306750163435936\n","step: 3140, loss: 0.05152837932109833\n","step: 3150, loss: 0.05419256165623665\n","step: 3160, loss: 0.15104305744171143\n","step: 3170, loss: 0.05683653801679611\n","step: 3180, loss: 0.09634952992200851\n","step: 3190, loss: 0.10605081915855408\n","step: 3200, loss: 0.058364108204841614\n","step: 3210, loss: 0.053829092532396317\n","step: 3220, loss: 0.10977141559123993\n","step: 3230, loss: 0.05497308820486069\n","step: 3240, loss: 0.16541767120361328\n","step: 3250, loss: 0.06948396563529968\n","step: 3260, loss: 0.11532186716794968\n","step: 3270, loss: 0.11997342854738235\n","step: 3280, loss: 0.09055956453084946\n","step: 3290, loss: 0.08142029494047165\n","step: 3300, loss: 0.052740659564733505\n","step: 3310, loss: 0.11877890676259995\n","step: 3320, loss: 0.2179986834526062\n","step: 3330, loss: 0.07197637110948563\n","step: 3340, loss: 0.028385654091835022\n","step: 3350, loss: 0.09836462140083313\n","step: 3360, loss: 0.025043435394763947\n","step: 3370, loss: 0.06792597472667694\n","step: 3380, loss: 0.12338997423648834\n","step: 3390, loss: 0.01677297055721283\n","step: 3400, loss: 0.08309616893529892\n","step: 3410, loss: 0.07252187281847\n","step: 3420, loss: 0.10409554839134216\n","step: 3430, loss: 0.051579639315605164\n","step: 3440, loss: 0.0839388370513916\n","step: 3450, loss: 0.08676251024007797\n","step: 3460, loss: 0.17960882186889648\n","step: 3470, loss: 0.08523145318031311\n","step: 3480, loss: 0.12954194843769073\n","step: 3490, loss: 0.15070147812366486\n","step: 3500, loss: 0.1312481164932251\n","step: 3510, loss: 0.0788731500506401\n","step: 3520, loss: 0.0824025571346283\n","step: 3530, loss: 0.056698232889175415\n","step: 3540, loss: 0.08807215839624405\n","step: 3550, loss: 0.0815713182091713\n","step: 3560, loss: 0.11358760297298431\n","step: 3570, loss: 0.05940786004066467\n","step: 3580, loss: 0.08831199258565903\n","step: 3590, loss: 0.05210110545158386\n","step: 3600, loss: 0.0606611967086792\n","step: 3610, loss: 0.09078431129455566\n","step: 3620, loss: 0.016090353950858116\n","step: 3630, loss: 0.03424656018614769\n","step: 3640, loss: 0.09022779762744904\n","step: 3650, loss: 0.09887810796499252\n","step: 3660, loss: 0.1416224092245102\n","step: 3670, loss: 0.06619541347026825\n","step: 3680, loss: 0.07588598132133484\n","step: 3690, loss: 0.05707233399152756\n","step: 3700, loss: 0.02238120324909687\n","step: 3710, loss: 0.08105487376451492\n","step: 3720, loss: 0.04769034683704376\n","step: 3730, loss: 0.14750732481479645\n","step: 3740, loss: 0.09603382647037506\n","step: 3750, loss: 0.1858288049697876\n","step: 3760, loss: 0.15765461325645447\n","step: 3770, loss: 0.03262050077319145\n","step: 3780, loss: 0.08180592209100723\n","step: 3790, loss: 0.047495048493146896\n","step: 3800, loss: 0.0560154989361763\n","step: 3810, loss: 0.08333560824394226\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      0.97      0.91        35\n","           2       0.44      0.81      0.57        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.91      0.86      0.89       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.59      0.94      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       0.97      0.98      0.98       689\n","          10       0.93      0.98      0.95       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.00      0.00      0.00        13\n","          14       0.30      1.00      0.46        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.80      0.87      0.83      1151\n","          17       0.95      0.95      0.95        41\n","          18       0.93      0.88      0.90        32\n","          19       0.00      0.00      0.00        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.71      0.82      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.38      0.55      0.44        44\n","          25       0.85      0.92      0.88       888\n","          26       0.88      0.78      0.82         9\n","          27       0.97      0.97      0.97        69\n","          28       0.98      0.99      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.93      0.83      0.87      1136\n","          31       0.59      0.68      0.63        19\n","          32       1.00      0.62      0.77         8\n","          33       0.60      0.97      0.74        86\n","          34       0.25      0.62      0.35        32\n","          35       0.99      0.99      0.99       474\n","          36       0.86      0.23      0.36       182\n","          37       0.91      0.93      0.92      1592\n","          38       0.98      0.97      0.98       404\n","          39       0.96      0.99      0.97       485\n","          40       0.93      0.85      0.89       573\n","          41       0.89      0.95      0.92       841\n","          42       0.97      0.98      0.98       575\n","          43       0.95      0.72      0.82       152\n","          44       0.87      0.92      0.90        75\n","          46       0.99      0.99      0.99        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.76      0.78      0.75     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 453\n","\n","Loop 55\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.8965487480163574\n","step: 10, loss: 1.867255687713623\n","step: 20, loss: 0.7641825675964355\n","step: 30, loss: 0.4129057228565216\n","step: 40, loss: 0.39036595821380615\n","step: 50, loss: 0.15351751446723938\n","step: 60, loss: 0.3347763419151306\n","step: 70, loss: 0.23730137944221497\n","step: 80, loss: 0.19183529913425446\n","step: 90, loss: 0.1530686318874359\n","step: 100, loss: 0.2544952929019928\n","step: 110, loss: 0.11204375326633453\n","step: 120, loss: 0.1348150074481964\n","step: 130, loss: 0.12346067279577255\n","step: 140, loss: 0.09768608957529068\n","step: 150, loss: 0.16451364755630493\n","step: 160, loss: 0.13873137533664703\n","step: 170, loss: 0.14260649681091309\n","step: 180, loss: 0.12139029055833817\n","step: 190, loss: 0.131741464138031\n","step: 200, loss: 0.19133946299552917\n","step: 210, loss: 0.14844904839992523\n","step: 220, loss: 0.2325112521648407\n","step: 230, loss: 0.10624198615550995\n","step: 240, loss: 0.06058590114116669\n","step: 250, loss: 0.06756109744310379\n","step: 260, loss: 0.08439792692661285\n","step: 270, loss: 0.169295996427536\n","step: 280, loss: 0.22850406169891357\n","step: 290, loss: 0.08496537059545517\n","step: 300, loss: 0.08863458037376404\n","step: 310, loss: 0.07293955981731415\n","step: 320, loss: 0.21246127784252167\n","step: 330, loss: 0.14636054635047913\n","step: 340, loss: 0.14244621992111206\n","step: 350, loss: 0.09213346242904663\n","step: 360, loss: 0.060423530638217926\n","step: 370, loss: 0.014437204226851463\n","step: 380, loss: 0.056629810482263565\n","step: 390, loss: 0.15416257083415985\n","step: 400, loss: 0.12311556190252304\n","step: 410, loss: 0.04128538817167282\n","step: 420, loss: 0.08734335005283356\n","step: 430, loss: 0.030670661479234695\n","step: 440, loss: 0.08657049387693405\n","step: 450, loss: 0.06324777007102966\n","step: 460, loss: 0.07613206654787064\n","step: 470, loss: 0.07614564150571823\n","step: 480, loss: 0.06327622383832932\n","step: 490, loss: 0.11595635861158371\n","step: 500, loss: 0.10599496960639954\n","step: 510, loss: 0.0642998144030571\n","step: 520, loss: 0.07165137678384781\n","step: 530, loss: 0.0986645519733429\n","step: 540, loss: 0.06794095039367676\n","step: 550, loss: 0.039220549166202545\n","step: 560, loss: 0.10320371389389038\n","step: 570, loss: 0.08733458817005157\n","step: 580, loss: 0.11582721769809723\n","step: 590, loss: 0.03381349518895149\n","step: 600, loss: 0.13696378469467163\n","step: 610, loss: 0.03412589430809021\n","step: 620, loss: 0.17932578921318054\n","step: 630, loss: 0.07354641705751419\n","step: 640, loss: 0.10888250172138214\n","step: 650, loss: 0.07402832061052322\n","step: 660, loss: 0.18959452211856842\n","step: 670, loss: 0.08219091594219208\n","step: 680, loss: 0.2001200020313263\n","step: 690, loss: 0.08726266771554947\n","step: 700, loss: 0.1592472791671753\n","step: 710, loss: 0.12507279217243195\n","step: 720, loss: 0.16579407453536987\n","step: 730, loss: 0.09337978810071945\n","step: 740, loss: 0.1379411667585373\n","step: 750, loss: 0.20370788872241974\n","step: 760, loss: 0.10454142838716507\n","step: 770, loss: 0.11419502645730972\n","step: 780, loss: 0.07123659551143646\n","step: 790, loss: 0.02998775616288185\n","step: 800, loss: 0.09299150109291077\n","step: 810, loss: 0.1522952914237976\n","step: 820, loss: 0.15399590134620667\n","step: 830, loss: 0.10882866382598877\n","step: 840, loss: 0.24892082810401917\n","step: 850, loss: 0.12240780889987946\n","step: 860, loss: 0.051690489053726196\n","step: 870, loss: 0.16932512819766998\n","step: 880, loss: 0.2889002859592438\n","step: 890, loss: 0.09170093387365341\n","step: 900, loss: 0.08257028460502625\n","step: 910, loss: 0.05763893574476242\n","step: 920, loss: 0.1280588060617447\n","step: 930, loss: 0.10063458234071732\n","step: 940, loss: 0.13652509450912476\n","step: 950, loss: 0.03923993185162544\n","step: 960, loss: 0.16334892809391022\n","step: 970, loss: 0.14254896342754364\n","step: 980, loss: 0.09209981560707092\n","step: 990, loss: 0.06487061828374863\n","step: 1000, loss: 0.05728519707918167\n","step: 1010, loss: 0.12448697537183762\n","step: 1020, loss: 0.14020869135856628\n","step: 1030, loss: 0.05837326869368553\n","step: 1040, loss: 0.04699452593922615\n","step: 1050, loss: 0.14188075065612793\n","step: 1060, loss: 0.11332819610834122\n","step: 1070, loss: 0.1387730836868286\n","step: 1080, loss: 0.13077808916568756\n","step: 1090, loss: 0.08232250064611435\n","step: 1100, loss: 0.12827207148075104\n","step: 1110, loss: 0.13339008390903473\n","step: 1120, loss: 0.10157454013824463\n","step: 1130, loss: 0.07848707586526871\n","step: 1140, loss: 0.12223619967699051\n","step: 1150, loss: 0.11499704420566559\n","step: 1160, loss: 0.09869375079870224\n","step: 1170, loss: 0.078400619328022\n","step: 1180, loss: 0.13304704427719116\n","step: 1190, loss: 0.1677853912115097\n","step: 1200, loss: 0.11352591216564178\n","step: 1210, loss: 0.054157283157110214\n","step: 1220, loss: 0.17809632420539856\n","step: 1230, loss: 0.04988956078886986\n","step: 1240, loss: 0.05996622145175934\n","step: 1250, loss: 0.03670777752995491\n","step: 1260, loss: 0.05795901268720627\n","step: 1270, loss: 0.05972772836685181\n","step: 1280, loss: 0.17657938599586487\n","step: 1290, loss: 0.07287129014730453\n","step: 1300, loss: 0.02072248049080372\n","step: 1310, loss: 0.09673389792442322\n","step: 1320, loss: 0.0195697583258152\n","step: 1330, loss: 0.08348463475704193\n","step: 1340, loss: 0.09134155511856079\n","step: 1350, loss: 0.08636832237243652\n","step: 1360, loss: 0.05437663942575455\n","step: 1370, loss: 0.11353126913309097\n","step: 1380, loss: 0.17969633638858795\n","step: 1390, loss: 0.09270676970481873\n","step: 1400, loss: 0.09916821122169495\n","step: 1410, loss: 0.09120918810367584\n","step: 1420, loss: 0.21316269040107727\n","step: 1430, loss: 0.09090326726436615\n","step: 1440, loss: 0.16113761067390442\n","step: 1450, loss: 0.1141609251499176\n","step: 1460, loss: 0.12475449591875076\n","step: 1470, loss: 0.1018303632736206\n","step: 1480, loss: 0.030921807512640953\n","step: 1490, loss: 0.07944486290216446\n","step: 1500, loss: 0.14193004369735718\n","step: 1510, loss: 0.06561306864023209\n","step: 1520, loss: 0.044244736433029175\n","step: 1530, loss: 0.09574471414089203\n","step: 1540, loss: 0.09744522720575333\n","step: 1550, loss: 0.17890122532844543\n","step: 1560, loss: 0.10860171169042587\n","step: 1570, loss: 0.0914684310555458\n","step: 1580, loss: 0.14351652562618256\n","step: 1590, loss: 0.033879008144140244\n","step: 1600, loss: 0.09783763438463211\n","step: 1610, loss: 0.08422645926475525\n","step: 1620, loss: 0.05570943281054497\n","step: 1630, loss: 0.1733274906873703\n","step: 1640, loss: 0.09487102925777435\n","step: 1650, loss: 0.12054155021905899\n","step: 1660, loss: 0.060129158198833466\n","step: 1670, loss: 0.0966666117310524\n","step: 1680, loss: 0.0563439279794693\n","step: 1690, loss: 0.04598549008369446\n","step: 1700, loss: 0.09023773670196533\n","step: 1710, loss: 0.10217896848917007\n","step: 1720, loss: 0.10670191049575806\n","step: 1730, loss: 0.07330968976020813\n","step: 1740, loss: 0.10208773612976074\n","step: 1750, loss: 0.10581061244010925\n","step: 1760, loss: 0.11105713993310928\n","step: 1770, loss: 0.06446265429258347\n","step: 1780, loss: 0.10224888473749161\n","step: 1790, loss: 0.033032480627298355\n","step: 1800, loss: 0.07836824655532837\n","step: 1810, loss: 0.09715132415294647\n","step: 1820, loss: 0.06307800859212875\n","step: 1830, loss: 0.16322380304336548\n","step: 1840, loss: 0.06167563423514366\n","step: 1850, loss: 0.08315999805927277\n","step: 1860, loss: 0.04505971819162369\n","step: 1870, loss: 0.11549420654773712\n","step: 1880, loss: 0.1064034253358841\n","step: 1890, loss: 0.05902564898133278\n","step: 1900, loss: 0.09620894491672516\n","step: 1910, loss: 0.03733130171895027\n","step: 1920, loss: 0.08317353576421738\n","step: 1930, loss: 0.029632125049829483\n","step: 1940, loss: 0.07044721394777298\n","step: 1950, loss: 0.3347626328468323\n","step: 1960, loss: 0.11247612535953522\n","step: 1970, loss: 0.14580035209655762\n","step: 1980, loss: 0.1683303862810135\n","step: 1990, loss: 0.10140211880207062\n","step: 2000, loss: 0.06987902522087097\n","step: 2010, loss: 0.09933381527662277\n","step: 2020, loss: 0.10739950090646744\n","step: 2030, loss: 0.045352209359407425\n","step: 2040, loss: 0.13630005717277527\n","step: 2050, loss: 0.15162505209445953\n","step: 2060, loss: 0.047385238111019135\n","step: 2070, loss: 0.08487077802419662\n","step: 2080, loss: 0.1520444005727768\n","step: 2090, loss: 0.08157248795032501\n","step: 2100, loss: 0.06923097372055054\n","step: 2110, loss: 0.23214475810527802\n","step: 2120, loss: 0.13536131381988525\n","step: 2130, loss: 0.032868389040231705\n","step: 2140, loss: 0.09167229384183884\n","step: 2150, loss: 0.1177126094698906\n","step: 2160, loss: 0.04739490896463394\n","step: 2170, loss: 0.043884970247745514\n","step: 2180, loss: 0.08082333207130432\n","step: 2190, loss: 0.1169695258140564\n","step: 2200, loss: 0.08724451065063477\n","step: 2210, loss: 0.06913259625434875\n","step: 2220, loss: 0.11547166109085083\n","step: 2230, loss: 0.08051512390375137\n","step: 2240, loss: 0.07429264485836029\n","step: 2250, loss: 0.06335491687059402\n","step: 2260, loss: 0.07464512437582016\n","step: 2270, loss: 0.04497490078210831\n","step: 2280, loss: 0.07952134311199188\n","step: 2290, loss: 0.02850569598376751\n","step: 2300, loss: 0.06825177371501923\n","step: 2310, loss: 0.019111143425107002\n","step: 2320, loss: 0.030527237802743912\n","step: 2330, loss: 0.1136292889714241\n","step: 2340, loss: 0.13454070687294006\n","step: 2350, loss: 0.1763773113489151\n","step: 2360, loss: 0.05077500641345978\n","step: 2370, loss: 0.07737584412097931\n","step: 2380, loss: 0.08356453478336334\n","step: 2390, loss: 0.08888105303049088\n","step: 2400, loss: 0.04806739091873169\n","step: 2410, loss: 0.09921624511480331\n","step: 2420, loss: 0.1315075308084488\n","step: 2430, loss: 0.15883372724056244\n","step: 2440, loss: 0.15045258402824402\n","step: 2450, loss: 0.1258034110069275\n","step: 2460, loss: 0.039125438779592514\n","step: 2470, loss: 0.07631491869688034\n","step: 2480, loss: 0.00995585322380066\n","step: 2490, loss: 0.049655091017484665\n","step: 2500, loss: 0.01989585906267166\n","step: 2510, loss: 0.12590283155441284\n","step: 2520, loss: 0.03296847268939018\n","step: 2530, loss: 0.12735223770141602\n","step: 2540, loss: 0.038208603858947754\n","step: 2550, loss: 0.05481958016753197\n","step: 2560, loss: 0.0769275426864624\n","step: 2570, loss: 0.08416642248630524\n","step: 2580, loss: 0.17692448198795319\n","step: 2590, loss: 0.09667246788740158\n","step: 2600, loss: 0.13960330188274384\n","step: 2610, loss: 0.06465368717908859\n","step: 2620, loss: 0.0715327262878418\n","step: 2630, loss: 0.07872678339481354\n","step: 2640, loss: 0.11554204672574997\n","step: 2650, loss: 0.14525963366031647\n","step: 2660, loss: 0.09664278477430344\n","step: 2670, loss: 0.06124989315867424\n","step: 2680, loss: 0.07137220352888107\n","step: 2690, loss: 0.0328562892973423\n","step: 2700, loss: 0.034140173345804214\n","step: 2710, loss: 0.042467258870601654\n","step: 2720, loss: 0.09821555763483047\n","step: 2730, loss: 0.024343125522136688\n","step: 2740, loss: 0.052750613540410995\n","step: 2750, loss: 0.13354377448558807\n","step: 2760, loss: 0.059035684913396835\n","step: 2770, loss: 0.08949237316846848\n","step: 2780, loss: 0.061214104294776917\n","step: 2790, loss: 0.03544655814766884\n","step: 2800, loss: 0.05001433938741684\n","step: 2810, loss: 0.0390436053276062\n","step: 2820, loss: 0.10378435999155045\n","step: 2830, loss: 0.05876950919628143\n","step: 2840, loss: 0.045532792806625366\n","step: 2850, loss: 0.11477585881948471\n","step: 2860, loss: 0.038666024804115295\n","step: 2870, loss: 0.04336048290133476\n","step: 2880, loss: 0.06010729819536209\n","step: 2890, loss: 0.15247634053230286\n","step: 2900, loss: 0.022211536765098572\n","step: 2910, loss: 0.13008080422878265\n","step: 2920, loss: 0.06874674558639526\n","step: 2930, loss: 0.06416929513216019\n","step: 2940, loss: 0.05538396164774895\n","step: 2950, loss: 0.06820187717676163\n","step: 2960, loss: 0.1025225892663002\n","step: 2970, loss: 0.04470517113804817\n","step: 2980, loss: 0.063595712184906\n","step: 2990, loss: 0.10111042857170105\n","step: 3000, loss: 0.038389451801776886\n","step: 3010, loss: 0.09678686410188675\n","step: 3020, loss: 0.06801628321409225\n","step: 3030, loss: 0.13747380673885345\n","step: 3040, loss: 0.0810026228427887\n","step: 3050, loss: 0.06270459294319153\n","step: 3060, loss: 0.13580748438835144\n","step: 3070, loss: 0.21247927844524384\n","step: 3080, loss: 0.05940642207860947\n","step: 3090, loss: 0.095749631524086\n","step: 3100, loss: 0.12182094156742096\n","step: 3110, loss: 0.05120076239109039\n","step: 3120, loss: 0.14244449138641357\n","step: 3130, loss: 0.07118913531303406\n","step: 3140, loss: 0.11631876230239868\n","step: 3150, loss: 0.10055185854434967\n","step: 3160, loss: 0.05545743182301521\n","step: 3170, loss: 0.13142970204353333\n","step: 3180, loss: 0.06632065773010254\n","step: 3190, loss: 0.06688118726015091\n","step: 3200, loss: 0.06597689539194107\n","step: 3210, loss: 0.15679503977298737\n","step: 3220, loss: 0.05365566164255142\n","step: 3230, loss: 0.07930058240890503\n","step: 3240, loss: 0.18160805106163025\n","step: 3250, loss: 0.03687942028045654\n","step: 3260, loss: 0.08432082086801529\n","step: 3270, loss: 0.035244014114141464\n","step: 3280, loss: 0.07435023039579391\n","step: 3290, loss: 0.0223536416888237\n","step: 3300, loss: 0.13053804636001587\n","step: 3310, loss: 0.05744898319244385\n","step: 3320, loss: 0.09850011765956879\n","step: 3330, loss: 0.03142761439085007\n","step: 3340, loss: 0.11350717395544052\n","step: 3350, loss: 0.0392470583319664\n","step: 3360, loss: 0.035855483263731\n","step: 3370, loss: 0.12107469886541367\n","step: 3380, loss: 0.01917717047035694\n","step: 3390, loss: 0.023776117712259293\n","step: 3400, loss: 0.03674866631627083\n","step: 3410, loss: 0.06613527983427048\n","step: 3420, loss: 0.13058097660541534\n","step: 3430, loss: 0.1183265671133995\n","step: 3440, loss: 0.0631171464920044\n","step: 3450, loss: 0.06341231614351273\n","step: 3460, loss: 0.06658068299293518\n","step: 3470, loss: 0.053351957350969315\n","step: 3480, loss: 0.054043084383010864\n","step: 3490, loss: 0.05293909087777138\n","step: 3500, loss: 0.11884111166000366\n","step: 3510, loss: 0.05534760653972626\n","step: 3520, loss: 0.09697065502405167\n","step: 3530, loss: 0.12080777436494827\n","step: 3540, loss: 0.10793193429708481\n","step: 3550, loss: 0.09494934976100922\n","step: 3560, loss: 0.086744524538517\n","step: 3570, loss: 0.10885924845933914\n","step: 3580, loss: 0.16478979587554932\n","step: 3590, loss: 0.05627303197979927\n","step: 3600, loss: 0.18112146854400635\n","step: 3610, loss: 0.10057632625102997\n","step: 3620, loss: 0.054947178810834885\n","step: 3630, loss: 0.10919059067964554\n","step: 3640, loss: 0.05624668300151825\n","step: 3650, loss: 0.11764304339885712\n","step: 3660, loss: 0.1163465678691864\n","step: 3670, loss: 0.041404396295547485\n","step: 3680, loss: 0.0744752362370491\n","step: 3690, loss: 0.13457827270030975\n","step: 3700, loss: 0.09897524118423462\n","step: 3710, loss: 0.08687600493431091\n","step: 3720, loss: 0.17074301838874817\n","step: 3730, loss: 0.058329202234745026\n","step: 3740, loss: 0.0806114450097084\n","step: 3750, loss: 0.0920315533876419\n","step: 3760, loss: 0.08309496194124222\n","step: 3770, loss: 0.032110974192619324\n","step: 3780, loss: 0.1419142335653305\n","step: 3790, loss: 0.06908392161130905\n","step: 3800, loss: 0.12608566880226135\n","step: 3810, loss: 0.13517563045024872\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.75      0.94      0.84        35\n","           2       0.81      0.17      0.28        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.82      0.84      0.83       291\n","           5       1.00      0.67      0.80       294\n","           6       1.00      0.98      0.99      1570\n","           7       0.59      0.94      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.96      0.98      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.79      0.85      0.81        13\n","          14       0.30      0.98      0.46        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.79      0.85      0.82      1151\n","          17       0.93      0.98      0.95        41\n","          18       0.91      0.97      0.94        32\n","          19       0.86      0.45      0.59        40\n","          20       1.00      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.92      0.73      0.81      4175\n","          23       0.67      0.97      0.79      2253\n","          24       0.34      0.59      0.43        44\n","          25       0.87      0.90      0.88       888\n","          26       1.00      0.67      0.80         9\n","          27       1.00      0.96      0.98        69\n","          28       1.00      0.99      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.86      0.90      0.88      1136\n","          31       0.58      0.58      0.58        19\n","          32       0.83      0.62      0.71         8\n","          33       0.86      0.83      0.84        86\n","          34       0.19      0.59      0.29        32\n","          35       0.99      0.97      0.98       474\n","          36       0.89      0.19      0.31       182\n","          37       0.89      0.94      0.92      1592\n","          38       0.90      0.99      0.94       404\n","          39       0.97      0.95      0.96       485\n","          40       0.95      0.76      0.84       573\n","          41       0.98      0.93      0.95       841\n","          42       0.97      0.99      0.98       575\n","          43       0.95      0.82      0.88       152\n","          44       0.92      0.92      0.92        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.78      0.77     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","Difference 431\n","\n","Loop 56\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 4.013138771057129\n","step: 10, loss: 2.2395570278167725\n","step: 20, loss: 0.9030024409294128\n","step: 30, loss: 0.526643693447113\n","step: 40, loss: 0.4186587631702423\n","step: 50, loss: 0.2710305452346802\n","step: 60, loss: 0.2857153117656708\n","step: 70, loss: 0.1367371380329132\n","step: 80, loss: 0.2738984525203705\n","step: 90, loss: 0.11650940775871277\n","step: 100, loss: 0.10337415337562561\n","step: 110, loss: 0.12969274818897247\n","step: 120, loss: 0.06326396018266678\n","step: 130, loss: 0.1254047006368637\n","step: 140, loss: 0.15379729866981506\n","step: 150, loss: 0.16816584765911102\n","step: 160, loss: 0.1063871905207634\n","step: 170, loss: 0.12878431379795074\n","step: 180, loss: 0.10424583405256271\n","step: 190, loss: 0.15119828283786774\n","step: 200, loss: 0.08398506045341492\n","step: 210, loss: 0.11412518471479416\n","step: 220, loss: 0.1276850402355194\n","step: 230, loss: 0.14920446276664734\n","step: 240, loss: 0.07400981336832047\n","step: 250, loss: 0.10633990913629532\n","step: 260, loss: 0.04026471823453903\n","step: 270, loss: 0.0854160338640213\n","step: 280, loss: 0.11819151043891907\n","step: 290, loss: 0.11610924452543259\n","step: 300, loss: 0.08957435190677643\n","step: 310, loss: 0.13307185471057892\n","step: 320, loss: 0.0728694498538971\n","step: 330, loss: 0.04966370761394501\n","step: 340, loss: 0.04708518087863922\n","step: 350, loss: 0.11448018252849579\n","step: 360, loss: 0.08123239129781723\n","step: 370, loss: 0.0862630158662796\n","step: 380, loss: 0.18648554384708405\n","step: 390, loss: 0.1270514875650406\n","step: 400, loss: 0.07464489340782166\n","step: 410, loss: 0.15471264719963074\n","step: 420, loss: 0.099859818816185\n","step: 430, loss: 0.08010245859622955\n","step: 440, loss: 0.02351997047662735\n","step: 450, loss: 0.11458480358123779\n","step: 460, loss: 0.09124390035867691\n","step: 470, loss: 0.10533910244703293\n","step: 480, loss: 0.155264750123024\n","step: 490, loss: 0.11269865930080414\n","step: 500, loss: 0.1448563039302826\n","step: 510, loss: 0.050791557878255844\n","step: 520, loss: 0.020196711644530296\n","step: 530, loss: 0.04283501207828522\n","step: 540, loss: 0.1058562621474266\n","step: 550, loss: 0.1449904441833496\n","step: 560, loss: 0.10726392269134521\n","step: 570, loss: 0.05121767148375511\n","step: 580, loss: 0.08033139258623123\n","step: 590, loss: 0.10984034836292267\n","step: 600, loss: 0.08121082186698914\n","step: 610, loss: 0.1132524386048317\n","step: 620, loss: 0.14057141542434692\n","step: 630, loss: 0.3063531816005707\n","step: 640, loss: 0.06231210008263588\n","step: 650, loss: 0.03764239698648453\n","step: 660, loss: 0.08817505091428757\n","step: 670, loss: 0.05511817708611488\n","step: 680, loss: 0.05988673120737076\n","step: 690, loss: 0.10703322291374207\n","step: 700, loss: 0.10086100548505783\n","step: 710, loss: 0.08230806142091751\n","step: 720, loss: 0.10721609741449356\n","step: 730, loss: 0.13490480184555054\n","step: 740, loss: 0.08794703334569931\n","step: 750, loss: 0.10959184169769287\n","step: 760, loss: 0.13314294815063477\n","step: 770, loss: 0.11916565895080566\n","step: 780, loss: 0.05247627571225166\n","step: 790, loss: 0.1416722685098648\n","step: 800, loss: 0.09348207712173462\n","step: 810, loss: 0.09995419532060623\n","step: 820, loss: 0.14339935779571533\n","step: 830, loss: 0.060485731810331345\n","step: 840, loss: 0.12516552209854126\n","step: 850, loss: 0.18316946923732758\n","step: 860, loss: 0.08887486159801483\n","step: 870, loss: 0.030492331832647324\n","step: 880, loss: 0.07276332378387451\n","step: 890, loss: 0.11019521951675415\n","step: 900, loss: 0.1320900321006775\n","step: 910, loss: 0.1758943349123001\n","step: 920, loss: 0.10279657691717148\n","step: 930, loss: 0.06969943642616272\n","step: 940, loss: 0.15888787806034088\n","step: 950, loss: 0.09718714654445648\n","step: 960, loss: 0.0530039519071579\n","step: 970, loss: 0.06736359000205994\n","step: 980, loss: 0.09004002809524536\n","step: 990, loss: 0.13231925666332245\n","step: 1000, loss: 0.12222762405872345\n","step: 1010, loss: 0.10553576052188873\n","step: 1020, loss: 0.17632056772708893\n","step: 1030, loss: 0.239669069647789\n","step: 1040, loss: 0.06704045832157135\n","step: 1050, loss: 0.06058434769511223\n","step: 1060, loss: 0.08651802688837051\n","step: 1070, loss: 0.13549461960792542\n","step: 1080, loss: 0.12109357118606567\n","step: 1090, loss: 0.12944240868091583\n","step: 1100, loss: 0.04690549150109291\n","step: 1110, loss: 0.10269656032323837\n","step: 1120, loss: 0.13668440282344818\n","step: 1130, loss: 0.01998238079249859\n","step: 1140, loss: 0.08607357740402222\n","step: 1150, loss: 0.04368377849459648\n","step: 1160, loss: 0.10329832881689072\n","step: 1170, loss: 0.1107914000749588\n","step: 1180, loss: 0.025147218257188797\n","step: 1190, loss: 0.03860819339752197\n","step: 1200, loss: 0.13945817947387695\n","step: 1210, loss: 0.1301576942205429\n","step: 1220, loss: 0.06733585894107819\n","step: 1230, loss: 0.10890352725982666\n","step: 1240, loss: 0.09463044255971909\n","step: 1250, loss: 0.046038687229156494\n","step: 1260, loss: 0.12568318843841553\n","step: 1270, loss: 0.16226889193058014\n","step: 1280, loss: 0.14316107332706451\n","step: 1290, loss: 0.0780801996588707\n","step: 1300, loss: 0.09652113169431686\n","step: 1310, loss: 0.07303401827812195\n","step: 1320, loss: 0.10103745758533478\n","step: 1330, loss: 0.060289978981018066\n","step: 1340, loss: 0.04892558604478836\n","step: 1350, loss: 0.041943009942770004\n","step: 1360, loss: 0.13178831338882446\n","step: 1370, loss: 0.1199844554066658\n","step: 1380, loss: 0.09051310271024704\n","step: 1390, loss: 0.06365641206502914\n","step: 1400, loss: 0.12237197905778885\n","step: 1410, loss: 0.04686534032225609\n","step: 1420, loss: 0.05780882388353348\n","step: 1430, loss: 0.12532471120357513\n","step: 1440, loss: 0.07113362103700638\n","step: 1450, loss: 0.0737459659576416\n","step: 1460, loss: 0.13032731413841248\n","step: 1470, loss: 0.07048918306827545\n","step: 1480, loss: 0.08317631483078003\n","step: 1490, loss: 0.28957852721214294\n","step: 1500, loss: 0.05223418027162552\n","step: 1510, loss: 0.03532785177230835\n","step: 1520, loss: 0.04553907364606857\n","step: 1530, loss: 0.09682463854551315\n","step: 1540, loss: 0.048302918672561646\n","step: 1550, loss: 0.08789847791194916\n","step: 1560, loss: 0.12833742797374725\n","step: 1570, loss: 0.13489526510238647\n","step: 1580, loss: 0.034365203231573105\n","step: 1590, loss: 0.02374427393078804\n","step: 1600, loss: 0.07435376942157745\n","step: 1610, loss: 0.2134687751531601\n","step: 1620, loss: 0.09164418280124664\n","step: 1630, loss: 0.05101965740323067\n","step: 1640, loss: 0.10302583128213882\n","step: 1650, loss: 0.1301942616701126\n","step: 1660, loss: 0.1256030946969986\n","step: 1670, loss: 0.05996590480208397\n","step: 1680, loss: 0.051870111376047134\n","step: 1690, loss: 0.12632565200328827\n","step: 1700, loss: 0.09375714510679245\n","step: 1710, loss: 0.083299420773983\n","step: 1720, loss: 0.03794889897108078\n","step: 1730, loss: 0.014549079351127148\n","step: 1740, loss: 0.08955416083335876\n","step: 1750, loss: 0.057647429406642914\n","step: 1760, loss: 0.1741504818201065\n","step: 1770, loss: 0.08760478347539902\n","step: 1780, loss: 0.09503357112407684\n","step: 1790, loss: 0.1138753741979599\n","step: 1800, loss: 0.1351708471775055\n","step: 1810, loss: 0.12492577731609344\n","step: 1820, loss: 0.1261511594057083\n","step: 1830, loss: 0.05112805217504501\n","step: 1840, loss: 0.04916946962475777\n","step: 1850, loss: 0.1678367704153061\n","step: 1860, loss: 0.07309258729219437\n","step: 1870, loss: 0.08654937893152237\n","step: 1880, loss: 0.10176295042037964\n","step: 1890, loss: 0.08437284827232361\n","step: 1900, loss: 0.05778126046061516\n","step: 1910, loss: 0.05692952126264572\n","step: 1920, loss: 0.10700614005327225\n","step: 1930, loss: 0.14541175961494446\n","step: 1940, loss: 0.04693100228905678\n","step: 1950, loss: 0.11134552210569382\n","step: 1960, loss: 0.15304963290691376\n","step: 1970, loss: 0.10643406212329865\n","step: 1980, loss: 0.21186189353466034\n","step: 1990, loss: 0.09364411234855652\n","step: 2000, loss: 0.09222200512886047\n","step: 2010, loss: 0.2556631863117218\n","step: 2020, loss: 0.07990291714668274\n","step: 2030, loss: 0.1031096875667572\n","step: 2040, loss: 0.08965365588665009\n","step: 2050, loss: 0.13540992140769958\n","step: 2060, loss: 0.08244143426418304\n","step: 2070, loss: 0.036506179720163345\n","step: 2080, loss: 0.018841348588466644\n","step: 2090, loss: 0.07496733218431473\n","step: 2100, loss: 0.09169060736894608\n","step: 2110, loss: 0.10465876013040543\n","step: 2120, loss: 0.07995473593473434\n","step: 2130, loss: 0.0628933310508728\n","step: 2140, loss: 0.17290544509887695\n","step: 2150, loss: 0.10728006809949875\n","step: 2160, loss: 0.09765753149986267\n","step: 2170, loss: 0.05473233386874199\n","step: 2180, loss: 0.09979414194822311\n","step: 2190, loss: 0.04977205768227577\n","step: 2200, loss: 0.14265502989292145\n","step: 2210, loss: 0.12233579158782959\n","step: 2220, loss: 0.04902654513716698\n","step: 2230, loss: 0.03364303335547447\n","step: 2240, loss: 0.09894651919603348\n","step: 2250, loss: 0.1073765903711319\n","step: 2260, loss: 0.08041051775217056\n","step: 2270, loss: 0.13460056483745575\n","step: 2280, loss: 0.16791941225528717\n","step: 2290, loss: 0.0897272527217865\n","step: 2300, loss: 0.07640838623046875\n","step: 2310, loss: 0.10247018188238144\n","step: 2320, loss: 0.01468165498226881\n","step: 2330, loss: 0.04865557327866554\n","step: 2340, loss: 0.10068079084157944\n","step: 2350, loss: 0.052808746695518494\n","step: 2360, loss: 0.07853158563375473\n","step: 2370, loss: 0.07087275385856628\n","step: 2380, loss: 0.14561188220977783\n","step: 2390, loss: 0.05716204270720482\n","step: 2400, loss: 0.024517236277461052\n","step: 2410, loss: 0.042727991938591\n","step: 2420, loss: 0.10749222338199615\n","step: 2430, loss: 0.04598810151219368\n","step: 2440, loss: 0.10143710672855377\n","step: 2450, loss: 0.10766758769750595\n","step: 2460, loss: 0.10169075429439545\n","step: 2470, loss: 0.13519544899463654\n","step: 2480, loss: 0.05371060594916344\n","step: 2490, loss: 0.10043710470199585\n","step: 2500, loss: 0.11242131143808365\n","step: 2510, loss: 0.07832735031843185\n","step: 2520, loss: 0.0779036208987236\n","step: 2530, loss: 0.0669037252664566\n","step: 2540, loss: 0.061751026660203934\n","step: 2550, loss: 0.08456554263830185\n","step: 2560, loss: 0.03213375434279442\n","step: 2570, loss: 0.17427241802215576\n","step: 2580, loss: 0.02793375588953495\n","step: 2590, loss: 0.05918128788471222\n","step: 2600, loss: 0.09585193544626236\n","step: 2610, loss: 0.12977229058742523\n","step: 2620, loss: 0.13583135604858398\n","step: 2630, loss: 0.08810645341873169\n","step: 2640, loss: 0.03831734508275986\n","step: 2650, loss: 0.01840759441256523\n","step: 2660, loss: 0.12664546072483063\n","step: 2670, loss: 0.0782170444726944\n","step: 2680, loss: 0.07284922897815704\n","step: 2690, loss: 0.06717400997877121\n","step: 2700, loss: 0.05453255772590637\n","step: 2710, loss: 0.13624368607997894\n","step: 2720, loss: 0.10210266709327698\n","step: 2730, loss: 0.1793958693742752\n","step: 2740, loss: 0.04777500405907631\n","step: 2750, loss: 0.08017928898334503\n","step: 2760, loss: 0.06657291203737259\n","step: 2770, loss: 0.040292900055646896\n","step: 2780, loss: 0.06716807186603546\n","step: 2790, loss: 0.017972677946090698\n","step: 2800, loss: 0.04033558443188667\n","step: 2810, loss: 0.05876270309090614\n","step: 2820, loss: 0.09471923112869263\n","step: 2830, loss: 0.0649065375328064\n","step: 2840, loss: 0.05362404137849808\n","step: 2850, loss: 0.11702665686607361\n","step: 2860, loss: 0.05179283022880554\n","step: 2870, loss: 0.08897952735424042\n","step: 2880, loss: 0.17836491763591766\n","step: 2890, loss: 0.06275583058595657\n","step: 2900, loss: 0.03509150817990303\n","step: 2910, loss: 0.05840893089771271\n","step: 2920, loss: 0.16137248277664185\n","step: 2930, loss: 0.06377880275249481\n","step: 2940, loss: 0.0708661824464798\n","step: 2950, loss: 0.1106470450758934\n","step: 2960, loss: 0.039461392909288406\n","step: 2970, loss: 0.11404412239789963\n","step: 2980, loss: 0.03678843751549721\n","step: 2990, loss: 0.10927576571702957\n","step: 3000, loss: 0.10744690150022507\n","step: 3010, loss: 0.04634225368499756\n","step: 3020, loss: 0.19228874146938324\n","step: 3030, loss: 0.1573726236820221\n","step: 3040, loss: 0.08953163027763367\n","step: 3050, loss: 0.03912416100502014\n","step: 3060, loss: 0.08902527391910553\n","step: 3070, loss: 0.041323523968458176\n","step: 3080, loss: 0.11038177460432053\n","step: 3090, loss: 0.05909768491983414\n","step: 3100, loss: 0.02765759453177452\n","step: 3110, loss: 0.10371743142604828\n","step: 3120, loss: 0.046483114361763\n","step: 3130, loss: 0.10798558592796326\n","step: 3140, loss: 0.06214400380849838\n","step: 3150, loss: 0.0804329663515091\n","step: 3160, loss: 0.04873741418123245\n","step: 3170, loss: 0.061673909425735474\n","step: 3180, loss: 0.03688456118106842\n","step: 3190, loss: 0.0590500608086586\n","step: 3200, loss: 0.05340230464935303\n","step: 3210, loss: 0.09551821649074554\n","step: 3220, loss: 0.12931257486343384\n","step: 3230, loss: 0.05919373407959938\n","step: 3240, loss: 0.07980860024690628\n","step: 3250, loss: 0.1295134425163269\n","step: 3260, loss: 0.15405848622322083\n","step: 3270, loss: 0.10481446981430054\n","step: 3280, loss: 0.09095830470323563\n","step: 3290, loss: 0.05020211637020111\n","step: 3300, loss: 0.14845281839370728\n","step: 3310, loss: 0.09808500111103058\n","step: 3320, loss: 0.04637271910905838\n","step: 3330, loss: 0.16873598098754883\n","step: 3340, loss: 0.09988851100206375\n","step: 3350, loss: 0.13487125933170319\n","step: 3360, loss: 0.0282024797052145\n","step: 3370, loss: 0.09990251064300537\n","step: 3380, loss: 0.129240944981575\n","step: 3390, loss: 0.1021120697259903\n","step: 3400, loss: 0.11998668313026428\n","step: 3410, loss: 0.3496001362800598\n","step: 3420, loss: 0.04177689179778099\n","step: 3430, loss: 0.16410966217517853\n","step: 3440, loss: 0.02734616957604885\n","step: 3450, loss: 0.07069779187440872\n","step: 3460, loss: 0.07876601070165634\n","step: 3470, loss: 0.12157497555017471\n","step: 3480, loss: 0.07795603573322296\n","step: 3490, loss: 0.06970217823982239\n","step: 3500, loss: 0.04558839276432991\n","step: 3510, loss: 0.08696874976158142\n","step: 3520, loss: 0.06589088588953018\n","step: 3530, loss: 0.13610506057739258\n","step: 3540, loss: 0.09925875812768936\n","step: 3550, loss: 0.1283370703458786\n","step: 3560, loss: 0.08991706371307373\n","step: 3570, loss: 0.029786095023155212\n","step: 3580, loss: 0.10242093354463577\n","step: 3590, loss: 0.10739956796169281\n","step: 3600, loss: 0.1549443006515503\n","step: 3610, loss: 0.04063035547733307\n","step: 3620, loss: 0.06270252168178558\n","step: 3630, loss: 0.07036075741052628\n","step: 3640, loss: 0.15417443215847015\n","step: 3650, loss: 0.12785953283309937\n","step: 3660, loss: 0.12597239017486572\n","step: 3670, loss: 0.13929881155490875\n","step: 3680, loss: 0.1191437765955925\n","step: 3690, loss: 0.03941105678677559\n","step: 3700, loss: 0.059954673051834106\n","step: 3710, loss: 0.09379764646291733\n","step: 3720, loss: 0.03308270126581192\n","step: 3730, loss: 0.05460391566157341\n","step: 3740, loss: 0.12734583020210266\n","step: 3750, loss: 0.13889282941818237\n","step: 3760, loss: 0.10594949871301651\n","step: 3770, loss: 0.11748068779706955\n","step: 3780, loss: 0.08595680445432663\n","step: 3790, loss: 0.09402971714735031\n","step: 3800, loss: 0.02595263533294201\n","step: 3810, loss: 0.09585703164339066\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.64      1.00      0.78        35\n","           2       0.74      0.68      0.71        77\n","           3       1.00      0.80      0.89      1030\n","           4       0.90      0.84      0.87       291\n","           5       1.00      0.73      0.84       294\n","           6       0.98      0.99      0.99      1570\n","           7       0.64      0.95      0.76       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.98       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.82      0.69      0.75        13\n","          14       0.24      1.00      0.39        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.85      0.87      0.86      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.82      1.00      0.90        32\n","          19       0.74      0.42      0.54        40\n","          20       1.00      1.00      1.00       584\n","          21       0.45      0.19      0.27        52\n","          22       0.94      0.72      0.81      4175\n","          23       0.65      0.97      0.78      2253\n","          24       0.30      0.48      0.37        44\n","          25       0.87      0.90      0.89       888\n","          26       0.62      0.56      0.59         9\n","          27       0.82      0.97      0.89        69\n","          28       0.99      0.99      0.99      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.94      0.86      0.90      1136\n","          31       0.48      0.53      0.50        19\n","          32       1.00      0.62      0.77         8\n","          33       0.70      0.92      0.79        86\n","          34       0.12      0.25      0.17        32\n","          35       0.98      0.99      0.98       474\n","          36       0.96      0.15      0.26       182\n","          37       0.89      0.95      0.92      1592\n","          38       0.92      0.98      0.95       404\n","          39       0.98      0.94      0.96       485\n","          40       0.94      0.92      0.93       573\n","          41       0.96      0.92      0.94       841\n","          42       0.98      0.99      0.98       575\n","          43       0.96      0.76      0.85       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.86      0.48      0.62        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.81      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 462\n","\n","Loop 57\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.970917224884033\n","step: 10, loss: 2.093954563140869\n","step: 20, loss: 1.0494707822799683\n","step: 30, loss: 0.5315368175506592\n","step: 40, loss: 0.330430805683136\n","step: 50, loss: 0.16933169960975647\n","step: 60, loss: 0.22597543895244598\n","step: 70, loss: 0.16711410880088806\n","step: 80, loss: 0.2433757781982422\n","step: 90, loss: 0.058360710740089417\n","step: 100, loss: 0.13158531486988068\n","step: 110, loss: 0.2603911757469177\n","step: 120, loss: 0.15230536460876465\n","step: 130, loss: 0.16011135280132294\n","step: 140, loss: 0.18889260292053223\n","step: 150, loss: 0.132660374045372\n","step: 160, loss: 0.1683526486158371\n","step: 170, loss: 0.0726243257522583\n","step: 180, loss: 0.17619886994361877\n","step: 190, loss: 0.2247796505689621\n","step: 200, loss: 0.12155357003211975\n","step: 210, loss: 0.15913860499858856\n","step: 220, loss: 0.1990828961133957\n","step: 230, loss: 0.1682073473930359\n","step: 240, loss: 0.2537221908569336\n","step: 250, loss: 0.18132051825523376\n","step: 260, loss: 0.08786503970623016\n","step: 270, loss: 0.04180524870753288\n","step: 280, loss: 0.1597132682800293\n","step: 290, loss: 0.1600009948015213\n","step: 300, loss: 0.10762786865234375\n","step: 310, loss: 0.11071030050516129\n","step: 320, loss: 0.13765324652194977\n","step: 330, loss: 0.2781763970851898\n","step: 340, loss: 0.11261149495840073\n","step: 350, loss: 0.06611239910125732\n","step: 360, loss: 0.07819528877735138\n","step: 370, loss: 0.24116167426109314\n","step: 380, loss: 0.17396077513694763\n","step: 390, loss: 0.0827176421880722\n","step: 400, loss: 0.17481373250484467\n","step: 410, loss: 0.11162380129098892\n","step: 420, loss: 0.0963578075170517\n","step: 430, loss: 0.16057135164737701\n","step: 440, loss: 0.15270626544952393\n","step: 450, loss: 0.07240985333919525\n","step: 460, loss: 0.06597567349672318\n","step: 470, loss: 0.05173970386385918\n","step: 480, loss: 0.09999153763055801\n","step: 490, loss: 0.08980288356542587\n","step: 500, loss: 0.17323113977909088\n","step: 510, loss: 0.10849573463201523\n","step: 520, loss: 0.05099806562066078\n","step: 530, loss: 0.11282901465892792\n","step: 540, loss: 0.04178455099463463\n","step: 550, loss: 0.07877112925052643\n","step: 560, loss: 0.10886389762163162\n","step: 570, loss: 0.09480578452348709\n","step: 580, loss: 0.05384732410311699\n","step: 590, loss: 0.11148940771818161\n","step: 600, loss: 0.04104561358690262\n","step: 610, loss: 0.1565874218940735\n","step: 620, loss: 0.10043960809707642\n","step: 630, loss: 0.1788947731256485\n","step: 640, loss: 0.13506659865379333\n","step: 650, loss: 0.14430099725723267\n","step: 660, loss: 0.0782896876335144\n","step: 670, loss: 0.11130750924348831\n","step: 680, loss: 0.023485945537686348\n","step: 690, loss: 0.06408926844596863\n","step: 700, loss: 0.06053948402404785\n","step: 710, loss: 0.1383456289768219\n","step: 720, loss: 0.1050073653459549\n","step: 730, loss: 0.114911749958992\n","step: 740, loss: 0.05199648067355156\n","step: 750, loss: 0.14866435527801514\n","step: 760, loss: 0.1090601310133934\n","step: 770, loss: 0.0710606649518013\n","step: 780, loss: 0.1228255182504654\n","step: 790, loss: 0.08549182862043381\n","step: 800, loss: 0.08210160583257675\n","step: 810, loss: 0.17737270891666412\n","step: 820, loss: 0.13916902244091034\n","step: 830, loss: 0.1883772909641266\n","step: 840, loss: 0.16990579664707184\n","step: 850, loss: 0.06058874726295471\n","step: 860, loss: 0.11696961522102356\n","step: 870, loss: 0.17039261758327484\n","step: 880, loss: 0.11773060262203217\n","step: 890, loss: 0.0913124829530716\n","step: 900, loss: 0.1757788360118866\n","step: 910, loss: 0.09316341578960419\n","step: 920, loss: 0.10692398250102997\n","step: 930, loss: 0.06703949719667435\n","step: 940, loss: 0.03769775852560997\n","step: 950, loss: 0.13780884444713593\n","step: 960, loss: 0.0518975555896759\n","step: 970, loss: 0.11069294810295105\n","step: 980, loss: 0.04428182169795036\n","step: 990, loss: 0.12444943934679031\n","step: 1000, loss: 0.0953839123249054\n","step: 1010, loss: 0.12473701685667038\n","step: 1020, loss: 0.0592358373105526\n","step: 1030, loss: 0.023363111540675163\n","step: 1040, loss: 0.08695708960294724\n","step: 1050, loss: 0.02723415568470955\n","step: 1060, loss: 0.06610970944166183\n","step: 1070, loss: 0.03308643773198128\n","step: 1080, loss: 0.058646123856306076\n","step: 1090, loss: 0.1826515793800354\n","step: 1100, loss: 0.03521471470594406\n","step: 1110, loss: 0.11552788317203522\n","step: 1120, loss: 0.08181964606046677\n","step: 1130, loss: 0.08359859883785248\n","step: 1140, loss: 0.09647433459758759\n","step: 1150, loss: 0.08893682807683945\n","step: 1160, loss: 0.0905182734131813\n","step: 1170, loss: 0.18321730196475983\n","step: 1180, loss: 0.04290708526968956\n","step: 1190, loss: 0.05697077140212059\n","step: 1200, loss: 0.18273763358592987\n","step: 1210, loss: 0.09924562275409698\n","step: 1220, loss: 0.08103727549314499\n","step: 1230, loss: 0.0515136644244194\n","step: 1240, loss: 0.11897237598896027\n","step: 1250, loss: 0.04310552775859833\n","step: 1260, loss: 0.055734582245349884\n","step: 1270, loss: 0.08882385492324829\n","step: 1280, loss: 0.04843486100435257\n","step: 1290, loss: 0.04454367235302925\n","step: 1300, loss: 0.04148372635245323\n","step: 1310, loss: 0.07208498567342758\n","step: 1320, loss: 0.07347188144922256\n","step: 1330, loss: 0.10346206277608871\n","step: 1340, loss: 0.13617610931396484\n","step: 1350, loss: 0.05082091689109802\n","step: 1360, loss: 0.07066825777292252\n","step: 1370, loss: 0.17435571551322937\n","step: 1380, loss: 0.06681320071220398\n","step: 1390, loss: 0.10001995414495468\n","step: 1400, loss: 0.14810501039028168\n","step: 1410, loss: 0.04374567046761513\n","step: 1420, loss: 0.09685009717941284\n","step: 1430, loss: 0.11308608949184418\n","step: 1440, loss: 0.09900642931461334\n","step: 1450, loss: 0.0582398846745491\n","step: 1460, loss: 0.08292022347450256\n","step: 1470, loss: 0.09276390075683594\n","step: 1480, loss: 0.08983591943979263\n","step: 1490, loss: 0.12021070718765259\n","step: 1500, loss: 0.09770140051841736\n","step: 1510, loss: 0.077409528195858\n","step: 1520, loss: 0.09764163941144943\n","step: 1530, loss: 0.05179130285978317\n","step: 1540, loss: 0.14396321773529053\n","step: 1550, loss: 0.09073470532894135\n","step: 1560, loss: 0.05337647348642349\n","step: 1570, loss: 0.11904948204755783\n","step: 1580, loss: 0.055351827293634415\n","step: 1590, loss: 0.17886048555374146\n","step: 1600, loss: 0.0970265194773674\n","step: 1610, loss: 0.15262076258659363\n","step: 1620, loss: 0.08309812843799591\n","step: 1630, loss: 0.17411835491657257\n","step: 1640, loss: 0.08490372449159622\n","step: 1650, loss: 0.07655227929353714\n","step: 1660, loss: 0.08730357140302658\n","step: 1670, loss: 0.07898150384426117\n","step: 1680, loss: 0.13196279108524323\n","step: 1690, loss: 0.09120551496744156\n","step: 1700, loss: 0.11822284758090973\n","step: 1710, loss: 0.10731387883424759\n","step: 1720, loss: 0.06210661306977272\n","step: 1730, loss: 0.0820448100566864\n","step: 1740, loss: 0.04019045829772949\n","step: 1750, loss: 0.06119246408343315\n","step: 1760, loss: 0.09070348739624023\n","step: 1770, loss: 0.09682893007993698\n","step: 1780, loss: 0.052487943321466446\n","step: 1790, loss: 0.025876594707369804\n","step: 1800, loss: 0.10211794823408127\n","step: 1810, loss: 0.0294139813631773\n","step: 1820, loss: 0.15116117894649506\n","step: 1830, loss: 0.07107200473546982\n","step: 1840, loss: 0.05864999070763588\n","step: 1850, loss: 0.04010003060102463\n","step: 1860, loss: 0.09822791814804077\n","step: 1870, loss: 0.07530815899372101\n","step: 1880, loss: 0.1274632215499878\n","step: 1890, loss: 0.06296920776367188\n","step: 1900, loss: 0.11400153487920761\n","step: 1910, loss: 0.12138813734054565\n","step: 1920, loss: 0.11122222244739532\n","step: 1930, loss: 0.1438249945640564\n","step: 1940, loss: 0.13119900226593018\n","step: 1950, loss: 0.15248122811317444\n","step: 1960, loss: 0.07955670356750488\n","step: 1970, loss: 0.08919238299131393\n","step: 1980, loss: 0.09104204177856445\n","step: 1990, loss: 0.07716897875070572\n","step: 2000, loss: 0.1328054666519165\n","step: 2010, loss: 0.1120389848947525\n","step: 2020, loss: 0.069283626973629\n","step: 2030, loss: 0.057026978582143784\n","step: 2040, loss: 0.09847239404916763\n","step: 2050, loss: 0.07990744709968567\n","step: 2060, loss: 0.20886537432670593\n","step: 2070, loss: 0.05784111097455025\n","step: 2080, loss: 0.059619754552841187\n","step: 2090, loss: 0.09217862784862518\n","step: 2100, loss: 0.030256299301981926\n","step: 2110, loss: 0.08066212385892868\n","step: 2120, loss: 0.07583771646022797\n","step: 2130, loss: 0.14506080746650696\n","step: 2140, loss: 0.12373913824558258\n","step: 2150, loss: 0.07012967020273209\n","step: 2160, loss: 0.02848993055522442\n","step: 2170, loss: 0.11067403852939606\n","step: 2180, loss: 0.053445760160684586\n","step: 2190, loss: 0.05929635092616081\n","step: 2200, loss: 0.050985187292099\n","step: 2210, loss: 0.061508659273386\n","step: 2220, loss: 0.13765689730644226\n","step: 2230, loss: 0.033048953860998154\n","step: 2240, loss: 0.11213238537311554\n","step: 2250, loss: 0.1231028139591217\n","step: 2260, loss: 0.13860763609409332\n","step: 2270, loss: 0.08393766731023788\n","step: 2280, loss: 0.11719903349876404\n","step: 2290, loss: 0.20881472527980804\n","step: 2300, loss: 0.0873086228966713\n","step: 2310, loss: 0.073386549949646\n","step: 2320, loss: 0.06276828050613403\n","step: 2330, loss: 0.06073770299553871\n","step: 2340, loss: 0.1202278882265091\n","step: 2350, loss: 0.1667831689119339\n","step: 2360, loss: 0.0625581368803978\n","step: 2370, loss: 0.18787643313407898\n","step: 2380, loss: 0.03277060389518738\n","step: 2390, loss: 0.11839824169874191\n","step: 2400, loss: 0.06558400392532349\n","step: 2410, loss: 0.05234793573617935\n","step: 2420, loss: 0.08550531417131424\n","step: 2430, loss: 0.107304148375988\n","step: 2440, loss: 0.08225297182798386\n","step: 2450, loss: 0.12654250860214233\n","step: 2460, loss: 0.09308501332998276\n","step: 2470, loss: 0.10444170236587524\n","step: 2480, loss: 0.1131119653582573\n","step: 2490, loss: 0.17502282559871674\n","step: 2500, loss: 0.0690283551812172\n","step: 2510, loss: 0.0594443641602993\n","step: 2520, loss: 0.11069422960281372\n","step: 2530, loss: 0.11607830971479416\n","step: 2540, loss: 0.11067021638154984\n","step: 2550, loss: 0.08353915065526962\n","step: 2560, loss: 0.030028391629457474\n","step: 2570, loss: 0.02707706019282341\n","step: 2580, loss: 0.08860824257135391\n","step: 2590, loss: 0.0610494390130043\n","step: 2600, loss: 0.09399780631065369\n","step: 2610, loss: 0.13308288156986237\n","step: 2620, loss: 0.10221118479967117\n","step: 2630, loss: 0.08117786049842834\n","step: 2640, loss: 0.08113237470388412\n","step: 2650, loss: 0.12865182757377625\n","step: 2660, loss: 0.09903200715780258\n","step: 2670, loss: 0.04985335096716881\n","step: 2680, loss: 0.037155650556087494\n","step: 2690, loss: 0.0786881372332573\n","step: 2700, loss: 0.09606051445007324\n","step: 2710, loss: 0.05632496997714043\n","step: 2720, loss: 0.026237647980451584\n","step: 2730, loss: 0.06522389501333237\n","step: 2740, loss: 0.03613593801856041\n","step: 2750, loss: 0.12343283742666245\n","step: 2760, loss: 0.0802622139453888\n","step: 2770, loss: 0.06381817907094955\n","step: 2780, loss: 0.17189747095108032\n","step: 2790, loss: 0.06598949432373047\n","step: 2800, loss: 0.07724825292825699\n","step: 2810, loss: 0.1258356124162674\n","step: 2820, loss: 0.11698256433010101\n","step: 2830, loss: 0.017062053084373474\n","step: 2840, loss: 0.10476326197385788\n","step: 2850, loss: 0.04596297815442085\n","step: 2860, loss: 0.06785637885332108\n","step: 2870, loss: 0.04009003937244415\n","step: 2880, loss: 0.03920825943350792\n","step: 2890, loss: 0.07347500324249268\n","step: 2900, loss: 0.054141879081726074\n","step: 2910, loss: 0.13368584215641022\n","step: 2920, loss: 0.07584531605243683\n","step: 2930, loss: 0.054064832627773285\n","step: 2940, loss: 0.09402405470609665\n","step: 2950, loss: 0.053517650812864304\n","step: 2960, loss: 0.14057786762714386\n","step: 2970, loss: 0.0471140518784523\n","step: 2980, loss: 0.0702037364244461\n","step: 2990, loss: 0.10440889745950699\n","step: 3000, loss: 0.04800120368599892\n","step: 3010, loss: 0.0682826042175293\n","step: 3020, loss: 0.056846797466278076\n","step: 3030, loss: 0.11216219514608383\n","step: 3040, loss: 0.11641290783882141\n","step: 3050, loss: 0.05261905863881111\n","step: 3060, loss: 0.14027410745620728\n","step: 3070, loss: 0.10066115856170654\n","step: 3080, loss: 0.16978491842746735\n","step: 3090, loss: 0.12536384165287018\n","step: 3100, loss: 0.03366583585739136\n","step: 3110, loss: 0.09843933582305908\n","step: 3120, loss: 0.11301396787166595\n","step: 3130, loss: 0.1440436691045761\n","step: 3140, loss: 0.13334538042545319\n","step: 3150, loss: 0.06920535862445831\n","step: 3160, loss: 0.08462412655353546\n","step: 3170, loss: 0.065580353140831\n","step: 3180, loss: 0.10092149674892426\n","step: 3190, loss: 0.09061279892921448\n","step: 3200, loss: 0.08806493133306503\n","step: 3210, loss: 0.11257858574390411\n","step: 3220, loss: 0.16250163316726685\n","step: 3230, loss: 0.13778962194919586\n","step: 3240, loss: 0.05872981622815132\n","step: 3250, loss: 0.0655272975564003\n","step: 3260, loss: 0.04941783845424652\n","step: 3270, loss: 0.03801962733268738\n","step: 3280, loss: 0.03127038851380348\n","step: 3290, loss: 0.05117299407720566\n","step: 3300, loss: 0.10849203914403915\n","step: 3310, loss: 0.10218934714794159\n","step: 3320, loss: 0.16243967413902283\n","step: 3330, loss: 0.09505581110715866\n","step: 3340, loss: 0.09015055000782013\n","step: 3350, loss: 0.13660097122192383\n","step: 3360, loss: 0.17113791406154633\n","step: 3370, loss: 0.06984875351190567\n","step: 3380, loss: 0.06555389612913132\n","step: 3390, loss: 0.10256414860486984\n","step: 3400, loss: 0.0737653374671936\n","step: 3410, loss: 0.05883060768246651\n","step: 3420, loss: 0.08402307331562042\n","step: 3430, loss: 0.13040634989738464\n","step: 3440, loss: 0.07663272321224213\n","step: 3450, loss: 0.097957544028759\n","step: 3460, loss: 0.07913060486316681\n","step: 3470, loss: 0.07510067522525787\n","step: 3480, loss: 0.12078440934419632\n","step: 3490, loss: 0.12676234543323517\n","step: 3500, loss: 0.18428952991962433\n","step: 3510, loss: 0.12356722354888916\n","step: 3520, loss: 0.06072002649307251\n","step: 3530, loss: 0.09148727357387543\n","step: 3540, loss: 0.08794266730546951\n","step: 3550, loss: 0.08929003775119781\n","step: 3560, loss: 0.10098898410797119\n","step: 3570, loss: 0.0514325350522995\n","step: 3580, loss: 0.07126735895872116\n","step: 3590, loss: 0.06994374841451645\n","step: 3600, loss: 0.10108374059200287\n","step: 3610, loss: 0.07739878445863724\n","step: 3620, loss: 0.1401526778936386\n","step: 3630, loss: 0.08984620869159698\n","step: 3640, loss: 0.05469658598303795\n","step: 3650, loss: 0.27541422843933105\n","step: 3660, loss: 0.023974817246198654\n","step: 3670, loss: 0.08363072574138641\n","step: 3680, loss: 0.08816802501678467\n","step: 3690, loss: 0.09094801545143127\n","step: 3700, loss: 0.05927163362503052\n","step: 3710, loss: 0.04273437336087227\n","step: 3720, loss: 0.09658729285001755\n","step: 3730, loss: 0.024788448587059975\n","step: 3740, loss: 0.006324187386780977\n","step: 3750, loss: 0.13275428116321564\n","step: 3760, loss: 0.1127513125538826\n","step: 3770, loss: 0.10644840449094772\n","step: 3780, loss: 0.10384158045053482\n","step: 3790, loss: 0.1353943794965744\n","step: 3800, loss: 0.04841388761997223\n","step: 3810, loss: 0.13712917268276215\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      0.91      0.96        35\n","           2       0.54      0.29      0.37        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.97      0.84      0.90       291\n","           5       0.97      0.84      0.90       294\n","           6       0.97      0.98      0.98      1570\n","           7       0.57      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.93      0.99      0.96       689\n","          10       0.92      0.98      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.67      0.92      0.77        13\n","          14       0.29      1.00      0.46        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.79      0.86      0.83      1151\n","          17       0.93      0.98      0.95        41\n","          18       0.82      1.00      0.90        32\n","          19       0.42      0.35      0.38        40\n","          20       0.99      1.00      1.00       584\n","          21       0.16      0.10      0.12        52\n","          22       0.95      0.78      0.86      4175\n","          23       0.75      0.96      0.84      2253\n","          24       0.41      0.59      0.48        44\n","          25       0.85      0.94      0.89       888\n","          26       1.00      0.89      0.94         9\n","          27       0.94      0.99      0.96        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.86      0.90      1136\n","          31       0.58      0.58      0.58        19\n","          32       1.00      0.75      0.86         8\n","          33       0.87      0.80      0.84        86\n","          34       0.24      0.53      0.33        32\n","          35       0.97      0.99      0.98       474\n","          36       0.74      0.15      0.25       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.98      0.98      0.98       404\n","          39       0.97      0.96      0.97       485\n","          40       0.92      0.77      0.84       573\n","          41       0.95      0.94      0.95       841\n","          42       0.99      0.98      0.99       575\n","          43       0.90      0.86      0.88       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.95      0.52      0.67        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.81      0.80     28417\n","weighted avg       0.92      0.91      0.91     28417\n","\n","Difference 464\n","\n","Loop 58\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9510927200317383\n","step: 10, loss: 1.6963247060775757\n","step: 20, loss: 0.7676147818565369\n","step: 30, loss: 0.5684288144111633\n","step: 40, loss: 0.1651923805475235\n","step: 50, loss: 0.24084553122520447\n","step: 60, loss: 0.18038272857666016\n","step: 70, loss: 0.12067651003599167\n","step: 80, loss: 0.2269279509782791\n","step: 90, loss: 0.1615825891494751\n","step: 100, loss: 0.16526082158088684\n","step: 110, loss: 0.12492214143276215\n","step: 120, loss: 0.07110109180212021\n","step: 130, loss: 0.29426854848861694\n","step: 140, loss: 0.17072153091430664\n","step: 150, loss: 0.17298105359077454\n","step: 160, loss: 0.11705880612134933\n","step: 170, loss: 0.13802337646484375\n","step: 180, loss: 0.16756004095077515\n","step: 190, loss: 0.2563578188419342\n","step: 200, loss: 0.12766103446483612\n","step: 210, loss: 0.23173223435878754\n","step: 220, loss: 0.21279790997505188\n","step: 230, loss: 0.09138934314250946\n","step: 240, loss: 0.099103644490242\n","step: 250, loss: 0.0988968014717102\n","step: 260, loss: 0.11432038247585297\n","step: 270, loss: 0.12096331268548965\n","step: 280, loss: 0.04677286371588707\n","step: 290, loss: 0.08901912719011307\n","step: 300, loss: 0.040517501533031464\n","step: 310, loss: 0.19439329206943512\n","step: 320, loss: 0.10983384400606155\n","step: 330, loss: 0.11270492523908615\n","step: 340, loss: 0.06540127098560333\n","step: 350, loss: 0.06495526432991028\n","step: 360, loss: 0.07838178426027298\n","step: 370, loss: 0.10303124040365219\n","step: 380, loss: 0.09063415229320526\n","step: 390, loss: 0.07191646844148636\n","step: 400, loss: 0.10503070056438446\n","step: 410, loss: 0.05632932484149933\n","step: 420, loss: 0.06496868282556534\n","step: 430, loss: 0.10593215376138687\n","step: 440, loss: 0.04306534305214882\n","step: 450, loss: 0.08677054196596146\n","step: 460, loss: 0.12904450297355652\n","step: 470, loss: 0.08395440876483917\n","step: 480, loss: 0.15417873859405518\n","step: 490, loss: 0.09437472373247147\n","step: 500, loss: 0.10449016839265823\n","step: 510, loss: 0.14245663583278656\n","step: 520, loss: 0.1461404412984848\n","step: 530, loss: 0.11503524333238602\n","step: 540, loss: 0.04850558564066887\n","step: 550, loss: 0.16665376722812653\n","step: 560, loss: 0.04191063717007637\n","step: 570, loss: 0.11912347376346588\n","step: 580, loss: 0.1593838334083557\n","step: 590, loss: 0.13571980595588684\n","step: 600, loss: 0.03508314862847328\n","step: 610, loss: 0.06177923455834389\n","step: 620, loss: 0.12903764843940735\n","step: 630, loss: 0.08226644992828369\n","step: 640, loss: 0.04552432894706726\n","step: 650, loss: 0.19011779129505157\n","step: 660, loss: 0.15884840488433838\n","step: 670, loss: 0.08241409063339233\n","step: 680, loss: 0.10046800225973129\n","step: 690, loss: 0.153152734041214\n","step: 700, loss: 0.12471330165863037\n","step: 710, loss: 0.12653447687625885\n","step: 720, loss: 0.028974909335374832\n","step: 730, loss: 0.1218940019607544\n","step: 740, loss: 0.08391682803630829\n","step: 750, loss: 0.09676751494407654\n","step: 760, loss: 0.05488918349146843\n","step: 770, loss: 0.13425834476947784\n","step: 780, loss: 0.06866496801376343\n","step: 790, loss: 0.1980687379837036\n","step: 800, loss: 0.0786375105381012\n","step: 810, loss: 0.07474584877490997\n","step: 820, loss: 0.051136042922735214\n","step: 830, loss: 0.13156123459339142\n","step: 840, loss: 0.06410590559244156\n","step: 850, loss: 0.06757429242134094\n","step: 860, loss: 0.12956281006336212\n","step: 870, loss: 0.1303010731935501\n","step: 880, loss: 0.10686802119016647\n","step: 890, loss: 0.15682128071784973\n","step: 900, loss: 0.10021678358316422\n","step: 910, loss: 0.088031105697155\n","step: 920, loss: 0.08880337327718735\n","step: 930, loss: 0.11068025976419449\n","step: 940, loss: 0.11509419232606888\n","step: 950, loss: 0.1670200377702713\n","step: 960, loss: 0.11859141290187836\n","step: 970, loss: 0.1294417530298233\n","step: 980, loss: 0.19064080715179443\n","step: 990, loss: 0.09987517446279526\n","step: 1000, loss: 0.13634173572063446\n","step: 1010, loss: 0.07524723559617996\n","step: 1020, loss: 0.0338512659072876\n","step: 1030, loss: 0.07026919722557068\n","step: 1040, loss: 0.09800837934017181\n","step: 1050, loss: 0.11557118594646454\n","step: 1060, loss: 0.1231316402554512\n","step: 1070, loss: 0.13390354812145233\n","step: 1080, loss: 0.07362998276948929\n","step: 1090, loss: 0.13026347756385803\n","step: 1100, loss: 0.16924363374710083\n","step: 1110, loss: 0.09974189847707748\n","step: 1120, loss: 0.1236913800239563\n","step: 1130, loss: 0.10517282783985138\n","step: 1140, loss: 0.08080112934112549\n","step: 1150, loss: 0.09152016043663025\n","step: 1160, loss: 0.04935754835605621\n","step: 1170, loss: 0.06519562751054764\n","step: 1180, loss: 0.08943639695644379\n","step: 1190, loss: 0.09810354560613632\n","step: 1200, loss: 0.07848978787660599\n","step: 1210, loss: 0.028388122096657753\n","step: 1220, loss: 0.12749166786670685\n","step: 1230, loss: 0.12526346743106842\n","step: 1240, loss: 0.09710830450057983\n","step: 1250, loss: 0.1723574548959732\n","step: 1260, loss: 0.17409095168113708\n","step: 1270, loss: 0.031884342432022095\n","step: 1280, loss: 0.09412319958209991\n","step: 1290, loss: 0.13969209790229797\n","step: 1300, loss: 0.07419070601463318\n","step: 1310, loss: 0.08478230237960815\n","step: 1320, loss: 0.1545691192150116\n","step: 1330, loss: 0.06418152898550034\n","step: 1340, loss: 0.20364566147327423\n","step: 1350, loss: 0.04579853266477585\n","step: 1360, loss: 0.02326221950352192\n","step: 1370, loss: 0.048686593770980835\n","step: 1380, loss: 0.09269538521766663\n","step: 1390, loss: 0.12230250239372253\n","step: 1400, loss: 0.04849635437130928\n","step: 1410, loss: 0.0942489430308342\n","step: 1420, loss: 0.07360900938510895\n","step: 1430, loss: 0.13691893219947815\n","step: 1440, loss: 0.12545844912528992\n","step: 1450, loss: 0.06674133241176605\n","step: 1460, loss: 0.06993414461612701\n","step: 1470, loss: 0.12221299856901169\n","step: 1480, loss: 0.16600419580936432\n","step: 1490, loss: 0.11324767768383026\n","step: 1500, loss: 0.048807188868522644\n","step: 1510, loss: 0.051515910774469376\n","step: 1520, loss: 0.09548942744731903\n","step: 1530, loss: 0.05658341571688652\n","step: 1540, loss: 0.03190062940120697\n","step: 1550, loss: 0.030041448771953583\n","step: 1560, loss: 0.06846027076244354\n","step: 1570, loss: 0.06170447915792465\n","step: 1580, loss: 0.12135010957717896\n","step: 1590, loss: 0.05896373093128204\n","step: 1600, loss: 0.09560281783342361\n","step: 1610, loss: 0.09406494349241257\n","step: 1620, loss: 0.10768823325634003\n","step: 1630, loss: 0.08228475600481033\n","step: 1640, loss: 0.01623552478849888\n","step: 1650, loss: 0.12427002936601639\n","step: 1660, loss: 0.08094312250614166\n","step: 1670, loss: 0.08068250119686127\n","step: 1680, loss: 0.11966433376073837\n","step: 1690, loss: 0.03507358580827713\n","step: 1700, loss: 0.19968366622924805\n","step: 1710, loss: 0.08330047130584717\n","step: 1720, loss: 0.0838569700717926\n","step: 1730, loss: 0.09082182496786118\n","step: 1740, loss: 0.16762566566467285\n","step: 1750, loss: 0.08757662773132324\n","step: 1760, loss: 0.11958765238523483\n","step: 1770, loss: 0.04168519005179405\n","step: 1780, loss: 0.11507770419120789\n","step: 1790, loss: 0.0501708947122097\n","step: 1800, loss: 0.13517381250858307\n","step: 1810, loss: 0.0561690628528595\n","step: 1820, loss: 0.09134397655725479\n","step: 1830, loss: 0.15793800354003906\n","step: 1840, loss: 0.21331162750720978\n","step: 1850, loss: 0.03749982640147209\n","step: 1860, loss: 0.09895900636911392\n","step: 1870, loss: 0.04262418672442436\n","step: 1880, loss: 0.041066430509090424\n","step: 1890, loss: 0.10190029442310333\n","step: 1900, loss: 0.0967029333114624\n","step: 1910, loss: 0.07936707884073257\n","step: 1920, loss: 0.09537373483181\n","step: 1930, loss: 0.05810476094484329\n","step: 1940, loss: 0.06793025881052017\n","step: 1950, loss: 0.0641869306564331\n","step: 1960, loss: 0.0511200912296772\n","step: 1970, loss: 0.03540750592947006\n","step: 1980, loss: 0.14155574142932892\n","step: 1990, loss: 0.07768058776855469\n","step: 2000, loss: 0.15726801753044128\n","step: 2010, loss: 0.07929760217666626\n","step: 2020, loss: 0.07625683397054672\n","step: 2030, loss: 0.11064565926790237\n","step: 2040, loss: 0.16235750913619995\n","step: 2050, loss: 0.08438008278608322\n","step: 2060, loss: 0.03905337676405907\n","step: 2070, loss: 0.044009532779455185\n","step: 2080, loss: 0.12937729060649872\n","step: 2090, loss: 0.08186433464288712\n","step: 2100, loss: 0.10216985642910004\n","step: 2110, loss: 0.13169436156749725\n","step: 2120, loss: 0.046213071793317795\n","step: 2130, loss: 0.03759738430380821\n","step: 2140, loss: 0.04814065992832184\n","step: 2150, loss: 0.05718442052602768\n","step: 2160, loss: 0.05625439062714577\n","step: 2170, loss: 0.03448910266160965\n","step: 2180, loss: 0.07468022406101227\n","step: 2190, loss: 0.09759049862623215\n","step: 2200, loss: 0.09435299038887024\n","step: 2210, loss: 0.1027204841375351\n","step: 2220, loss: 0.09894207864999771\n","step: 2230, loss: 0.07524829357862473\n","step: 2240, loss: 0.10199116915464401\n","step: 2250, loss: 0.05381231755018234\n","step: 2260, loss: 0.06503447145223618\n","step: 2270, loss: 0.10192595422267914\n","step: 2280, loss: 0.11333129554986954\n","step: 2290, loss: 0.21372666954994202\n","step: 2300, loss: 0.037938863039016724\n","step: 2310, loss: 0.06694825738668442\n","step: 2320, loss: 0.2174343317747116\n","step: 2330, loss: 0.051366183906793594\n","step: 2340, loss: 0.10241338610649109\n","step: 2350, loss: 0.10636446624994278\n","step: 2360, loss: 0.03766275569796562\n","step: 2370, loss: 0.061649199575185776\n","step: 2380, loss: 0.09164194762706757\n","step: 2390, loss: 0.09902903437614441\n","step: 2400, loss: 0.07769482582807541\n","step: 2410, loss: 0.11139222979545593\n","step: 2420, loss: 0.13519833981990814\n","step: 2430, loss: 0.0865166038274765\n","step: 2440, loss: 0.16953830420970917\n","step: 2450, loss: 0.05178691819310188\n","step: 2460, loss: 0.053134772926568985\n","step: 2470, loss: 0.1116856038570404\n","step: 2480, loss: 0.11148108541965485\n","step: 2490, loss: 0.10515178740024567\n","step: 2500, loss: 0.18316411972045898\n","step: 2510, loss: 0.10613542050123215\n","step: 2520, loss: 0.024160467088222504\n","step: 2530, loss: 0.10545382648706436\n","step: 2540, loss: 0.1850709766149521\n","step: 2550, loss: 0.08945553004741669\n","step: 2560, loss: 0.1399276703596115\n","step: 2570, loss: 0.18239660561084747\n","step: 2580, loss: 0.07587675750255585\n","step: 2590, loss: 0.08340945094823837\n","step: 2600, loss: 0.05667882040143013\n","step: 2610, loss: 0.04156417399644852\n","step: 2620, loss: 0.13882555067539215\n","step: 2630, loss: 0.0829542875289917\n","step: 2640, loss: 0.09323028475046158\n","step: 2650, loss: 0.09547729045152664\n","step: 2660, loss: 0.1433342695236206\n","step: 2670, loss: 0.18665608763694763\n","step: 2680, loss: 0.13552561402320862\n","step: 2690, loss: 0.07835131138563156\n","step: 2700, loss: 0.024842403829097748\n","step: 2710, loss: 0.07863543182611465\n","step: 2720, loss: 0.10310478508472443\n","step: 2730, loss: 0.19349737465381622\n","step: 2740, loss: 0.10016891360282898\n","step: 2750, loss: 0.09160260111093521\n","step: 2760, loss: 0.14057840406894684\n","step: 2770, loss: 0.024518899619579315\n","step: 2780, loss: 0.04628491774201393\n","step: 2790, loss: 0.0981147363781929\n","step: 2800, loss: 0.040616121143102646\n","step: 2810, loss: 0.05029640346765518\n","step: 2820, loss: 0.08154953271150589\n","step: 2830, loss: 0.09643634408712387\n","step: 2840, loss: 0.05566415563225746\n","step: 2850, loss: 0.08569195121526718\n","step: 2860, loss: 0.04988410323858261\n","step: 2870, loss: 0.0468360111117363\n","step: 2880, loss: 0.041449639946222305\n","step: 2890, loss: 0.09134214371442795\n","step: 2900, loss: 0.026245269924402237\n","step: 2910, loss: 0.011574405245482922\n","step: 2920, loss: 0.10461150854825974\n","step: 2930, loss: 0.0475749671459198\n","step: 2940, loss: 0.057479750365018845\n","step: 2950, loss: 0.04499131441116333\n","step: 2960, loss: 0.05762633681297302\n","step: 2970, loss: 0.13506369292736053\n","step: 2980, loss: 0.05809580162167549\n","step: 2990, loss: 0.0624944344162941\n","step: 3000, loss: 0.09400597214698792\n","step: 3010, loss: 0.06504983454942703\n","step: 3020, loss: 0.09030419588088989\n","step: 3030, loss: 0.15249407291412354\n","step: 3040, loss: 0.1080918088555336\n","step: 3050, loss: 0.07117962837219238\n","step: 3060, loss: 0.04594147205352783\n","step: 3070, loss: 0.09481888264417648\n","step: 3080, loss: 0.07612268626689911\n","step: 3090, loss: 0.06477762758731842\n","step: 3100, loss: 0.026332613080739975\n","step: 3110, loss: 0.04535994678735733\n","step: 3120, loss: 0.10510946065187454\n","step: 3130, loss: 0.09357888996601105\n","step: 3140, loss: 0.09448496997356415\n","step: 3150, loss: 0.1235029473900795\n","step: 3160, loss: 0.15832431614398956\n","step: 3170, loss: 0.05704807490110397\n","step: 3180, loss: 0.06779132038354874\n","step: 3190, loss: 0.0626010149717331\n","step: 3200, loss: 0.04306303709745407\n","step: 3210, loss: 0.05452433228492737\n","step: 3220, loss: 0.08748674392700195\n","step: 3230, loss: 0.036732904613018036\n","step: 3240, loss: 0.07147126644849777\n","step: 3250, loss: 0.06551041454076767\n","step: 3260, loss: 0.07431646436452866\n","step: 3270, loss: 0.12704485654830933\n","step: 3280, loss: 0.06325744092464447\n","step: 3290, loss: 0.12372025102376938\n","step: 3300, loss: 0.11338402330875397\n","step: 3310, loss: 0.05386054515838623\n","step: 3320, loss: 0.042300641536712646\n","step: 3330, loss: 0.030893392860889435\n","step: 3340, loss: 0.07390230149030685\n","step: 3350, loss: 0.06422653794288635\n","step: 3360, loss: 0.0598120242357254\n","step: 3370, loss: 0.10954292118549347\n","step: 3380, loss: 0.06024136021733284\n","step: 3390, loss: 0.06396423280239105\n","step: 3400, loss: 0.04910598322749138\n","step: 3410, loss: 0.1030350774526596\n","step: 3420, loss: 0.07660923898220062\n","step: 3430, loss: 0.0686044842004776\n","step: 3440, loss: 0.041991524398326874\n","step: 3450, loss: 0.06737938523292542\n","step: 3460, loss: 0.0576915480196476\n","step: 3470, loss: 0.11188102513551712\n","step: 3480, loss: 0.05600268766283989\n","step: 3490, loss: 0.16664178669452667\n","step: 3500, loss: 0.20813880860805511\n","step: 3510, loss: 0.09599619358778\n","step: 3520, loss: 0.10145653039216995\n","step: 3530, loss: 0.06899549067020416\n","step: 3540, loss: 0.08872626721858978\n","step: 3550, loss: 0.04740193858742714\n","step: 3560, loss: 0.0672026202082634\n","step: 3570, loss: 0.0825079008936882\n","step: 3580, loss: 0.2013225257396698\n","step: 3590, loss: 0.054655689746141434\n","step: 3600, loss: 0.04971199110150337\n","step: 3610, loss: 0.07861050963401794\n","step: 3620, loss: 0.06534326076507568\n","step: 3630, loss: 0.014532478526234627\n","step: 3640, loss: 0.06278092414140701\n","step: 3650, loss: 0.06741869449615479\n","step: 3660, loss: 0.08548960834741592\n","step: 3670, loss: 0.051202766597270966\n","step: 3680, loss: 0.04847937822341919\n","step: 3690, loss: 0.12334908545017242\n","step: 3700, loss: 0.06822334975004196\n","step: 3710, loss: 0.04827307164669037\n","step: 3720, loss: 0.019408036023378372\n","step: 3730, loss: 0.057478372007608414\n","step: 3740, loss: 0.09667137265205383\n","step: 3750, loss: 0.0356815829873085\n","step: 3760, loss: 0.11141570657491684\n","step: 3770, loss: 0.09884737432003021\n","step: 3780, loss: 0.12382674217224121\n","step: 3790, loss: 0.04900882765650749\n","step: 3800, loss: 0.03414333984255791\n","step: 3810, loss: 0.08776848763227463\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.64      0.39      0.48        77\n","           3       1.00      0.79      0.89      1030\n","           4       1.00      0.83      0.91       291\n","           5       0.92      0.84      0.88       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.55      0.94      0.69       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.98       689\n","          10       0.92      0.97      0.95       901\n","          11       0.98      0.99      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.75      0.69      0.72        13\n","          14       0.25      1.00      0.41        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.88      0.82      0.85      1151\n","          17       0.97      0.93      0.95        41\n","          18       0.94      0.97      0.95        32\n","          19       0.23      0.35      0.28        40\n","          20       0.99      1.00      0.99       584\n","          21       0.10      0.13      0.11        52\n","          22       0.93      0.74      0.83      4175\n","          23       0.69      0.97      0.80      2253\n","          24       0.27      0.36      0.31        44\n","          25       0.90      0.91      0.91       888\n","          26       1.00      1.00      1.00         9\n","          27       0.94      0.99      0.96        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.92      0.82      0.87      1136\n","          31       0.52      0.74      0.61        19\n","          32       0.80      0.50      0.62         8\n","          33       0.58      0.98      0.72        86\n","          34       0.05      0.09      0.07        32\n","          35       0.97      0.99      0.98       474\n","          36       0.38      0.16      0.23       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.95      0.98      0.96       404\n","          39       1.00      0.89      0.94       485\n","          40       0.93      0.90      0.91       573\n","          41       0.95      0.94      0.94       841\n","          42       0.99      0.98      0.99       575\n","          43       0.94      0.89      0.91       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.98      0.99        82\n","          48       1.00      0.63      0.78        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.79      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 466\n","\n","Loop 59\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.96004056930542\n","step: 10, loss: 1.8525991439819336\n","step: 20, loss: 0.8041223883628845\n","step: 30, loss: 0.4342963695526123\n","step: 40, loss: 0.22150608897209167\n","step: 50, loss: 0.1396239995956421\n","step: 60, loss: 0.08905162662267685\n","step: 70, loss: 0.23213863372802734\n","step: 80, loss: 0.16425423324108124\n","step: 90, loss: 0.11698860675096512\n","step: 100, loss: 0.1722438931465149\n","step: 110, loss: 0.2138989269733429\n","step: 120, loss: 0.1908452957868576\n","step: 130, loss: 0.21975775063037872\n","step: 140, loss: 0.11640804260969162\n","step: 150, loss: 0.19088958203792572\n","step: 160, loss: 0.2261100560426712\n","step: 170, loss: 0.0546133853495121\n","step: 180, loss: 0.22028818726539612\n","step: 190, loss: 0.19984419643878937\n","step: 200, loss: 0.09553410857915878\n","step: 210, loss: 0.1273011863231659\n","step: 220, loss: 0.14103415608406067\n","step: 230, loss: 0.09666953235864639\n","step: 240, loss: 0.12707912921905518\n","step: 250, loss: 0.08807364851236343\n","step: 260, loss: 0.1739995926618576\n","step: 270, loss: 0.09620564430952072\n","step: 280, loss: 0.1526954025030136\n","step: 290, loss: 0.14501945674419403\n","step: 300, loss: 0.055752817541360855\n","step: 310, loss: 0.11176800727844238\n","step: 320, loss: 0.11479868739843369\n","step: 330, loss: 0.13198329508304596\n","step: 340, loss: 0.06915628910064697\n","step: 350, loss: 0.1387793868780136\n","step: 360, loss: 0.06352649629116058\n","step: 370, loss: 0.07940196245908737\n","step: 380, loss: 0.10103342682123184\n","step: 390, loss: 0.07379712164402008\n","step: 400, loss: 0.1575039178133011\n","step: 410, loss: 0.09108733385801315\n","step: 420, loss: 0.16791599988937378\n","step: 430, loss: 0.24114744365215302\n","step: 440, loss: 0.11013013869524002\n","step: 450, loss: 0.0986635759472847\n","step: 460, loss: 0.17743712663650513\n","step: 470, loss: 0.09887374937534332\n","step: 480, loss: 0.11392153054475784\n","step: 490, loss: 0.07036780565977097\n","step: 500, loss: 0.17405210435390472\n","step: 510, loss: 0.14716559648513794\n","step: 520, loss: 0.15122835338115692\n","step: 530, loss: 0.10335015505552292\n","step: 540, loss: 0.11729195713996887\n","step: 550, loss: 0.07374808192253113\n","step: 560, loss: 0.06892061233520508\n","step: 570, loss: 0.05986250191926956\n","step: 580, loss: 0.11824464797973633\n","step: 590, loss: 0.07839137315750122\n","step: 600, loss: 0.21583232283592224\n","step: 610, loss: 0.10335094481706619\n","step: 620, loss: 0.06791115552186966\n","step: 630, loss: 0.08161312341690063\n","step: 640, loss: 0.19837985932826996\n","step: 650, loss: 0.12151265144348145\n","step: 660, loss: 0.04041023179888725\n","step: 670, loss: 0.08606041967868805\n","step: 680, loss: 0.09115944057703018\n","step: 690, loss: 0.17766143381595612\n","step: 700, loss: 0.1041640192270279\n","step: 710, loss: 0.0743732824921608\n","step: 720, loss: 0.1395643800497055\n","step: 730, loss: 0.09297831356525421\n","step: 740, loss: 0.07218796759843826\n","step: 750, loss: 0.11288830637931824\n","step: 760, loss: 0.09208223223686218\n","step: 770, loss: 0.10958202928304672\n","step: 780, loss: 0.08947769552469254\n","step: 790, loss: 0.07875795662403107\n","step: 800, loss: 0.10987575352191925\n","step: 810, loss: 0.2817356586456299\n","step: 820, loss: 0.10621362179517746\n","step: 830, loss: 0.07797392457723618\n","step: 840, loss: 0.1393560916185379\n","step: 850, loss: 0.06401876360177994\n","step: 860, loss: 0.18802644312381744\n","step: 870, loss: 0.1028781533241272\n","step: 880, loss: 0.10564889013767242\n","step: 890, loss: 0.1832464039325714\n","step: 900, loss: 0.12685105204582214\n","step: 910, loss: 0.13123849034309387\n","step: 920, loss: 0.05125296488404274\n","step: 930, loss: 0.04660922661423683\n","step: 940, loss: 0.10868962854146957\n","step: 950, loss: 0.0770595446228981\n","step: 960, loss: 0.09898946434259415\n","step: 970, loss: 0.14058439433574677\n","step: 980, loss: 0.18954944610595703\n","step: 990, loss: 0.12569579482078552\n","step: 1000, loss: 0.11321565508842468\n","step: 1010, loss: 0.010241620242595673\n","step: 1020, loss: 0.08124860376119614\n","step: 1030, loss: 0.12360449880361557\n","step: 1040, loss: 0.03221073001623154\n","step: 1050, loss: 0.1786820888519287\n","step: 1060, loss: 0.11259113997220993\n","step: 1070, loss: 0.09751317650079727\n","step: 1080, loss: 0.1285775601863861\n","step: 1090, loss: 0.12656497955322266\n","step: 1100, loss: 0.138514444231987\n","step: 1110, loss: 0.06617369502782822\n","step: 1120, loss: 0.09493324905633926\n","step: 1130, loss: 0.02592964470386505\n","step: 1140, loss: 0.172874316573143\n","step: 1150, loss: 0.1620652675628662\n","step: 1160, loss: 0.07558876276016235\n","step: 1170, loss: 0.09127886593341827\n","step: 1180, loss: 0.0941460058093071\n","step: 1190, loss: 0.15668584406375885\n","step: 1200, loss: 0.05060045048594475\n","step: 1210, loss: 0.14540666341781616\n","step: 1220, loss: 0.1271556168794632\n","step: 1230, loss: 0.1304463893175125\n","step: 1240, loss: 0.062317829579114914\n","step: 1250, loss: 0.07778730988502502\n","step: 1260, loss: 0.11538224667310715\n","step: 1270, loss: 0.02811937779188156\n","step: 1280, loss: 0.14247751235961914\n","step: 1290, loss: 0.14078742265701294\n","step: 1300, loss: 0.09705343097448349\n","step: 1310, loss: 0.08954956382513046\n","step: 1320, loss: 0.14750319719314575\n","step: 1330, loss: 0.08042343705892563\n","step: 1340, loss: 0.06985798478126526\n","step: 1350, loss: 0.19626836478710175\n","step: 1360, loss: 0.0733485147356987\n","step: 1370, loss: 0.05776973068714142\n","step: 1380, loss: 0.15919432044029236\n","step: 1390, loss: 0.08534781634807587\n","step: 1400, loss: 0.12548771500587463\n","step: 1410, loss: 0.1359412670135498\n","step: 1420, loss: 0.05216744542121887\n","step: 1430, loss: 0.09289590269327164\n","step: 1440, loss: 0.057548414915800095\n","step: 1450, loss: 0.07990601658821106\n","step: 1460, loss: 0.057510267943143845\n","step: 1470, loss: 0.10000342130661011\n","step: 1480, loss: 0.10270178318023682\n","step: 1490, loss: 0.11922156065702438\n","step: 1500, loss: 0.10704885423183441\n","step: 1510, loss: 0.06642752885818481\n","step: 1520, loss: 0.1439695656299591\n","step: 1530, loss: 0.10452006012201309\n","step: 1540, loss: 0.12335661053657532\n","step: 1550, loss: 0.03401663526892662\n","step: 1560, loss: 0.059362635016441345\n","step: 1570, loss: 0.0956193283200264\n","step: 1580, loss: 0.2092515230178833\n","step: 1590, loss: 0.08494462817907333\n","step: 1600, loss: 0.15367598831653595\n","step: 1610, loss: 0.051840513944625854\n","step: 1620, loss: 0.09843948483467102\n","step: 1630, loss: 0.05972975119948387\n","step: 1640, loss: 0.07099060714244843\n","step: 1650, loss: 0.17927776277065277\n","step: 1660, loss: 0.06139827147126198\n","step: 1670, loss: 0.1053490862250328\n","step: 1680, loss: 0.11771519482135773\n","step: 1690, loss: 0.08787404000759125\n","step: 1700, loss: 0.23746712505817413\n","step: 1710, loss: 0.07533842325210571\n","step: 1720, loss: 0.10298893600702286\n","step: 1730, loss: 0.06858030706644058\n","step: 1740, loss: 0.04477943480014801\n","step: 1750, loss: 0.03858352452516556\n","step: 1760, loss: 0.10796115547418594\n","step: 1770, loss: 0.07756395637989044\n","step: 1780, loss: 0.11969233304262161\n","step: 1790, loss: 0.08077900856733322\n","step: 1800, loss: 0.04911811277270317\n","step: 1810, loss: 0.1725229173898697\n","step: 1820, loss: 0.19858142733573914\n","step: 1830, loss: 0.08868706971406937\n","step: 1840, loss: 0.12052509188652039\n","step: 1850, loss: 0.09043345600366592\n","step: 1860, loss: 0.037143126130104065\n","step: 1870, loss: 0.08784221112728119\n","step: 1880, loss: 0.09453009068965912\n","step: 1890, loss: 0.05434777960181236\n","step: 1900, loss: 0.08646798878908157\n","step: 1910, loss: 0.10906300693750381\n","step: 1920, loss: 0.06004885956645012\n","step: 1930, loss: 0.10776236653327942\n","step: 1940, loss: 0.15350498259067535\n","step: 1950, loss: 0.16169683635234833\n","step: 1960, loss: 0.0869186520576477\n","step: 1970, loss: 0.0821898952126503\n","step: 1980, loss: 0.06461536139249802\n","step: 1990, loss: 0.046495966613292694\n","step: 2000, loss: 0.11694008857011795\n","step: 2010, loss: 0.0633058249950409\n","step: 2020, loss: 0.10824726521968842\n","step: 2030, loss: 0.11771517992019653\n","step: 2040, loss: 0.034925416111946106\n","step: 2050, loss: 0.1774081140756607\n","step: 2060, loss: 0.09193414449691772\n","step: 2070, loss: 0.08654362708330154\n","step: 2080, loss: 0.163692444562912\n","step: 2090, loss: 0.1276487410068512\n","step: 2100, loss: 0.09657450020313263\n","step: 2110, loss: 0.057111576199531555\n","step: 2120, loss: 0.08068599551916122\n","step: 2130, loss: 0.09815006703138351\n","step: 2140, loss: 0.11059790104627609\n","step: 2150, loss: 0.10950246453285217\n","step: 2160, loss: 0.11217165738344193\n","step: 2170, loss: 0.1258736401796341\n","step: 2180, loss: 0.14066962897777557\n","step: 2190, loss: 0.11923057585954666\n","step: 2200, loss: 0.0925898551940918\n","step: 2210, loss: 0.10563869774341583\n","step: 2220, loss: 0.040704868733882904\n","step: 2230, loss: 0.07337045669555664\n","step: 2240, loss: 0.07821233570575714\n","step: 2250, loss: 0.05440627411007881\n","step: 2260, loss: 0.1712358593940735\n","step: 2270, loss: 0.030493225902318954\n","step: 2280, loss: 0.026521753519773483\n","step: 2290, loss: 0.06740149855613708\n","step: 2300, loss: 0.09108041226863861\n","step: 2310, loss: 0.033138301223516464\n","step: 2320, loss: 0.07141074538230896\n","step: 2330, loss: 0.10001091659069061\n","step: 2340, loss: 0.13811877369880676\n","step: 2350, loss: 0.024282032623887062\n","step: 2360, loss: 0.07512570172548294\n","step: 2370, loss: 0.12705335021018982\n","step: 2380, loss: 0.09734480082988739\n","step: 2390, loss: 0.16144633293151855\n","step: 2400, loss: 0.11984342336654663\n","step: 2410, loss: 0.1362619549036026\n","step: 2420, loss: 0.06660152971744537\n","step: 2430, loss: 0.12400944530963898\n","step: 2440, loss: 0.0566411018371582\n","step: 2450, loss: 0.16649240255355835\n","step: 2460, loss: 0.12057889252901077\n","step: 2470, loss: 0.043614283204078674\n","step: 2480, loss: 0.10032088309526443\n","step: 2490, loss: 0.0376921109855175\n","step: 2500, loss: 0.06017111986875534\n","step: 2510, loss: 0.11009550839662552\n","step: 2520, loss: 0.08501473069190979\n","step: 2530, loss: 0.06754127144813538\n","step: 2540, loss: 0.05149569362401962\n","step: 2550, loss: 0.05309711769223213\n","step: 2560, loss: 0.07537122815847397\n","step: 2570, loss: 0.028756603598594666\n","step: 2580, loss: 0.058150988072156906\n","step: 2590, loss: 0.04826602339744568\n","step: 2600, loss: 0.15079651772975922\n","step: 2610, loss: 0.10791627317667007\n","step: 2620, loss: 0.09761328250169754\n","step: 2630, loss: 0.16808247566223145\n","step: 2640, loss: 0.1170860081911087\n","step: 2650, loss: 0.07166942954063416\n","step: 2660, loss: 0.18326766788959503\n","step: 2670, loss: 0.06386525928974152\n","step: 2680, loss: 0.16592948138713837\n","step: 2690, loss: 0.03253242000937462\n","step: 2700, loss: 0.09215092658996582\n","step: 2710, loss: 0.05423145741224289\n","step: 2720, loss: 0.11049206554889679\n","step: 2730, loss: 0.07720483839511871\n","step: 2740, loss: 0.06355031579732895\n","step: 2750, loss: 0.0722195953130722\n","step: 2760, loss: 0.09353543817996979\n","step: 2770, loss: 0.05633993074297905\n","step: 2780, loss: 0.06638804078102112\n","step: 2790, loss: 0.09102486073970795\n","step: 2800, loss: 0.06339820474386215\n","step: 2810, loss: 0.06866168230772018\n","step: 2820, loss: 0.05491798371076584\n","step: 2830, loss: 0.0658424124121666\n","step: 2840, loss: 0.024623218923807144\n","step: 2850, loss: 0.1417466253042221\n","step: 2860, loss: 0.13135331869125366\n","step: 2870, loss: 0.06465566903352737\n","step: 2880, loss: 0.08195142447948456\n","step: 2890, loss: 0.059204235672950745\n","step: 2900, loss: 0.06820687651634216\n","step: 2910, loss: 0.03276549652218819\n","step: 2920, loss: 0.08351504802703857\n","step: 2930, loss: 0.09094835817813873\n","step: 2940, loss: 0.05060999095439911\n","step: 2950, loss: 0.1055324524641037\n","step: 2960, loss: 0.09741010516881943\n","step: 2970, loss: 0.11936893314123154\n","step: 2980, loss: 0.11421018093824387\n","step: 2990, loss: 0.09783107787370682\n","step: 3000, loss: 0.057021234184503555\n","step: 3010, loss: 0.03980806842446327\n","step: 3020, loss: 0.08280858397483826\n","step: 3030, loss: 0.0317661389708519\n","step: 3040, loss: 0.06129099428653717\n","step: 3050, loss: 0.10986208915710449\n","step: 3060, loss: 0.03566470369696617\n","step: 3070, loss: 0.08177058398723602\n","step: 3080, loss: 0.08866405487060547\n","step: 3090, loss: 0.0702052190899849\n","step: 3100, loss: 0.044679176062345505\n","step: 3110, loss: 0.0845450907945633\n","step: 3120, loss: 0.07563410699367523\n","step: 3130, loss: 0.11542309820652008\n","step: 3140, loss: 0.07735202461481094\n","step: 3150, loss: 0.06866589933633804\n","step: 3160, loss: 0.037799619138240814\n","step: 3170, loss: 0.12608487904071808\n","step: 3180, loss: 0.032624345272779465\n","step: 3190, loss: 0.03754623606801033\n","step: 3200, loss: 0.12708105146884918\n","step: 3210, loss: 0.12377607077360153\n","step: 3220, loss: 0.1110384464263916\n","step: 3230, loss: 0.05478349328041077\n","step: 3240, loss: 0.01486531738191843\n","step: 3250, loss: 0.08361942321062088\n","step: 3260, loss: 0.1017909124493599\n","step: 3270, loss: 0.07163802534341812\n","step: 3280, loss: 0.09582395106554031\n","step: 3290, loss: 0.019156621769070625\n","step: 3300, loss: 0.0646786019206047\n","step: 3310, loss: 0.08292645215988159\n","step: 3320, loss: 0.0754343643784523\n","step: 3330, loss: 0.04842020571231842\n","step: 3340, loss: 0.04701133072376251\n","step: 3350, loss: 0.08719506114721298\n","step: 3360, loss: 0.030519619584083557\n","step: 3370, loss: 0.13480864465236664\n","step: 3380, loss: 0.08291614800691605\n","step: 3390, loss: 0.13142630457878113\n","step: 3400, loss: 0.10919762402772903\n","step: 3410, loss: 0.07722831517457962\n","step: 3420, loss: 0.09988933056592941\n","step: 3430, loss: 0.08050096780061722\n","step: 3440, loss: 0.07465147972106934\n","step: 3450, loss: 0.18311578035354614\n","step: 3460, loss: 0.07381244003772736\n","step: 3470, loss: 0.06132827699184418\n","step: 3480, loss: 0.0716981291770935\n","step: 3490, loss: 0.07794221490621567\n","step: 3500, loss: 0.08831380307674408\n","step: 3510, loss: 0.05133384093642235\n","step: 3520, loss: 0.07443088293075562\n","step: 3530, loss: 0.07711860537528992\n","step: 3540, loss: 0.25068166851997375\n","step: 3550, loss: 0.05751330405473709\n","step: 3560, loss: 0.08606269210577011\n","step: 3570, loss: 0.06232674419879913\n","step: 3580, loss: 0.12719452381134033\n","step: 3590, loss: 0.0594620518386364\n","step: 3600, loss: 0.04344841465353966\n","step: 3610, loss: 0.05054289475083351\n","step: 3620, loss: 0.10423880070447922\n","step: 3630, loss: 0.08642058074474335\n","step: 3640, loss: 0.09601231664419174\n","step: 3650, loss: 0.026705380529165268\n","step: 3660, loss: 0.061145056039094925\n","step: 3670, loss: 0.1387718915939331\n","step: 3680, loss: 0.1626882404088974\n","step: 3690, loss: 0.06498575210571289\n","step: 3700, loss: 0.13107770681381226\n","step: 3710, loss: 0.11578737199306488\n","step: 3720, loss: 0.07768356055021286\n","step: 3730, loss: 0.07520325481891632\n","step: 3740, loss: 0.044879063963890076\n","step: 3750, loss: 0.08200367540121078\n","step: 3760, loss: 0.07034140825271606\n","step: 3770, loss: 0.1575591117143631\n","step: 3780, loss: 0.0940929427742958\n","step: 3790, loss: 0.04960348457098007\n","step: 3800, loss: 0.08388041704893112\n","step: 3810, loss: 0.0803157240152359\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.85      0.97      0.91        35\n","           2       0.73      0.45      0.56        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.98      0.84      0.91       291\n","           5       0.83      0.84      0.84       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       689\n","          10       0.97      0.98      0.97       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.50      0.92      0.65        13\n","          14       0.37      1.00      0.54        43\n","          15       0.96      0.98      0.97      2778\n","          16       0.82      0.84      0.83      1151\n","          17       0.95      0.88      0.91        41\n","          18       0.94      1.00      0.97        32\n","          19       0.40      0.78      0.53        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.97      0.72      0.82      4175\n","          23       0.69      0.97      0.80      2253\n","          24       0.34      0.68      0.45        44\n","          25       0.85      0.91      0.88       888\n","          26       0.88      0.78      0.82         9\n","          27       0.96      0.99      0.97        69\n","          28       1.00      0.98      0.99      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.83      0.88      1136\n","          31       0.53      0.89      0.67        19\n","          32       1.00      0.75      0.86         8\n","          33       0.64      0.95      0.76        86\n","          34       0.21      0.44      0.29        32\n","          35       0.99      0.99      0.99       474\n","          36       0.61      0.19      0.29       182\n","          37       0.85      0.96      0.90      1592\n","          38       0.96      0.98      0.97       404\n","          39       0.98      0.91      0.94       485\n","          40       0.89      0.95      0.92       573\n","          41       0.93      0.94      0.94       841\n","          42       0.97      0.99      0.98       575\n","          43       0.94      0.96      0.95       152\n","          44       0.99      0.92      0.95        75\n","          46       1.00      0.87      0.93        82\n","          48       0.81      0.71      0.76        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.83      0.80     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","Difference 464\n","\n","Loop 60\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Train from scratch...\n","step: 0, loss: 3.9919679164886475\n","step: 10, loss: 2.1831295490264893\n","step: 20, loss: 0.8834181427955627\n","step: 30, loss: 0.40551456809043884\n","step: 40, loss: 0.5093245506286621\n","step: 50, loss: 0.17259877920150757\n","step: 60, loss: 0.2642151117324829\n","step: 70, loss: 0.2267920970916748\n","step: 80, loss: 0.09236352890729904\n","step: 90, loss: 0.06847624480724335\n","step: 100, loss: 0.193571075797081\n","step: 110, loss: 0.21791543066501617\n","step: 120, loss: 0.15678715705871582\n","step: 130, loss: 0.1794290393590927\n","step: 140, loss: 0.08837181329727173\n","step: 150, loss: 0.34447041153907776\n","step: 160, loss: 0.15022188425064087\n","step: 170, loss: 0.14880697429180145\n","step: 180, loss: 0.07858103513717651\n","step: 190, loss: 0.14007481932640076\n","step: 200, loss: 0.10545501112937927\n","step: 210, loss: 0.12647585570812225\n","step: 220, loss: 0.1506822556257248\n","step: 230, loss: 0.12373344600200653\n","step: 240, loss: 0.07537040114402771\n","step: 250, loss: 0.1313236653804779\n","step: 260, loss: 0.10953597724437714\n","step: 270, loss: 0.1117440015077591\n","step: 280, loss: 0.23510731756687164\n","step: 290, loss: 0.11017806828022003\n","step: 300, loss: 0.0649166852235794\n","step: 310, loss: 0.12272248417139053\n","step: 320, loss: 0.1251184046268463\n","step: 330, loss: 0.11357289552688599\n","step: 340, loss: 0.1253901869058609\n","step: 350, loss: 0.09958915412425995\n","step: 360, loss: 0.14220312237739563\n","step: 370, loss: 0.12179186195135117\n","step: 380, loss: 0.21451842784881592\n","step: 390, loss: 0.14063675701618195\n","step: 400, loss: 0.06594614684581757\n","step: 410, loss: 0.031146924942731857\n","step: 420, loss: 0.07616791874170303\n","step: 430, loss: 0.09979155659675598\n","step: 440, loss: 0.10885078459978104\n","step: 450, loss: 0.09345436096191406\n","step: 460, loss: 0.08460211753845215\n","step: 470, loss: 0.17635716497898102\n","step: 480, loss: 0.15310387313365936\n","step: 490, loss: 0.07972949743270874\n","step: 500, loss: 0.19423235952854156\n","step: 510, loss: 0.10285276174545288\n","step: 520, loss: 0.15103431046009064\n","step: 530, loss: 0.1780918538570404\n","step: 540, loss: 0.08176881074905396\n","step: 550, loss: 0.07597024738788605\n","step: 560, loss: 0.10388192534446716\n","step: 570, loss: 0.07225464284420013\n","step: 580, loss: 0.05112890526652336\n","step: 590, loss: 0.02281113527715206\n","step: 600, loss: 0.08876131474971771\n","step: 610, loss: 0.1308925598859787\n","step: 620, loss: 0.06870073080062866\n","step: 630, loss: 0.09261917322874069\n","step: 640, loss: 0.16252842545509338\n","step: 650, loss: 0.09329279512166977\n","step: 660, loss: 0.16080716252326965\n","step: 670, loss: 0.12961265444755554\n","step: 680, loss: 0.022018257528543472\n","step: 690, loss: 0.1331915408372879\n","step: 700, loss: 0.09250937402248383\n","step: 710, loss: 0.0931730717420578\n","step: 720, loss: 0.218556746840477\n","step: 730, loss: 0.07342219352722168\n","step: 740, loss: 0.09911899268627167\n","step: 750, loss: 0.09635283797979355\n","step: 760, loss: 0.21313142776489258\n","step: 770, loss: 0.04410320520401001\n","step: 780, loss: 0.14478258788585663\n","step: 790, loss: 0.10817424952983856\n","step: 800, loss: 0.11586878448724747\n","step: 810, loss: 0.056014012545347214\n","step: 820, loss: 0.07444737106561661\n","step: 830, loss: 0.17158672213554382\n","step: 840, loss: 0.12493044883012772\n","step: 850, loss: 0.11508650332689285\n","step: 860, loss: 0.17022250592708588\n","step: 870, loss: 0.044214390218257904\n","step: 880, loss: 0.029893964529037476\n","step: 890, loss: 0.11875341832637787\n","step: 900, loss: 0.13183051347732544\n","step: 910, loss: 0.07613116502761841\n","step: 920, loss: 0.0319756381213665\n","step: 930, loss: 0.26449066400527954\n","step: 940, loss: 0.11567876487970352\n","step: 950, loss: 0.1149371787905693\n"]}],"source":["topn = 500\n","i = 0\n","last_top_sen = set()\n","top_words = domain_dev_word_lst[:topn]\n","new_top_sen = set([tuple(sen) for sen in top_words])\n","\n","while len(new_top_sen.difference(last_top_sen)) > 50:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","\n","  domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","  domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  if i == 1:\n","    last_top_sen = set()\n","  else:\n","    last_top_sen = new_top_sen\n","\n","  top_words_ids, top_tags_ids, remain_words, remain_tags = gen_pseudo_data(model, domain_dev_iter, topn)\n","  new_top_sen = set([tuple(sen) for sen in top_words_ids])\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    words = []\n","    tags = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        tags.append(tag_ids[k])\n","    top_words.append(words)\n","    top_tags.append(tags)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)\n","\n","  print(\"Difference\", len(new_top_sen.difference(last_top_sen)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2FBwm-BIV1L"},"outputs":[],"source":["model_file = os.path.join(model_dir, \"nonfixed_model.pt\")\n","torch.save(model.state_dict(), model_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-t80AQ7wb4U"},"outputs":[],"source":["# from collections import Counter\n","\n","# c = Counter([tuple(sen) for sen in top_words])\n","# print( c.most_common(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4ZuWMYTojmJ"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJxcl1cLn7rb"},"outputs":[],"source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvkSQ18M7p2H"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ngux9I-orPX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEACZ4DaGGRA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2DFUhCDorSf"},"outputs":[],"source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHUO7r7-tsoo"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}