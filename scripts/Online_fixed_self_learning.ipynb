{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1669695793391,"user_tz":300,"elapsed":20391,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"eef649f6-ee65-47ed-c008-25d2c376e6e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Collecting boto3\n","  Downloading boto3-1.26.17-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 1.3 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.17\n","  Downloading botocore-1.29.17-py3-none-any.whl (10.0 MB)\n","\u001b[K     |████████████████████████████████| 10.0 MB 23.5 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 75.2 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.17 botocore-1.29.17 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 21.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.10.3\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from utils import read_conll_file, read_data\n","\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1669695826925,"user_tz":300,"elapsed":33404,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"4c40f0e8-0118-4c87-fe59-499c8d705a62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"CPKysG3i3nsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KqectYYC30N","executionInfo":{"status":"ok","timestamp":1669695830047,"user_tz":300,"elapsed":3124,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"bb238309-cefc-421d-9e1b-94c3cab39143"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1669695830047,"user_tz":300,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"10448e13-61fc-4747-f1ed-aa921a29cb87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669695831979,"user_tz":300,"elapsed":13,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"67325939-779f-4d6f-ccd5-07bd3771f181"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 14197180.29B/s]\n"]}]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator, average=\"weighted\"): #macro, micro\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","\n","    pred_lst = []\n","    true_lst = []\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            for s in y_hat.cpu().numpy().tolist():\n","              pred_lst.extend(s)\n","            for s in y.numpy().tolist():\n","              true_lst.extend(s)\n","\n","    precision_value = multiclass_precision(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    recall_value = multiclass_recall(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    f1_value = multiclass_f1_score(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    acc = multiclass_accuracy(\n","        torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","        average=average)    \n","\n","\n","    return precision_value, recall_value, f1_value, acc"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669695856880,"user_tz":300,"elapsed":24910,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"53ad7118-03b3-4b2f-fb14-87482c5310f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:14<00:00, 28705387.68B/s]\n"]}]},{"cell_type":"code","source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.00001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run this part only when there is no base_model before to save time\n","if 'base_model.pt' not in os.listdir(model_dir):\n","  train(model, train_iter, optimizer, criterion)\n","  eval(model, test_iter)"],"metadata":{"id":"B0x3gRfi9iyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# save the model only if we have not train the base model before\n","# all the models share the same base model\n","if 'base_model.pt' not in os.listdir(model_dir):\n","  torch.save(model.state_dict(), model_file)"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value = eval(model, test_iter)\n","print(wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value)"],"metadata":{"id":"hAD3Wd574v6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669695944761,"user_tz":300,"elapsed":19400,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"213f90e3-002f-48e3-df44-1ed745cb3377"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9771) tensor(0.9743) tensor(0.9751) tensor(0.9743)\n"]}]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"],"metadata":{"id":"agIHM1TmEYl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"mGm3QLNcD8bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain = \"answers\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"],"metadata":{"id":"fwvivWyzEHOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"],"metadata":{"id":"UHAOs-fdEHMO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669695976942,"user_tz":300,"elapsed":770,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"2c0ac430-af75-49ba-93c0-618bc57eb3b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"]}]},{"cell_type":"code","source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []\n","domain_acc_value_lst = []"],"metadata":{"id":"3JGnpmRNHHmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)\n","domain_acc_value_lst.append(domain_acc_value)"],"metadata":{"id":"cHb8ZM-VjG-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"S1HrHo0LEhWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","          \n","          # Rank by LLD\n","          # lld = torch.prod(max_prob, 1)\n","          # LLD.extend(lld)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn]\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","\n","  return new_train_x, new_train_y, remain_train_x, remain_train_y"],"metadata":{"id":"vzWOF5sCGjgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topn = 300\n","i = 0\n","while len(domain_dev_word_lst) >= topn:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","  print(\"domain_dev_word_lst\", len(domain_dev_word_lst))\n","\n","  if i == 1:\n","    domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=False,\n","                                num_workers=1,\n","                                collate_fn=pad)\n","  else:\n","    domain_dev_dataset = PosDataset_new(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=True,\n","                                num_workers=1,\n","                                collate_fn=pad_new)\n","    \n","  initial = True if i==1 else False\n","  new_train_x, new_train_y, domain_dev_word_lst, domain_dev_tag_lst = gen_pseudo_data(model, domain_dev_iter, topn, initial)\n","\n","  new_train_dataset = PosDataset_new(new_train_x, new_train_y)\n","\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad_new)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train_new(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)"],"metadata":{"id":"zl3VMmnVVD-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pAWlrkaMV85R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"g4ZuWMYTojmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"],"metadata":{"id":"jJxcl1cLn7rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"],"metadata":{"id":"6ryA3bq0ot4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"O2DFUhCDorSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install -U kaleido"],"metadata":{"id":"tJvl7lQqqKdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import files\n","# go.Figure.write_image(fig, domain + \"_300.png\")\n","# files.download(domain + \"_300.png\") "],"metadata":{"id":"PLs82qPoc_TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2Ngux9I-orPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uEACZ4DaGGRA"},"execution_count":null,"outputs":[]}]}