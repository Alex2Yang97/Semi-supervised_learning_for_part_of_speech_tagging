{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1667577411660,"user_tz":240,"elapsed":10976,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"80f93b4a-1962-4bdd-aac8-7d8dff6f7a27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 15.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.26.2-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 58.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting botocore<1.30.0,>=1.29.2\n","  Downloading botocore-1.29.2-py3-none-any.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 51.5 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.3 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 62.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.2->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.2->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 72.2 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.2 botocore-1.29.2 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1667577455463,"user_tz":240,"elapsed":43819,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"5ebabf41-bdb3-4503-82cf-96f7521e5e7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os"],"metadata":{"id":"JAp_R4aG3nuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","answer_dir = os.path.join(data_dir, \"pos_fine\", \"answers\")\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","labeled_dir = os.path.join(data_dir, \"unlabeled\")\n","\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"id":"CPKysG3i3nsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import codecs"],"metadata":{"id":"9zR0GZJA3np5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)"],"metadata":{"id":"WKWluD5y3nnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")"],"metadata":{"id":"bOIJvUda3xt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst = []\n","wsj_train_tag_lst = []\n","wsj_tags = []\n","for word, tag in read_conll_file(wsj_train_file):\n","  wsj_train_word_lst.append(word)\n","  wsj_train_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj train\", len(wsj_train_word_lst))\n","\n","wsj_dev_word_lst = []\n","wsj_dev_tag_lst = []\n","for word, tag in read_conll_file(wsj_dev_file):\n","  wsj_dev_word_lst.append(word)\n","  wsj_dev_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj dev\", len(wsj_dev_word_lst))\n","print(\"The number of tags in wsj\", len(set(wsj_tags)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-_9MiDj3xrl","executionInfo":{"status":"ok","timestamp":1667577460260,"user_tz":240,"elapsed":4800,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"d5ff33e0-cef8-4224-880e-c19f1d59f177"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of sentences in wsj train 30060\n","The number of sentences in wsj dev 1336\n","The number of tags in wsj 48\n"]}]},{"cell_type":"code","source":["import random"],"metadata":{"id":"MPyeN0066vBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(0)\n","random.shuffle(wsj_train_word_lst)\n","random.seed(0)\n","random.shuffle(wsj_train_tag_lst)"],"metadata":{"id":"9N8fDl0u72zK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labeled_train_words = wsj_train_word_lst[:10000]\n","labeled_train_tags = wsj_train_tag_lst[:10000]\n","unlabeled_words = wsj_train_word_lst[10000:]\n","unlabeled_tags = wsj_train_tag_lst[10000:]\n","\n","print(len(labeled_train_words))\n","print(len(unlabeled_words))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKLOMMXw6j7U","executionInfo":{"status":"ok","timestamp":1667577460262,"user_tz":240,"elapsed":11,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"7a7ff7de-6371-4755-e8ff-ee304498224b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000\n","20060\n"]}]},{"cell_type":"code","source":["wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1667577460262,"user_tz":240,"elapsed":8,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"1e4e715a-64dd-4a75-b496-7e62aeaab8c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_","executionInfo":{"status":"ok","timestamp":1667577611254,"user_tz":240,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a35fe475-07e6-4e4a-c6fc-3af9e6e11894"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 843955.77B/s]\n"]}]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"accuracy\", acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K","executionInfo":{"status":"ok","timestamp":1667577647855,"user_tz":240,"elapsed":25442,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"56e40da7-4b87-4a50-f250-3650091ffdc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:13<00:00, 29760412.11B/s]\n"]}]},{"cell_type":"code","source":["train_dataset = PosDataset(labeled_train_words, labeled_train_tags)\n","eval_dataset = PosDataset(wsj_dev_word_lst, wsj_dev_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0x3gRfi9iyA","executionInfo":{"status":"ok","timestamp":1667530952317,"user_tz":240,"elapsed":171637,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"36cb20a2-b067-4b8a-844d-03a319f2f05d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 3.9463701248168945\n","step: 10, loss: 2.018392324447632\n","step: 20, loss: 0.7985055446624756\n","step: 30, loss: 0.3254476487636566\n","step: 40, loss: 0.35109612345695496\n","step: 50, loss: 0.324329674243927\n","step: 60, loss: 0.22176219522953033\n","step: 70, loss: 0.15460249781608582\n","step: 80, loss: 0.14656797051429749\n","step: 90, loss: 0.16489265859127045\n","step: 100, loss: 0.1346680372953415\n","step: 110, loss: 0.08465590327978134\n","step: 120, loss: 0.10497339069843292\n","step: 130, loss: 0.10838736593723297\n","step: 140, loss: 0.05528959259390831\n","step: 150, loss: 0.149887353181839\n","step: 160, loss: 0.12817040085792542\n","step: 170, loss: 0.15120059251785278\n","step: 180, loss: 0.0764465257525444\n","step: 190, loss: 0.11801405251026154\n","step: 200, loss: 0.2753466069698334\n","step: 210, loss: 0.15532943606376648\n","step: 220, loss: 0.09293542057275772\n","step: 230, loss: 0.20889462530612946\n","step: 240, loss: 0.10068797320127487\n","step: 250, loss: 0.11133162677288055\n","step: 260, loss: 0.06397204846143723\n","step: 270, loss: 0.1471710205078125\n","step: 280, loss: 0.17567862570285797\n","step: 290, loss: 0.12582702934741974\n","step: 300, loss: 0.04387989640235901\n","step: 310, loss: 0.08508626371622086\n","step: 320, loss: 0.16948840022087097\n","step: 330, loss: 0.1826862096786499\n","step: 340, loss: 0.08693129569292068\n","step: 350, loss: 0.08077458292245865\n","step: 360, loss: 0.20031647384166718\n","step: 370, loss: 0.06989233195781708\n","step: 380, loss: 0.07673043012619019\n","step: 390, loss: 0.09169420599937439\n","step: 400, loss: 0.15129858255386353\n","step: 410, loss: 0.15359793603420258\n","step: 420, loss: 0.12477322667837143\n","step: 430, loss: 0.05913373455405235\n","step: 440, loss: 0.04941558465361595\n","step: 450, loss: 0.06804697960615158\n","step: 460, loss: 0.23915426433086395\n","step: 470, loss: 0.087680883705616\n","step: 480, loss: 0.15910114347934723\n","step: 490, loss: 0.055645715445280075\n","step: 500, loss: 0.08483164757490158\n","step: 510, loss: 0.23748822510242462\n","step: 520, loss: 0.07024213671684265\n","step: 530, loss: 0.07285576313734055\n","step: 540, loss: 0.1401839703321457\n","step: 550, loss: 0.07305583357810974\n","step: 560, loss: 0.11708609014749527\n","step: 570, loss: 0.11152693629264832\n","step: 580, loss: 0.08572288602590561\n","step: 590, loss: 0.08527123183012009\n","step: 600, loss: 0.11207011342048645\n","step: 610, loss: 0.11088380217552185\n","step: 620, loss: 0.1507250964641571\n","step: 630, loss: 0.11987660825252533\n","step: 640, loss: 0.06696980446577072\n","step: 650, loss: 0.05441972240805626\n","step: 660, loss: 0.040285155177116394\n","step: 670, loss: 0.1093902662396431\n","step: 680, loss: 0.17333678901195526\n","step: 690, loss: 0.1162593737244606\n","step: 700, loss: 0.10012947767972946\n","step: 710, loss: 0.11739713698625565\n","step: 720, loss: 0.04626651480793953\n","step: 730, loss: 0.13761110603809357\n","step: 740, loss: 0.07089204341173172\n","step: 750, loss: 0.053261738270521164\n","step: 760, loss: 0.12509897351264954\n","step: 770, loss: 0.1622602343559265\n","step: 780, loss: 0.07879282534122467\n","step: 790, loss: 0.14323019981384277\n","step: 800, loss: 0.12970563769340515\n","step: 810, loss: 0.1377682089805603\n","step: 820, loss: 0.04815518483519554\n","step: 830, loss: 0.07404565811157227\n","step: 840, loss: 0.036612577736377716\n","step: 850, loss: 0.15742330253124237\n","step: 860, loss: 0.07593424618244171\n","step: 870, loss: 0.09183698147535324\n","step: 880, loss: 0.03353463113307953\n","step: 890, loss: 0.055893223732709885\n","step: 900, loss: 0.033632855862379074\n","step: 910, loss: 0.18164601922035217\n","step: 920, loss: 0.05225243419408798\n","step: 930, loss: 0.10587413609027863\n","step: 940, loss: 0.0926731675863266\n","step: 950, loss: 0.09171240031719208\n","step: 960, loss: 0.039945028722286224\n","step: 970, loss: 0.12517954409122467\n","step: 980, loss: 0.05934847146272659\n","step: 990, loss: 0.07798055559396744\n","step: 1000, loss: 0.19395774602890015\n","step: 1010, loss: 0.06719694286584854\n","step: 1020, loss: 0.03666206821799278\n","step: 1030, loss: 0.0581904873251915\n","step: 1040, loss: 0.12915684282779694\n","step: 1050, loss: 0.07867871224880219\n","step: 1060, loss: 0.09085194021463394\n","step: 1070, loss: 0.044612858444452286\n","step: 1080, loss: 0.1372515708208084\n","step: 1090, loss: 0.06533464789390564\n","step: 1100, loss: 0.04667338356375694\n","step: 1110, loss: 0.05642789602279663\n","step: 1120, loss: 0.028041040524840355\n","step: 1130, loss: 0.16413693130016327\n","step: 1140, loss: 0.087201327085495\n","step: 1150, loss: 0.07856663316488266\n","step: 1160, loss: 0.060282882302999496\n","step: 1170, loss: 0.08096848428249359\n","step: 1180, loss: 0.1303519904613495\n","step: 1190, loss: 0.07748404145240784\n","step: 1200, loss: 0.06130843982100487\n","step: 1210, loss: 0.0825730636715889\n","step: 1220, loss: 0.07486564666032791\n","step: 1230, loss: 0.06044561415910721\n","step: 1240, loss: 0.19164125621318817\n","accuracy 0.969618596534962\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.99      1.00      1.00       125\n","           2       0.99      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.67      1.00      0.80        53\n","           5       1.00      0.42      0.59        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.96      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       1.00      1.00      1.00      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.96      0.98       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.92      0.91      0.92      1819\n","          17       0.88      0.93      0.90       114\n","          18       0.99      0.96      0.97        76\n","          19       0.40      0.33      0.36         6\n","          20       1.00      1.00      1.00       271\n","          22       0.96      0.98      0.97      4732\n","          23       0.96      0.99      0.97      3086\n","          24       0.04      0.03      0.04        29\n","          25       0.99      0.98      0.99      2049\n","          26       0.07      0.17      0.10         6\n","          27       0.96      0.99      0.98       328\n","          28       0.99      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.89      0.92      1126\n","          31       0.84      0.76      0.80        76\n","          32       0.89      0.94      0.92        18\n","          33       0.74      0.81      0.77       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.95      0.97       418\n","          36       1.00      0.60      0.75         5\n","          37       0.99      0.94      0.97       805\n","          38       0.98      0.96      0.97      1309\n","          39       0.96      0.86      0.91       490\n","          40       0.92      0.90      0.91       706\n","          41       0.92      0.98      0.95       301\n","          42       0.99      0.99      0.99       590\n","          43       0.95      0.95      0.95       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.89      0.87      0.87     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"model.pt\")\n","# torch.save(model.state_dict(), model_file)"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","\n","model.load_state_dict(torch.load(model_file))\n","eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAD3Wd574v6Q","executionInfo":{"status":"ok","timestamp":1667577682084,"user_tz":240,"elapsed":19789,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"e0c339dd-7c83-46d5-cbbe-d4a3cb37bdb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy 0.969618596534962\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.99      1.00      1.00       125\n","           2       0.99      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.67      1.00      0.80        53\n","           5       1.00      0.42      0.59        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.96      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       1.00      1.00      1.00      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.96      0.98       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.92      0.91      0.92      1819\n","          17       0.88      0.93      0.90       114\n","          18       0.99      0.96      0.97        76\n","          19       0.40      0.33      0.36         6\n","          20       1.00      1.00      1.00       271\n","          22       0.96      0.98      0.97      4732\n","          23       0.96      0.99      0.97      3086\n","          24       0.04      0.03      0.04        29\n","          25       0.99      0.98      0.99      2049\n","          26       0.07      0.17      0.10         6\n","          27       0.96      0.99      0.98       328\n","          28       0.99      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.89      0.92      1126\n","          31       0.84      0.76      0.80        76\n","          32       0.89      0.94      0.92        18\n","          33       0.74      0.81      0.77       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.95      0.97       418\n","          36       1.00      0.60      0.75         5\n","          37       0.99      0.94      0.97       805\n","          38       0.98      0.96      0.97      1309\n","          39       0.96      0.86      0.91       490\n","          40       0.92      0.90      0.91       706\n","          41       0.92      0.98      0.95       301\n","          42       0.99      0.99      0.99       590\n","          43       0.95      0.95      0.95       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.89      0.87      0.87     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["unlabeled_dataset = PosDataset(unlabeled_words, unlabeled_tags)\n","\n","unlabeled_iter = data.DataLoader(dataset=unlabeled_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)"],"metadata":{"id":"DYCxNf1k5YFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","LLD = []\n","new_x_lst = []\n","new_y_lst = []\n","i = 0\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(unlabeled_iter):\n","\n","      words, x, is_heads, tags, y, seqlens = batch\n","\n","      logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","      # Save prediction as new training dataset\n","      softmax_value = torch.softmax(logits, dim=2)\n","      max_prob = torch.amax(softmax_value, dim=2)\n","      lld = torch.prod(max_prob, 1)\n","      LLD.extend(lld)\n","\n","      new_x_lst.extend(x.tolist())\n","      new_y_lst.extend(y_hat.tolist())\n","\n","      Words.extend(words)\n","      Is_heads.extend(is_heads)\n","      Tags.extend(tags)\n","      Y.extend(y.numpy().tolist())\n","      Y_hat.extend(y_hat.cpu().numpy().tolist())"],"metadata":{"id":"Tq8DyhZz9Msv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(Words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEFXDp5MZWVh","executionInfo":{"status":"ok","timestamp":1667578033523,"user_tz":240,"elapsed":447,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"ad1e18e9-877a-484e-ff1b-998bd169f7b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20060"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["ind = list(range(len(LLD)))\n","ind = [x for _, x in sorted(zip(LLD, ind), reverse=True)]\n","\n","select_ind = ind[: 2000]\n","not_select_ind = ind[2000:]\n","\n","new_train_x = [new_x_lst[i] for i in select_ind]\n","new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","remain_train_y = [new_y_lst[i] for i in not_select_ind]"],"metadata":{"id":"kYeLIaD6M1sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_sentences = [Words[s] for s in select_ind]"],"metadata":{"id":"loetGPC3Zplk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_sentences_set = set(top_sentences)"],"metadata":{"id":"72ywM4gzblxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(top_sentences_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuNtOCaLbobv","executionInfo":{"status":"ok","timestamp":1667578632498,"user_tz":240,"elapsed":2,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"d75acf13-87ec-4d96-94ab-749ce85d83aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens"],"metadata":{"id":"tazNmMoPgAEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_train_dataset = PosDataset_new(new_train_x, new_train_y)\n","\n","new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad_new)"],"metadata":{"id":"aD4gN8kR7qC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"BUErqWSi7qAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"6PJrl9qR7p9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_new(model, new_train_iter, optimizer, criterion)\n","eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lBuUT9P7p7D","executionInfo":{"status":"ok","timestamp":1667578572694,"user_tz":240,"elapsed":40491,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"ceb00e89-f79a-46c5-a5c3-83d0a030a171"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.13175822794437408\n","step: 10, loss: 0.06500213593244553\n","step: 20, loss: 0.04235837236046791\n","step: 30, loss: 0.06930667906999588\n","step: 40, loss: 0.049075935035943985\n","step: 50, loss: 0.05700163543224335\n","step: 60, loss: 0.03679328411817551\n","step: 70, loss: 0.06033544987440109\n","step: 80, loss: 0.054879385977983475\n","step: 90, loss: 0.05069451034069061\n","step: 100, loss: 0.06683169305324554\n","step: 110, loss: 0.07210827618837357\n","step: 120, loss: 0.09831839799880981\n","step: 130, loss: 0.06754110008478165\n","step: 140, loss: 0.09271911531686783\n","step: 150, loss: 0.050322026014328\n","step: 160, loss: 0.0321730300784111\n","step: 170, loss: 0.07775381952524185\n","step: 180, loss: 0.046156466007232666\n","step: 190, loss: 0.07832826673984528\n","step: 200, loss: 0.09453294426202774\n","step: 210, loss: 0.06591164320707321\n","step: 220, loss: 0.07453957945108414\n","step: 230, loss: 0.08637044578790665\n","step: 240, loss: 0.0495699904859066\n","accuracy 0.9687461049482737\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.78      1.00      0.88        53\n","           5       1.00      0.98      0.99        55\n","           6       0.99      1.00      1.00      1321\n","           7       0.97      0.97      0.97       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      0.99      0.99      1270\n","          11       1.00      0.99      1.00      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.94      0.97       409\n","          15       0.96      0.99      0.98      3419\n","          16       0.92      0.92      0.92      1819\n","          17       0.89      0.96      0.92       114\n","          18       0.97      0.92      0.95        76\n","          19       0.00      0.00      0.00         6\n","          20       1.00      1.00      1.00       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.97      0.98      0.97      3086\n","          24       0.13      0.17      0.15        29\n","          25       0.99      0.98      0.99      2049\n","          26       0.18      0.67      0.29         6\n","          27       0.97      0.98      0.98       328\n","          28       1.00      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.89      0.91      1126\n","          31       0.90      0.80      0.85        76\n","          32       0.77      0.94      0.85        18\n","          33       0.65      0.83      0.73       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.94      0.96       418\n","          36       1.00      0.40      0.57         5\n","          37       0.99      0.92      0.95       805\n","          38       0.98      0.95      0.97      1309\n","          39       0.88      0.90      0.89       490\n","          40       0.90      0.91      0.90       706\n","          41       0.88      0.97      0.93       301\n","          42       0.97      0.99      0.98       590\n","          43       0.98      0.94      0.96       154\n","          44       0.99      1.00      0.99        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.88      0.88      0.87     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["model.eval()\n","\n","Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","LLD = []\n","new_x_lst = []\n","new_y_lst = []\n","i = 0\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(unlabeled_iter):\n","\n","      words, x, is_heads, tags, y, seqlens = batch\n","\n","      logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","      # Save prediction as new training dataset\n","      softmax_value = torch.softmax(logits, dim=2)\n","      max_prob = torch.amax(softmax_value, dim=2)\n","      lld = torch.prod(max_prob, 1)\n","      LLD.extend(lld)\n","\n","      new_x_lst.extend(x.tolist())\n","      new_y_lst.extend(y_hat.tolist())\n","\n","      Words.extend(words)\n","      Is_heads.extend(is_heads)\n","      Tags.extend(tags)\n","      Y.extend(y.numpy().tolist())\n","      Y_hat.extend(y_hat.cpu().numpy().tolist())"],"metadata":{"id":"A1Zj97KxbTMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind = list(range(len(LLD)))\n","ind = [x for _, x in sorted(zip(LLD, ind), reverse=True)]\n","\n","select_ind = ind[: 2000]\n","not_select_ind = ind[2000:]\n","\n","new_train_x = [new_x_lst[i] for i in select_ind]\n","new_train_y = [new_y_lst[i] for i in select_ind]"],"metadata":{"id":"gxPe9WeGbTJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_top_sentences = [Words[s] for s in select_ind]\n","new_top_sentences_set = set(new_top_sentences)"],"metadata":{"id":"gtL_sKIUbtyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# new_top_sentences - top_sentences\n","new_diff = new_top_sentences_set.difference(top_sentences_set)\n","# top_sentences - new_top_sentences\n","old_diff = top_sentences_set.difference(new_top_sentences_set)"],"metadata":{"id":"xqfkFTxFbtt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(sentences_diff)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcb_5yJSbTGy","executionInfo":{"status":"ok","timestamp":1667578816965,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"80b18688-ef33-4add-dad0-6edb5b701fda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1739"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":[],"metadata":{"id":"nqjIRPGrbMh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loop_i = 0\n","while len(new_top_sentences_set.difference(top_sentences_set)) <= 200:\n","  loop_i += 1\n","  print(\"\\nLoop\", loop_i)\n","  print(\"Remain dataset\", len(remain_train_x))\n","\n","  # Generate dataset \n","  new_unlabeled_dataset = PosDataset_new(remain_train_x, remain_train_y)\n","\n","  new_unlabeled_iter = data.DataLoader(dataset=new_unlabeled_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad_new)\n","\n","  # Prediction\n","  model.eval()\n","\n","  LLD = []\n","  new_x_lst = []\n","  new_y_lst = []\n","  i = 0\n","\n","  with torch.no_grad():\n","      for i, batch in enumerate(new_unlabeled_iter):\n","\n","        x, y, seqlens = batch\n","\n","        logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","        # Save prediction as new training dataset\n","        softmax_value = torch.softmax(logits, dim=2)\n","        max_prob = torch.amax(softmax_value, dim=2)\n","        lld = torch.prod(max_prob, 1)\n","        LLD.extend(lld)\n","\n","        new_x_lst.extend(x.tolist())\n","        new_y_lst.extend(y_hat.tolist())\n","\n","  ind = list(range(len(LLD)))\n","  ind = [x for _, x in sorted(zip(LLD, ind), reverse=True)]\n","  select_ind = ind[: 2000]\n","  not_select_ind = ind[2000:]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","  # train model\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train_new(model, new_train_iter, optimizer, criterion)\n","\n","\n","  eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9sQ8lGr7p4a","executionInfo":{"status":"ok","timestamp":1667532531264,"user_tz":240,"elapsed":1239564,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"c7ad12a2-f91b-480c-d68f-5e26fc9c1868"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loop 1\n","Remain dataset 18060\n","step: 0, loss: 0.03387511149048805\n","step: 10, loss: 0.08041409403085709\n","step: 20, loss: 0.07576928287744522\n","step: 30, loss: 0.03719585761427879\n","step: 40, loss: 0.03793807327747345\n","step: 50, loss: 0.05613357201218605\n","step: 60, loss: 0.0617285817861557\n","step: 70, loss: 0.09802041202783585\n","step: 80, loss: 0.047864172607660294\n","step: 90, loss: 0.09668195992708206\n","step: 100, loss: 0.0840291678905487\n","step: 110, loss: 0.04634398967027664\n","step: 120, loss: 0.03393184393644333\n","step: 130, loss: 0.041940852999687195\n","step: 140, loss: 0.02925848588347435\n","step: 150, loss: 0.06991099566221237\n","step: 160, loss: 0.043631669133901596\n","step: 170, loss: 0.05638584494590759\n","step: 180, loss: 0.05049470439553261\n","step: 190, loss: 0.05955478176474571\n","step: 200, loss: 0.06083463132381439\n","step: 210, loss: 0.0639224424958229\n","step: 220, loss: 0.03133399039506912\n","step: 230, loss: 0.04272076115012169\n","step: 240, loss: 0.08042771369218826\n","accuracy 0.9664402343263119\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.99      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.49      1.00      0.65        53\n","           5       1.00      0.02      0.04        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.94      0.99      0.97       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      1.00      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.97      0.98       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.88      0.94      0.91      1819\n","          17       0.83      0.95      0.89       114\n","          18       0.99      0.97      0.98        76\n","          19       0.00      0.00      0.00         6\n","          20       0.97      1.00      0.98       271\n","          22       0.98      0.96      0.97      4732\n","          23       0.96      0.99      0.97      3086\n","          24       0.08      0.07      0.07        29\n","          25       0.99      0.97      0.98      2049\n","          26       0.17      0.67      0.28         6\n","          27       0.96      1.00      0.98       328\n","          28       1.00      0.99      1.00       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.87      0.90      1126\n","          31       0.78      0.71      0.74        76\n","          32       0.94      0.94      0.94        18\n","          33       0.64      0.92      0.76       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.94      0.97       418\n","          36       0.80      0.80      0.80         5\n","          37       0.98      0.95      0.96       805\n","          38       0.98      0.96      0.97      1309\n","          39       0.94      0.87      0.90       490\n","          40       0.92      0.88      0.90       706\n","          41       0.94      0.98      0.96       301\n","          42       1.00      0.97      0.98       590\n","          43       0.95      0.95      0.95       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.87      0.87      0.85     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 2\n","Remain dataset 16060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.033802710473537445\n","step: 10, loss: 0.03264637291431427\n","step: 20, loss: 0.025558797642588615\n","step: 30, loss: 0.03916459158062935\n","step: 40, loss: 0.029730170965194702\n","step: 50, loss: 0.028933893889188766\n","step: 60, loss: 0.06623674184083939\n","step: 70, loss: 0.02319684997200966\n","step: 80, loss: 0.03547313064336777\n","step: 90, loss: 0.05208820477128029\n","step: 100, loss: 0.014772629365324974\n","step: 110, loss: 0.025128137320280075\n","step: 120, loss: 0.050528813153505325\n","step: 130, loss: 0.031239036470651627\n","step: 140, loss: 0.04336753115057945\n","step: 150, loss: 0.0375884473323822\n","step: 160, loss: 0.042078934609889984\n","step: 170, loss: 0.06188686564564705\n","step: 180, loss: 0.03733464330434799\n","step: 190, loss: 0.05363009124994278\n","step: 200, loss: 0.013346710242331028\n","step: 210, loss: 0.01733534038066864\n","step: 220, loss: 0.027978630736470222\n","step: 230, loss: 0.04664081335067749\n","step: 240, loss: 0.09549206495285034\n","accuracy 0.9689019070173251\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.80      1.00      0.89        53\n","           5       1.00      0.73      0.84        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.99      0.99      0.99       187\n","           9       1.00      0.99      0.99       809\n","          10       1.00      1.00      1.00      1270\n","          11       0.99      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.99      0.99       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.89      0.92      0.91      1819\n","          17       0.80      0.94      0.86       114\n","          18       0.99      0.97      0.98        76\n","          19       0.50      0.50      0.50         6\n","          20       0.99      1.00      0.99       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.96      0.99      0.97      3086\n","          24       0.00      0.00      0.00        29\n","          25       0.99      0.99      0.99      2049\n","          26       0.12      0.33      0.17         6\n","          27       0.96      1.00      0.98       328\n","          28       1.00      0.99      1.00       514\n","          29       1.00      1.00      1.00       215\n","          30       0.93      0.89      0.91      1126\n","          31       0.84      0.62      0.71        76\n","          32       0.94      0.94      0.94        18\n","          33       0.72      0.81      0.76       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.95      0.97       418\n","          36       1.00      0.60      0.75         5\n","          37       0.98      0.95      0.97       805\n","          38       0.97      0.96      0.97      1309\n","          39       0.94      0.85      0.89       490\n","          40       0.93      0.86      0.89       706\n","          41       0.94      0.96      0.95       301\n","          42       1.00      0.99      0.99       590\n","          43       0.96      0.99      0.97       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      0.88      0.93         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.89      0.88      0.88     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 3\n","Remain dataset 14060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.07083073258399963\n","step: 10, loss: 0.012032496742904186\n","step: 20, loss: 0.025874201208353043\n","step: 30, loss: 0.009324867278337479\n","step: 40, loss: 0.012447481974959373\n","step: 50, loss: 0.020384514704346657\n","step: 60, loss: 0.045017410069704056\n","step: 70, loss: 0.05141768977046013\n","step: 80, loss: 0.05812250077724457\n","step: 90, loss: 0.04314720630645752\n","step: 100, loss: 0.01929996721446514\n","step: 110, loss: 0.021622737869620323\n","step: 120, loss: 0.03527675196528435\n","step: 130, loss: 0.03984002768993378\n","step: 140, loss: 0.031749576330184937\n","step: 150, loss: 0.03320395573973656\n","step: 160, loss: 0.007386897224932909\n","step: 170, loss: 0.02778368815779686\n","step: 180, loss: 0.04370421543717384\n","step: 190, loss: 0.14823903143405914\n","step: 200, loss: 0.03785046562552452\n","step: 210, loss: 0.019526682794094086\n","step: 220, loss: 0.022905811667442322\n","step: 230, loss: 0.05178769305348396\n","step: 240, loss: 0.05423211678862572\n","accuracy 0.9670322821887075\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       0.99      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.54      1.00      0.70        53\n","           5       1.00      0.20      0.33        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.97      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      0.99      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.98      0.98      0.98       409\n","          15       0.96      0.99      0.98      3419\n","          16       0.90      0.93      0.92      1819\n","          17       0.88      0.84      0.86       114\n","          18       0.99      0.95      0.97        76\n","          19       0.00      0.00      0.00         6\n","          20       1.00      1.00      1.00       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.97      0.98      0.97      3086\n","          24       0.12      0.14      0.13        29\n","          25       0.99      0.98      0.99      2049\n","          26       0.14      0.50      0.22         6\n","          27       0.96      0.99      0.98       328\n","          28       1.00      0.99      1.00       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.87      0.90      1126\n","          31       0.73      0.82      0.77        76\n","          32       0.85      0.94      0.89        18\n","          33       0.68      0.79      0.73       121\n","          34       1.00      0.55      0.71        11\n","          35       0.98      0.95      0.97       418\n","          36       0.50      0.20      0.29         5\n","          37       0.98      0.93      0.96       805\n","          38       0.98      0.95      0.97      1309\n","          39       0.94      0.88      0.91       490\n","          40       0.91      0.90      0.90       706\n","          41       0.89      0.97      0.93       301\n","          42       0.99      0.99      0.99       590\n","          43       0.97      0.94      0.96       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.86      0.85      0.84     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 4\n","Remain dataset 12060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.02108386717736721\n","step: 10, loss: 0.050651915371418\n","step: 20, loss: 0.007521605119109154\n","step: 30, loss: 0.016523627564311028\n","step: 40, loss: 0.013251928612589836\n","step: 50, loss: 0.005366346798837185\n","step: 60, loss: 0.005943673197180033\n","step: 70, loss: 0.02088145725429058\n","step: 80, loss: 0.03494458645582199\n","step: 90, loss: 0.034228838980197906\n","step: 100, loss: 0.01206861063838005\n","step: 110, loss: 0.007750891614705324\n","step: 120, loss: 0.007328256499022245\n","step: 130, loss: 0.03158141300082207\n","step: 140, loss: 0.010884732007980347\n","step: 150, loss: 0.051326919347047806\n","step: 160, loss: 0.008128147572278976\n","step: 170, loss: 0.0051239244639873505\n","step: 180, loss: 0.05718662217259407\n","step: 190, loss: 0.03377663716673851\n","step: 200, loss: 0.017172183841466904\n","step: 210, loss: 0.02524889074265957\n","step: 220, loss: 0.021683650091290474\n","step: 230, loss: 0.06200620159506798\n","step: 240, loss: 0.03088442049920559\n","accuracy 0.9672192446715693\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.99      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.55      1.00      0.71        53\n","           5       1.00      0.25      0.41        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.97      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      1.00      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       1.00      0.98      0.99       409\n","          15       0.97      0.98      0.98      3419\n","          16       0.89      0.94      0.91      1819\n","          17       0.90      0.90      0.90       114\n","          18       0.96      0.96      0.96        76\n","          19       0.00      0.00      0.00         6\n","          20       0.99      1.00      0.99       271\n","          22       0.98      0.96      0.97      4732\n","          23       0.96      0.98      0.97      3086\n","          24       0.00      0.00      0.00        29\n","          25       0.99      0.98      0.98      2049\n","          26       0.06      0.17      0.09         6\n","          27       0.96      1.00      0.98       328\n","          28       1.00      0.99      1.00       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.88      0.91      1126\n","          31       0.81      0.78      0.79        76\n","          32       0.89      0.89      0.89        18\n","          33       0.69      0.79      0.74       121\n","          34       1.00      1.00      1.00        11\n","          35       0.93      0.98      0.96       418\n","          36       0.60      0.60      0.60         5\n","          37       0.96      0.96      0.96       805\n","          38       0.97      0.97      0.97      1309\n","          39       0.95      0.86      0.91       490\n","          40       0.93      0.87      0.90       706\n","          41       0.92      0.97      0.94       301\n","          42       0.99      0.99      0.99       590\n","          43       0.95      0.99      0.97       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.86      0.86      0.85     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 5\n","Remain dataset 10060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.025636395439505577\n","step: 10, loss: 0.045986395329236984\n","step: 20, loss: 0.00938976276665926\n","step: 30, loss: 0.022229839116334915\n","step: 40, loss: 0.026090459898114204\n","step: 50, loss: 0.03520989418029785\n","step: 60, loss: 0.01663173921406269\n","step: 70, loss: 0.018812404945492744\n","step: 80, loss: 0.013605202548205853\n","step: 90, loss: 0.006451743189245462\n","step: 100, loss: 0.02026980370283127\n","step: 110, loss: 0.01470857858657837\n","step: 120, loss: 0.0156256016343832\n","step: 130, loss: 0.022077707573771477\n","step: 140, loss: 0.032479990273714066\n","step: 150, loss: 0.03268139064311981\n","step: 160, loss: 0.0027504833415150642\n","step: 170, loss: 0.019885551184415817\n","step: 180, loss: 0.020512402057647705\n","step: 190, loss: 0.00865151546895504\n","step: 200, loss: 0.009515722282230854\n","step: 210, loss: 0.01685095950961113\n","step: 220, loss: 0.011765964329242706\n","step: 230, loss: 0.0040495204739272594\n","step: 240, loss: 0.05209420621395111\n","accuracy 0.9680605758444473\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.57      1.00      0.73        53\n","           5       1.00      0.27      0.43        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.99      0.99      0.99       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      0.99      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       1.00      1.00      1.00        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.99      0.99       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.91      0.93      0.92      1819\n","          17       0.87      0.91      0.89       114\n","          18       0.99      0.97      0.98        76\n","          19       0.00      0.00      0.00         6\n","          20       0.99      1.00      0.99       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.95      0.99      0.97      3086\n","          24       0.00      0.00      0.00        29\n","          25       0.99      0.98      0.99      2049\n","          26       0.00      0.00      0.00         6\n","          27       0.96      1.00      0.98       328\n","          28       1.00      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.95      0.86      0.90      1126\n","          31       0.81      0.74      0.77        76\n","          32       0.94      0.94      0.94        18\n","          33       0.65      0.83      0.73       121\n","          34       1.00      1.00      1.00        11\n","          35       0.97      0.98      0.97       418\n","          36       0.75      0.60      0.67         5\n","          37       0.98      0.93      0.95       805\n","          38       0.97      0.97      0.97      1309\n","          39       0.91      0.88      0.90       490\n","          40       0.92      0.90      0.91       706\n","          41       0.89      0.97      0.93       301\n","          42       0.99      0.99      0.99       590\n","          43       0.98      0.92      0.95       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.86      0.85      0.85     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 6\n","Remain dataset 8060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.008811681531369686\n","step: 10, loss: 0.004967300686985254\n","step: 20, loss: 0.010247076861560345\n","step: 30, loss: 0.02494324930012226\n","step: 40, loss: 0.01871597208082676\n","step: 50, loss: 0.014718884602189064\n","step: 60, loss: 0.013306375592947006\n","step: 70, loss: 0.04965516924858093\n","step: 80, loss: 0.018658939749002457\n","step: 90, loss: 0.022405246272683144\n","step: 100, loss: 0.009587600827217102\n","step: 110, loss: 0.02225385792553425\n","step: 120, loss: 0.01887815073132515\n","step: 130, loss: 0.01211230643093586\n","step: 140, loss: 0.017995372414588928\n","step: 150, loss: 0.0040978300385177135\n","step: 160, loss: 0.0031719361431896687\n","step: 170, loss: 0.011597157455980778\n","step: 180, loss: 0.008554761298000813\n","step: 190, loss: 0.05121753364801407\n","step: 200, loss: 0.024423623457551003\n","step: 210, loss: 0.02632645145058632\n","step: 220, loss: 0.057850975543260574\n","step: 230, loss: 0.0009608551627025008\n","step: 240, loss: 0.019125912338495255\n","accuracy 0.9665337155677427\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.50      1.00      0.66        53\n","           5       1.00      0.11      0.20        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.96      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       1.00      0.99      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       0.97      1.00      0.98        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.97      0.98       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.93      0.91      0.92      1819\n","          17       0.88      0.94      0.91       114\n","          18       0.99      0.93      0.96        76\n","          19       0.00      0.00      0.00         6\n","          20       0.99      1.00      0.99       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.94      0.99      0.97      3086\n","          24       0.00      0.00      0.00        29\n","          25       0.99      0.98      0.98      2049\n","          26       0.06      0.17      0.09         6\n","          27       0.96      1.00      0.98       328\n","          28       0.99      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.88      0.91      1126\n","          31       0.86      0.74      0.79        76\n","          32       0.81      0.94      0.87        18\n","          33       0.60      0.83      0.70       121\n","          34       1.00      0.91      0.95        11\n","          35       0.99      0.96      0.97       418\n","          36       1.00      0.60      0.75         5\n","          37       0.99      0.92      0.95       805\n","          38       0.98      0.95      0.96      1309\n","          39       0.94      0.87      0.90       490\n","          40       0.90      0.92      0.91       706\n","          41       0.91      0.95      0.93       301\n","          42       1.00      0.99      0.99       590\n","          43       0.95      0.97      0.96       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.86      0.85      0.85     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 7\n","Remain dataset 6060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.005595264956355095\n","step: 10, loss: 0.009686592034995556\n","step: 20, loss: 0.017664678394794464\n","step: 30, loss: 0.029816340655088425\n","step: 40, loss: 0.028868388384580612\n","step: 50, loss: 0.007976054213941097\n","step: 60, loss: 0.01799927093088627\n","step: 70, loss: 0.0021552378311753273\n","step: 80, loss: 0.06063456833362579\n","step: 90, loss: 0.009784297086298466\n","step: 100, loss: 0.016946442425251007\n","step: 110, loss: 0.03092639520764351\n","step: 120, loss: 0.019843896850943565\n","step: 130, loss: 0.02744329534471035\n","step: 140, loss: 0.012680510058999062\n","step: 150, loss: 0.006045272573828697\n","step: 160, loss: 0.020579975098371506\n","step: 170, loss: 0.01112629845738411\n","step: 180, loss: 0.03582637384533882\n","step: 190, loss: 0.053486377000808716\n","step: 200, loss: 0.017145372927188873\n","step: 210, loss: 0.024113325402140617\n","step: 220, loss: 0.0051592434756457806\n","step: 230, loss: 0.005388400051742792\n","step: 240, loss: 0.004426217172294855\n","accuracy 0.967717811292534\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       125\n","           2       0.99      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.77      1.00      0.87        53\n","           5       1.00      0.73      0.84        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.96      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      1.00      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       0.97      1.00      0.98        29\n","          13       0.00      0.00      0.00         2\n","          14       1.00      0.98      0.99       409\n","          15       0.97      0.99      0.98      3419\n","          16       0.91      0.92      0.92      1819\n","          17       0.86      0.94      0.90       114\n","          18       0.97      0.96      0.97        76\n","          19       0.00      0.00      0.00         6\n","          20       0.98      1.00      0.99       271\n","          22       0.97      0.96      0.97      4732\n","          23       0.95      0.99      0.97      3086\n","          24       0.05      0.03      0.04        29\n","          25       0.99      0.97      0.98      2049\n","          26       0.06      0.17      0.09         6\n","          27       0.96      0.99      0.98       328\n","          28       0.99      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.88      0.91      1126\n","          31       0.86      0.71      0.78        76\n","          32       0.89      0.89      0.89        18\n","          33       0.75      0.78      0.76       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.98      0.98       418\n","          36       1.00      0.40      0.57         5\n","          37       0.98      0.93      0.95       805\n","          38       0.98      0.95      0.97      1309\n","          39       0.91      0.90      0.91       490\n","          40       0.90      0.90      0.90       706\n","          41       0.91      0.95      0.93       301\n","          42       0.99      0.97      0.98       590\n","          43       0.97      0.97      0.97       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.88      0.86      0.86     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 8\n","Remain dataset 4060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.012664043344557285\n","step: 10, loss: 0.008763986639678478\n","step: 20, loss: 0.015848470851778984\n","step: 30, loss: 0.0011201751185581088\n","step: 40, loss: 0.008707894943654537\n","step: 50, loss: 0.011126556433737278\n","step: 60, loss: 0.008696245029568672\n","step: 70, loss: 0.007858016528189182\n","step: 80, loss: 0.020427048206329346\n","step: 90, loss: 0.01654486544430256\n","step: 100, loss: 0.0008282059570774436\n","step: 110, loss: 0.048812828958034515\n","step: 120, loss: 0.009565841406583786\n","step: 130, loss: 0.015809889882802963\n","step: 140, loss: 0.014786138199269772\n","step: 150, loss: 0.002903173677623272\n","step: 160, loss: 0.03439748287200928\n","step: 170, loss: 0.02109651267528534\n","step: 180, loss: 0.00883923564106226\n","step: 190, loss: 0.015442810952663422\n","step: 200, loss: 0.007040585856884718\n","step: 210, loss: 0.02165823243558407\n","step: 220, loss: 0.004981494974344969\n","step: 230, loss: 0.012980828061699867\n","step: 240, loss: 0.008891004137694836\n","accuracy 0.966876480119656\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      0.98      0.99       125\n","           2       1.00      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.58      1.00      0.74        53\n","           5       1.00      0.35      0.51        55\n","           6       1.00      1.00      1.00      1321\n","           7       0.96      0.99      0.98       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      1.00      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       0.97      1.00      0.98        29\n","          13       0.00      0.00      0.00         2\n","          14       1.00      0.98      0.99       409\n","          15       0.96      0.99      0.97      3419\n","          16       0.92      0.91      0.92      1819\n","          17       0.93      0.93      0.93       114\n","          18       0.99      0.96      0.97        76\n","          19       0.00      0.00      0.00         6\n","          20       0.99      1.00      0.99       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.95      0.98      0.97      3086\n","          24       0.07      0.07      0.07        29\n","          25       1.00      0.97      0.98      2049\n","          26       0.00      0.00      0.00         6\n","          27       0.96      1.00      0.98       328\n","          28       1.00      0.99      1.00       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.88      0.91      1126\n","          31       0.86      0.82      0.84        76\n","          32       0.94      0.94      0.94        18\n","          33       0.66      0.83      0.74       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.92      0.95       418\n","          36       1.00      0.40      0.57         5\n","          37       0.98      0.93      0.95       805\n","          38       0.97      0.97      0.97      1309\n","          39       0.94      0.88      0.91       490\n","          40       0.92      0.91      0.91       706\n","          41       0.93      0.94      0.94       301\n","          42       0.97      0.99      0.98       590\n","          43       0.98      0.94      0.96       154\n","          44       0.99      1.00      0.99        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.87      0.85      0.86     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n","\n","Loop 9\n","Remain dataset 2060\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["step: 0, loss: 0.007702747825533152\n","step: 10, loss: 0.02734122984111309\n","step: 20, loss: 0.002830310259014368\n","step: 30, loss: 0.00601162388920784\n","step: 40, loss: 0.03372558206319809\n","step: 50, loss: 0.032372117042541504\n","step: 60, loss: 0.019446486607193947\n","step: 70, loss: 0.017916223034262657\n","step: 80, loss: 0.02014981210231781\n","step: 90, loss: 0.017721671611070633\n","step: 100, loss: 0.024141157045960426\n","step: 110, loss: 0.015087915584445\n","step: 120, loss: 0.018708279356360435\n","step: 130, loss: 0.003214342286810279\n","step: 140, loss: 0.008994173258543015\n","step: 150, loss: 0.015451240353286266\n","step: 160, loss: 0.00577939348295331\n","step: 170, loss: 0.009637003764510155\n","step: 180, loss: 0.0058058626018464565\n","step: 190, loss: 0.027805214747786522\n","step: 200, loss: 0.008106912486255169\n","step: 210, loss: 0.02094203047454357\n","step: 220, loss: 0.005504540167748928\n","step: 230, loss: 0.03242998570203781\n","step: 240, loss: 0.040669117122888565\n","accuracy 0.9676554904649134\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.98      1.00      0.99       125\n","           2       0.99      1.00      1.00       236\n","           3       1.00      1.00      1.00      1629\n","           4       0.56      1.00      0.72        53\n","           5       1.00      0.31      0.47        55\n","           6       0.99      1.00      0.99      1321\n","           7       0.97      0.92      0.95       187\n","           9       1.00      0.99      0.99       809\n","          10       0.99      1.00      0.99      1270\n","          11       1.00      0.99      0.99      2768\n","          12       0.97      0.97      0.97        29\n","          13       0.00      0.00      0.00         2\n","          14       0.99      0.97      0.98       409\n","          15       0.96      0.99      0.97      3419\n","          16       0.91      0.93      0.92      1819\n","          17       0.95      0.85      0.90       114\n","          18       0.99      0.97      0.98        76\n","          19       0.00      0.00      0.00         6\n","          20       0.99      1.00      0.99       271\n","          22       0.97      0.97      0.97      4732\n","          23       0.96      0.98      0.97      3086\n","          24       0.12      0.10      0.11        29\n","          25       0.99      0.98      0.98      2049\n","          26       0.06      0.17      0.09         6\n","          27       0.96      1.00      0.98       328\n","          28       0.99      0.99      0.99       514\n","          29       1.00      1.00      1.00       215\n","          30       0.94      0.88      0.91      1126\n","          31       0.77      0.87      0.81        76\n","          32       0.94      0.94      0.94        18\n","          33       0.74      0.77      0.76       121\n","          34       1.00      1.00      1.00        11\n","          35       0.99      0.94      0.96       418\n","          36       0.75      0.60      0.67         5\n","          37       0.98      0.94      0.96       805\n","          38       0.97      0.97      0.97      1309\n","          39       0.96      0.86      0.91       490\n","          40       0.92      0.90      0.91       706\n","          41       0.92      0.96      0.94       301\n","          42       1.00      0.99      0.99       590\n","          43       0.96      0.93      0.94       154\n","          44       1.00      1.00      1.00        76\n","          45       1.00      1.00      1.00         8\n","          46       1.00      1.00      1.00        78\n","          48       1.00      1.00      1.00       243\n","\n","    accuracy                           0.97     32092\n","   macro avg       0.87      0.86      0.86     32092\n","weighted avg       0.97      0.97      0.97     32092\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["# acc_lst = [\n","#     0.973106892370215,\n","#     0.9689019070173251,\n","#     0.9664402343263119,\n","#     0.9689019070173251,\n","#     0.9670322821887075,\n","#     0.9672192446715693,\n","#     0.9680605758444473,\n","#     0.9665337155677427,\n","#     0.967717811292534,\n","#     0.966876480119656,\n","#     0.9676554904649134,\n","#     ]"],"metadata":{"id":"CMgRZX7AFe0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.scatterplot(x=list(range(len(acc_lst))), y=acc_lst)\n","sns.lineplot(x=list(range(len(acc_lst))), y=acc_lst)"],"metadata":{"id":"2DEMIx2r3xf2","executionInfo":{"status":"ok","timestamp":1667573062079,"user_tz":240,"elapsed":338,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/","height":282},"outputId":"589819f5-858a-4a67-c6e1-c652756284a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f6805a90650>"]},"metadata":{},"execution_count":44},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"uEACZ4DaGGRA"},"execution_count":null,"outputs":[]}]}