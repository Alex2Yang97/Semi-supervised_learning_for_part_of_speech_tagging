{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1669225331792,"user_tz":300,"elapsed":3531,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"f1131f3a-718b-422e-8d41-ae59cff2a95d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.26.15)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.15 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.15)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.15->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.15->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.15->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from utils import read_conll_file, read_data\n","\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1669225333944,"user_tz":300,"elapsed":2156,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"46849e36-bf11-4ec2-dea8-9bc27416510a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"CPKysG3i3nsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KqectYYC30N","executionInfo":{"status":"ok","timestamp":1669225336113,"user_tz":300,"elapsed":2171,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"67320955-9edc-41da-dba4-9cbdde18af8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1669225336113,"user_tz":300,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"1f62c313-c77f-47a2-c365-4cf621552a0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens = tokenizer.tokenize(\"mistakenly\")\n","# tokens"],"metadata":{"id":"T9SUpDvLH2vE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tid = tokenizer.convert_tokens_to_ids(tokens)\n","# tid"],"metadata":{"id":"ggEO3eQKH54f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizer.convert_ids_to_tokens([234,2000,3000,22893])"],"metadata":{"id":"WRlUglUjIF36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator, average=\"macro\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n","    precision_value = precision_score(y_true, y_pred, average=average)\n","    recall_value = recall_score(y_true, y_pred, average=average)\n","    f1_value = f1_score(y_true, y_pred, average=average)\n","\n","    return precision_value, recall_value, f1_value"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"],"metadata":{"id":"B0x3gRfi9iyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# torch.save(model.state_dict(), model_file)"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value = eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAD3Wd574v6Q","executionInfo":{"status":"ok","timestamp":1669225371443,"user_tz":300,"elapsed":21307,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"0451f10d-9982-45af-e50a-8bff182b913c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       178\n","           2       1.00      1.00      1.00       352\n","           3       1.00      1.00      1.00      2000\n","           4       1.00      1.00      1.00        60\n","           5       1.00      1.00      1.00        60\n","           6       1.00      1.00      1.00      1613\n","           7       1.00      1.00      1.00       223\n","           9       1.00      0.99      1.00       935\n","          10       0.98      1.00      0.99      1266\n","          11       0.99      1.00      0.99      3309\n","          12       1.00      1.00      1.00        46\n","          13       1.00      0.20      0.33        20\n","          14       1.00      0.99      1.00       511\n","          15       0.97      0.99      0.98      4250\n","          16       0.97      0.89      0.93      2423\n","          17       0.96      0.93      0.94       139\n","          18       0.92      0.93      0.93        73\n","          19       1.00      0.75      0.86         4\n","          20       1.00      0.99      1.00       413\n","          22       0.98      0.98      0.98      5545\n","          23       0.99      0.96      0.97      4133\n","          24       0.21      0.71      0.32        45\n","          25       0.99      0.98      0.98      2316\n","          26       0.71      1.00      0.83        15\n","          27       0.99      0.99      0.99       373\n","          28       1.00      0.99      0.99       766\n","          29       0.99      1.00      1.00       357\n","          30       0.95      0.91      0.93      1405\n","          31       0.88      0.85      0.86        82\n","          32       0.85      0.89      0.87        37\n","          33       0.85      0.87      0.86       126\n","          34       0.75      0.82      0.78        11\n","          35       0.99      0.99      0.99       588\n","          36       0.88      1.00      0.93         7\n","          37       0.98      0.98      0.98      1124\n","          38       0.99      0.96      0.98      1162\n","          39       0.93      0.97      0.95       608\n","          40       0.82      0.99      0.90       829\n","          41       0.97      0.98      0.97       563\n","          42       0.98      0.99      0.99       873\n","          43       0.93      0.97      0.95       191\n","          44       1.00      1.00      1.00        90\n","          45       1.00      1.00      1.00        14\n","          46       1.00      0.99      0.99        89\n","          48       1.00      1.00      1.00       366\n","\n","    accuracy                           0.97     39590\n","   macro avg       0.94      0.94      0.93     39590\n","weighted avg       0.98      0.97      0.98     39590\n","\n"]}]},{"cell_type":"code","source":["# wsj_precision_value, wsj_recall_value, wsj_f1_value"],"metadata":{"id":"2M6AkbHMD6VJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"],"metadata":{"id":"agIHM1TmEYl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"mGm3QLNcD8bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain = \"emails\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"],"metadata":{"id":"fwvivWyzEHOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHAOs-fdEHMO","executionInfo":{"status":"ok","timestamp":1669225371444,"user_tz":300,"elapsed":24,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"4724a8fe-d942-49b5-c581-2b14074e5725"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 2450\n","The number of tags 49\n","The number of samples: 2450\n","The number of tags 48\n","after filter tag 2427\n","after filter tag 2402\n"]}]},{"cell_type":"code","source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []"],"metadata":{"id":"3JGnpmRNHHmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHb8ZM-VjG-7","executionInfo":{"status":"ok","timestamp":1669225379882,"user_tz":300,"elapsed":8459,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"d12d656c-8b7e-4430-9d15-d4ccd649d318"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.94      0.86        35\n","           2       0.87      0.52      0.65        77\n","           3       1.00      0.79      0.88      1030\n","           4       1.00      0.84      0.91       291\n","           5       0.91      0.84      0.87       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.98       689\n","          10       0.93      0.98      0.96       901\n","          11       0.96      1.00      0.98      2111\n","          12       0.98      0.96      0.97        47\n","          13       0.60      0.46      0.52        13\n","          14       0.28      1.00      0.44        43\n","          15       0.93      0.98      0.95      2778\n","          16       0.90      0.80      0.85      1151\n","          17       0.91      0.95      0.93        41\n","          18       0.94      1.00      0.97        32\n","          19       0.68      0.57      0.62        40\n","          20       1.00      0.96      0.98       584\n","          21       0.00      0.00      0.00        52\n","          22       0.93      0.75      0.83      4175\n","          23       0.73      0.95      0.83      2253\n","          24       0.37      0.68      0.48        44\n","          25       0.87      0.92      0.89       888\n","          26       1.00      0.78      0.88         9\n","          27       0.71      0.99      0.82        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.81      0.86      0.83      1136\n","          31       0.55      0.63      0.59        19\n","          32       1.00      0.75      0.86         8\n","          33       0.75      0.97      0.84        86\n","          34       0.19      0.38      0.25        32\n","          35       0.97      1.00      0.98       474\n","          36       1.00      0.14      0.24       182\n","          37       0.89      0.96      0.92      1592\n","          38       0.98      0.96      0.97       404\n","          39       0.97      0.93      0.95       485\n","          40       0.90      0.98      0.94       573\n","          41       0.93      0.94      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.94      0.89      0.92       152\n","          44       0.87      0.92      0.90        75\n","          46       0.96      0.99      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.79      0.80      0.78     28417\n","weighted avg       0.91      0.91      0.90     28417\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"S1HrHo0LEhWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","  new_prob_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","          new_prob_lst.extend(max_prob.tolist())\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","          new_prob_lst.extend(max_prob.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn]\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","  new_train_prob = [new_prob_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","  remain_train_prob = [new_prob_lst[i] for i in not_select_ind]\n","\n","  return new_train_x, new_train_y, new_train_prob, remain_train_x, remain_train_y, remain_train_prob"],"metadata":{"id":"vzWOF5sCGjgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                            batch_size=8,\n","                            shuffle=False,\n","                            num_workers=1,\n","                            collate_fn=pad)"],"metadata":{"id":"5ENZbc9qph1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.eval()\n","\n","# LLD = []\n","# MEAN_PROB = []\n","# new_x_lst = []\n","# new_y_lst = []\n","\n","# with torch.no_grad():\n","#     for i, batch in enumerate(domain_dev_iter):\n","\n","#       _, x, _, _, y, _ = batch\n","#       sen_len = y.bool().sum(axis=1)\n","\n","#       logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","#       # Save prediction as new training dataset\n","#       softmax_value = torch.softmax(logits, dim=2)\n","#       max_prob = torch.amax(softmax_value, dim=2)\n","\n","#       # Rank by mean probability\n","#       res_prob = y.bool().to(device) * max_prob.to(device)\n","#       sum_prob = res_prob.sum(axis=1)\n","#       mean_prob = sum_prob / sen_len.to(device)\n","#       MEAN_PROB.extend(mean_prob)\n","      \n","#       new_x_lst.extend(x.tolist())\n","#       new_y_lst.extend(y_hat.tolist())\n","#       break"],"metadata":{"id":"3g8F4GmiqT_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y.shape"],"metadata":{"id":"m_IOJD1WqfyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y"],"metadata":{"id":"JEc5Kr0kuiO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y_hat"],"metadata":{"id":"BvjGahErusg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max_prob"],"metadata":{"id":"rE5ZfLDUql2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z-orZnnXqlzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5CZH4eCqlwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(domain_dev_word_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e6O5XWvVqluB","executionInfo":{"status":"ok","timestamp":1669225380327,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"b0a0c3cf-fa81-445c-bea7-a8dc07f1e702"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2427"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["threshold_prob = 0.9\n","\n","# 1. topn token overall\n","# 2. select from each POS class 1% , 2%, 5%\n","\n","\n","topn = len(domain_dev_word_lst)\n","i = 0\n","while i <= 10:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","  print(\"domain_dev_word_lst\", len(domain_dev_word_lst))\n","\n","  domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","  domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                              batch_size=8,\n","                              shuffle=False,\n","                              num_workers=1,\n","                                collate_fn=pad)\n","  \n","  top_words_ids, top_tags_ids, top_prob_lst, _, _, _ = gen_pseudo_data(model, domain_dev_iter, topn)\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  top_prob = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    prob_lst = top_prob_lst[t]\n","    words = []\n","    tags = []\n","    probs = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        \n","        if prob_lst[k] >= threshold_prob:\n","          tags.append(tag_ids[k])\n","        else:\n","          tags.append('<pad>')\n","\n","        probs.append(prob_lst[k])\n","        \n","    top_words.append(words)\n","    top_tags.append(tags)\n","    top_prob.append(probs)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zl3VMmnVVD-8","outputId":"b44af5d8-d1b5-462e-834f-99d77226c604","executionInfo":{"status":"ok","timestamp":1669231138777,"user_tz":300,"elapsed":5758454,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loop 1\n","domain_dev_word_lst 2427\n","Train from scratch...\n","step: 0, loss: 3.8578107357025146\n","step: 10, loss: 2.022843837738037\n","step: 20, loss: 0.9560863375663757\n","step: 30, loss: 0.35957786440849304\n","step: 40, loss: 0.4113931655883789\n","step: 50, loss: 0.19826169312000275\n","step: 60, loss: 0.293730765581131\n","step: 70, loss: 0.2802572548389435\n","step: 80, loss: 0.31896159052848816\n","step: 90, loss: 0.13969437777996063\n","step: 100, loss: 0.2858290672302246\n","step: 110, loss: 0.20158474147319794\n","step: 120, loss: 0.1962890326976776\n","step: 130, loss: 0.06106842681765556\n","step: 140, loss: 0.20354318618774414\n","step: 150, loss: 0.31151801347732544\n","step: 160, loss: 0.06656526029109955\n","step: 170, loss: 0.06906750053167343\n","step: 180, loss: 0.25116419792175293\n","step: 190, loss: 0.11046190559864044\n","step: 200, loss: 0.14170722663402557\n","step: 210, loss: 0.1346970796585083\n","step: 220, loss: 0.042788807302713394\n","step: 230, loss: 0.04952073097229004\n","step: 240, loss: 0.1223900094628334\n","step: 250, loss: 0.09885464608669281\n","step: 260, loss: 0.0690731555223465\n","step: 270, loss: 0.06130285561084747\n","step: 280, loss: 0.11499898880720139\n","step: 290, loss: 0.16384275257587433\n","step: 300, loss: 0.20130828022956848\n","step: 310, loss: 0.2796606123447418\n","step: 320, loss: 0.11113334447145462\n","step: 330, loss: 0.13601991534233093\n","step: 340, loss: 0.21135108172893524\n","step: 350, loss: 0.04586553946137428\n","step: 360, loss: 0.12460656464099884\n","step: 370, loss: 0.10911688953638077\n","step: 380, loss: 0.07694423198699951\n","step: 390, loss: 0.10829591751098633\n","step: 400, loss: 0.14342229068279266\n","step: 410, loss: 0.145392045378685\n","step: 420, loss: 0.13193035125732422\n","step: 430, loss: 0.17262114584445953\n","step: 440, loss: 0.10041370987892151\n","step: 450, loss: 0.04687751829624176\n","step: 460, loss: 0.1738479733467102\n","step: 470, loss: 0.10764795541763306\n","step: 480, loss: 0.17235973477363586\n","step: 490, loss: 0.06517378985881805\n","step: 500, loss: 0.15485888719558716\n","step: 510, loss: 0.1031239852309227\n","step: 520, loss: 0.07628394663333893\n","step: 530, loss: 0.1410137563943863\n","step: 540, loss: 0.10905254632234573\n","step: 550, loss: 0.055534087121486664\n","step: 560, loss: 0.08408632129430771\n","step: 570, loss: 0.08804702013731003\n","step: 580, loss: 0.03544735908508301\n","step: 590, loss: 0.1392727941274643\n","step: 600, loss: 0.11453811824321747\n","step: 610, loss: 0.10974744707345963\n","step: 620, loss: 0.10104607790708542\n","step: 630, loss: 0.08818846940994263\n","step: 640, loss: 0.14059731364250183\n","step: 650, loss: 0.12525995075702667\n","step: 660, loss: 0.10365927964448929\n","step: 670, loss: 0.15644578635692596\n","step: 680, loss: 0.06012248620390892\n","step: 690, loss: 0.08944889158010483\n","step: 700, loss: 0.13349328935146332\n","step: 710, loss: 0.08301086723804474\n","step: 720, loss: 0.05712466314435005\n","step: 730, loss: 0.1896476149559021\n","step: 740, loss: 0.08024618774652481\n","step: 750, loss: 0.12612083554267883\n","step: 760, loss: 0.06916314363479614\n","step: 770, loss: 0.06557893753051758\n","step: 780, loss: 0.07603997737169266\n","step: 790, loss: 0.08525347709655762\n","step: 800, loss: 0.12253766506910324\n","step: 810, loss: 0.14126531779766083\n","step: 820, loss: 0.08503396809101105\n","step: 830, loss: 0.11175621300935745\n","step: 840, loss: 0.08405585587024689\n","step: 850, loss: 0.12290102988481522\n","step: 860, loss: 0.046225544065237045\n","step: 870, loss: 0.17818643152713776\n","step: 880, loss: 0.12988610565662384\n","step: 890, loss: 0.07426775246858597\n","step: 900, loss: 0.06298289448022842\n","step: 910, loss: 0.25515037775039673\n","step: 920, loss: 0.13663649559020996\n","step: 930, loss: 0.05536198243498802\n","step: 940, loss: 0.1865324079990387\n","step: 950, loss: 0.13531197607517242\n","step: 960, loss: 0.17321012914180756\n","step: 970, loss: 0.044938959181308746\n","step: 980, loss: 0.2427334189414978\n","step: 990, loss: 0.17648449540138245\n","step: 1000, loss: 0.17174263298511505\n","step: 1010, loss: 0.09610241651535034\n","step: 1020, loss: 0.022219523787498474\n","step: 1030, loss: 0.07703886181116104\n","step: 1040, loss: 0.12499827146530151\n","step: 1050, loss: 0.13974711298942566\n","step: 1060, loss: 0.06296940892934799\n","step: 1070, loss: 0.20040784776210785\n","step: 1080, loss: 0.05609199404716492\n","step: 1090, loss: 0.13580796122550964\n","step: 1100, loss: 0.05821263790130615\n","step: 1110, loss: 0.21338658034801483\n","step: 1120, loss: 0.061208367347717285\n","step: 1130, loss: 0.027509206905961037\n","step: 1140, loss: 0.1286820024251938\n","step: 1150, loss: 0.08770846575498581\n","step: 1160, loss: 0.10112960636615753\n","step: 1170, loss: 0.13254807889461517\n","step: 1180, loss: 0.10233169794082642\n","step: 1190, loss: 0.03766612336039543\n","step: 1200, loss: 0.10870703309774399\n","step: 1210, loss: 0.0835627093911171\n","step: 1220, loss: 0.11216776818037033\n","step: 1230, loss: 0.09978225827217102\n","step: 1240, loss: 0.13242198526859283\n","step: 1250, loss: 0.07586110383272171\n","step: 1260, loss: 0.1357058435678482\n","step: 1270, loss: 0.07028815895318985\n","step: 1280, loss: 0.14167556166648865\n","step: 1290, loss: 0.10071233659982681\n","step: 1300, loss: 0.06052348017692566\n","step: 1310, loss: 0.11616973578929901\n","step: 1320, loss: 0.20481878519058228\n","step: 1330, loss: 0.08224564045667648\n","step: 1340, loss: 0.06811047345399857\n","step: 1350, loss: 0.07685094326734543\n","step: 1360, loss: 0.1958320140838623\n","step: 1370, loss: 0.08274728059768677\n","step: 1380, loss: 0.044417090713977814\n","step: 1390, loss: 0.1382206529378891\n","step: 1400, loss: 0.06168731302022934\n","step: 1410, loss: 0.04802952706813812\n","step: 1420, loss: 0.047009676694869995\n","step: 1430, loss: 0.13280834257602692\n","step: 1440, loss: 0.07775331288576126\n","step: 1450, loss: 0.04936491698026657\n","step: 1460, loss: 0.27587243914604187\n","step: 1470, loss: 0.11493312567472458\n","step: 1480, loss: 0.06349150091409683\n","step: 1490, loss: 0.06917428225278854\n","step: 1500, loss: 0.17944052815437317\n","step: 1510, loss: 0.047000106424093246\n","step: 1520, loss: 0.12902401387691498\n","step: 1530, loss: 0.09785666316747665\n","step: 1540, loss: 0.05439431965351105\n","step: 1550, loss: 0.08248067647218704\n","step: 1560, loss: 0.16699522733688354\n","step: 1570, loss: 0.04814023897051811\n","step: 1580, loss: 0.17547161877155304\n","step: 1590, loss: 0.04853411391377449\n","step: 1600, loss: 0.09372851252555847\n","step: 1610, loss: 0.10089936852455139\n","step: 1620, loss: 0.08106766641139984\n","step: 1630, loss: 0.10450608283281326\n","step: 1640, loss: 0.1216096505522728\n","step: 1650, loss: 0.13894148170948029\n","step: 1660, loss: 0.09354212880134583\n","step: 1670, loss: 0.1320502907037735\n","step: 1680, loss: 0.09549376368522644\n","step: 1690, loss: 0.04555236175656319\n","step: 1700, loss: 0.031236393377184868\n","step: 1710, loss: 0.05380149930715561\n","step: 1720, loss: 0.07780515402555466\n","step: 1730, loss: 0.10054870694875717\n","step: 1740, loss: 0.16154293715953827\n","step: 1750, loss: 0.23497359454631805\n","step: 1760, loss: 0.03986270725727081\n","step: 1770, loss: 0.10658552497625351\n","step: 1780, loss: 0.10188983380794525\n","step: 1790, loss: 0.15630830824375153\n","step: 1800, loss: 0.09885666519403458\n","step: 1810, loss: 0.09603895992040634\n","step: 1820, loss: 0.13300444185733795\n","step: 1830, loss: 0.08409220725297928\n","step: 1840, loss: 0.13607150316238403\n","step: 1850, loss: 0.05583964288234711\n","step: 1860, loss: 0.11275538057088852\n","step: 1870, loss: 0.08203454315662384\n","step: 1880, loss: 0.11072620004415512\n","step: 1890, loss: 0.0741114467382431\n","step: 1900, loss: 0.08158015459775925\n","step: 1910, loss: 0.08599715679883957\n","step: 1920, loss: 0.08769463747739792\n","step: 1930, loss: 0.021638181060552597\n","step: 1940, loss: 0.12440107017755508\n","step: 1950, loss: 0.06070917844772339\n","step: 1960, loss: 0.06959868222475052\n","step: 1970, loss: 0.11485068500041962\n","step: 1980, loss: 0.13996699452400208\n","step: 1990, loss: 0.0436975434422493\n","step: 2000, loss: 0.0455109141767025\n","step: 2010, loss: 0.11203509569168091\n","step: 2020, loss: 0.16051848232746124\n","step: 2030, loss: 0.0452897772192955\n","step: 2040, loss: 0.0590498261153698\n","step: 2050, loss: 0.13581368327140808\n","step: 2060, loss: 0.05016002431511879\n","step: 2070, loss: 0.07316919416189194\n","step: 2080, loss: 0.03816340118646622\n","step: 2090, loss: 0.06261187791824341\n","step: 2100, loss: 0.06923197954893112\n","step: 2110, loss: 0.14728419482707977\n","step: 2120, loss: 0.09916719049215317\n","step: 2130, loss: 0.07025663554668427\n","step: 2140, loss: 0.12674027681350708\n","step: 2150, loss: 0.13206464052200317\n","step: 2160, loss: 0.11254706978797913\n","step: 2170, loss: 0.10389741510152817\n","step: 2180, loss: 0.13169853389263153\n","step: 2190, loss: 0.0876554623246193\n","step: 2200, loss: 0.07337680459022522\n","step: 2210, loss: 0.1661045104265213\n","step: 2220, loss: 0.0623437874019146\n","step: 2230, loss: 0.15343718230724335\n","step: 2240, loss: 0.05197696015238762\n","step: 2250, loss: 0.16811752319335938\n","step: 2260, loss: 0.12844598293304443\n","step: 2270, loss: 0.07521829009056091\n","step: 2280, loss: 0.0706915408372879\n","step: 2290, loss: 0.11637086421251297\n","step: 2300, loss: 0.05866297706961632\n","step: 2310, loss: 0.12401822954416275\n","step: 2320, loss: 0.0644117221236229\n","step: 2330, loss: 0.13946257531642914\n","step: 2340, loss: 0.1401691436767578\n","step: 2350, loss: 0.11473651230335236\n","step: 2360, loss: 0.1494632065296173\n","step: 2370, loss: 0.03945060446858406\n","step: 2380, loss: 0.1176242008805275\n","step: 2390, loss: 0.038988981395959854\n","step: 2400, loss: 0.11754855513572693\n","step: 2410, loss: 0.037984758615493774\n","step: 2420, loss: 0.12764409184455872\n","step: 2430, loss: 0.0795726403594017\n","step: 2440, loss: 0.07630420476198196\n","step: 2450, loss: 0.167823925614357\n","step: 2460, loss: 0.0648702085018158\n","step: 2470, loss: 0.07148034125566483\n","step: 2480, loss: 0.03940948471426964\n","step: 2490, loss: 0.06926972419023514\n","step: 2500, loss: 0.0658794492483139\n","step: 2510, loss: 0.13507863879203796\n","step: 2520, loss: 0.12771794199943542\n","step: 2530, loss: 0.07289941608905792\n","step: 2540, loss: 0.20161134004592896\n","step: 2550, loss: 0.15333446860313416\n","step: 2560, loss: 0.11341013759374619\n","step: 2570, loss: 0.07741380482912064\n","step: 2580, loss: 0.16938918828964233\n","step: 2590, loss: 0.3716610372066498\n","step: 2600, loss: 0.09735186398029327\n","step: 2610, loss: 0.13838663697242737\n","step: 2620, loss: 0.0844707190990448\n","step: 2630, loss: 0.08811139315366745\n","step: 2640, loss: 0.08657212555408478\n","step: 2650, loss: 0.06461165100336075\n","step: 2660, loss: 0.2283608466386795\n","step: 2670, loss: 0.09726808220148087\n","step: 2680, loss: 0.11672509461641312\n","step: 2690, loss: 0.10419244319200516\n","step: 2700, loss: 0.06595645099878311\n","step: 2710, loss: 0.04755968600511551\n","step: 2720, loss: 0.033026814460754395\n","step: 2730, loss: 0.0437614805996418\n","step: 2740, loss: 0.11722574383020401\n","step: 2750, loss: 0.14973294734954834\n","step: 2760, loss: 0.08102995157241821\n","step: 2770, loss: 0.05751101300120354\n","step: 2780, loss: 0.10638707876205444\n","step: 2790, loss: 0.07679422199726105\n","step: 2800, loss: 0.06971900165081024\n","step: 2810, loss: 0.0806647315621376\n","step: 2820, loss: 0.05664515495300293\n","step: 2830, loss: 0.2660132348537445\n","step: 2840, loss: 0.04366395249962807\n","step: 2850, loss: 0.04372132569551468\n","step: 2860, loss: 0.14176544547080994\n","step: 2870, loss: 0.1723555475473404\n","step: 2880, loss: 0.08027351647615433\n","step: 2890, loss: 0.18950675427913666\n","step: 2900, loss: 0.03874294087290764\n","step: 2910, loss: 0.11767567694187164\n","step: 2920, loss: 0.061912212520837784\n","step: 2930, loss: 0.089561827480793\n","step: 2940, loss: 0.07711867988109589\n","step: 2950, loss: 0.07288647443056107\n","step: 2960, loss: 0.17014190554618835\n","step: 2970, loss: 0.1748596727848053\n","step: 2980, loss: 0.04728436842560768\n","step: 2990, loss: 0.042937953025102615\n","step: 3000, loss: 0.04776977375149727\n","step: 3010, loss: 0.13884949684143066\n","step: 3020, loss: 0.1941423863172531\n","step: 3030, loss: 0.14228922128677368\n","step: 3040, loss: 0.12878333032131195\n","step: 3050, loss: 0.0692591443657875\n","step: 3060, loss: 0.06530708819627762\n","step: 3070, loss: 0.05474511533975601\n","step: 3080, loss: 0.11264031380414963\n","step: 3090, loss: 0.10875412076711655\n","step: 3100, loss: 0.12401193380355835\n","step: 3110, loss: 0.1601066291332245\n","step: 3120, loss: 0.11814338713884354\n","step: 3130, loss: 0.05843440815806389\n","step: 3140, loss: 0.05799606069922447\n","step: 3150, loss: 0.11604029685258865\n","step: 3160, loss: 0.15862153470516205\n","step: 3170, loss: 0.1379413604736328\n","step: 3180, loss: 0.023456336930394173\n","step: 3190, loss: 0.12507590651512146\n","step: 3200, loss: 0.011642697267234325\n","step: 3210, loss: 0.06642434000968933\n","step: 3220, loss: 0.08925313502550125\n","step: 3230, loss: 0.030109835788607597\n","step: 3240, loss: 0.049835145473480225\n","step: 3250, loss: 0.12800155580043793\n","step: 3260, loss: 0.17216172814369202\n","step: 3270, loss: 0.012522118166089058\n","step: 3280, loss: 0.06577380746603012\n","step: 3290, loss: 0.1042424663901329\n","step: 3300, loss: 0.08422748744487762\n","step: 3310, loss: 0.037149690091609955\n","step: 3320, loss: 0.1252942681312561\n","step: 3330, loss: 0.09689085185527802\n","step: 3340, loss: 0.05639092996716499\n","step: 3350, loss: 0.0670180469751358\n","step: 3360, loss: 0.04837911203503609\n","step: 3370, loss: 0.07473155111074448\n","step: 3380, loss: 0.1887529343366623\n","step: 3390, loss: 0.05796848237514496\n","step: 3400, loss: 0.15322209894657135\n","step: 3410, loss: 0.037789106369018555\n","step: 3420, loss: 0.0520516112446785\n","step: 3430, loss: 0.05193537473678589\n","step: 3440, loss: 0.058776430785655975\n","step: 3450, loss: 0.06482710689306259\n","step: 3460, loss: 0.09063971787691116\n","step: 3470, loss: 0.08436387032270432\n","step: 3480, loss: 0.07418946176767349\n","step: 3490, loss: 0.11793877929449081\n","step: 3500, loss: 0.20716792345046997\n","step: 3510, loss: 0.09967442601919174\n","step: 3520, loss: 0.1365935355424881\n","step: 3530, loss: 0.06860083341598511\n","step: 3540, loss: 0.07811526209115982\n","step: 3550, loss: 0.130024254322052\n","step: 3560, loss: 0.04035687819123268\n","step: 3570, loss: 0.10822160542011261\n","step: 3580, loss: 0.14191637933254242\n","step: 3590, loss: 0.05557209625840187\n","step: 3600, loss: 0.10527897626161575\n","step: 3610, loss: 0.06387567520141602\n","step: 3620, loss: 0.03186854347586632\n","step: 3630, loss: 0.0865674614906311\n","step: 3640, loss: 0.07854743301868439\n","step: 3650, loss: 0.097282275557518\n","step: 3660, loss: 0.028886526823043823\n","step: 3670, loss: 0.055330585688352585\n","step: 3680, loss: 0.04134318232536316\n","step: 3690, loss: 0.04338601976633072\n","step: 3700, loss: 0.11812246590852737\n","step: 3710, loss: 0.13146774470806122\n","step: 3720, loss: 0.09226958453655243\n","step: 3730, loss: 0.1148597002029419\n","step: 3740, loss: 0.06154526025056839\n","step: 3750, loss: 0.10715582966804504\n","step: 3760, loss: 0.12222922593355179\n","step: 3770, loss: 0.12469012290239334\n","step: 3780, loss: 0.3213248550891876\n","step: 3790, loss: 0.06749099493026733\n","step: 3800, loss: 0.08295433223247528\n","step: 3810, loss: 0.10225190222263336\n","step: 3820, loss: 0.06599409133195877\n","step: 3830, loss: 0.1342829018831253\n","step: 3840, loss: 0.12709297239780426\n","step: 3850, loss: 0.10489947348833084\n","step: 3860, loss: 0.08899273723363876\n","step: 3870, loss: 0.03629383072257042\n","step: 3880, loss: 0.09287334978580475\n","step: 3890, loss: 0.08109085261821747\n","step: 3900, loss: 0.0682714655995369\n","step: 3910, loss: 0.13645325601100922\n","step: 3920, loss: 0.03813751041889191\n","step: 3930, loss: 0.06534995883703232\n","step: 3940, loss: 0.016359372064471245\n","step: 3950, loss: 0.06992332637310028\n","step: 3960, loss: 0.04138944298028946\n","step: 3970, loss: 0.11442480236291885\n","step: 3980, loss: 0.09919451177120209\n","step: 3990, loss: 0.1452983170747757\n","step: 4000, loss: 0.11772036552429199\n","step: 4010, loss: 0.15052922070026398\n","step: 4020, loss: 0.022031571716070175\n","step: 4030, loss: 0.10054720193147659\n","step: 4040, loss: 0.0607205405831337\n","step: 4050, loss: 0.08745812624692917\n","step: 4060, loss: 0.05939061567187309\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.88      1.00      0.93        35\n","           2       0.98      0.75      0.85        77\n","           3       1.00      0.77      0.87      1030\n","           4       0.96      0.85      0.90       291\n","           5       0.94      0.84      0.89       294\n","           6       0.98      0.99      0.98      1570\n","           7       0.53      0.94      0.68       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.92      0.98      0.95       901\n","          11       0.97      1.00      0.98      2111\n","          12       0.96      0.98      0.97        47\n","          13       1.00      0.62      0.76        13\n","          14       0.28      1.00      0.43        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.84      0.83      0.84      1151\n","          17       0.80      0.98      0.88        41\n","          18       0.89      0.97      0.93        32\n","          19       0.70      0.47      0.57        40\n","          20       1.00      0.91      0.95       584\n","          21       1.00      0.02      0.04        52\n","          22       0.96      0.71      0.82      4175\n","          23       0.67      0.98      0.79      2253\n","          24       0.42      0.50      0.46        44\n","          25       0.85      0.93      0.89       888\n","          26       0.75      1.00      0.86         9\n","          27       0.94      0.99      0.96        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.91      0.84      0.87      1136\n","          31       0.65      0.68      0.67        19\n","          32       1.00      0.62      0.77         8\n","          33       0.66      0.98      0.79        86\n","          34       0.18      0.41      0.25        32\n","          35       0.98      0.99      0.99       474\n","          36       0.92      0.13      0.23       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.98      0.97      0.98       404\n","          39       0.98      0.95      0.96       485\n","          40       0.92      0.86      0.89       573\n","          41       0.91      0.93      0.92       841\n","          42       0.99      0.98      0.99       575\n","          43       0.94      0.89      0.92       152\n","          44       0.88      0.93      0.90        75\n","          46       1.00      0.98      0.99        82\n","          48       0.38      0.06      0.11        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.83      0.81      0.79     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","\n","Loop 2\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9744224548339844\n","step: 10, loss: 1.8904787302017212\n","step: 20, loss: 0.8285925388336182\n","step: 30, loss: 0.4231554865837097\n","step: 40, loss: 0.399410218000412\n","step: 50, loss: 0.252452552318573\n","step: 60, loss: 0.1653655469417572\n","step: 70, loss: 0.2522847056388855\n","step: 80, loss: 0.2739276885986328\n","step: 90, loss: 0.19627831876277924\n","step: 100, loss: 0.19765301048755646\n","step: 110, loss: 0.18636098504066467\n","step: 120, loss: 0.2103412002325058\n","step: 130, loss: 0.09848518669605255\n","step: 140, loss: 0.07492765784263611\n","step: 150, loss: 0.10969369858503342\n","step: 160, loss: 0.16029424965381622\n","step: 170, loss: 0.21435096859931946\n","step: 180, loss: 0.1558590531349182\n","step: 190, loss: 0.0823933482170105\n","step: 200, loss: 0.126363605260849\n","step: 210, loss: 0.28107380867004395\n","step: 220, loss: 0.1996993124485016\n","step: 230, loss: 0.1396445482969284\n","step: 240, loss: 0.37917211651802063\n","step: 250, loss: 0.20601120591163635\n","step: 260, loss: 0.28652194142341614\n","step: 270, loss: 0.09673602133989334\n","step: 280, loss: 0.13788741827011108\n","step: 290, loss: 0.15735456347465515\n","step: 300, loss: 0.0870056077837944\n","step: 310, loss: 0.20293733477592468\n","step: 320, loss: 0.07991606742143631\n","step: 330, loss: 0.14336436986923218\n","step: 340, loss: 0.12936528027057648\n","step: 350, loss: 0.10953553020954132\n","step: 360, loss: 0.11870123445987701\n","step: 370, loss: 0.0760381668806076\n","step: 380, loss: 0.13280630111694336\n","step: 390, loss: 0.06596120446920395\n","step: 400, loss: 0.14147770404815674\n","step: 410, loss: 0.13909561932086945\n","step: 420, loss: 0.06290443986654282\n","step: 430, loss: 0.0945766493678093\n","step: 440, loss: 0.04113346338272095\n","step: 450, loss: 0.0981501042842865\n","step: 460, loss: 0.10243159532546997\n","step: 470, loss: 0.1183219701051712\n","step: 480, loss: 0.07845132797956467\n","step: 490, loss: 0.060032833367586136\n","step: 500, loss: 0.08279775083065033\n","step: 510, loss: 0.1607258915901184\n","step: 520, loss: 0.07591082900762558\n","step: 530, loss: 0.1006685122847557\n","step: 540, loss: 0.11335793882608414\n","step: 550, loss: 0.05135178193449974\n","step: 560, loss: 0.05285844951868057\n","step: 570, loss: 0.09125617146492004\n","step: 580, loss: 0.09388286620378494\n","step: 590, loss: 0.1679978221654892\n","step: 600, loss: 0.07663629204034805\n","step: 610, loss: 0.06734181195497513\n","step: 620, loss: 0.16721178591251373\n","step: 630, loss: 0.13758176565170288\n","step: 640, loss: 0.10552700608968735\n","step: 650, loss: 0.2173835188150406\n","step: 660, loss: 0.042277805507183075\n","step: 670, loss: 0.17566199600696564\n","step: 680, loss: 0.15997722744941711\n","step: 690, loss: 0.1311814934015274\n","step: 700, loss: 0.08515243977308273\n","step: 710, loss: 0.09340447187423706\n","step: 720, loss: 0.03573998436331749\n","step: 730, loss: 0.05254306271672249\n","step: 740, loss: 0.18772456049919128\n","step: 750, loss: 0.14983704686164856\n","step: 760, loss: 0.19302017986774445\n","step: 770, loss: 0.11557386070489883\n","step: 780, loss: 0.05943699926137924\n","step: 790, loss: 0.07420995831489563\n","step: 800, loss: 0.1465710997581482\n","step: 810, loss: 0.10012252628803253\n","step: 820, loss: 0.17264555394649506\n","step: 830, loss: 0.08490002900362015\n","step: 840, loss: 0.05107931047677994\n","step: 850, loss: 0.07980179041624069\n","step: 860, loss: 0.07873737066984177\n","step: 870, loss: 0.07403310388326645\n","step: 880, loss: 0.15559864044189453\n","step: 890, loss: 0.064299575984478\n","step: 900, loss: 0.04318207502365112\n","step: 910, loss: 0.09839188307523727\n","step: 920, loss: 0.17913195490837097\n","step: 930, loss: 0.11275119334459305\n","step: 940, loss: 0.11995435506105423\n","step: 950, loss: 0.1299952119588852\n","step: 960, loss: 0.06933128088712692\n","step: 970, loss: 0.08115477114915848\n","step: 980, loss: 0.11486967653036118\n","step: 990, loss: 0.14891579747200012\n","step: 1000, loss: 0.1433062106370926\n","step: 1010, loss: 0.08214880526065826\n","step: 1020, loss: 0.14190034568309784\n","step: 1030, loss: 0.062252841889858246\n","step: 1040, loss: 0.04448538273572922\n","step: 1050, loss: 0.11778394877910614\n","step: 1060, loss: 0.07694365084171295\n","step: 1070, loss: 0.11160367727279663\n","step: 1080, loss: 0.07940174639225006\n","step: 1090, loss: 0.026250693947076797\n","step: 1100, loss: 0.06246566399931908\n","step: 1110, loss: 0.1852688193321228\n","step: 1120, loss: 0.047183141112327576\n","step: 1130, loss: 0.14831548929214478\n","step: 1140, loss: 0.07623250037431717\n","step: 1150, loss: 0.07234488427639008\n","step: 1160, loss: 0.120606929063797\n","step: 1170, loss: 0.05274786800146103\n","step: 1180, loss: 0.12327505648136139\n","step: 1190, loss: 0.1424272358417511\n","step: 1200, loss: 0.09428053349256516\n","step: 1210, loss: 0.20958220958709717\n","step: 1220, loss: 0.07690966874361038\n","step: 1230, loss: 0.07920030504465103\n","step: 1240, loss: 0.31548774242401123\n","step: 1250, loss: 0.07163175195455551\n","step: 1260, loss: 0.10329611599445343\n","step: 1270, loss: 0.08601643145084381\n","step: 1280, loss: 0.1403856873512268\n","step: 1290, loss: 0.07635925710201263\n","step: 1300, loss: 0.07036789506673813\n","step: 1310, loss: 0.059061937034130096\n","step: 1320, loss: 0.07930084317922592\n","step: 1330, loss: 0.10397298634052277\n","step: 1340, loss: 0.11503762751817703\n","step: 1350, loss: 0.09965003281831741\n","step: 1360, loss: 0.08778325468301773\n","step: 1370, loss: 0.13997113704681396\n","step: 1380, loss: 0.0757024735212326\n","step: 1390, loss: 0.21879762411117554\n","step: 1400, loss: 0.11054597795009613\n","step: 1410, loss: 0.30960363149642944\n","step: 1420, loss: 0.06862182915210724\n","step: 1430, loss: 0.18577010929584503\n","step: 1440, loss: 0.138563334941864\n","step: 1450, loss: 0.14293015003204346\n","step: 1460, loss: 0.13830013573169708\n","step: 1470, loss: 0.11692436039447784\n","step: 1480, loss: 0.08589653670787811\n","step: 1490, loss: 0.08143651485443115\n","step: 1500, loss: 0.06532470881938934\n","step: 1510, loss: 0.16048789024353027\n","step: 1520, loss: 0.06617800146341324\n","step: 1530, loss: 0.1360345482826233\n","step: 1540, loss: 0.034774161875247955\n","step: 1550, loss: 0.07814200222492218\n","step: 1560, loss: 0.04166095331311226\n","step: 1570, loss: 0.1296371966600418\n","step: 1580, loss: 0.03478633239865303\n","step: 1590, loss: 0.057879604399204254\n","step: 1600, loss: 0.29447412490844727\n","step: 1610, loss: 0.10836201161146164\n","step: 1620, loss: 0.06832276284694672\n","step: 1630, loss: 0.15655721724033356\n","step: 1640, loss: 0.1148388683795929\n","step: 1650, loss: 0.17772886157035828\n","step: 1660, loss: 0.08277928829193115\n","step: 1670, loss: 0.029959751293063164\n","step: 1680, loss: 0.08083398640155792\n","step: 1690, loss: 0.11938957124948502\n","step: 1700, loss: 0.05027179419994354\n","step: 1710, loss: 0.16408568620681763\n","step: 1720, loss: 0.10145150870084763\n","step: 1730, loss: 0.10761977732181549\n","step: 1740, loss: 0.09844397753477097\n","step: 1750, loss: 0.06957419961690903\n","step: 1760, loss: 0.11389771103858948\n","step: 1770, loss: 0.10522380471229553\n","step: 1780, loss: 0.08096618950366974\n","step: 1790, loss: 0.1511787474155426\n","step: 1800, loss: 0.03443848714232445\n","step: 1810, loss: 0.06619907915592194\n","step: 1820, loss: 0.1450541764497757\n","step: 1830, loss: 0.03311550244688988\n","step: 1840, loss: 0.022447064518928528\n","step: 1850, loss: 0.10145752876996994\n","step: 1860, loss: 0.05810851976275444\n","step: 1870, loss: 0.06612922251224518\n","step: 1880, loss: 0.08054329454898834\n","step: 1890, loss: 0.019445333629846573\n","step: 1900, loss: 0.10114450007677078\n","step: 1910, loss: 0.12986594438552856\n","step: 1920, loss: 0.03207806870341301\n","step: 1930, loss: 0.07195628434419632\n","step: 1940, loss: 0.07973871380090714\n","step: 1950, loss: 0.10458878427743912\n","step: 1960, loss: 0.08748094737529755\n","step: 1970, loss: 0.029396886005997658\n","step: 1980, loss: 0.035190682858228683\n","step: 1990, loss: 0.06642080843448639\n","step: 2000, loss: 0.16392260789871216\n","step: 2010, loss: 0.06353006511926651\n","step: 2020, loss: 0.11162569373846054\n","step: 2030, loss: 0.07277871668338776\n","step: 2040, loss: 0.06397417187690735\n","step: 2050, loss: 0.03893353417515755\n","step: 2060, loss: 0.13307657837867737\n","step: 2070, loss: 0.13594472408294678\n","step: 2080, loss: 0.16097156703472137\n","step: 2090, loss: 0.13010956346988678\n","step: 2100, loss: 0.08548055589199066\n","step: 2110, loss: 0.06043412163853645\n","step: 2120, loss: 0.09604208916425705\n","step: 2130, loss: 0.2222553938627243\n","step: 2140, loss: 0.1328878551721573\n","step: 2150, loss: 0.05881442502140999\n","step: 2160, loss: 0.13983365893363953\n","step: 2170, loss: 0.2304268628358841\n","step: 2180, loss: 0.07263567298650742\n","step: 2190, loss: 0.12556910514831543\n","step: 2200, loss: 0.07175009697675705\n","step: 2210, loss: 0.08948343992233276\n","step: 2220, loss: 0.13458989560604095\n","step: 2230, loss: 0.029570696875452995\n","step: 2240, loss: 0.061402615159749985\n","step: 2250, loss: 0.1312355250120163\n","step: 2260, loss: 0.09558502584695816\n","step: 2270, loss: 0.07799158990383148\n","step: 2280, loss: 0.09389595687389374\n","step: 2290, loss: 0.12284599989652634\n","step: 2300, loss: 0.247209832072258\n","step: 2310, loss: 0.04712957516312599\n","step: 2320, loss: 0.047195013612508774\n","step: 2330, loss: 0.19214902818202972\n","step: 2340, loss: 0.09397727251052856\n","step: 2350, loss: 0.07198930531740189\n","step: 2360, loss: 0.09161754697561264\n","step: 2370, loss: 0.05075887218117714\n","step: 2380, loss: 0.11744453758001328\n","step: 2390, loss: 0.08429551869630814\n","step: 2400, loss: 0.08308665454387665\n","step: 2410, loss: 0.11341957002878189\n","step: 2420, loss: 0.019661923870444298\n","step: 2430, loss: 0.08406536281108856\n","step: 2440, loss: 0.11161850392818451\n","step: 2450, loss: 0.10503720492124557\n","step: 2460, loss: 0.08644869923591614\n","step: 2470, loss: 0.08411186933517456\n","step: 2480, loss: 0.09426379948854446\n","step: 2490, loss: 0.057524293661117554\n","step: 2500, loss: 0.06935061514377594\n","step: 2510, loss: 0.053621724247932434\n","step: 2520, loss: 0.0653693899512291\n","step: 2530, loss: 0.12748324871063232\n","step: 2540, loss: 0.057077959179878235\n","step: 2550, loss: 0.06202562153339386\n","step: 2560, loss: 0.07822021842002869\n","step: 2570, loss: 0.05942035838961601\n","step: 2580, loss: 0.1526080071926117\n","step: 2590, loss: 0.10221225768327713\n","step: 2600, loss: 0.09514675289392471\n","step: 2610, loss: 0.06817664951086044\n","step: 2620, loss: 0.11126092076301575\n","step: 2630, loss: 0.10100575536489487\n","step: 2640, loss: 0.10780622065067291\n","step: 2650, loss: 0.1520048975944519\n","step: 2660, loss: 0.09936326742172241\n","step: 2670, loss: 0.05765022337436676\n","step: 2680, loss: 0.11187689006328583\n","step: 2690, loss: 0.05454271659255028\n","step: 2700, loss: 0.06857560575008392\n","step: 2710, loss: 0.08484595268964767\n","step: 2720, loss: 0.18904797732830048\n","step: 2730, loss: 0.05078285187482834\n","step: 2740, loss: 0.10293399542570114\n","step: 2750, loss: 0.1012224406003952\n","step: 2760, loss: 0.09070640802383423\n","step: 2770, loss: 0.08657553791999817\n","step: 2780, loss: 0.0485592819750309\n","step: 2790, loss: 0.10232944041490555\n","step: 2800, loss: 0.08756660670042038\n","step: 2810, loss: 0.09362073987722397\n","step: 2820, loss: 0.1270604133605957\n","step: 2830, loss: 0.07243610918521881\n","step: 2840, loss: 0.136666938662529\n","step: 2850, loss: 0.10319969803094864\n","step: 2860, loss: 0.0750342309474945\n","step: 2870, loss: 0.04495777189731598\n","step: 2880, loss: 0.04957134276628494\n","step: 2890, loss: 0.057614345103502274\n","step: 2900, loss: 0.21018388867378235\n","step: 2910, loss: 0.11436732858419418\n","step: 2920, loss: 0.026414591819047928\n","step: 2930, loss: 0.06319377571344376\n","step: 2940, loss: 0.08849798887968063\n","step: 2950, loss: 0.07091661542654037\n","step: 2960, loss: 0.19134443998336792\n","step: 2970, loss: 0.06561122834682465\n","step: 2980, loss: 0.027858292683959007\n","step: 2990, loss: 0.15543732047080994\n","step: 3000, loss: 0.0566275492310524\n","step: 3010, loss: 0.041685305535793304\n","step: 3020, loss: 0.02112746424973011\n","step: 3030, loss: 0.11887333542108536\n","step: 3040, loss: 0.02454354055225849\n","step: 3050, loss: 0.05993923917412758\n","step: 3060, loss: 0.028023498132824898\n","step: 3070, loss: 0.16848540306091309\n","step: 3080, loss: 0.12483720481395721\n","step: 3090, loss: 0.033507198095321655\n","step: 3100, loss: 0.10701422393321991\n","step: 3110, loss: 0.07196300476789474\n","step: 3120, loss: 0.06747052818536758\n","step: 3130, loss: 0.21710538864135742\n","step: 3140, loss: 0.09358389675617218\n","step: 3150, loss: 0.11942072212696075\n","step: 3160, loss: 0.02048848569393158\n","step: 3170, loss: 0.06460614502429962\n","step: 3180, loss: 0.05497879534959793\n","step: 3190, loss: 0.15836958587169647\n","step: 3200, loss: 0.06623745709657669\n","step: 3210, loss: 0.06441447883844376\n","step: 3220, loss: 0.028981909155845642\n","step: 3230, loss: 0.09002883732318878\n","step: 3240, loss: 0.08256997168064117\n","step: 3250, loss: 0.09657549113035202\n","step: 3260, loss: 0.12237411737442017\n","step: 3270, loss: 0.06796686351299286\n","step: 3280, loss: 0.11699250340461731\n","step: 3290, loss: 0.006136590614914894\n","step: 3300, loss: 0.0899810865521431\n","step: 3310, loss: 0.08585356175899506\n","step: 3320, loss: 0.06931284070014954\n","step: 3330, loss: 0.040532033890485764\n","step: 3340, loss: 0.08221109956502914\n","step: 3350, loss: 0.16558296978473663\n","step: 3360, loss: 0.10629697889089584\n","step: 3370, loss: 0.10521405190229416\n","step: 3380, loss: 0.0975131019949913\n","step: 3390, loss: 0.027109406888484955\n","step: 3400, loss: 0.049333397299051285\n","step: 3410, loss: 0.08384597301483154\n","step: 3420, loss: 0.12752768397331238\n","step: 3430, loss: 0.096384197473526\n","step: 3440, loss: 0.06949982792139053\n","step: 3450, loss: 0.09694215655326843\n","step: 3460, loss: 0.08191858232021332\n","step: 3470, loss: 0.10176539421081543\n","step: 3480, loss: 0.07142727077007294\n","step: 3490, loss: 0.052647288888692856\n","step: 3500, loss: 0.04648499935865402\n","step: 3510, loss: 0.17730171978473663\n","step: 3520, loss: 0.12962479889392853\n","step: 3530, loss: 0.06481967121362686\n","step: 3540, loss: 0.12606124579906464\n","step: 3550, loss: 0.0637178048491478\n","step: 3560, loss: 0.05782472342252731\n","step: 3570, loss: 0.05403353273868561\n","step: 3580, loss: 0.02473825216293335\n","step: 3590, loss: 0.0933842808008194\n","step: 3600, loss: 0.10423154383897781\n","step: 3610, loss: 0.05626041814684868\n","step: 3620, loss: 0.058332864195108414\n","step: 3630, loss: 0.021894965320825577\n","step: 3640, loss: 0.10102821886539459\n","step: 3650, loss: 0.05912439897656441\n","step: 3660, loss: 0.0871289074420929\n","step: 3670, loss: 0.020164115354418755\n","step: 3680, loss: 0.20847859978675842\n","step: 3690, loss: 0.05620556324720383\n","step: 3700, loss: 0.09040343016386032\n","step: 3710, loss: 0.11616526544094086\n","step: 3720, loss: 0.049489524215459824\n","step: 3730, loss: 0.1844872534275055\n","step: 3740, loss: 0.1273578703403473\n","step: 3750, loss: 0.08038771152496338\n","step: 3760, loss: 0.0872744545340538\n","step: 3770, loss: 0.11864028126001358\n","step: 3780, loss: 0.0933796763420105\n","step: 3790, loss: 0.12081290036439896\n","step: 3800, loss: 0.12852412462234497\n","step: 3810, loss: 0.07608062773942947\n","step: 3820, loss: 0.09853016585111618\n","step: 3830, loss: 0.06092881038784981\n","step: 3840, loss: 0.13369275629520416\n","step: 3850, loss: 0.029869571328163147\n","step: 3860, loss: 0.022867964580655098\n","step: 3870, loss: 0.06059718132019043\n","step: 3880, loss: 0.03150278329849243\n","step: 3890, loss: 0.12959474325180054\n","step: 3900, loss: 0.24181944131851196\n","step: 3910, loss: 0.03907868266105652\n","step: 3920, loss: 0.11197851598262787\n","step: 3930, loss: 0.0786672979593277\n","step: 3940, loss: 0.08071305602788925\n","step: 3950, loss: 0.11345991492271423\n","step: 3960, loss: 0.06430453807115555\n","step: 3970, loss: 0.10656817257404327\n","step: 3980, loss: 0.04430712014436722\n","step: 3990, loss: 0.05100266635417938\n","step: 4000, loss: 0.16224069893360138\n","step: 4010, loss: 0.11744187027215958\n","step: 4020, loss: 0.08979310840368271\n","step: 4030, loss: 0.08680989593267441\n","step: 4040, loss: 0.06623780727386475\n","step: 4050, loss: 0.06896988302469254\n","step: 4060, loss: 0.06346163153648376\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.74      0.87      0.80        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.99      0.84      0.91       291\n","           5       0.90      0.84      0.87       294\n","           6       0.99      0.98      0.99      1570\n","           7       0.58      0.94      0.71       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.35      0.69      0.46        13\n","          14       0.33      1.00      0.50        43\n","          15       0.93      0.98      0.95      2778\n","          16       0.91      0.81      0.86      1151\n","          17       0.83      0.98      0.90        41\n","          18       0.91      1.00      0.96        32\n","          19       0.74      0.35      0.47        40\n","          20       0.99      0.97      0.98       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.72      0.82      4175\n","          23       0.67      0.98      0.79      2253\n","          24       0.27      0.50      0.35        44\n","          25       0.87      0.89      0.88       888\n","          26       0.82      1.00      0.90         9\n","          27       0.84      1.00      0.91        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.92      0.84      0.88      1136\n","          31       0.62      0.53      0.57        19\n","          32       1.00      0.50      0.67         8\n","          33       0.73      0.93      0.82        86\n","          34       0.26      0.62      0.36        32\n","          35       0.97      1.00      0.98       474\n","          36       0.96      0.14      0.24       182\n","          37       0.88      0.95      0.92      1592\n","          38       0.98      0.97      0.98       404\n","          39       0.96      0.93      0.95       485\n","          40       0.89      0.96      0.92       573\n","          41       0.93      0.95      0.94       841\n","          42       0.98      0.98      0.98       575\n","          43       0.96      0.74      0.84       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.44      0.09      0.15        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.80      0.81      0.78     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","\n","Loop 3\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9339003562927246\n","step: 10, loss: 2.289106845855713\n","step: 20, loss: 0.7053303718566895\n","step: 30, loss: 0.6224756240844727\n","step: 40, loss: 0.3166123330593109\n","step: 50, loss: 0.5051404237747192\n","step: 60, loss: 0.13824541866779327\n","step: 70, loss: 0.08874711394309998\n","step: 80, loss: 0.13650724291801453\n","step: 90, loss: 0.14978356659412384\n","step: 100, loss: 0.14438968896865845\n","step: 110, loss: 0.25198033452033997\n","step: 120, loss: 0.07747942209243774\n","step: 130, loss: 0.12324719876050949\n","step: 140, loss: 0.22597134113311768\n","step: 150, loss: 0.27523043751716614\n","step: 160, loss: 0.15472295880317688\n","step: 170, loss: 0.09249476343393326\n","step: 180, loss: 0.17971962690353394\n","step: 190, loss: 0.06000003591179848\n","step: 200, loss: 0.21116024255752563\n","step: 210, loss: 0.1884876787662506\n","step: 220, loss: 0.15460163354873657\n","step: 230, loss: 0.09133506566286087\n","step: 240, loss: 0.17950715124607086\n","step: 250, loss: 0.1579282283782959\n","step: 260, loss: 0.07437818497419357\n","step: 270, loss: 0.14969293773174286\n","step: 280, loss: 0.12466371059417725\n","step: 290, loss: 0.1547568440437317\n","step: 300, loss: 0.0881371721625328\n","step: 310, loss: 0.17403274774551392\n","step: 320, loss: 0.09495767951011658\n","step: 330, loss: 0.1559533029794693\n","step: 340, loss: 0.04019966721534729\n","step: 350, loss: 0.13899539411067963\n","step: 360, loss: 0.0880427211523056\n","step: 370, loss: 0.05602578446269035\n","step: 380, loss: 0.21625788509845734\n","step: 390, loss: 0.11059137433767319\n","step: 400, loss: 0.20770764350891113\n","step: 410, loss: 0.10050936043262482\n","step: 420, loss: 0.1779627799987793\n","step: 430, loss: 0.22812499105930328\n","step: 440, loss: 0.15269921720027924\n","step: 450, loss: 0.03846278786659241\n","step: 460, loss: 0.12712720036506653\n","step: 470, loss: 0.1853308379650116\n","step: 480, loss: 0.04840990528464317\n","step: 490, loss: 0.1143856942653656\n","step: 500, loss: 0.13436289131641388\n","step: 510, loss: 0.12395542860031128\n","step: 520, loss: 0.04436875879764557\n","step: 530, loss: 0.016966836526989937\n","step: 540, loss: 0.17693553864955902\n","step: 550, loss: 0.03214969485998154\n","step: 560, loss: 0.19668158888816833\n","step: 570, loss: 0.09674756973981857\n","step: 580, loss: 0.1310449093580246\n","step: 590, loss: 0.03564944490790367\n","step: 600, loss: 0.05630488693714142\n","step: 610, loss: 0.13886255025863647\n","step: 620, loss: 0.07613485306501389\n","step: 630, loss: 0.06740028411149979\n","step: 640, loss: 0.1484876275062561\n","step: 650, loss: 0.1943875402212143\n","step: 660, loss: 0.10754035413265228\n","step: 670, loss: 0.2134823054075241\n","step: 680, loss: 0.08791709691286087\n","step: 690, loss: 0.021440429612994194\n","step: 700, loss: 0.05914076045155525\n","step: 710, loss: 0.047971926629543304\n","step: 720, loss: 0.1312355399131775\n","step: 730, loss: 0.05452454090118408\n","step: 740, loss: 0.037067610770463943\n","step: 750, loss: 0.04328063875436783\n","step: 760, loss: 0.08494937419891357\n","step: 770, loss: 0.0832926407456398\n","step: 780, loss: 0.10106508433818817\n","step: 790, loss: 0.14313970506191254\n","step: 800, loss: 0.039095427840948105\n","step: 810, loss: 0.05982315540313721\n","step: 820, loss: 0.09882840514183044\n","step: 830, loss: 0.022387944161891937\n","step: 840, loss: 0.1216140165925026\n","step: 850, loss: 0.09690172970294952\n","step: 860, loss: 0.1019720807671547\n","step: 870, loss: 0.15774653851985931\n","step: 880, loss: 0.04438761621713638\n","step: 890, loss: 0.18092116713523865\n","step: 900, loss: 0.0781412273645401\n","step: 910, loss: 0.11677384376525879\n","step: 920, loss: 0.09099576622247696\n","step: 930, loss: 0.13676874339580536\n","step: 940, loss: 0.11551181972026825\n","step: 950, loss: 0.14656201004981995\n","step: 960, loss: 0.1329759806394577\n","step: 970, loss: 0.07352671027183533\n","step: 980, loss: 0.14475098252296448\n","step: 990, loss: 0.08827681094408035\n","step: 1000, loss: 0.07892431318759918\n","step: 1010, loss: 0.19989243149757385\n","step: 1020, loss: 0.042566604912281036\n","step: 1030, loss: 0.11333581805229187\n","step: 1040, loss: 0.09015951305627823\n","step: 1050, loss: 0.1323188841342926\n","step: 1060, loss: 0.10792391002178192\n","step: 1070, loss: 0.11768556386232376\n","step: 1080, loss: 0.11617054790258408\n","step: 1090, loss: 0.1485498696565628\n","step: 1100, loss: 0.20971371233463287\n","step: 1110, loss: 0.05125947669148445\n","step: 1120, loss: 0.05247218534350395\n","step: 1130, loss: 0.11436016857624054\n","step: 1140, loss: 0.09024900197982788\n","step: 1150, loss: 0.10139116644859314\n","step: 1160, loss: 0.03387020155787468\n","step: 1170, loss: 0.1778934746980667\n","step: 1180, loss: 0.08918939530849457\n","step: 1190, loss: 0.10337283462285995\n","step: 1200, loss: 0.10024945437908173\n","step: 1210, loss: 0.09056774526834488\n","step: 1220, loss: 0.12640678882598877\n","step: 1230, loss: 0.07267310470342636\n","step: 1240, loss: 0.0685867965221405\n","step: 1250, loss: 0.08237352222204208\n","step: 1260, loss: 0.07516303658485413\n","step: 1270, loss: 0.08591904491186142\n","step: 1280, loss: 0.17148584127426147\n","step: 1290, loss: 0.11197501420974731\n","step: 1300, loss: 0.10700978338718414\n","step: 1310, loss: 0.17255957424640656\n","step: 1320, loss: 0.08630739152431488\n","step: 1330, loss: 0.1126934215426445\n","step: 1340, loss: 0.17551879584789276\n","step: 1350, loss: 0.049548398703336716\n","step: 1360, loss: 0.05310087278485298\n","step: 1370, loss: 0.11599993705749512\n","step: 1380, loss: 0.05986444652080536\n","step: 1390, loss: 0.06429923325777054\n","step: 1400, loss: 0.18333446979522705\n","step: 1410, loss: 0.08781588077545166\n","step: 1420, loss: 0.11814490705728531\n","step: 1430, loss: 0.06369756162166595\n","step: 1440, loss: 0.10855834186077118\n","step: 1450, loss: 0.034281857311725616\n","step: 1460, loss: 0.05985217168927193\n","step: 1470, loss: 0.12228970229625702\n","step: 1480, loss: 0.12827666103839874\n","step: 1490, loss: 0.06502439826726913\n","step: 1500, loss: 0.15063904225826263\n","step: 1510, loss: 0.07911936193704605\n","step: 1520, loss: 0.09591936320066452\n","step: 1530, loss: 0.09374557435512543\n","step: 1540, loss: 0.03818461298942566\n","step: 1550, loss: 0.05606045573949814\n","step: 1560, loss: 0.08587341755628586\n","step: 1570, loss: 0.08775748312473297\n","step: 1580, loss: 0.07420790940523148\n","step: 1590, loss: 0.08402753621339798\n","step: 1600, loss: 0.11777432262897491\n","step: 1610, loss: 0.0453442744910717\n","step: 1620, loss: 0.11443626880645752\n","step: 1630, loss: 0.12417180836200714\n","step: 1640, loss: 0.09294450283050537\n","step: 1650, loss: 0.14515921473503113\n","step: 1660, loss: 0.02044447883963585\n","step: 1670, loss: 0.03902382031083107\n","step: 1680, loss: 0.06253506988286972\n","step: 1690, loss: 0.08027991652488708\n","step: 1700, loss: 0.1508249193429947\n","step: 1710, loss: 0.028635963797569275\n","step: 1720, loss: 0.07380757480859756\n","step: 1730, loss: 0.07711482793092728\n","step: 1740, loss: 0.08656105399131775\n","step: 1750, loss: 0.07543064653873444\n","step: 1760, loss: 0.09563751518726349\n","step: 1770, loss: 0.07961025089025497\n","step: 1780, loss: 0.04605608433485031\n","step: 1790, loss: 0.11960786581039429\n","step: 1800, loss: 0.06095215305685997\n","step: 1810, loss: 0.1128411814570427\n","step: 1820, loss: 0.09620367735624313\n","step: 1830, loss: 0.03672415390610695\n","step: 1840, loss: 0.1433582603931427\n","step: 1850, loss: 0.08425119519233704\n","step: 1860, loss: 0.1226535513997078\n","step: 1870, loss: 0.07978162914514542\n","step: 1880, loss: 0.05021458491683006\n","step: 1890, loss: 0.08820564299821854\n","step: 1900, loss: 0.07188486307859421\n","step: 1910, loss: 0.06730406731367111\n","step: 1920, loss: 0.0746319517493248\n","step: 1930, loss: 0.10707976669073105\n","step: 1940, loss: 0.3462275564670563\n","step: 1950, loss: 0.1356537789106369\n","step: 1960, loss: 0.09734674543142319\n","step: 1970, loss: 0.0721169039607048\n","step: 1980, loss: 0.08955429494380951\n","step: 1990, loss: 0.10071253031492233\n","step: 2000, loss: 0.06633664667606354\n","step: 2010, loss: 0.14569303393363953\n","step: 2020, loss: 0.060456082224845886\n","step: 2030, loss: 0.06285364180803299\n","step: 2040, loss: 0.11098047345876694\n","step: 2050, loss: 0.10686415433883667\n","step: 2060, loss: 0.23711299896240234\n","step: 2070, loss: 0.06283212453126907\n","step: 2080, loss: 0.08794094622135162\n","step: 2090, loss: 0.11146114021539688\n","step: 2100, loss: 0.05026330053806305\n","step: 2110, loss: 0.0894690677523613\n","step: 2120, loss: 0.12770019471645355\n","step: 2130, loss: 0.061145149171352386\n","step: 2140, loss: 0.13813453912734985\n","step: 2150, loss: 0.07420583814382553\n","step: 2160, loss: 0.2627265751361847\n","step: 2170, loss: 0.14478710293769836\n","step: 2180, loss: 0.13288848102092743\n","step: 2190, loss: 0.18340730667114258\n","step: 2200, loss: 0.09756866097450256\n","step: 2210, loss: 0.040854472666978836\n","step: 2220, loss: 0.1285550743341446\n","step: 2230, loss: 0.10174692422151566\n","step: 2240, loss: 0.02495056577026844\n","step: 2250, loss: 0.06705927848815918\n","step: 2260, loss: 0.15195928514003754\n","step: 2270, loss: 0.07372117042541504\n","step: 2280, loss: 0.08372954279184341\n","step: 2290, loss: 0.10301487147808075\n","step: 2300, loss: 0.16881327331066132\n","step: 2310, loss: 0.11094604432582855\n","step: 2320, loss: 0.060912687331438065\n","step: 2330, loss: 0.054161593317985535\n","step: 2340, loss: 0.01989905536174774\n","step: 2350, loss: 0.09003319591283798\n","step: 2360, loss: 0.11536817252635956\n","step: 2370, loss: 0.1135765090584755\n","step: 2380, loss: 0.1277492493391037\n","step: 2390, loss: 0.08256186544895172\n","step: 2400, loss: 0.14803694188594818\n","step: 2410, loss: 0.061131272464990616\n","step: 2420, loss: 0.06865712255239487\n","step: 2430, loss: 0.08814510703086853\n","step: 2440, loss: 0.08213403820991516\n","step: 2450, loss: 0.08186992257833481\n","step: 2460, loss: 0.21788917481899261\n","step: 2470, loss: 0.08801838755607605\n","step: 2480, loss: 0.0697188675403595\n","step: 2490, loss: 0.07651019841432571\n","step: 2500, loss: 0.13986262679100037\n","step: 2510, loss: 0.10173799842596054\n","step: 2520, loss: 0.1584119200706482\n","step: 2530, loss: 0.08036689460277557\n","step: 2540, loss: 0.06097523123025894\n","step: 2550, loss: 0.04988550767302513\n","step: 2560, loss: 0.0841539204120636\n","step: 2570, loss: 0.16374754905700684\n","step: 2580, loss: 0.06744024902582169\n","step: 2590, loss: 0.025177927687764168\n","step: 2600, loss: 0.0782531127333641\n","step: 2610, loss: 0.040887922048568726\n","step: 2620, loss: 0.08854281157255173\n","step: 2630, loss: 0.05313935875892639\n","step: 2640, loss: 0.031855467706918716\n","step: 2650, loss: 0.08765489608049393\n","step: 2660, loss: 0.029143506661057472\n","step: 2670, loss: 0.078516885638237\n","step: 2680, loss: 0.05935431644320488\n","step: 2690, loss: 0.07286053895950317\n","step: 2700, loss: 0.07403302937746048\n","step: 2710, loss: 0.0961255133152008\n","step: 2720, loss: 0.09982095658779144\n","step: 2730, loss: 0.04589298367500305\n","step: 2740, loss: 0.06672932207584381\n","step: 2750, loss: 0.20302292704582214\n","step: 2760, loss: 0.11229075491428375\n","step: 2770, loss: 0.034957148134708405\n","step: 2780, loss: 0.017234064638614655\n","step: 2790, loss: 0.13135088980197906\n","step: 2800, loss: 0.10423324257135391\n","step: 2810, loss: 0.0779053196310997\n","step: 2820, loss: 0.10226992517709732\n","step: 2830, loss: 0.04674052074551582\n","step: 2840, loss: 0.12067737430334091\n","step: 2850, loss: 0.12702831625938416\n","step: 2860, loss: 0.07480836659669876\n","step: 2870, loss: 0.07752814143896103\n","step: 2880, loss: 0.03233702480792999\n","step: 2890, loss: 0.08912775665521622\n","step: 2900, loss: 0.0703689306974411\n","step: 2910, loss: 0.05603772774338722\n","step: 2920, loss: 0.12180858105421066\n","step: 2930, loss: 0.011693904176354408\n","step: 2940, loss: 0.07242275774478912\n","step: 2950, loss: 0.05443114414811134\n","step: 2960, loss: 0.11091214418411255\n","step: 2970, loss: 0.2352985441684723\n","step: 2980, loss: 0.09681805968284607\n","step: 2990, loss: 0.1266688108444214\n","step: 3000, loss: 0.0728466659784317\n","step: 3010, loss: 0.029309701174497604\n","step: 3020, loss: 0.12972807884216309\n","step: 3030, loss: 0.09210759401321411\n","step: 3040, loss: 0.040449585765600204\n","step: 3050, loss: 0.1002165749669075\n","step: 3060, loss: 0.059454165399074554\n","step: 3070, loss: 0.10333321243524551\n","step: 3080, loss: 0.11227746307849884\n","step: 3090, loss: 0.0748434066772461\n","step: 3100, loss: 0.039721954613924026\n","step: 3110, loss: 0.14566515386104584\n","step: 3120, loss: 0.07473281025886536\n","step: 3130, loss: 0.06074654310941696\n","step: 3140, loss: 0.06495477259159088\n","step: 3150, loss: 0.07470592111349106\n","step: 3160, loss: 0.10828573256731033\n","step: 3170, loss: 0.08458999544382095\n","step: 3180, loss: 0.07394719123840332\n","step: 3190, loss: 0.21948431432247162\n","step: 3200, loss: 0.028769053518772125\n","step: 3210, loss: 0.05642939731478691\n","step: 3220, loss: 0.05595200136303902\n","step: 3230, loss: 0.16116644442081451\n","step: 3240, loss: 0.07583946734666824\n","step: 3250, loss: 0.08545060455799103\n","step: 3260, loss: 0.12015979737043381\n","step: 3270, loss: 0.08239928632974625\n","step: 3280, loss: 0.10543056577444077\n","step: 3290, loss: 0.056431472301483154\n","step: 3300, loss: 0.08082426339387894\n","step: 3310, loss: 0.07352667301893234\n","step: 3320, loss: 0.11882363259792328\n","step: 3330, loss: 0.0359790213406086\n","step: 3340, loss: 0.0901225283741951\n","step: 3350, loss: 0.08957736194133759\n","step: 3360, loss: 0.075987808406353\n","step: 3370, loss: 0.15107044577598572\n","step: 3380, loss: 0.03775089606642723\n","step: 3390, loss: 0.15122514963150024\n","step: 3400, loss: 0.19682005047798157\n","step: 3410, loss: 0.10453176498413086\n","step: 3420, loss: 0.09365898370742798\n","step: 3430, loss: 0.1122397780418396\n","step: 3440, loss: 0.042215608060359955\n","step: 3450, loss: 0.1174086406826973\n","step: 3460, loss: 0.12402740120887756\n","step: 3470, loss: 0.07612313330173492\n","step: 3480, loss: 0.09860079735517502\n","step: 3490, loss: 0.05240263044834137\n","step: 3500, loss: 0.05262620002031326\n","step: 3510, loss: 0.04444192349910736\n","step: 3520, loss: 0.06997919827699661\n","step: 3530, loss: 0.029288288205862045\n","step: 3540, loss: 0.0782037302851677\n","step: 3550, loss: 0.0938563123345375\n","step: 3560, loss: 0.016529960557818413\n","step: 3570, loss: 0.06712248921394348\n","step: 3580, loss: 0.03737162426114082\n","step: 3590, loss: 0.191367506980896\n","step: 3600, loss: 0.057726334780454636\n","step: 3610, loss: 0.1383993923664093\n","step: 3620, loss: 0.1534171998500824\n","step: 3630, loss: 0.10285727679729462\n","step: 3640, loss: 0.12976126372814178\n","step: 3650, loss: 0.05987871438264847\n","step: 3660, loss: 0.1477559506893158\n","step: 3670, loss: 0.12091724574565887\n","step: 3680, loss: 0.10702932626008987\n","step: 3690, loss: 0.09326813369989395\n","step: 3700, loss: 0.04043702781200409\n","step: 3710, loss: 0.04402238503098488\n","step: 3720, loss: 0.10853561013936996\n","step: 3730, loss: 0.07419930398464203\n","step: 3740, loss: 0.12800189852714539\n","step: 3750, loss: 0.04426785558462143\n","step: 3760, loss: 0.09997982531785965\n","step: 3770, loss: 0.05497707054018974\n","step: 3780, loss: 0.09102398157119751\n","step: 3790, loss: 0.07745888084173203\n","step: 3800, loss: 0.0892249345779419\n","step: 3810, loss: 0.051438577473163605\n","step: 3820, loss: 0.07698144018650055\n","step: 3830, loss: 0.13512493669986725\n","step: 3840, loss: 0.08083821088075638\n","step: 3850, loss: 0.010175744071602821\n","step: 3860, loss: 0.24187566339969635\n","step: 3870, loss: 0.08369473367929459\n","step: 3880, loss: 0.1270526647567749\n","step: 3890, loss: 0.06484317034482956\n","step: 3900, loss: 0.12110207974910736\n","step: 3910, loss: 0.042166270315647125\n","step: 3920, loss: 0.03104073740541935\n","step: 3930, loss: 0.1030924916267395\n","step: 3940, loss: 0.07629844546318054\n","step: 3950, loss: 0.06506426632404327\n","step: 3960, loss: 0.11661513894796371\n","step: 3970, loss: 0.06312409788370132\n","step: 3980, loss: 0.06155773252248764\n","step: 3990, loss: 0.07096181064844131\n","step: 4000, loss: 0.026319531723856926\n","step: 4010, loss: 0.08053886890411377\n","step: 4020, loss: 0.08776989579200745\n","step: 4030, loss: 0.034994009882211685\n","step: 4040, loss: 0.08540025353431702\n","step: 4050, loss: 0.22864699363708496\n","step: 4060, loss: 0.08222115784883499\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.83      0.83      0.83        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.95      0.84      0.89       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.99      0.99      1570\n","           7       0.55      0.94      0.69       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.18      0.54      0.27        13\n","          14       0.31      1.00      0.48        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.84      1.00      0.91        41\n","          18       0.97      0.91      0.94        32\n","          19       0.87      0.65      0.74        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.96      0.73      0.83      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.28      0.36      0.32        44\n","          25       0.85      0.92      0.89       888\n","          26       1.00      0.78      0.88         9\n","          27       0.95      1.00      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.93      0.88      0.91      1136\n","          31       0.58      0.58      0.58        19\n","          32       0.70      0.88      0.78         8\n","          33       0.64      0.97      0.77        86\n","          34       0.24      0.53      0.33        32\n","          35       0.99      0.99      0.99       474\n","          36       0.79      0.14      0.24       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.96      0.98      0.97       404\n","          39       0.96      0.96      0.96       485\n","          40       0.94      0.89      0.92       573\n","          41       0.96      0.93      0.94       841\n","          42       0.99      0.98      0.99       575\n","          43       0.97      0.82      0.89       152\n","          44       0.88      0.95      0.91        75\n","          46       1.00      0.99      0.99        82\n","          48       0.13      0.04      0.06        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.79      0.81      0.78     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","Loop 4\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9027211666107178\n","step: 10, loss: 1.8218066692352295\n","step: 20, loss: 0.5341317653656006\n","step: 30, loss: 0.373980313539505\n","step: 40, loss: 0.2784781754016876\n","step: 50, loss: 0.14584752917289734\n","step: 60, loss: 0.2054794728755951\n","step: 70, loss: 0.3061464726924896\n","step: 80, loss: 0.25042346119880676\n","step: 90, loss: 0.3166840672492981\n","step: 100, loss: 0.21608568727970123\n","step: 110, loss: 0.08802615106105804\n","step: 120, loss: 0.12345367670059204\n","step: 130, loss: 0.14347471296787262\n","step: 140, loss: 0.18266521394252777\n","step: 150, loss: 0.03799182176589966\n","step: 160, loss: 0.1504223495721817\n","step: 170, loss: 0.11642548441886902\n","step: 180, loss: 0.34273630380630493\n","step: 190, loss: 0.10991539806127548\n","step: 200, loss: 0.16636116802692413\n","step: 210, loss: 0.07686644792556763\n","step: 220, loss: 0.10445191711187363\n","step: 230, loss: 0.16015705466270447\n","step: 240, loss: 0.09220515191555023\n","step: 250, loss: 0.16339239478111267\n","step: 260, loss: 0.21591594815254211\n","step: 270, loss: 0.1088315099477768\n","step: 280, loss: 0.10950484126806259\n","step: 290, loss: 0.1863653063774109\n","step: 300, loss: 0.06955345720052719\n","step: 310, loss: 0.17376141250133514\n","step: 320, loss: 0.0890979990363121\n","step: 330, loss: 0.10122592002153397\n","step: 340, loss: 0.11224646121263504\n","step: 350, loss: 0.20243771374225616\n","step: 360, loss: 0.08498246967792511\n","step: 370, loss: 0.08550462126731873\n","step: 380, loss: 0.11280395090579987\n","step: 390, loss: 0.09682045876979828\n","step: 400, loss: 0.21319837868213654\n","step: 410, loss: 0.1197037547826767\n","step: 420, loss: 0.1455095410346985\n","step: 430, loss: 0.20162302255630493\n","step: 440, loss: 0.0756838321685791\n","step: 450, loss: 0.014229348860681057\n","step: 460, loss: 0.04628434032201767\n","step: 470, loss: 0.1634949892759323\n","step: 480, loss: 0.1609482765197754\n","step: 490, loss: 0.14324110746383667\n","step: 500, loss: 0.14018060266971588\n","step: 510, loss: 0.09032377600669861\n","step: 520, loss: 0.10868396610021591\n","step: 530, loss: 0.06677685678005219\n","step: 540, loss: 0.07005850970745087\n","step: 550, loss: 0.060401447117328644\n","step: 560, loss: 0.07056478410959244\n","step: 570, loss: 0.09468549489974976\n","step: 580, loss: 0.194478377699852\n","step: 590, loss: 0.12558279931545258\n","step: 600, loss: 0.0629388689994812\n","step: 610, loss: 0.23137329518795013\n","step: 620, loss: 0.07335692644119263\n","step: 630, loss: 0.09618373215198517\n","step: 640, loss: 0.2091674506664276\n","step: 650, loss: 0.08185787498950958\n","step: 660, loss: 0.059210509061813354\n","step: 670, loss: 0.1480204164981842\n","step: 680, loss: 0.1116543784737587\n","step: 690, loss: 0.10634779185056686\n","step: 700, loss: 0.1140131726861\n","step: 710, loss: 0.10970284044742584\n","step: 720, loss: 0.2668176293373108\n","step: 730, loss: 0.029164355248212814\n","step: 740, loss: 0.03170784190297127\n","step: 750, loss: 0.10617657750844955\n","step: 760, loss: 0.09087934345006943\n","step: 770, loss: 0.032070547342300415\n","step: 780, loss: 0.14450626075267792\n","step: 790, loss: 0.1595963090658188\n","step: 800, loss: 0.08035264164209366\n","step: 810, loss: 0.1611049920320511\n","step: 820, loss: 0.14755825698375702\n","step: 830, loss: 0.08661223948001862\n","step: 840, loss: 0.05139985680580139\n","step: 850, loss: 0.07026185095310211\n","step: 860, loss: 0.11396521329879761\n","step: 870, loss: 0.05730046331882477\n","step: 880, loss: 0.19270111620426178\n","step: 890, loss: 0.2186843901872635\n","step: 900, loss: 0.10336658358573914\n","step: 910, loss: 0.17126598954200745\n","step: 920, loss: 0.11295436322689056\n","step: 930, loss: 0.023104455322027206\n","step: 940, loss: 0.10176651179790497\n","step: 950, loss: 0.13478370010852814\n","step: 960, loss: 0.11431463807821274\n","step: 970, loss: 0.06372368335723877\n","step: 980, loss: 0.059846844524145126\n","step: 990, loss: 0.12446604669094086\n","step: 1000, loss: 0.07865919917821884\n","step: 1010, loss: 0.055757030844688416\n","step: 1020, loss: 0.11352290213108063\n","step: 1030, loss: 0.07113580405712128\n","step: 1040, loss: 0.04090462997555733\n","step: 1050, loss: 0.12039609998464584\n","step: 1060, loss: 0.046297721564769745\n","step: 1070, loss: 0.10878132283687592\n","step: 1080, loss: 0.07771170884370804\n","step: 1090, loss: 0.042587634176015854\n","step: 1100, loss: 0.11214274913072586\n","step: 1110, loss: 0.10793481022119522\n","step: 1120, loss: 0.0793214961886406\n","step: 1130, loss: 0.12261684983968735\n","step: 1140, loss: 0.1743854284286499\n","step: 1150, loss: 0.05375472456216812\n","step: 1160, loss: 0.09837643802165985\n","step: 1170, loss: 0.1395447999238968\n","step: 1180, loss: 0.11726520955562592\n","step: 1190, loss: 0.08233392238616943\n","step: 1200, loss: 0.1414860188961029\n","step: 1210, loss: 0.0795823186635971\n","step: 1220, loss: 0.13314998149871826\n","step: 1230, loss: 0.06327923387289047\n","step: 1240, loss: 0.10502775758504868\n","step: 1250, loss: 0.09772007167339325\n","step: 1260, loss: 0.09887883067131042\n","step: 1270, loss: 0.12294599413871765\n","step: 1280, loss: 0.1330898106098175\n","step: 1290, loss: 0.12045367807149887\n","step: 1300, loss: 0.10509444028139114\n","step: 1310, loss: 0.10329028218984604\n","step: 1320, loss: 0.13319702446460724\n","step: 1330, loss: 0.09907249361276627\n","step: 1340, loss: 0.10164809226989746\n","step: 1350, loss: 0.04776934161782265\n","step: 1360, loss: 0.18077804148197174\n","step: 1370, loss: 0.10620113462209702\n","step: 1380, loss: 0.10976897180080414\n","step: 1390, loss: 0.07869902998209\n","step: 1400, loss: 0.035828977823257446\n","step: 1410, loss: 0.08743426203727722\n","step: 1420, loss: 0.12263041734695435\n","step: 1430, loss: 0.075968436896801\n","step: 1440, loss: 0.07097497582435608\n","step: 1450, loss: 0.06272822618484497\n","step: 1460, loss: 0.09841466695070267\n","step: 1470, loss: 0.13754066824913025\n","step: 1480, loss: 0.15189313888549805\n","step: 1490, loss: 0.11899746209383011\n","step: 1500, loss: 0.11292090266942978\n","step: 1510, loss: 0.05528898164629936\n","step: 1520, loss: 0.14451102912425995\n","step: 1530, loss: 0.13996723294258118\n","step: 1540, loss: 0.046032942831516266\n","step: 1550, loss: 0.09805034101009369\n","step: 1560, loss: 0.09320022910833359\n","step: 1570, loss: 0.05984591692686081\n","step: 1580, loss: 0.04845857620239258\n","step: 1590, loss: 0.12165039032697678\n","step: 1600, loss: 0.11815541982650757\n","step: 1610, loss: 0.12734355032444\n","step: 1620, loss: 0.09827826172113419\n","step: 1630, loss: 0.13820840418338776\n","step: 1640, loss: 0.07193831354379654\n","step: 1650, loss: 0.09168434143066406\n","step: 1660, loss: 0.12422750145196915\n","step: 1670, loss: 0.0783424973487854\n","step: 1680, loss: 0.15843167901039124\n","step: 1690, loss: 0.047073785215616226\n","step: 1700, loss: 0.08768166601657867\n","step: 1710, loss: 0.07122298330068588\n","step: 1720, loss: 0.0466136559844017\n","step: 1730, loss: 0.14609603583812714\n","step: 1740, loss: 0.13484187424182892\n","step: 1750, loss: 0.06534279882907867\n","step: 1760, loss: 0.0890577957034111\n","step: 1770, loss: 0.20326241850852966\n","step: 1780, loss: 0.1299726665019989\n","step: 1790, loss: 0.07654494792222977\n","step: 1800, loss: 0.08431302011013031\n","step: 1810, loss: 0.06019839644432068\n","step: 1820, loss: 0.059631768614053726\n","step: 1830, loss: 0.09155271202325821\n","step: 1840, loss: 0.031771834939718246\n","step: 1850, loss: 0.029985325410962105\n","step: 1860, loss: 0.11967969685792923\n","step: 1870, loss: 0.6509661674499512\n","step: 1880, loss: 0.042291950434446335\n","step: 1890, loss: 0.08596794307231903\n","step: 1900, loss: 0.06333529949188232\n","step: 1910, loss: 0.21641656756401062\n","step: 1920, loss: 0.08921407908201218\n","step: 1930, loss: 0.11753888428211212\n","step: 1940, loss: 0.09259307384490967\n","step: 1950, loss: 0.030514931306242943\n","step: 1960, loss: 0.031118987128138542\n","step: 1970, loss: 0.10868910700082779\n","step: 1980, loss: 0.10332541912794113\n","step: 1990, loss: 0.041893597692251205\n","step: 2000, loss: 0.08345834165811539\n","step: 2010, loss: 0.12053725123405457\n","step: 2020, loss: 0.06468696892261505\n","step: 2030, loss: 0.0620604008436203\n","step: 2040, loss: 0.0616283193230629\n","step: 2050, loss: 0.15145553648471832\n","step: 2060, loss: 0.028628891333937645\n","step: 2070, loss: 0.12011782824993134\n","step: 2080, loss: 0.13630029559135437\n","step: 2090, loss: 0.10414110869169235\n","step: 2100, loss: 0.07931819558143616\n","step: 2110, loss: 0.20249110460281372\n","step: 2120, loss: 0.0869482159614563\n","step: 2130, loss: 0.10239937156438828\n","step: 2140, loss: 0.1472111940383911\n","step: 2150, loss: 0.13285256922245026\n","step: 2160, loss: 0.11732880771160126\n","step: 2170, loss: 0.047783203423023224\n","step: 2180, loss: 0.04584954306483269\n","step: 2190, loss: 0.07386867702007294\n","step: 2200, loss: 0.10511215776205063\n","step: 2210, loss: 0.07464388012886047\n","step: 2220, loss: 0.07482707500457764\n","step: 2230, loss: 0.11514424532651901\n","step: 2240, loss: 0.028009537607431412\n","step: 2250, loss: 0.04883068799972534\n","step: 2260, loss: 0.10025665909051895\n","step: 2270, loss: 0.09978177398443222\n","step: 2280, loss: 0.05295957997441292\n","step: 2290, loss: 0.13372546434402466\n","step: 2300, loss: 0.057264938950538635\n","step: 2310, loss: 0.04176536574959755\n","step: 2320, loss: 0.08627012372016907\n","step: 2330, loss: 0.055213525891304016\n","step: 2340, loss: 0.13091997802257538\n","step: 2350, loss: 0.10402495414018631\n","step: 2360, loss: 0.036956530064344406\n","step: 2370, loss: 0.0832073837518692\n","step: 2380, loss: 0.05332321673631668\n","step: 2390, loss: 0.08951981365680695\n","step: 2400, loss: 0.4647339880466461\n","step: 2410, loss: 0.03394955396652222\n","step: 2420, loss: 0.06299419701099396\n","step: 2430, loss: 0.11136552691459656\n","step: 2440, loss: 0.05201125890016556\n","step: 2450, loss: 0.015520519576966763\n","step: 2460, loss: 0.062400855123996735\n","step: 2470, loss: 0.1033228188753128\n","step: 2480, loss: 0.2141866236925125\n","step: 2490, loss: 0.12843160331249237\n","step: 2500, loss: 0.08680309355258942\n","step: 2510, loss: 0.12951718270778656\n","step: 2520, loss: 0.13924676179885864\n","step: 2530, loss: 0.16317330300807953\n","step: 2540, loss: 0.07176142930984497\n","step: 2550, loss: 0.111533023416996\n","step: 2560, loss: 0.08829153329133987\n","step: 2570, loss: 0.05714160203933716\n","step: 2580, loss: 0.06849076598882675\n","step: 2590, loss: 0.25936204195022583\n","step: 2600, loss: 0.0745982825756073\n","step: 2610, loss: 0.11590608954429626\n","step: 2620, loss: 0.060079175978899\n","step: 2630, loss: 0.1303175836801529\n","step: 2640, loss: 0.09615285694599152\n","step: 2650, loss: 0.027253320440649986\n","step: 2660, loss: 0.05959567055106163\n","step: 2670, loss: 0.12972316145896912\n","step: 2680, loss: 0.10583942383527756\n","step: 2690, loss: 0.03439486026763916\n","step: 2700, loss: 0.09183105826377869\n","step: 2710, loss: 0.10629922151565552\n","step: 2720, loss: 0.021922649815678596\n","step: 2730, loss: 0.04926970973610878\n","step: 2740, loss: 0.03499946370720863\n","step: 2750, loss: 0.11198385804891586\n","step: 2760, loss: 0.07269471138715744\n","step: 2770, loss: 0.03340239077806473\n","step: 2780, loss: 0.11383059620857239\n","step: 2790, loss: 0.06362740695476532\n","step: 2800, loss: 0.09929179400205612\n","step: 2810, loss: 0.08588807284832001\n","step: 2820, loss: 0.07169390469789505\n","step: 2830, loss: 0.16313309967517853\n","step: 2840, loss: 0.06130683049559593\n","step: 2850, loss: 0.05783054605126381\n","step: 2860, loss: 0.03842312470078468\n","step: 2870, loss: 0.043722864240407944\n","step: 2880, loss: 0.0482851043343544\n","step: 2890, loss: 0.1313326507806778\n","step: 2900, loss: 0.07222507148981094\n","step: 2910, loss: 0.037555720657110214\n","step: 2920, loss: 0.08363126963376999\n","step: 2930, loss: 0.061135176569223404\n","step: 2940, loss: 0.08049757778644562\n","step: 2950, loss: 0.031997814774513245\n","step: 2960, loss: 0.10255248844623566\n","step: 2970, loss: 0.09624571353197098\n","step: 2980, loss: 0.27735963463783264\n","step: 2990, loss: 0.12141212075948715\n","step: 3000, loss: 0.04857466742396355\n","step: 3010, loss: 0.10176324099302292\n","step: 3020, loss: 0.08043733984231949\n","step: 3030, loss: 0.1757335364818573\n","step: 3040, loss: 0.10116074234247208\n","step: 3050, loss: 0.030304431915283203\n","step: 3060, loss: 0.13243822753429413\n","step: 3070, loss: 0.07518582046031952\n","step: 3080, loss: 0.10095721483230591\n","step: 3090, loss: 0.06503140926361084\n","step: 3100, loss: 0.04127996042370796\n","step: 3110, loss: 0.07225559651851654\n","step: 3120, loss: 0.11249089241027832\n","step: 3130, loss: 0.08763992786407471\n","step: 3140, loss: 0.10017213970422745\n","step: 3150, loss: 0.026781819760799408\n","step: 3160, loss: 0.051366616040468216\n","step: 3170, loss: 0.05192282050848007\n","step: 3180, loss: 0.14740560948848724\n","step: 3190, loss: 0.0571671687066555\n","step: 3200, loss: 0.030265742912888527\n","step: 3210, loss: 0.020614366978406906\n","step: 3220, loss: 0.05422630161046982\n","step: 3230, loss: 0.028094442561268806\n","step: 3240, loss: 0.09075739234685898\n","step: 3250, loss: 0.046671319752931595\n","step: 3260, loss: 0.038891684263944626\n","step: 3270, loss: 0.04961486905813217\n","step: 3280, loss: 0.10255184024572372\n","step: 3290, loss: 0.06657319515943527\n","step: 3300, loss: 0.10288185626268387\n","step: 3310, loss: 0.04900585860013962\n","step: 3320, loss: 0.0380602665245533\n","step: 3330, loss: 0.060684364289045334\n","step: 3340, loss: 0.028472641482949257\n","step: 3350, loss: 0.11098581552505493\n","step: 3360, loss: 0.04784125089645386\n","step: 3370, loss: 0.052907951176166534\n","step: 3380, loss: 0.10062601417303085\n","step: 3390, loss: 0.17770296335220337\n","step: 3400, loss: 0.17042428255081177\n","step: 3410, loss: 0.049364008009433746\n","step: 3420, loss: 0.08124132454395294\n","step: 3430, loss: 0.06119619682431221\n","step: 3440, loss: 0.10594099014997482\n","step: 3450, loss: 0.12007839232683182\n","step: 3460, loss: 0.04839161038398743\n","step: 3470, loss: 0.1484793871641159\n","step: 3480, loss: 0.18787647783756256\n","step: 3490, loss: 0.1006605476140976\n","step: 3500, loss: 0.06739246100187302\n","step: 3510, loss: 0.09206492453813553\n","step: 3520, loss: 0.053051743656396866\n","step: 3530, loss: 0.10481412708759308\n","step: 3540, loss: 0.07135143131017685\n","step: 3550, loss: 0.1915600597858429\n","step: 3560, loss: 0.08900391310453415\n","step: 3570, loss: 0.1407724767923355\n","step: 3580, loss: 0.126307412981987\n","step: 3590, loss: 0.16720031201839447\n","step: 3600, loss: 0.06928461790084839\n","step: 3610, loss: 0.08363514393568039\n","step: 3620, loss: 0.057070426642894745\n","step: 3630, loss: 0.10641421377658844\n","step: 3640, loss: 0.07582534849643707\n","step: 3650, loss: 0.1062217578291893\n","step: 3660, loss: 0.12235350161790848\n","step: 3670, loss: 0.18634967505931854\n","step: 3680, loss: 0.08992906659841537\n","step: 3690, loss: 0.12016179412603378\n","step: 3700, loss: 0.02383311465382576\n","step: 3710, loss: 0.10019674897193909\n","step: 3720, loss: 0.0541248619556427\n","step: 3730, loss: 0.17208631336688995\n","step: 3740, loss: 0.053909145295619965\n","step: 3750, loss: 0.021088041365146637\n","step: 3760, loss: 0.11178171634674072\n","step: 3770, loss: 0.07509741932153702\n","step: 3780, loss: 0.06882915645837784\n","step: 3790, loss: 0.3626590073108673\n","step: 3800, loss: 0.20326335728168488\n","step: 3810, loss: 0.17194488644599915\n","step: 3820, loss: 0.13262279331684113\n","step: 3830, loss: 0.20778964459896088\n","step: 3840, loss: 0.10574779659509659\n","step: 3850, loss: 0.11699480563402176\n","step: 3860, loss: 0.12337988615036011\n","step: 3870, loss: 0.027302419766783714\n","step: 3880, loss: 0.10506962239742279\n","step: 3890, loss: 0.06952346116304398\n","step: 3900, loss: 0.13316349685192108\n","step: 3910, loss: 0.045589279383420944\n","step: 3920, loss: 0.05203527957201004\n","step: 3930, loss: 0.09667950868606567\n","step: 3940, loss: 0.1386321634054184\n","step: 3950, loss: 0.11351542919874191\n","step: 3960, loss: 0.07117457687854767\n","step: 3970, loss: 0.054465726017951965\n","step: 3980, loss: 0.1591252237558365\n","step: 3990, loss: 0.1467960774898529\n","step: 4000, loss: 0.0940806195139885\n","step: 4010, loss: 0.045117925852537155\n","step: 4020, loss: 0.08172820508480072\n","step: 4030, loss: 0.09905087947845459\n","step: 4040, loss: 0.1115250289440155\n","step: 4050, loss: 0.06659898161888123\n","step: 4060, loss: 0.12101688235998154\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.89      0.97      0.93        35\n","           2       0.86      0.79      0.82        77\n","           3       0.99      0.79      0.88      1030\n","           4       0.93      0.83      0.88       291\n","           5       0.87      0.84      0.86       294\n","           6       0.97      0.99      0.98      1570\n","           7       0.54      0.94      0.68       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.94      0.96      0.95       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.92      0.98      0.95        47\n","          13       0.57      0.31      0.40        13\n","          14       0.38      1.00      0.55        43\n","          15       0.93      0.98      0.96      2778\n","          16       0.84      0.86      0.85      1151\n","          17       0.93      0.98      0.95        41\n","          18       0.93      0.84      0.89        32\n","          19       0.41      0.33      0.36        40\n","          20       0.98      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.16      0.07      0.10        44\n","          25       0.86      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.88      1.00      0.94        69\n","          28       1.00      1.00      1.00      1864\n","          29       1.00      0.99      0.99       344\n","          30       0.95      0.86      0.90      1136\n","          31       0.57      0.63      0.60        19\n","          32       0.86      0.75      0.80         8\n","          33       0.73      0.95      0.83        86\n","          34       0.29      0.69      0.41        32\n","          35       0.98      0.99      0.99       474\n","          36       0.73      0.16      0.27       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.98      0.98      0.98       404\n","          39       0.96      0.95      0.96       485\n","          40       0.94      0.83      0.88       573\n","          41       0.96      0.92      0.94       841\n","          42       0.99      0.98      0.99       575\n","          43       1.00      0.82      0.90       152\n","          44       0.87      0.97      0.92        75\n","          46       1.00      0.96      0.98        82\n","          48       0.00      0.00      0.00        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.78      0.79      0.78     28417\n","weighted avg       0.91      0.90      0.90     28417\n","\n","\n","Loop 5\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9450647830963135\n","step: 10, loss: 1.9303308725357056\n","step: 20, loss: 0.7436556220054626\n","step: 30, loss: 0.35262247920036316\n","step: 40, loss: 0.2850682735443115\n","step: 50, loss: 0.2496727854013443\n","step: 60, loss: 0.2910245954990387\n","step: 70, loss: 0.11595608294010162\n","step: 80, loss: 0.2279968112707138\n","step: 90, loss: 0.28462380170822144\n","step: 100, loss: 0.1593732237815857\n","step: 110, loss: 0.0955706387758255\n","step: 120, loss: 0.24801349639892578\n","step: 130, loss: 0.15364909172058105\n","step: 140, loss: 0.09673310071229935\n","step: 150, loss: 0.11215564608573914\n","step: 160, loss: 0.1718411147594452\n","step: 170, loss: 0.08355002105236053\n","step: 180, loss: 0.17112946510314941\n","step: 190, loss: 0.08040986955165863\n","step: 200, loss: 0.21723614633083344\n","step: 210, loss: 0.17928163707256317\n","step: 220, loss: 0.06091427430510521\n","step: 230, loss: 0.13611041009426117\n","step: 240, loss: 0.1116798147559166\n","step: 250, loss: 0.0772329643368721\n","step: 260, loss: 0.08671128749847412\n","step: 270, loss: 0.1750318706035614\n","step: 280, loss: 0.1595516800880432\n","step: 290, loss: 0.11396964639425278\n","step: 300, loss: 0.12149212509393692\n","step: 310, loss: 0.09684478491544724\n","step: 320, loss: 0.1339367777109146\n","step: 330, loss: 0.03621817007660866\n","step: 340, loss: 0.1345490962266922\n","step: 350, loss: 0.19288428127765656\n","step: 360, loss: 0.18344072997570038\n","step: 370, loss: 0.1290445327758789\n","step: 380, loss: 0.3379075527191162\n","step: 390, loss: 0.2186790555715561\n","step: 400, loss: 0.22909380495548248\n","step: 410, loss: 0.16305391490459442\n","step: 420, loss: 0.12195531278848648\n","step: 430, loss: 0.19323159754276276\n","step: 440, loss: 0.10615236312150955\n","step: 450, loss: 0.22497741878032684\n","step: 460, loss: 0.13257266581058502\n","step: 470, loss: 0.16904929280281067\n","step: 480, loss: 0.07878989726305008\n","step: 490, loss: 0.12125000357627869\n","step: 500, loss: 0.07478407025337219\n","step: 510, loss: 0.1228070929646492\n","step: 520, loss: 0.15415559709072113\n","step: 530, loss: 0.24679630994796753\n","step: 540, loss: 0.14482487738132477\n","step: 550, loss: 0.15151837468147278\n","step: 560, loss: 0.04326820373535156\n","step: 570, loss: 0.0997101292014122\n","step: 580, loss: 0.09353155642747879\n","step: 590, loss: 0.09722945839166641\n","step: 600, loss: 0.10936851799488068\n","step: 610, loss: 0.05854890123009682\n","step: 620, loss: 0.03324484825134277\n","step: 630, loss: 0.10038339346647263\n","step: 640, loss: 0.09665220975875854\n","step: 650, loss: 0.09065756946802139\n","step: 660, loss: 0.11219187825918198\n","step: 670, loss: 0.02563418261706829\n","step: 680, loss: 0.053231071680784225\n","step: 690, loss: 0.08606947213411331\n","step: 700, loss: 0.03555119410157204\n","step: 710, loss: 0.10358923673629761\n","step: 720, loss: 0.03547002375125885\n","step: 730, loss: 0.07318613678216934\n","step: 740, loss: 0.20636919140815735\n","step: 750, loss: 0.12656505405902863\n","step: 760, loss: 0.10913123190402985\n","step: 770, loss: 0.07335487753152847\n","step: 780, loss: 0.04005902633070946\n","step: 790, loss: 0.042050935328006744\n","step: 800, loss: 0.07300740480422974\n","step: 810, loss: 0.03764072433114052\n","step: 820, loss: 0.12231572717428207\n","step: 830, loss: 0.14315418899059296\n","step: 840, loss: 0.21061548590660095\n","step: 850, loss: 0.14115920662879944\n","step: 860, loss: 0.0660918727517128\n","step: 870, loss: 0.08658435940742493\n","step: 880, loss: 0.2556152045726776\n","step: 890, loss: 0.20883947610855103\n","step: 900, loss: 0.16035881638526917\n","step: 910, loss: 0.1631876826286316\n","step: 920, loss: 0.04846756532788277\n","step: 930, loss: 0.137027770280838\n","step: 940, loss: 0.05843354016542435\n","step: 950, loss: 0.09397359937429428\n","step: 960, loss: 0.11145295202732086\n","step: 970, loss: 0.11484766751527786\n","step: 980, loss: 0.10483811795711517\n","step: 990, loss: 0.141551673412323\n","step: 1000, loss: 0.05843052268028259\n","step: 1010, loss: 0.10300037264823914\n","step: 1020, loss: 0.07008715718984604\n","step: 1030, loss: 0.1536780744791031\n","step: 1040, loss: 0.028458507731556892\n","step: 1050, loss: 0.0677461326122284\n","step: 1060, loss: 0.10488007217645645\n","step: 1070, loss: 0.05240704491734505\n","step: 1080, loss: 0.2181953638792038\n","step: 1090, loss: 0.11273021996021271\n","step: 1100, loss: 0.09828045219182968\n","step: 1110, loss: 0.06136029213666916\n","step: 1120, loss: 0.04840874299407005\n","step: 1130, loss: 0.06809300929307938\n","step: 1140, loss: 0.06369973719120026\n","step: 1150, loss: 0.10563316941261292\n","step: 1160, loss: 0.0503777414560318\n","step: 1170, loss: 0.19102945923805237\n","step: 1180, loss: 0.07628179341554642\n","step: 1190, loss: 0.07315194606781006\n","step: 1200, loss: 0.09437020123004913\n","step: 1210, loss: 0.03627496585249901\n","step: 1220, loss: 0.16527274250984192\n","step: 1230, loss: 0.07828120142221451\n","step: 1240, loss: 0.06232655048370361\n","step: 1250, loss: 0.07212261855602264\n","step: 1260, loss: 0.055561307817697525\n","step: 1270, loss: 0.051535118371248245\n","step: 1280, loss: 0.22260494530200958\n","step: 1290, loss: 0.2629409432411194\n","step: 1300, loss: 0.10701911896467209\n","step: 1310, loss: 0.10955845564603806\n","step: 1320, loss: 0.05786750838160515\n","step: 1330, loss: 0.08328074216842651\n","step: 1340, loss: 0.12041989713907242\n","step: 1350, loss: 0.045801419764757156\n","step: 1360, loss: 0.033980146050453186\n","step: 1370, loss: 0.07916553318500519\n","step: 1380, loss: 0.03862680867314339\n","step: 1390, loss: 0.10273920744657516\n","step: 1400, loss: 0.11580689251422882\n","step: 1410, loss: 0.032611627131700516\n","step: 1420, loss: 0.0484146922826767\n","step: 1430, loss: 0.1250414252281189\n","step: 1440, loss: 0.06415785104036331\n","step: 1450, loss: 0.12877421081066132\n","step: 1460, loss: 0.07119151204824448\n","step: 1470, loss: 0.26787105202674866\n","step: 1480, loss: 0.1055169403553009\n","step: 1490, loss: 0.08394897729158401\n","step: 1500, loss: 0.02948850765824318\n","step: 1510, loss: 0.1408860832452774\n","step: 1520, loss: 0.11019203066825867\n","step: 1530, loss: 0.07744930684566498\n","step: 1540, loss: 0.09369336813688278\n","step: 1550, loss: 0.09285346418619156\n","step: 1560, loss: 0.17360574007034302\n","step: 1570, loss: 0.14925073087215424\n","step: 1580, loss: 0.12757225334644318\n","step: 1590, loss: 0.1413031816482544\n","step: 1600, loss: 0.16990704834461212\n","step: 1610, loss: 0.1345490962266922\n","step: 1620, loss: 0.12570929527282715\n","step: 1630, loss: 0.07629051059484482\n","step: 1640, loss: 0.054903801530599594\n","step: 1650, loss: 0.15119145810604095\n","step: 1660, loss: 0.12215976417064667\n","step: 1670, loss: 0.16411252319812775\n","step: 1680, loss: 0.12380658835172653\n","step: 1690, loss: 0.13856767117977142\n","step: 1700, loss: 0.031890109181404114\n","step: 1710, loss: 0.042831793427467346\n","step: 1720, loss: 0.1518765687942505\n","step: 1730, loss: 0.049447495490312576\n","step: 1740, loss: 0.119500532746315\n","step: 1750, loss: 0.052502021193504333\n","step: 1760, loss: 0.08915821462869644\n","step: 1770, loss: 0.02159075438976288\n","step: 1780, loss: 0.06907061487436295\n","step: 1790, loss: 0.13215725123882294\n","step: 1800, loss: 0.21549834311008453\n","step: 1810, loss: 0.14274482429027557\n","step: 1820, loss: 0.022619569674134254\n","step: 1830, loss: 0.08158959448337555\n","step: 1840, loss: 0.11947856843471527\n","step: 1850, loss: 0.18858769536018372\n","step: 1860, loss: 0.07088446617126465\n","step: 1870, loss: 0.045172080397605896\n","step: 1880, loss: 0.01869991607964039\n","step: 1890, loss: 0.13252229988574982\n","step: 1900, loss: 0.12098407000303268\n","step: 1910, loss: 0.045318715274333954\n","step: 1920, loss: 0.048222996294498444\n","step: 1930, loss: 0.10103263705968857\n","step: 1940, loss: 0.11137248575687408\n","step: 1950, loss: 0.046334341168403625\n","step: 1960, loss: 0.09428881853818893\n","step: 1970, loss: 0.0885116383433342\n","step: 1980, loss: 0.061110466718673706\n","step: 1990, loss: 0.1105734258890152\n","step: 2000, loss: 0.10255831480026245\n","step: 2010, loss: 0.05225661024451256\n","step: 2020, loss: 0.10509242862462997\n","step: 2030, loss: 0.11600019037723541\n","step: 2040, loss: 0.11414895951747894\n","step: 2050, loss: 0.14507341384887695\n","step: 2060, loss: 0.1916998326778412\n","step: 2070, loss: 0.10433518886566162\n","step: 2080, loss: 0.04487563297152519\n","step: 2090, loss: 0.1079523116350174\n","step: 2100, loss: 0.059058479964733124\n","step: 2110, loss: 0.04816547781229019\n","step: 2120, loss: 0.0783127173781395\n","step: 2130, loss: 0.08288712799549103\n","step: 2140, loss: 0.11031448096036911\n","step: 2150, loss: 0.04674573615193367\n","step: 2160, loss: 0.11615847796201706\n","step: 2170, loss: 0.0403597466647625\n","step: 2180, loss: 0.12693005800247192\n","step: 2190, loss: 0.1341400295495987\n","step: 2200, loss: 0.03627118095755577\n","step: 2210, loss: 0.10528209805488586\n","step: 2220, loss: 0.11780260503292084\n","step: 2230, loss: 0.049276500940322876\n","step: 2240, loss: 0.028258439153432846\n","step: 2250, loss: 0.10405807197093964\n","step: 2260, loss: 0.13914768397808075\n","step: 2270, loss: 0.046355217695236206\n","step: 2280, loss: 0.15209659934043884\n","step: 2290, loss: 0.11450698226690292\n","step: 2300, loss: 0.07136018574237823\n","step: 2310, loss: 0.13373437523841858\n","step: 2320, loss: 0.06693306565284729\n","step: 2330, loss: 0.11672089248895645\n","step: 2340, loss: 0.023045074194669724\n","step: 2350, loss: 0.07717796415090561\n","step: 2360, loss: 0.07729320973157883\n","step: 2370, loss: 0.13608525693416595\n","step: 2380, loss: 0.09864974021911621\n","step: 2390, loss: 0.10472051054239273\n","step: 2400, loss: 0.1303274929523468\n","step: 2410, loss: 0.12257689982652664\n","step: 2420, loss: 0.18828871846199036\n","step: 2430, loss: 0.044097721576690674\n","step: 2440, loss: 0.12510979175567627\n","step: 2450, loss: 0.15171672403812408\n","step: 2460, loss: 0.21704749763011932\n","step: 2470, loss: 0.05056288465857506\n","step: 2480, loss: 0.10186943411827087\n","step: 2490, loss: 0.0643777921795845\n","step: 2500, loss: 0.11327402293682098\n","step: 2510, loss: 0.11036071926355362\n","step: 2520, loss: 0.020929336547851562\n","step: 2530, loss: 0.06301086395978928\n","step: 2540, loss: 0.07236611843109131\n","step: 2550, loss: 0.1024809405207634\n","step: 2560, loss: 0.12080161273479462\n","step: 2570, loss: 0.048550840467214584\n","step: 2580, loss: 0.07989680022001266\n","step: 2590, loss: 0.09818895161151886\n","step: 2600, loss: 0.06771626323461533\n","step: 2610, loss: 0.08708731085062027\n","step: 2620, loss: 0.17621010541915894\n","step: 2630, loss: 0.11843124032020569\n","step: 2640, loss: 0.06484648585319519\n","step: 2650, loss: 0.10068096965551376\n","step: 2660, loss: 0.12058985978364944\n","step: 2670, loss: 0.16132164001464844\n","step: 2680, loss: 0.12266415357589722\n","step: 2690, loss: 0.2078651487827301\n","step: 2700, loss: 0.156188502907753\n","step: 2710, loss: 0.17857691645622253\n","step: 2720, loss: 0.2005026787519455\n","step: 2730, loss: 0.16509923338890076\n","step: 2740, loss: 0.037143055349588394\n","step: 2750, loss: 0.11265712231397629\n","step: 2760, loss: 0.04919459670782089\n","step: 2770, loss: 0.10583523660898209\n","step: 2780, loss: 0.05580480024218559\n","step: 2790, loss: 0.12557341158390045\n","step: 2800, loss: 0.10795233398675919\n","step: 2810, loss: 0.11974336206912994\n","step: 2820, loss: 0.0861794650554657\n","step: 2830, loss: 0.05744371563196182\n","step: 2840, loss: 0.11065109074115753\n","step: 2850, loss: 0.0994923859834671\n","step: 2860, loss: 0.12293537706136703\n","step: 2870, loss: 0.11165101081132889\n","step: 2880, loss: 0.12529775500297546\n","step: 2890, loss: 0.058559589087963104\n","step: 2900, loss: 0.0591374970972538\n","step: 2910, loss: 0.02597888745367527\n","step: 2920, loss: 0.12692025303840637\n","step: 2930, loss: 0.04720325767993927\n","step: 2940, loss: 0.1172516793012619\n","step: 2950, loss: 0.10344939678907394\n","step: 2960, loss: 0.053557172417640686\n","step: 2970, loss: 0.2317403107881546\n","step: 2980, loss: 0.09007629007101059\n","step: 2990, loss: 0.04512118548154831\n","step: 3000, loss: 0.08476396650075912\n","step: 3010, loss: 0.14174094796180725\n","step: 3020, loss: 0.15623293817043304\n","step: 3030, loss: 0.08709625899791718\n","step: 3040, loss: 0.09281852841377258\n","step: 3050, loss: 0.14332358539104462\n","step: 3060, loss: 0.04813123494386673\n","step: 3070, loss: 0.08511994779109955\n","step: 3080, loss: 0.12802806496620178\n","step: 3090, loss: 0.21936438977718353\n","step: 3100, loss: 0.14666882157325745\n","step: 3110, loss: 0.07082638889551163\n","step: 3120, loss: 0.11479781568050385\n","step: 3130, loss: 0.09570667892694473\n","step: 3140, loss: 0.07712047547101974\n","step: 3150, loss: 0.050931431353092194\n","step: 3160, loss: 0.12291814386844635\n","step: 3170, loss: 0.06687118858098984\n","step: 3180, loss: 0.15759117901325226\n","step: 3190, loss: 0.05479418858885765\n","step: 3200, loss: 0.035156071186065674\n","step: 3210, loss: 0.028886448591947556\n","step: 3220, loss: 0.02642870880663395\n","step: 3230, loss: 0.025053445249795914\n","step: 3240, loss: 0.10767193138599396\n","step: 3250, loss: 0.07152917236089706\n","step: 3260, loss: 0.11240296065807343\n","step: 3270, loss: 0.03899989277124405\n","step: 3280, loss: 0.011550077237188816\n","step: 3290, loss: 0.08528255671262741\n","step: 3300, loss: 0.04940100386738777\n","step: 3310, loss: 0.14734116196632385\n","step: 3320, loss: 0.0495183952152729\n","step: 3330, loss: 0.06884883344173431\n","step: 3340, loss: 0.10275112837553024\n","step: 3350, loss: 0.1609785109758377\n","step: 3360, loss: 0.1250917613506317\n","step: 3370, loss: 0.0683678537607193\n","step: 3380, loss: 0.044062934815883636\n","step: 3390, loss: 0.16018544137477875\n","step: 3400, loss: 0.09420014917850494\n","step: 3410, loss: 0.19050970673561096\n","step: 3420, loss: 0.036968983709812164\n","step: 3430, loss: 0.12769442796707153\n","step: 3440, loss: 0.10980630666017532\n","step: 3450, loss: 0.10031133890151978\n","step: 3460, loss: 0.16327379643917084\n","step: 3470, loss: 0.12080056965351105\n","step: 3480, loss: 0.1189589723944664\n","step: 3490, loss: 0.06973467022180557\n","step: 3500, loss: 0.06675988435745239\n","step: 3510, loss: 0.06511586159467697\n","step: 3520, loss: 0.05282658338546753\n","step: 3530, loss: 0.030855918303132057\n","step: 3540, loss: 0.16272757947444916\n","step: 3550, loss: 0.07577391713857651\n","step: 3560, loss: 0.07823322713375092\n","step: 3570, loss: 0.12307946383953094\n","step: 3580, loss: 0.10747222602367401\n","step: 3590, loss: 0.04683830216526985\n","step: 3600, loss: 0.17527785897254944\n","step: 3610, loss: 0.10136730968952179\n","step: 3620, loss: 0.12154066562652588\n","step: 3630, loss: 0.1414046585559845\n","step: 3640, loss: 0.1663220375776291\n","step: 3650, loss: 0.07844121009111404\n","step: 3660, loss: 0.10746415704488754\n","step: 3670, loss: 0.07242833077907562\n","step: 3680, loss: 0.14437203109264374\n","step: 3690, loss: 0.08201699703931808\n","step: 3700, loss: 0.08013948798179626\n","step: 3710, loss: 0.11471495032310486\n","step: 3720, loss: 0.050933901220560074\n","step: 3730, loss: 0.19397911429405212\n","step: 3740, loss: 0.10017867386341095\n","step: 3750, loss: 0.1350424438714981\n","step: 3760, loss: 0.11669740080833435\n","step: 3770, loss: 0.15620364248752594\n","step: 3780, loss: 0.07803694158792496\n","step: 3790, loss: 0.1253160983324051\n","step: 3800, loss: 0.03786079213023186\n","step: 3810, loss: 0.03290259465575218\n","step: 3820, loss: 0.04771331325173378\n","step: 3830, loss: 0.06776334345340729\n","step: 3840, loss: 0.05327674001455307\n","step: 3850, loss: 0.20226290822029114\n","step: 3860, loss: 0.11696621030569077\n","step: 3870, loss: 0.05535084009170532\n","step: 3880, loss: 0.12964265048503876\n","step: 3890, loss: 0.08546434342861176\n","step: 3900, loss: 0.07817067950963974\n","step: 3910, loss: 0.13192303478717804\n","step: 3920, loss: 0.04729991778731346\n","step: 3930, loss: 0.041980329900979996\n","step: 3940, loss: 0.1611575335264206\n","step: 3950, loss: 0.09832482039928436\n","step: 3960, loss: 0.1054021343588829\n","step: 3970, loss: 0.05983419716358185\n","step: 3980, loss: 0.08107966929674149\n","step: 3990, loss: 0.07488957047462463\n","step: 4000, loss: 0.09474264085292816\n","step: 4010, loss: 0.03209930658340454\n","step: 4020, loss: 0.07990285009145737\n","step: 4030, loss: 0.09832630306482315\n","step: 4040, loss: 0.10511093586683273\n","step: 4050, loss: 0.17771819233894348\n","step: 4060, loss: 0.10805202275514603\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      0.91      0.96        35\n","           2       0.90      0.95      0.92        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.95      0.82      0.88       291\n","           5       0.87      0.84      0.86       294\n","           6       0.92      1.00      0.96      1570\n","           7       0.78      0.91      0.84       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      1.00      0.99       689\n","          10       0.94      0.97      0.96       901\n","          11       0.98      0.99      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.78      0.54      0.64        13\n","          14       0.38      1.00      0.55        43\n","          15       0.93      0.98      0.96      2778\n","          16       0.85      0.85      0.85      1151\n","          17       0.95      0.93      0.94        41\n","          18       0.86      1.00      0.93        32\n","          19       1.00      0.38      0.55        40\n","          20       0.99      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.75      0.84      4175\n","          23       0.69      0.97      0.81      2253\n","          24       0.30      0.61      0.41        44\n","          25       0.86      0.90      0.88       888\n","          26       1.00      0.78      0.88         9\n","          27       0.96      0.97      0.96        69\n","          28       0.99      1.00      1.00      1864\n","          29       0.99      0.97      0.98       344\n","          30       0.96      0.84      0.89      1136\n","          31       0.68      0.79      0.73        19\n","          32       1.00      0.38      0.55         8\n","          33       0.72      0.94      0.81        86\n","          34       0.28      0.72      0.41        32\n","          35       0.99      0.99      0.99       474\n","          36       0.93      0.15      0.26       182\n","          37       0.88      0.97      0.92      1592\n","          38       0.94      0.99      0.96       404\n","          39       0.96      0.96      0.96       485\n","          40       0.94      0.83      0.88       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.95      0.81      0.88       152\n","          44       0.93      0.92      0.93        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.06      0.12        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.85      0.80      0.79     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","Loop 6\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9125754833221436\n","step: 10, loss: 2.266775131225586\n","step: 20, loss: 0.8810528516769409\n","step: 30, loss: 0.44713687896728516\n","step: 40, loss: 0.23138757050037384\n","step: 50, loss: 0.2086188942193985\n","step: 60, loss: 0.22301948070526123\n","step: 70, loss: 1.3270195722579956\n","step: 80, loss: 0.32700708508491516\n","step: 90, loss: 0.1893535703420639\n","step: 100, loss: 0.18158401548862457\n","step: 110, loss: 0.1648378074169159\n","step: 120, loss: 0.30230948328971863\n","step: 130, loss: 0.16539664566516876\n","step: 140, loss: 0.3052779734134674\n","step: 150, loss: 0.19958344101905823\n","step: 160, loss: 0.12110505998134613\n","step: 170, loss: 0.11179114133119583\n","step: 180, loss: 0.1142435371875763\n","step: 190, loss: 0.12621475756168365\n","step: 200, loss: 0.17072132229804993\n","step: 210, loss: 0.13609035313129425\n","step: 220, loss: 0.23101240396499634\n","step: 230, loss: 0.17650935053825378\n","step: 240, loss: 0.16503679752349854\n","step: 250, loss: 0.08404826372861862\n","step: 260, loss: 0.1914597451686859\n","step: 270, loss: 0.13785189390182495\n","step: 280, loss: 0.11494728177785873\n","step: 290, loss: 0.0640091821551323\n","step: 300, loss: 0.11610043793916702\n","step: 310, loss: 0.10159212350845337\n","step: 320, loss: 0.08523251116275787\n","step: 330, loss: 0.1963721364736557\n","step: 340, loss: 0.09448219835758209\n","step: 350, loss: 0.17676691710948944\n","step: 360, loss: 0.0751417875289917\n","step: 370, loss: 0.2104509472846985\n","step: 380, loss: 0.09642218798398972\n","step: 390, loss: 0.18775826692581177\n","step: 400, loss: 0.10815560072660446\n","step: 410, loss: 0.09939190000295639\n","step: 420, loss: 0.09013578295707703\n","step: 430, loss: 0.07190171629190445\n","step: 440, loss: 0.04054097831249237\n","step: 450, loss: 0.08250758051872253\n","step: 460, loss: 0.0843096524477005\n","step: 470, loss: 0.13831716775894165\n","step: 480, loss: 0.08507722616195679\n","step: 490, loss: 0.14093619585037231\n","step: 500, loss: 0.08389639109373093\n","step: 510, loss: 0.13513661921024323\n","step: 520, loss: 0.1257321536540985\n","step: 530, loss: 0.08383658528327942\n","step: 540, loss: 0.05652155354619026\n","step: 550, loss: 0.10487955808639526\n","step: 560, loss: 0.09637486189603806\n","step: 570, loss: 0.07238636910915375\n","step: 580, loss: 0.11249826103448868\n","step: 590, loss: 0.11987505108118057\n","step: 600, loss: 0.07076617330312729\n","step: 610, loss: 0.10476954281330109\n","step: 620, loss: 0.10501427203416824\n","step: 630, loss: 0.15873610973358154\n","step: 640, loss: 0.07607530057430267\n","step: 650, loss: 0.047710347920656204\n","step: 660, loss: 0.0870952233672142\n","step: 670, loss: 0.165690615773201\n","step: 680, loss: 0.24500256776809692\n","step: 690, loss: 0.24184460937976837\n","step: 700, loss: 0.3463377356529236\n","step: 710, loss: 0.12015606462955475\n","step: 720, loss: 0.061607759445905685\n","step: 730, loss: 0.09991548210382462\n","step: 740, loss: 0.10305791348218918\n","step: 750, loss: 0.1466091275215149\n","step: 760, loss: 0.09433804452419281\n","step: 770, loss: 0.15973398089408875\n","step: 780, loss: 0.10827969759702682\n","step: 790, loss: 0.16039244830608368\n","step: 800, loss: 0.102040134370327\n","step: 810, loss: 0.09344011545181274\n","step: 820, loss: 0.10966965556144714\n","step: 830, loss: 0.11761514842510223\n","step: 840, loss: 0.09132082760334015\n","step: 850, loss: 0.05548909306526184\n","step: 860, loss: 0.1814960539340973\n","step: 870, loss: 0.13984143733978271\n","step: 880, loss: 0.13817505538463593\n","step: 890, loss: 0.05632255598902702\n","step: 900, loss: 0.06192941963672638\n","step: 910, loss: 0.0782025158405304\n","step: 920, loss: 0.12092441320419312\n","step: 930, loss: 0.04110153764486313\n","step: 940, loss: 0.1674989014863968\n","step: 950, loss: 0.04699418321251869\n","step: 960, loss: 0.046199798583984375\n","step: 970, loss: 0.08587736636400223\n","step: 980, loss: 0.07526983320713043\n","step: 990, loss: 0.23570136725902557\n","step: 1000, loss: 0.11872373521327972\n","step: 1010, loss: 0.06331837922334671\n","step: 1020, loss: 0.11525767296552658\n","step: 1030, loss: 0.08372441679239273\n","step: 1040, loss: 0.061409417539834976\n","step: 1050, loss: 0.10059209913015366\n","step: 1060, loss: 0.12064075469970703\n","step: 1070, loss: 0.15533292293548584\n","step: 1080, loss: 0.10097822546958923\n","step: 1090, loss: 0.10188759118318558\n","step: 1100, loss: 0.08115948736667633\n","step: 1110, loss: 0.11802934110164642\n","step: 1120, loss: 0.1039169430732727\n","step: 1130, loss: 0.09024835377931595\n","step: 1140, loss: 0.16585154831409454\n","step: 1150, loss: 0.07886631041765213\n","step: 1160, loss: 0.04250416159629822\n","step: 1170, loss: 0.1778116077184677\n","step: 1180, loss: 0.13015589118003845\n","step: 1190, loss: 0.15236784517765045\n","step: 1200, loss: 0.052216608077287674\n","step: 1210, loss: 0.05739477276802063\n","step: 1220, loss: 0.049762193113565445\n","step: 1230, loss: 0.14174918830394745\n","step: 1240, loss: 0.05298709496855736\n","step: 1250, loss: 0.09046131372451782\n","step: 1260, loss: 0.06541310250759125\n","step: 1270, loss: 0.2238997220993042\n","step: 1280, loss: 0.21745209395885468\n","step: 1290, loss: 0.06850750744342804\n","step: 1300, loss: 0.09728863835334778\n","step: 1310, loss: 0.10530932247638702\n","step: 1320, loss: 0.09066740423440933\n","step: 1330, loss: 0.16677582263946533\n","step: 1340, loss: 0.062413886189460754\n","step: 1350, loss: 0.09318003803491592\n","step: 1360, loss: 0.31245657801628113\n","step: 1370, loss: 0.07841511070728302\n","step: 1380, loss: 0.026830513030290604\n","step: 1390, loss: 0.10496412217617035\n","step: 1400, loss: 0.055782753974199295\n","step: 1410, loss: 0.05354771018028259\n","step: 1420, loss: 0.19448235630989075\n","step: 1430, loss: 0.08924561738967896\n","step: 1440, loss: 0.04678788781166077\n","step: 1450, loss: 0.17525538802146912\n","step: 1460, loss: 0.058538686484098434\n","step: 1470, loss: 0.061488695442676544\n","step: 1480, loss: 0.20141145586967468\n","step: 1490, loss: 0.036463625729084015\n","step: 1500, loss: 0.06662002950906754\n","step: 1510, loss: 0.10968685150146484\n","step: 1520, loss: 0.05045842006802559\n","step: 1530, loss: 0.08717537671327591\n","step: 1540, loss: 0.0851438045501709\n","step: 1550, loss: 0.08026362955570221\n","step: 1560, loss: 0.12498394399881363\n","step: 1570, loss: 0.04690485820174217\n","step: 1580, loss: 0.2157907336950302\n","step: 1590, loss: 0.14787691831588745\n","step: 1600, loss: 0.14718249440193176\n","step: 1610, loss: 0.11924310773611069\n","step: 1620, loss: 0.10958712548017502\n","step: 1630, loss: 0.13278502225875854\n","step: 1640, loss: 0.07987836748361588\n","step: 1650, loss: 0.08817658573389053\n","step: 1660, loss: 0.029816335067152977\n","step: 1670, loss: 0.028495410457253456\n","step: 1680, loss: 0.13265161216259003\n","step: 1690, loss: 0.13821592926979065\n","step: 1700, loss: 0.05813204497098923\n","step: 1710, loss: 0.09322518110275269\n","step: 1720, loss: 0.05035089701414108\n","step: 1730, loss: 0.1255970448255539\n","step: 1740, loss: 0.10696426033973694\n","step: 1750, loss: 0.21530219912528992\n","step: 1760, loss: 0.07387730479240417\n","step: 1770, loss: 0.10391101986169815\n","step: 1780, loss: 0.13591445982456207\n","step: 1790, loss: 0.046208977699279785\n","step: 1800, loss: 0.029939502477645874\n","step: 1810, loss: 0.04532347619533539\n","step: 1820, loss: 0.1308230608701706\n","step: 1830, loss: 0.06462588906288147\n","step: 1840, loss: 0.1306142956018448\n","step: 1850, loss: 0.034170541912317276\n","step: 1860, loss: 0.05950787290930748\n","step: 1870, loss: 0.11412104219198227\n","step: 1880, loss: 0.07398001104593277\n","step: 1890, loss: 0.1232113465666771\n","step: 1900, loss: 0.2328280359506607\n","step: 1910, loss: 0.11908838897943497\n","step: 1920, loss: 0.1712094247341156\n","step: 1930, loss: 0.03357884660363197\n","step: 1940, loss: 0.062004052102565765\n","step: 1950, loss: 0.08339215070009232\n","step: 1960, loss: 0.06153833493590355\n","step: 1970, loss: 0.14665982127189636\n","step: 1980, loss: 0.17322133481502533\n","step: 1990, loss: 0.11535178124904633\n","step: 2000, loss: 0.04786169156432152\n","step: 2010, loss: 0.04188677296042442\n","step: 2020, loss: 0.11493591964244843\n","step: 2030, loss: 0.04003904387354851\n","step: 2040, loss: 0.14636817574501038\n","step: 2050, loss: 0.06171863153576851\n","step: 2060, loss: 0.06630438566207886\n","step: 2070, loss: 0.10441339761018753\n","step: 2080, loss: 0.045448824763298035\n","step: 2090, loss: 0.04432874172925949\n","step: 2100, loss: 0.1266447752714157\n","step: 2110, loss: 0.11100094020366669\n","step: 2120, loss: 0.10902091860771179\n","step: 2130, loss: 0.020214349031448364\n","step: 2140, loss: 0.050315503031015396\n","step: 2150, loss: 0.12373621016740799\n","step: 2160, loss: 0.21861520409584045\n","step: 2170, loss: 0.09399742633104324\n","step: 2180, loss: 0.050011418759822845\n","step: 2190, loss: 0.10737959295511246\n","step: 2200, loss: 0.12977387011051178\n","step: 2210, loss: 0.0506509505212307\n","step: 2220, loss: 0.1351114958524704\n","step: 2230, loss: 0.14275220036506653\n","step: 2240, loss: 0.11260602623224258\n","step: 2250, loss: 0.12122838199138641\n","step: 2260, loss: 0.19767019152641296\n","step: 2270, loss: 0.09521476924419403\n","step: 2280, loss: 0.044663023203611374\n","step: 2290, loss: 0.03162289783358574\n","step: 2300, loss: 0.08313677459955215\n","step: 2310, loss: 0.10061431676149368\n","step: 2320, loss: 0.11513631790876389\n","step: 2330, loss: 0.08755581825971603\n","step: 2340, loss: 0.15189535915851593\n","step: 2350, loss: 0.05417395383119583\n","step: 2360, loss: 0.07689754664897919\n","step: 2370, loss: 0.4186658561229706\n","step: 2380, loss: 0.10635622590780258\n","step: 2390, loss: 0.18529334664344788\n","step: 2400, loss: 0.07180521637201309\n","step: 2410, loss: 0.10161662846803665\n","step: 2420, loss: 0.08937983214855194\n","step: 2430, loss: 0.05518259480595589\n","step: 2440, loss: 0.06233453005552292\n","step: 2450, loss: 0.08551669865846634\n","step: 2460, loss: 0.23740239441394806\n","step: 2470, loss: 0.11015041917562485\n","step: 2480, loss: 0.13393308222293854\n","step: 2490, loss: 0.09504970908164978\n","step: 2500, loss: 0.10171344876289368\n","step: 2510, loss: 0.11400529742240906\n","step: 2520, loss: 0.10692578554153442\n","step: 2530, loss: 0.0738120749592781\n","step: 2540, loss: 0.23901109397411346\n","step: 2550, loss: 0.042592015117406845\n","step: 2560, loss: 0.08871012926101685\n","step: 2570, loss: 0.1535761058330536\n","step: 2580, loss: 0.08621403574943542\n","step: 2590, loss: 0.04696493595838547\n","step: 2600, loss: 0.09765658527612686\n","step: 2610, loss: 0.14981935918331146\n","step: 2620, loss: 0.1637299656867981\n","step: 2630, loss: 0.14318609237670898\n","step: 2640, loss: 0.06697359681129456\n","step: 2650, loss: 0.07025285810232162\n","step: 2660, loss: 0.12449058145284653\n","step: 2670, loss: 0.06812753528356552\n","step: 2680, loss: 0.04330214485526085\n","step: 2690, loss: 0.06733855605125427\n","step: 2700, loss: 0.04161077365279198\n","step: 2710, loss: 0.028486110270023346\n","step: 2720, loss: 0.1074361652135849\n","step: 2730, loss: 0.1219322457909584\n","step: 2740, loss: 0.1259879469871521\n","step: 2750, loss: 0.0767732784152031\n","step: 2760, loss: 0.12230770289897919\n","step: 2770, loss: 0.07692986726760864\n","step: 2780, loss: 0.19267183542251587\n","step: 2790, loss: 0.04753074795007706\n","step: 2800, loss: 0.13003602623939514\n","step: 2810, loss: 0.09212692826986313\n","step: 2820, loss: 0.05171150341629982\n","step: 2830, loss: 0.010413233190774918\n","step: 2840, loss: 0.05900384113192558\n","step: 2850, loss: 0.1312342882156372\n","step: 2860, loss: 0.05115601792931557\n","step: 2870, loss: 0.16701142489910126\n","step: 2880, loss: 0.03819048032164574\n","step: 2890, loss: 0.2839463949203491\n","step: 2900, loss: 0.20258186757564545\n","step: 2910, loss: 0.10149753093719482\n","step: 2920, loss: 0.1287928968667984\n","step: 2930, loss: 0.10030604153871536\n","step: 2940, loss: 0.12755323946475983\n","step: 2950, loss: 0.10502368956804276\n","step: 2960, loss: 0.1047937422990799\n","step: 2970, loss: 0.1554049849510193\n","step: 2980, loss: 0.14272648096084595\n","step: 2990, loss: 0.0681459829211235\n","step: 3000, loss: 0.06432358920574188\n","step: 3010, loss: 0.11478918045759201\n","step: 3020, loss: 0.22299057245254517\n","step: 3030, loss: 0.15581773221492767\n","step: 3040, loss: 0.24609170854091644\n","step: 3050, loss: 0.07261066883802414\n","step: 3060, loss: 0.047561999410390854\n","step: 3070, loss: 0.05917710065841675\n","step: 3080, loss: 0.08610657602548599\n","step: 3090, loss: 0.032625991851091385\n","step: 3100, loss: 0.1412060260772705\n","step: 3110, loss: 0.09072742611169815\n","step: 3120, loss: 0.10593366622924805\n","step: 3130, loss: 0.06432399898767471\n","step: 3140, loss: 0.03399128466844559\n","step: 3150, loss: 0.16733872890472412\n","step: 3160, loss: 0.07714991271495819\n","step: 3170, loss: 0.12381518632173538\n","step: 3180, loss: 0.041444696485996246\n","step: 3190, loss: 0.06739071756601334\n","step: 3200, loss: 0.030896484851837158\n","step: 3210, loss: 0.10495208203792572\n","step: 3220, loss: 0.09475447982549667\n","step: 3230, loss: 0.06948684900999069\n","step: 3240, loss: 0.12032927572727203\n","step: 3250, loss: 0.05224495381116867\n","step: 3260, loss: 0.10259594768285751\n","step: 3270, loss: 0.13203151524066925\n","step: 3280, loss: 0.07098519802093506\n","step: 3290, loss: 0.03528162091970444\n","step: 3300, loss: 0.07407783716917038\n","step: 3310, loss: 0.05304381996393204\n","step: 3320, loss: 0.09493449330329895\n","step: 3330, loss: 0.0641184002161026\n","step: 3340, loss: 0.03561985865235329\n","step: 3350, loss: 0.11083374172449112\n","step: 3360, loss: 0.10833127796649933\n","step: 3370, loss: 0.08108197897672653\n","step: 3380, loss: 0.06317152827978134\n","step: 3390, loss: 0.15673020482063293\n","step: 3400, loss: 0.025937926024198532\n","step: 3410, loss: 0.07113870233297348\n","step: 3420, loss: 0.08474302291870117\n","step: 3430, loss: 0.09252426028251648\n","step: 3440, loss: 0.1259569674730301\n","step: 3450, loss: 0.06680995225906372\n","step: 3460, loss: 0.10875006765127182\n","step: 3470, loss: 0.05105607956647873\n","step: 3480, loss: 0.11841519176959991\n","step: 3490, loss: 0.06102479621767998\n","step: 3500, loss: 0.07353254407644272\n","step: 3510, loss: 0.031023811548948288\n","step: 3520, loss: 0.0672047957777977\n","step: 3530, loss: 0.10521844029426575\n","step: 3540, loss: 0.09087290614843369\n","step: 3550, loss: 0.13401584327220917\n","step: 3560, loss: 0.072407566010952\n","step: 3570, loss: 0.13002587854862213\n","step: 3580, loss: 0.10278499871492386\n","step: 3590, loss: 0.25530609488487244\n","step: 3600, loss: 0.05147229880094528\n","step: 3610, loss: 0.06579946726560593\n","step: 3620, loss: 0.07337260991334915\n","step: 3630, loss: 0.03560204803943634\n","step: 3640, loss: 0.10327436029911041\n","step: 3650, loss: 0.05546169355511665\n","step: 3660, loss: 0.07587403059005737\n","step: 3670, loss: 0.06529048830270767\n","step: 3680, loss: 0.05473313108086586\n","step: 3690, loss: 0.08173352479934692\n","step: 3700, loss: 0.06264206767082214\n","step: 3710, loss: 0.04037299007177353\n","step: 3720, loss: 0.13094019889831543\n","step: 3730, loss: 0.16169415414333344\n","step: 3740, loss: 0.05818996578454971\n","step: 3750, loss: 0.06973124295473099\n","step: 3760, loss: 0.11879535764455795\n","step: 3770, loss: 0.07467670738697052\n","step: 3780, loss: 0.07333101332187653\n","step: 3790, loss: 0.11868017911911011\n","step: 3800, loss: 0.01939721219241619\n","step: 3810, loss: 0.09609126299619675\n","step: 3820, loss: 0.1984821856021881\n","step: 3830, loss: 0.1596917361021042\n","step: 3840, loss: 0.11782678961753845\n","step: 3850, loss: 0.11457988619804382\n","step: 3860, loss: 0.11346683651208878\n","step: 3870, loss: 0.06463496387004852\n","step: 3880, loss: 0.03920846804976463\n","step: 3890, loss: 0.03296948969364166\n","step: 3900, loss: 0.06784140318632126\n","step: 3910, loss: 0.1297450065612793\n","step: 3920, loss: 0.11251077055931091\n","step: 3930, loss: 0.04857964068651199\n","step: 3940, loss: 0.06517713516950607\n","step: 3950, loss: 0.09322645515203476\n","step: 3960, loss: 0.05782997980713844\n","step: 3970, loss: 0.09693201631307602\n","step: 3980, loss: 0.10004858672618866\n","step: 3990, loss: 0.06454695761203766\n","step: 4000, loss: 0.10808925330638885\n","step: 4010, loss: 0.08958008885383606\n","step: 4020, loss: 0.0866173654794693\n","step: 4030, loss: 0.18758709728717804\n","step: 4040, loss: 0.10900473594665527\n","step: 4050, loss: 0.06412278860807419\n","step: 4060, loss: 0.15033233165740967\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.79      0.94      0.86        35\n","           2       0.89      0.91      0.90        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.99      0.84      0.91       291\n","           5       0.89      0.84      0.86       294\n","           6       0.96      0.99      0.97      1570\n","           7       0.63      0.94      0.75       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.95      0.97      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.86      0.92      0.89        13\n","          14       0.32      1.00      0.49        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.84      0.85      0.85      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.91      0.97      0.94        32\n","          19       0.50      0.05      0.09        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.74      0.83      4175\n","          23       0.67      0.97      0.79      2253\n","          24       0.28      0.64      0.39        44\n","          25       0.86      0.90      0.88       888\n","          26       1.00      0.44      0.62         9\n","          27       0.99      1.00      0.99        69\n","          28       0.99      1.00      0.99      1864\n","          29       1.00      0.97      0.98       344\n","          30       0.93      0.89      0.91      1136\n","          31       0.58      0.58      0.58        19\n","          32       1.00      0.62      0.77         8\n","          33       0.87      0.84      0.85        86\n","          34       0.24      0.56      0.34        32\n","          35       0.97      0.99      0.98       474\n","          36       1.00      0.14      0.24       182\n","          37       0.89      0.95      0.92      1592\n","          38       0.90      0.99      0.95       404\n","          39       0.97      0.94      0.95       485\n","          40       0.92      0.78      0.84       573\n","          41       0.95      0.94      0.95       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.91      0.93       152\n","          44       0.90      0.92      0.91        75\n","          46       0.95      0.98      0.96        82\n","          48       0.83      0.06      0.12        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.82      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","\n","Loop 7\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9424540996551514\n","step: 10, loss: 2.002934694290161\n","step: 20, loss: 0.9450781345367432\n","step: 30, loss: 0.49137580394744873\n","step: 40, loss: 0.28753459453582764\n","step: 50, loss: 0.4099986255168915\n","step: 60, loss: 0.21073055267333984\n","step: 70, loss: 0.16521331667900085\n","step: 80, loss: 0.150138720870018\n","step: 90, loss: 0.3428265452384949\n","step: 100, loss: 0.19429075717926025\n","step: 110, loss: 0.15412501990795135\n","step: 120, loss: 0.12337151914834976\n","step: 130, loss: 0.17618608474731445\n","step: 140, loss: 0.1268484741449356\n","step: 150, loss: 0.06639576703310013\n","step: 160, loss: 0.2297154664993286\n","step: 170, loss: 0.13366635143756866\n","step: 180, loss: 0.0923285037279129\n","step: 190, loss: 0.0953238308429718\n","step: 200, loss: 0.07968135923147202\n","step: 210, loss: 0.23057647049427032\n","step: 220, loss: 0.07571388781070709\n","step: 230, loss: 0.18365436792373657\n","step: 240, loss: 0.27481237053871155\n","step: 250, loss: 0.1395493894815445\n","step: 260, loss: 0.3155767321586609\n","step: 270, loss: 0.2183888852596283\n","step: 280, loss: 0.06865126639604568\n","step: 290, loss: 0.08468453586101532\n","step: 300, loss: 0.08953335136175156\n","step: 310, loss: 0.09493851661682129\n","step: 320, loss: 0.09153401106595993\n","step: 330, loss: 0.13241618871688843\n","step: 340, loss: 0.06667619198560715\n","step: 350, loss: 0.12545232474803925\n","step: 360, loss: 0.33319568634033203\n","step: 370, loss: 0.10146444290876389\n","step: 380, loss: 0.08451937884092331\n","step: 390, loss: 0.06049564108252525\n","step: 400, loss: 0.1141754686832428\n","step: 410, loss: 0.06672421842813492\n","step: 420, loss: 0.029468541964888573\n","step: 430, loss: 0.1140272319316864\n","step: 440, loss: 0.13550320267677307\n","step: 450, loss: 0.15383145213127136\n","step: 460, loss: 0.044056449085474014\n","step: 470, loss: 0.13279715180397034\n","step: 480, loss: 0.14195851981639862\n","step: 490, loss: 0.1259724646806717\n","step: 500, loss: 0.0830109715461731\n","step: 510, loss: 0.13909049332141876\n","step: 520, loss: 0.07213666290044785\n","step: 530, loss: 0.19620999693870544\n","step: 540, loss: 0.10272786766290665\n","step: 550, loss: 0.0801795944571495\n","step: 560, loss: 0.07591480761766434\n","step: 570, loss: 0.0448748916387558\n","step: 580, loss: 0.10367774218320847\n","step: 590, loss: 0.07581177353858948\n","step: 600, loss: 0.10212507098913193\n","step: 610, loss: 0.08783166110515594\n","step: 620, loss: 0.07945990562438965\n","step: 630, loss: 0.07418164610862732\n","step: 640, loss: 0.058989517390728\n","step: 650, loss: 0.05183866247534752\n","step: 660, loss: 0.15319788455963135\n","step: 670, loss: 0.08311155438423157\n","step: 680, loss: 0.054712675511837006\n","step: 690, loss: 0.047936368733644485\n","step: 700, loss: 0.03808799758553505\n","step: 710, loss: 0.11222429573535919\n","step: 720, loss: 0.08023033291101456\n","step: 730, loss: 0.11496429145336151\n","step: 740, loss: 0.15334998071193695\n","step: 750, loss: 0.14715637266635895\n","step: 760, loss: 0.11503303796052933\n","step: 770, loss: 0.23907774686813354\n","step: 780, loss: 0.08832202106714249\n","step: 790, loss: 0.15712560713291168\n","step: 800, loss: 0.0907929539680481\n","step: 810, loss: 0.0682428702712059\n","step: 820, loss: 0.09986738860607147\n","step: 830, loss: 0.07028046995401382\n","step: 840, loss: 0.10845071822404861\n","step: 850, loss: 0.19056622684001923\n","step: 860, loss: 0.09244763851165771\n","step: 870, loss: 0.08157818019390106\n","step: 880, loss: 0.11348609626293182\n","step: 890, loss: 0.03518034145236015\n","step: 900, loss: 0.13739338517189026\n","step: 910, loss: 0.2159007042646408\n","step: 920, loss: 0.08298084884881973\n","step: 930, loss: 0.08526858687400818\n","step: 940, loss: 0.09912221878767014\n","step: 950, loss: 0.1155920922756195\n","step: 960, loss: 0.17534033954143524\n","step: 970, loss: 0.1286301612854004\n","step: 980, loss: 0.07954767346382141\n","step: 990, loss: 0.019791819155216217\n","step: 1000, loss: 0.10112739354372025\n","step: 1010, loss: 0.042723093181848526\n","step: 1020, loss: 0.20549072325229645\n","step: 1030, loss: 0.14333650469779968\n","step: 1040, loss: 0.1209864392876625\n","step: 1050, loss: 0.0623459592461586\n","step: 1060, loss: 0.16974933445453644\n","step: 1070, loss: 0.11774635314941406\n","step: 1080, loss: 0.05032023787498474\n","step: 1090, loss: 0.0517696812748909\n","step: 1100, loss: 0.2577664852142334\n","step: 1110, loss: 0.09852156043052673\n","step: 1120, loss: 0.24670268595218658\n","step: 1130, loss: 0.029312945902347565\n","step: 1140, loss: 0.06472539156675339\n","step: 1150, loss: 0.10907093435525894\n","step: 1160, loss: 0.06882833689451218\n","step: 1170, loss: 0.11491509526968002\n","step: 1180, loss: 0.17680594325065613\n","step: 1190, loss: 0.12927252054214478\n","step: 1200, loss: 0.15946625173091888\n","step: 1210, loss: 0.08216866850852966\n","step: 1220, loss: 0.05228077620267868\n","step: 1230, loss: 0.11645704507827759\n","step: 1240, loss: 0.14812691509723663\n","step: 1250, loss: 0.11843816190958023\n","step: 1260, loss: 0.14179390668869019\n","step: 1270, loss: 0.16826258599758148\n","step: 1280, loss: 0.045612581074237823\n","step: 1290, loss: 0.10432443767786026\n","step: 1300, loss: 0.2880942225456238\n","step: 1310, loss: 0.11039788275957108\n","step: 1320, loss: 0.1141812726855278\n","step: 1330, loss: 0.10485267639160156\n","step: 1340, loss: 0.08802307397127151\n","step: 1350, loss: 0.17792999744415283\n","step: 1360, loss: 0.15873602032661438\n","step: 1370, loss: 0.0856715738773346\n","step: 1380, loss: 0.056986089795827866\n","step: 1390, loss: 0.12705880403518677\n","step: 1400, loss: 0.08274848759174347\n","step: 1410, loss: 0.13798312842845917\n","step: 1420, loss: 0.19467835128307343\n","step: 1430, loss: 0.11284103989601135\n","step: 1440, loss: 0.07641280442476273\n","step: 1450, loss: 0.07872039079666138\n","step: 1460, loss: 0.04897882416844368\n","step: 1470, loss: 0.11039262264966965\n","step: 1480, loss: 0.08352097123861313\n","step: 1490, loss: 0.1870208978652954\n","step: 1500, loss: 0.17603377997875214\n","step: 1510, loss: 0.11974354088306427\n","step: 1520, loss: 0.07699432224035263\n","step: 1530, loss: 0.12159574776887894\n","step: 1540, loss: 0.05599885806441307\n","step: 1550, loss: 0.10357286781072617\n","step: 1560, loss: 0.08291035145521164\n","step: 1570, loss: 0.10664454102516174\n","step: 1580, loss: 0.1251264065504074\n","step: 1590, loss: 0.12304927408695221\n","step: 1600, loss: 0.09954244643449783\n","step: 1610, loss: 0.05992281809449196\n","step: 1620, loss: 0.02502560056746006\n","step: 1630, loss: 0.10716219991445541\n","step: 1640, loss: 0.12940382957458496\n","step: 1650, loss: 0.04202261567115784\n","step: 1660, loss: 0.10681594163179398\n","step: 1670, loss: 0.08590385317802429\n","step: 1680, loss: 0.16979201138019562\n","step: 1690, loss: 0.07509225606918335\n","step: 1700, loss: 0.06849745661020279\n","step: 1710, loss: 0.058047614991664886\n","step: 1720, loss: 0.03867517039179802\n","step: 1730, loss: 0.10442236065864563\n","step: 1740, loss: 0.07948876917362213\n","step: 1750, loss: 0.10453686118125916\n","step: 1760, loss: 0.05872605741024017\n","step: 1770, loss: 0.22763460874557495\n","step: 1780, loss: 0.12737584114074707\n","step: 1790, loss: 0.06863678246736526\n","step: 1800, loss: 0.1142108291387558\n","step: 1810, loss: 0.10177861154079437\n","step: 1820, loss: 0.10898740589618683\n","step: 1830, loss: 0.0902877226471901\n","step: 1840, loss: 0.06558944284915924\n","step: 1850, loss: 0.064248226583004\n","step: 1860, loss: 0.023019656538963318\n","step: 1870, loss: 0.08778133988380432\n","step: 1880, loss: 0.12633705139160156\n","step: 1890, loss: 0.10440895706415176\n","step: 1900, loss: 0.11392275243997574\n","step: 1910, loss: 0.10895103961229324\n","step: 1920, loss: 0.054190631955862045\n","step: 1930, loss: 0.021717432886362076\n","step: 1940, loss: 0.049761347472667694\n","step: 1950, loss: 0.07248186320066452\n","step: 1960, loss: 0.09377273172140121\n","step: 1970, loss: 0.07930944114923477\n","step: 1980, loss: 0.09766987711191177\n","step: 1990, loss: 0.04983656480908394\n","step: 2000, loss: 0.12427164614200592\n","step: 2010, loss: 0.12263450771570206\n","step: 2020, loss: 0.15789957344532013\n","step: 2030, loss: 0.15164507925510406\n","step: 2040, loss: 0.05628833547234535\n","step: 2050, loss: 0.08552629500627518\n","step: 2060, loss: 0.10661206394433975\n","step: 2070, loss: 0.06628914922475815\n","step: 2080, loss: 0.073033906519413\n","step: 2090, loss: 0.16847796738147736\n","step: 2100, loss: 0.08864179998636246\n","step: 2110, loss: 0.16658610105514526\n","step: 2120, loss: 0.092229925096035\n","step: 2130, loss: 0.1104709804058075\n","step: 2140, loss: 0.122406505048275\n","step: 2150, loss: 0.05454442650079727\n","step: 2160, loss: 0.11153563112020493\n","step: 2170, loss: 0.17044508457183838\n","step: 2180, loss: 0.032677244395017624\n","step: 2190, loss: 0.1137378141283989\n","step: 2200, loss: 0.11485162377357483\n","step: 2210, loss: 0.09665817022323608\n","step: 2220, loss: 0.058681659400463104\n","step: 2230, loss: 0.02793041244149208\n","step: 2240, loss: 0.19930419325828552\n","step: 2250, loss: 0.058289993554353714\n","step: 2260, loss: 0.05079113319516182\n","step: 2270, loss: 0.14482459425926208\n","step: 2280, loss: 0.02800627611577511\n","step: 2290, loss: 0.0943487212061882\n","step: 2300, loss: 0.06512217968702316\n","step: 2310, loss: 0.13014955818653107\n","step: 2320, loss: 0.0587889738380909\n","step: 2330, loss: 0.04690352454781532\n","step: 2340, loss: 0.14567744731903076\n","step: 2350, loss: 0.08563201874494553\n","step: 2360, loss: 0.10746315121650696\n","step: 2370, loss: 0.12309940904378891\n","step: 2380, loss: 0.07264795899391174\n","step: 2390, loss: 0.020134206861257553\n","step: 2400, loss: 0.05838184803724289\n","step: 2410, loss: 0.09007621556520462\n","step: 2420, loss: 0.15760086476802826\n","step: 2430, loss: 0.06826043128967285\n","step: 2440, loss: 0.02294149436056614\n","step: 2450, loss: 0.08189629763364792\n","step: 2460, loss: 0.05161682143807411\n","step: 2470, loss: 0.11555148661136627\n","step: 2480, loss: 0.163392573595047\n","step: 2490, loss: 0.141248419880867\n","step: 2500, loss: 0.09982132166624069\n","step: 2510, loss: 0.14920245110988617\n","step: 2520, loss: 0.10153187066316605\n","step: 2530, loss: 0.047268543392419815\n","step: 2540, loss: 0.15659090876579285\n","step: 2550, loss: 0.06864996254444122\n","step: 2560, loss: 0.11207839846611023\n","step: 2570, loss: 0.10532687604427338\n","step: 2580, loss: 0.07963158935308456\n","step: 2590, loss: 0.17130570113658905\n","step: 2600, loss: 0.07315466552972794\n","step: 2610, loss: 0.05431431531906128\n","step: 2620, loss: 0.06579434871673584\n","step: 2630, loss: 0.11215180903673172\n","step: 2640, loss: 0.035630885511636734\n","step: 2650, loss: 0.09976848214864731\n","step: 2660, loss: 0.07381933182477951\n","step: 2670, loss: 0.03546285629272461\n","step: 2680, loss: 0.14252124726772308\n","step: 2690, loss: 0.14663700759410858\n","step: 2700, loss: 0.07206214219331741\n","step: 2710, loss: 0.07486031949520111\n","step: 2720, loss: 0.05821514502167702\n","step: 2730, loss: 0.1234438344836235\n","step: 2740, loss: 0.09373123943805695\n","step: 2750, loss: 0.08169927448034286\n","step: 2760, loss: 0.060346562415361404\n","step: 2770, loss: 0.09485902637243271\n","step: 2780, loss: 0.021785546094179153\n","step: 2790, loss: 0.0682327002286911\n","step: 2800, loss: 0.056884583085775375\n","step: 2810, loss: 0.06843873113393784\n","step: 2820, loss: 0.1207592785358429\n","step: 2830, loss: 0.11933080852031708\n","step: 2840, loss: 0.03954833745956421\n","step: 2850, loss: 0.17114470899105072\n","step: 2860, loss: 0.05162892863154411\n","step: 2870, loss: 0.13045892119407654\n","step: 2880, loss: 0.11338918656110764\n","step: 2890, loss: 0.10086965560913086\n","step: 2900, loss: 0.05562208220362663\n","step: 2910, loss: 0.09992071241140366\n","step: 2920, loss: 0.13906286656856537\n","step: 2930, loss: 0.12564536929130554\n","step: 2940, loss: 0.13002844154834747\n","step: 2950, loss: 0.04711889103055\n","step: 2960, loss: 0.2211071252822876\n","step: 2970, loss: 0.10333093255758286\n","step: 2980, loss: 0.12540745735168457\n","step: 2990, loss: 0.059440817683935165\n","step: 3000, loss: 0.09911660104990005\n","step: 3010, loss: 0.04365084692835808\n","step: 3020, loss: 0.06981652975082397\n","step: 3030, loss: 0.09549508988857269\n","step: 3040, loss: 0.09048935770988464\n","step: 3050, loss: 0.07023775577545166\n","step: 3060, loss: 0.1877426952123642\n","step: 3070, loss: 0.09015699476003647\n","step: 3080, loss: 0.0815102830529213\n","step: 3090, loss: 0.12038740515708923\n","step: 3100, loss: 0.08918939530849457\n","step: 3110, loss: 0.0755847916007042\n","step: 3120, loss: 0.029296012595295906\n","step: 3130, loss: 0.07571426779031754\n","step: 3140, loss: 0.05520898103713989\n","step: 3150, loss: 0.13957829773426056\n","step: 3160, loss: 0.15706577897071838\n","step: 3170, loss: 0.08773045986890793\n","step: 3180, loss: 0.11536634713411331\n","step: 3190, loss: 0.08113180845975876\n","step: 3200, loss: 0.03779706358909607\n","step: 3210, loss: 0.06810681521892548\n","step: 3220, loss: 0.061634570360183716\n","step: 3230, loss: 0.023392170667648315\n","step: 3240, loss: 0.07263679057359695\n","step: 3250, loss: 0.17362754046916962\n","step: 3260, loss: 0.042738981544971466\n","step: 3270, loss: 0.09084060788154602\n","step: 3280, loss: 0.10288810729980469\n","step: 3290, loss: 0.06431816518306732\n","step: 3300, loss: 0.06571252644062042\n","step: 3310, loss: 0.0821167379617691\n","step: 3320, loss: 0.1911502629518509\n","step: 3330, loss: 0.06110554561018944\n","step: 3340, loss: 0.052854061126708984\n","step: 3350, loss: 0.029584627598524094\n","step: 3360, loss: 0.07429012656211853\n","step: 3370, loss: 0.13355793058872223\n","step: 3380, loss: 0.06092725694179535\n","step: 3390, loss: 0.09957210719585419\n","step: 3400, loss: 0.1158355176448822\n","step: 3410, loss: 0.13428954780101776\n","step: 3420, loss: 0.13374972343444824\n","step: 3430, loss: 0.058225855231285095\n","step: 3440, loss: 0.026083558797836304\n","step: 3450, loss: 0.06311794370412827\n","step: 3460, loss: 0.0840209573507309\n","step: 3470, loss: 0.08920271694660187\n","step: 3480, loss: 0.06367411464452744\n","step: 3490, loss: 0.12241856008768082\n","step: 3500, loss: 0.059702519327402115\n","step: 3510, loss: 0.10913963615894318\n","step: 3520, loss: 0.0670272707939148\n","step: 3530, loss: 0.06675830483436584\n","step: 3540, loss: 0.1351659595966339\n","step: 3550, loss: 0.08943183720111847\n","step: 3560, loss: 0.16528481245040894\n","step: 3570, loss: 0.09034815430641174\n","step: 3580, loss: 0.0489872507750988\n","step: 3590, loss: 0.07942274212837219\n","step: 3600, loss: 0.05731382220983505\n","step: 3610, loss: 0.05788038298487663\n","step: 3620, loss: 0.16085641086101532\n","step: 3630, loss: 0.04643950238823891\n","step: 3640, loss: 0.05498695746064186\n","step: 3650, loss: 0.10169719904661179\n","step: 3660, loss: 0.09141318500041962\n","step: 3670, loss: 0.11030998826026917\n","step: 3680, loss: 0.06751008331775665\n","step: 3690, loss: 0.0422697477042675\n","step: 3700, loss: 0.10385710746049881\n","step: 3710, loss: 0.043697789311409\n","step: 3720, loss: 0.1393088698387146\n","step: 3730, loss: 0.1339370310306549\n","step: 3740, loss: 0.06810230016708374\n","step: 3750, loss: 0.12270490825176239\n","step: 3760, loss: 0.09864916652441025\n","step: 3770, loss: 0.08893385529518127\n","step: 3780, loss: 0.06888893991708755\n","step: 3790, loss: 0.1515483409166336\n","step: 3800, loss: 0.08210825175046921\n","step: 3810, loss: 0.33812153339385986\n","step: 3820, loss: 0.03853604942560196\n","step: 3830, loss: 0.04060618579387665\n","step: 3840, loss: 0.05666251853108406\n","step: 3850, loss: 0.10536227375268936\n","step: 3860, loss: 0.13518889248371124\n","step: 3870, loss: 0.08607766032218933\n","step: 3880, loss: 0.050254590809345245\n","step: 3890, loss: 0.07946603745222092\n","step: 3900, loss: 0.11399195343255997\n","step: 3910, loss: 0.07876212149858475\n","step: 3920, loss: 0.08347123116254807\n","step: 3930, loss: 0.03544075787067413\n","step: 3940, loss: 0.09475772827863693\n","step: 3950, loss: 0.04964360222220421\n","step: 3960, loss: 0.07953515648841858\n","step: 3970, loss: 0.13692739605903625\n","step: 3980, loss: 0.030581161379814148\n","step: 3990, loss: 0.03744577616453171\n","step: 4000, loss: 0.0465804748237133\n","step: 4010, loss: 0.04659627377986908\n","step: 4020, loss: 0.08683472126722336\n","step: 4030, loss: 0.12709881365299225\n","step: 4040, loss: 0.032619960606098175\n","step: 4050, loss: 0.03871748223900795\n","step: 4060, loss: 0.07398273050785065\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.96      0.92      0.94        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.99      0.84      0.91       291\n","           5       0.94      0.84      0.89       294\n","           6       0.93      0.99      0.96      1570\n","           7       0.58      0.95      0.72       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.93      0.98      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.25      0.92      0.39        13\n","          14       0.30      1.00      0.46        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.88      0.83      0.86      1151\n","          17       0.87      0.98      0.92        41\n","          18       0.94      0.97      0.95        32\n","          19       0.47      0.23      0.31        40\n","          20       0.99      1.00      1.00       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.73      0.83      4175\n","          23       0.71      0.95      0.81      2253\n","          24       0.40      0.73      0.51        44\n","          25       0.86      0.93      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       0.94      0.99      0.96        69\n","          28       0.99      1.00      1.00      1864\n","          29       1.00      0.98      0.99       344\n","          30       0.90      0.89      0.90      1136\n","          31       0.72      0.68      0.70        19\n","          32       0.83      0.62      0.71         8\n","          33       0.77      0.81      0.79        86\n","          34       0.13      0.25      0.17        32\n","          35       0.98      1.00      0.99       474\n","          36       1.00      0.15      0.26       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.97      0.97      0.97       404\n","          39       0.95      0.97      0.96       485\n","          40       0.89      0.97      0.93       573\n","          41       0.94      0.94      0.94       841\n","          42       0.99      0.99      0.99       575\n","          43       0.96      0.88      0.91       152\n","          44       0.87      0.92      0.90        75\n","          46       1.00      0.96      0.98        82\n","          48       0.86      0.08      0.14        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.82      0.78     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","Loop 8\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.8351759910583496\n","step: 10, loss: 2.1066806316375732\n","step: 20, loss: 0.9604615569114685\n","step: 30, loss: 0.3924427628517151\n","step: 40, loss: 0.30013036727905273\n","step: 50, loss: 0.23292863368988037\n","step: 60, loss: 0.18906502425670624\n","step: 70, loss: 0.24661128222942352\n","step: 80, loss: 0.21913781762123108\n","step: 90, loss: 0.19464431703090668\n","step: 100, loss: 0.18617886304855347\n","step: 110, loss: 0.08205337822437286\n","step: 120, loss: 0.08649905771017075\n","step: 130, loss: 0.18566110730171204\n","step: 140, loss: 0.10276804864406586\n","step: 150, loss: 0.12429571896791458\n","step: 160, loss: 0.14546851813793182\n","step: 170, loss: 0.12877807021141052\n","step: 180, loss: 0.11385523527860641\n","step: 190, loss: 0.05925017595291138\n","step: 200, loss: 0.15535056591033936\n","step: 210, loss: 0.08999836444854736\n","step: 220, loss: 0.07373537123203278\n","step: 230, loss: 0.17364977300167084\n","step: 240, loss: 0.170032799243927\n","step: 250, loss: 0.09139011055231094\n","step: 260, loss: 0.16414499282836914\n","step: 270, loss: 0.06684555858373642\n","step: 280, loss: 0.06197912618517876\n","step: 290, loss: 0.0773928165435791\n","step: 300, loss: 0.062119144946336746\n","step: 310, loss: 0.08889616280794144\n","step: 320, loss: 0.15495407581329346\n","step: 330, loss: 0.04928475245833397\n","step: 340, loss: 0.2720758616924286\n","step: 350, loss: 0.11977890133857727\n","step: 360, loss: 0.08755797147750854\n","step: 370, loss: 0.06603150069713593\n","step: 380, loss: 0.07782907783985138\n","step: 390, loss: 0.12327677011489868\n","step: 400, loss: 0.10217227786779404\n","step: 410, loss: 0.07313727587461472\n","step: 420, loss: 0.1452825516462326\n","step: 430, loss: 0.11239616572856903\n","step: 440, loss: 0.09371161460876465\n","step: 450, loss: 0.05718639865517616\n","step: 460, loss: 0.17589543759822845\n","step: 470, loss: 0.08776181191205978\n","step: 480, loss: 0.10570501536130905\n","step: 490, loss: 0.1650991588830948\n","step: 500, loss: 0.2029869705438614\n","step: 510, loss: 0.1021447628736496\n","step: 520, loss: 0.03900125250220299\n","step: 530, loss: 0.09068458527326584\n","step: 540, loss: 0.10665329545736313\n","step: 550, loss: 0.0795709565281868\n","step: 560, loss: 0.13396617770195007\n","step: 570, loss: 0.1945057064294815\n","step: 580, loss: 0.1322138011455536\n","step: 590, loss: 0.04709106683731079\n","step: 600, loss: 0.08576898276805878\n","step: 610, loss: 0.14767758548259735\n","step: 620, loss: 0.05037776008248329\n","step: 630, loss: 0.1458783894777298\n","step: 640, loss: 0.09611604362726212\n","step: 650, loss: 0.0701647698879242\n","step: 660, loss: 0.1789168119430542\n","step: 670, loss: 0.24548684060573578\n","step: 680, loss: 0.10387732833623886\n","step: 690, loss: 0.1002766415476799\n","step: 700, loss: 0.0899670347571373\n","step: 710, loss: 0.07045967876911163\n","step: 720, loss: 0.15622569620609283\n","step: 730, loss: 0.1324172019958496\n","step: 740, loss: 0.08638294786214828\n","step: 750, loss: 0.07078604400157928\n","step: 760, loss: 0.18820257484912872\n","step: 770, loss: 0.08539506793022156\n","step: 780, loss: 0.07770448923110962\n","step: 790, loss: 0.10379066318273544\n","step: 800, loss: 0.08509615808725357\n","step: 810, loss: 0.26178663969039917\n","step: 820, loss: 0.09428413957357407\n","step: 830, loss: 0.2178974151611328\n","step: 840, loss: 0.08458312600851059\n","step: 850, loss: 0.037946056574583054\n","step: 860, loss: 0.15895888209342957\n","step: 870, loss: 0.15518224239349365\n","step: 880, loss: 0.060643840581178665\n","step: 890, loss: 0.13175462186336517\n","step: 900, loss: 0.125161275267601\n","step: 910, loss: 0.08404714614152908\n","step: 920, loss: 0.05838925763964653\n","step: 930, loss: 0.04853564128279686\n","step: 940, loss: 0.021863164380192757\n","step: 950, loss: 0.09550496935844421\n","step: 960, loss: 0.09504258632659912\n","step: 970, loss: 0.07345728576183319\n","step: 980, loss: 0.06947233527898788\n","step: 990, loss: 0.09199706465005875\n","step: 1000, loss: 0.10195207595825195\n","step: 1010, loss: 0.19277167320251465\n","step: 1020, loss: 0.08850497007369995\n","step: 1030, loss: 0.06080517917871475\n","step: 1040, loss: 0.09813794493675232\n","step: 1050, loss: 0.16019557416439056\n","step: 1060, loss: 0.16340287029743195\n","step: 1070, loss: 0.12709002196788788\n","step: 1080, loss: 0.08594320714473724\n","step: 1090, loss: 0.0354473777115345\n","step: 1100, loss: 0.08719329535961151\n","step: 1110, loss: 0.11323485523462296\n","step: 1120, loss: 0.13917671144008636\n","step: 1130, loss: 0.104693703353405\n","step: 1140, loss: 0.051118504256010056\n","step: 1150, loss: 0.1467234045267105\n","step: 1160, loss: 0.058125413954257965\n","step: 1170, loss: 0.12881414592266083\n","step: 1180, loss: 0.0886964350938797\n","step: 1190, loss: 0.06452405452728271\n","step: 1200, loss: 0.0600169375538826\n","step: 1210, loss: 0.07976271957159042\n","step: 1220, loss: 0.06830369681119919\n","step: 1230, loss: 0.05975606292486191\n","step: 1240, loss: 0.10558544099330902\n","step: 1250, loss: 0.05853787064552307\n","step: 1260, loss: 0.12744319438934326\n","step: 1270, loss: 0.08262588828802109\n","step: 1280, loss: 0.06093691661953926\n","step: 1290, loss: 0.06549108773469925\n","step: 1300, loss: 0.09895991533994675\n","step: 1310, loss: 0.14393165707588196\n","step: 1320, loss: 0.0781119093298912\n","step: 1330, loss: 0.10208234190940857\n","step: 1340, loss: 0.061425283551216125\n","step: 1350, loss: 0.06514285504817963\n","step: 1360, loss: 0.05231248214840889\n","step: 1370, loss: 0.16076858341693878\n","step: 1380, loss: 0.05059390142560005\n","step: 1390, loss: 0.051086004823446274\n","step: 1400, loss: 0.1289070099592209\n","step: 1410, loss: 0.08754771202802658\n","step: 1420, loss: 0.07444947957992554\n","step: 1430, loss: 0.19124697148799896\n","step: 1440, loss: 0.1171230673789978\n","step: 1450, loss: 0.15563444793224335\n","step: 1460, loss: 0.045894842594861984\n","step: 1470, loss: 0.08829233050346375\n","step: 1480, loss: 0.10949141532182693\n","step: 1490, loss: 0.1266937553882599\n","step: 1500, loss: 0.15360912680625916\n","step: 1510, loss: 0.10333291441202164\n","step: 1520, loss: 0.0795370563864708\n","step: 1530, loss: 0.07777143269777298\n","step: 1540, loss: 0.08687614649534225\n","step: 1550, loss: 0.07233409583568573\n","step: 1560, loss: 0.09410296380519867\n","step: 1570, loss: 0.03507149964570999\n","step: 1580, loss: 0.048669494688510895\n","step: 1590, loss: 0.2848646938800812\n","step: 1600, loss: 0.12353745847940445\n","step: 1610, loss: 0.09835463762283325\n","step: 1620, loss: 0.06182441487908363\n","step: 1630, loss: 0.03440237417817116\n","step: 1640, loss: 0.12138785421848297\n","step: 1650, loss: 0.0617678202688694\n","step: 1660, loss: 0.06305763870477676\n","step: 1670, loss: 0.08241580426692963\n","step: 1680, loss: 0.08864585310220718\n","step: 1690, loss: 0.1493287980556488\n","step: 1700, loss: 0.10656941682100296\n","step: 1710, loss: 0.04125780612230301\n","step: 1720, loss: 0.12227590382099152\n","step: 1730, loss: 0.0555996336042881\n","step: 1740, loss: 0.10908828675746918\n","step: 1750, loss: 0.12570597231388092\n","step: 1760, loss: 0.03299429267644882\n","step: 1770, loss: 0.039801184087991714\n","step: 1780, loss: 0.030918948352336884\n","step: 1790, loss: 0.07897975295782089\n","step: 1800, loss: 0.03684321790933609\n","step: 1810, loss: 0.09733393788337708\n","step: 1820, loss: 0.21551696956157684\n","step: 1830, loss: 0.03267895430326462\n","step: 1840, loss: 0.10764966160058975\n","step: 1850, loss: 0.13913944363594055\n","step: 1860, loss: 0.08261321485042572\n","step: 1870, loss: 0.06584405153989792\n","step: 1880, loss: 0.05073002353310585\n","step: 1890, loss: 0.10328676551580429\n","step: 1900, loss: 0.09032141417264938\n","step: 1910, loss: 0.05761244520545006\n","step: 1920, loss: 0.13882826268672943\n","step: 1930, loss: 0.13357561826705933\n","step: 1940, loss: 0.06846269220113754\n","step: 1950, loss: 0.028819706290960312\n","step: 1960, loss: 0.06170553341507912\n","step: 1970, loss: 0.06903761625289917\n","step: 1980, loss: 0.11684556305408478\n","step: 1990, loss: 0.019805820658802986\n","step: 2000, loss: 0.04598800837993622\n","step: 2010, loss: 0.11945488303899765\n","step: 2020, loss: 0.0629415437579155\n","step: 2030, loss: 0.07682888209819794\n","step: 2040, loss: 0.09321743249893188\n","step: 2050, loss: 0.04796627536416054\n","step: 2060, loss: 0.0884200856089592\n","step: 2070, loss: 0.10660435259342194\n","step: 2080, loss: 0.13815192878246307\n","step: 2090, loss: 0.14476598799228668\n","step: 2100, loss: 0.08276213705539703\n","step: 2110, loss: 0.5594847798347473\n","step: 2120, loss: 0.19579240679740906\n","step: 2130, loss: 0.10063128173351288\n","step: 2140, loss: 0.08649040013551712\n","step: 2150, loss: 0.0459144189953804\n","step: 2160, loss: 0.09821465611457825\n","step: 2170, loss: 0.08060389012098312\n","step: 2180, loss: 0.08069710433483124\n","step: 2190, loss: 0.10055240988731384\n","step: 2200, loss: 0.10044095665216446\n","step: 2210, loss: 0.10744721442461014\n","step: 2220, loss: 0.09744706749916077\n","step: 2230, loss: 0.19904454052448273\n","step: 2240, loss: 0.23383736610412598\n","step: 2250, loss: 0.08698675036430359\n","step: 2260, loss: 0.05294322967529297\n","step: 2270, loss: 0.09173592180013657\n","step: 2280, loss: 0.07974990457296371\n","step: 2290, loss: 0.07467421144247055\n","step: 2300, loss: 0.08446338027715683\n","step: 2310, loss: 0.04280947893857956\n","step: 2320, loss: 0.07608173787593842\n","step: 2330, loss: 0.09178592264652252\n","step: 2340, loss: 0.1012738049030304\n","step: 2350, loss: 0.09018365293741226\n","step: 2360, loss: 0.1554122269153595\n","step: 2370, loss: 0.07414697855710983\n","step: 2380, loss: 0.07827633619308472\n","step: 2390, loss: 0.15234099328517914\n","step: 2400, loss: 0.08703385293483734\n","step: 2410, loss: 0.08835557103157043\n","step: 2420, loss: 0.024908604100346565\n","step: 2430, loss: 0.04066372662782669\n","step: 2440, loss: 0.06744860857725143\n","step: 2450, loss: 0.17690055072307587\n","step: 2460, loss: 0.09409376978874207\n","step: 2470, loss: 0.14913128316402435\n","step: 2480, loss: 0.07075704634189606\n","step: 2490, loss: 0.0885823667049408\n","step: 2500, loss: 0.035523224622011185\n","step: 2510, loss: 0.05438832566142082\n","step: 2520, loss: 0.05932140350341797\n","step: 2530, loss: 0.08032720535993576\n","step: 2540, loss: 0.11037392914295197\n","step: 2550, loss: 0.030964475125074387\n","step: 2560, loss: 0.07330790907144547\n","step: 2570, loss: 0.0751316174864769\n","step: 2580, loss: 0.1560957431793213\n","step: 2590, loss: 0.07048521935939789\n","step: 2600, loss: 0.12131433188915253\n","step: 2610, loss: 0.08172185719013214\n","step: 2620, loss: 0.12434369325637817\n","step: 2630, loss: 0.08328213542699814\n","step: 2640, loss: 0.08609411120414734\n","step: 2650, loss: 0.10084303468465805\n","step: 2660, loss: 0.18774418532848358\n","step: 2670, loss: 0.07565104216337204\n","step: 2680, loss: 0.09592418372631073\n","step: 2690, loss: 0.2468022108078003\n","step: 2700, loss: 0.10422337800264359\n","step: 2710, loss: 0.08372481912374496\n","step: 2720, loss: 0.10672038793563843\n","step: 2730, loss: 0.0667804405093193\n","step: 2740, loss: 0.031072605401277542\n","step: 2750, loss: 0.05927729234099388\n","step: 2760, loss: 0.03548634424805641\n","step: 2770, loss: 0.15939714014530182\n","step: 2780, loss: 0.10480549931526184\n","step: 2790, loss: 0.10987493395805359\n","step: 2800, loss: 0.15180745720863342\n","step: 2810, loss: 0.15563136339187622\n","step: 2820, loss: 0.03742056339979172\n","step: 2830, loss: 0.07175352424383163\n","step: 2840, loss: 0.11654175072908401\n","step: 2850, loss: 0.05514625087380409\n","step: 2860, loss: 0.06651372462511063\n","step: 2870, loss: 0.0930391401052475\n","step: 2880, loss: 0.06360224634408951\n","step: 2890, loss: 0.07734882831573486\n","step: 2900, loss: 0.03018071874976158\n","step: 2910, loss: 0.06993146240711212\n","step: 2920, loss: 0.09790902584791183\n","step: 2930, loss: 0.07313670217990875\n","step: 2940, loss: 0.1368018537759781\n","step: 2950, loss: 0.10254625976085663\n","step: 2960, loss: 0.1403796672821045\n","step: 2970, loss: 0.05279478430747986\n","step: 2980, loss: 0.16391077637672424\n","step: 2990, loss: 0.0721905454993248\n","step: 3000, loss: 0.017210429534316063\n","step: 3010, loss: 0.058833587914705276\n","step: 3020, loss: 0.07770819962024689\n","step: 3030, loss: 0.27086329460144043\n","step: 3040, loss: 0.14325885474681854\n","step: 3050, loss: 0.12472778558731079\n","step: 3060, loss: 0.06375095248222351\n","step: 3070, loss: 0.10144505649805069\n","step: 3080, loss: 0.2208709865808487\n","step: 3090, loss: 0.10555807501077652\n","step: 3100, loss: 0.10850878059864044\n","step: 3110, loss: 0.037547286599874496\n","step: 3120, loss: 0.12915313243865967\n","step: 3130, loss: 0.13120101392269135\n","step: 3140, loss: 0.1183067336678505\n","step: 3150, loss: 0.03568475320935249\n","step: 3160, loss: 0.10360715538263321\n","step: 3170, loss: 0.10424113273620605\n","step: 3180, loss: 0.057758815586566925\n","step: 3190, loss: 0.09366726130247116\n","step: 3200, loss: 0.13301847875118256\n","step: 3210, loss: 0.10883840918540955\n","step: 3220, loss: 0.13549640774726868\n","step: 3230, loss: 0.1699357032775879\n","step: 3240, loss: 0.13306477665901184\n","step: 3250, loss: 0.12806417047977448\n","step: 3260, loss: 0.07361525297164917\n","step: 3270, loss: 0.06770390272140503\n","step: 3280, loss: 0.0431375652551651\n","step: 3290, loss: 0.16236072778701782\n","step: 3300, loss: 0.09766178578138351\n","step: 3310, loss: 0.09996162354946136\n","step: 3320, loss: 0.06371022760868073\n","step: 3330, loss: 0.12642104923725128\n","step: 3340, loss: 0.06631580740213394\n","step: 3350, loss: 0.04614539071917534\n","step: 3360, loss: 0.15286672115325928\n","step: 3370, loss: 0.12948095798492432\n","step: 3380, loss: 0.18665696680545807\n","step: 3390, loss: 0.07513343542814255\n","step: 3400, loss: 0.0540170855820179\n","step: 3410, loss: 0.06486314535140991\n","step: 3420, loss: 0.18789556622505188\n","step: 3430, loss: 0.08823606371879578\n","step: 3440, loss: 0.13481423258781433\n","step: 3450, loss: 0.1280578225851059\n","step: 3460, loss: 0.07935353368520737\n","step: 3470, loss: 0.2390003800392151\n","step: 3480, loss: 0.03684908151626587\n","step: 3490, loss: 0.15495426952838898\n","step: 3500, loss: 0.045881226658821106\n","step: 3510, loss: 0.0528201125562191\n","step: 3520, loss: 0.04386736452579498\n","step: 3530, loss: 0.08718330413103104\n","step: 3540, loss: 0.2502456605434418\n","step: 3550, loss: 0.03263888880610466\n","step: 3560, loss: 0.1441321074962616\n","step: 3570, loss: 0.09362853318452835\n","step: 3580, loss: 0.12852232158184052\n","step: 3590, loss: 0.11724897474050522\n","step: 3600, loss: 0.11869914829730988\n","step: 3610, loss: 0.12969878315925598\n","step: 3620, loss: 0.08615817874670029\n","step: 3630, loss: 0.11597879976034164\n","step: 3640, loss: 0.12991146743297577\n","step: 3650, loss: 0.14576716721057892\n","step: 3660, loss: 0.03622990846633911\n","step: 3670, loss: 0.0746673196554184\n","step: 3680, loss: 0.0833229199051857\n","step: 3690, loss: 0.10115474462509155\n","step: 3700, loss: 0.0501350574195385\n","step: 3710, loss: 0.07990393787622452\n","step: 3720, loss: 0.07204323261976242\n","step: 3730, loss: 0.02393910102546215\n","step: 3740, loss: 0.09337598830461502\n","step: 3750, loss: 0.07997070252895355\n","step: 3760, loss: 0.051099661737680435\n","step: 3770, loss: 0.018510878086090088\n","step: 3780, loss: 0.04501719772815704\n","step: 3790, loss: 0.10497085005044937\n","step: 3800, loss: 0.09893785417079926\n","step: 3810, loss: 0.06422161310911179\n","step: 3820, loss: 0.38567402958869934\n","step: 3830, loss: 0.05135383829474449\n","step: 3840, loss: 0.058925025165081024\n","step: 3850, loss: 0.060583047568798065\n","step: 3860, loss: 0.05960783734917641\n","step: 3870, loss: 0.18759407103061676\n","step: 3880, loss: 0.15975916385650635\n","step: 3890, loss: 0.038236431777477264\n","step: 3900, loss: 0.07550778239965439\n","step: 3910, loss: 0.054364703595638275\n","step: 3920, loss: 0.024705497547984123\n","step: 3930, loss: 0.07778260856866837\n","step: 3940, loss: 0.19942525029182434\n","step: 3950, loss: 0.031139833852648735\n","step: 3960, loss: 0.04857983812689781\n","step: 3970, loss: 0.10679485648870468\n","step: 3980, loss: 0.16909195482730865\n","step: 3990, loss: 0.08188865333795547\n","step: 4000, loss: 0.10219137370586395\n","step: 4010, loss: 0.1542651206254959\n","step: 4020, loss: 0.07967700809240341\n","step: 4030, loss: 0.09700369834899902\n","step: 4040, loss: 0.06525024771690369\n","step: 4050, loss: 0.13311469554901123\n","step: 4060, loss: 0.0322175957262516\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.78      1.00      0.88        35\n","           2       0.96      0.90      0.93        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.96      0.84      0.89       291\n","           5       0.96      0.80      0.87       294\n","           6       0.94      0.98      0.96      1570\n","           7       0.61      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.98      0.99       689\n","          10       0.96      0.97      0.96       901\n","          11       0.99      0.99      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       1.00      0.08      0.14        13\n","          14       0.25      1.00      0.40        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.87      0.82      0.85      1151\n","          17       0.91      0.98      0.94        41\n","          18       0.94      0.97      0.95        32\n","          19       0.82      0.82      0.82        40\n","          20       0.99      0.98      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.73      0.82      4175\n","          23       0.66      0.97      0.79      2253\n","          24       0.26      0.27      0.26        44\n","          25       0.86      0.91      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.99      1.00      0.99        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.83      0.89      1136\n","          31       0.55      0.63      0.59        19\n","          32       1.00      0.62      0.77         8\n","          33       0.60      0.98      0.74        86\n","          34       0.22      0.50      0.31        32\n","          35       0.98      0.99      0.99       474\n","          36       1.00      0.10      0.19       182\n","          37       0.89      0.94      0.91      1592\n","          38       0.97      0.98      0.97       404\n","          39       0.94      0.97      0.96       485\n","          40       0.89      0.95      0.92       573\n","          41       0.94      0.94      0.94       841\n","          42       0.98      0.99      0.99       575\n","          43       0.98      0.82      0.89       152\n","          44       0.86      0.97      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       1.00      0.09      0.16        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.83      0.80      0.78     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n","\n","Loop 9\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9851202964782715\n","step: 10, loss: 2.224785566329956\n","step: 20, loss: 0.9794295430183411\n","step: 30, loss: 1.119818091392517\n","step: 40, loss: 0.26466700434684753\n","step: 50, loss: 0.2720685601234436\n","step: 60, loss: 0.23667916655540466\n","step: 70, loss: 0.2318653166294098\n","step: 80, loss: 0.21900655329227448\n","step: 90, loss: 0.32603392004966736\n","step: 100, loss: 0.14908678829669952\n","step: 110, loss: 0.3333498537540436\n","step: 120, loss: 0.09357237070798874\n","step: 130, loss: 0.1587657332420349\n","step: 140, loss: 0.252299427986145\n","step: 150, loss: 0.11623068153858185\n","step: 160, loss: 0.312703400850296\n","step: 170, loss: 0.11389130353927612\n","step: 180, loss: 0.19228552281856537\n","step: 190, loss: 0.26803186535835266\n","step: 200, loss: 0.2172648161649704\n","step: 210, loss: 0.07492701709270477\n","step: 220, loss: 0.12881827354431152\n","step: 230, loss: 0.13540121912956238\n","step: 240, loss: 0.052153799682855606\n","step: 250, loss: 0.1450689435005188\n","step: 260, loss: 0.045438844710588455\n","step: 270, loss: 0.09178336709737778\n","step: 280, loss: 0.14402802288532257\n","step: 290, loss: 0.18730004131793976\n","step: 300, loss: 0.15290239453315735\n","step: 310, loss: 0.15119668841362\n","step: 320, loss: 0.09590043127536774\n","step: 330, loss: 0.12334795296192169\n","step: 340, loss: 0.09569177031517029\n","step: 350, loss: 0.08164406567811966\n","step: 360, loss: 0.2551974356174469\n","step: 370, loss: 0.1357089877128601\n","step: 380, loss: 0.0889459103345871\n","step: 390, loss: 0.22665078938007355\n","step: 400, loss: 0.06395325064659119\n","step: 410, loss: 0.23415815830230713\n","step: 420, loss: 0.170698344707489\n","step: 430, loss: 0.07892074435949326\n","step: 440, loss: 0.06106780841946602\n","step: 450, loss: 0.1221093013882637\n","step: 460, loss: 0.07509232312440872\n","step: 470, loss: 0.09173905849456787\n","step: 480, loss: 0.07734248787164688\n","step: 490, loss: 0.12059282511472702\n","step: 500, loss: 0.10199782252311707\n","step: 510, loss: 0.16582180559635162\n","step: 520, loss: 0.08514972031116486\n","step: 530, loss: 0.04704361408948898\n","step: 540, loss: 0.14056000113487244\n","step: 550, loss: 0.10696632415056229\n","step: 560, loss: 0.08121731877326965\n","step: 570, loss: 0.06912335753440857\n","step: 580, loss: 0.093340203166008\n","step: 590, loss: 0.11304587125778198\n","step: 600, loss: 0.089970663189888\n","step: 610, loss: 0.0773131474852562\n","step: 620, loss: 0.0429987758398056\n","step: 630, loss: 0.04340604692697525\n","step: 640, loss: 0.05609581992030144\n","step: 650, loss: 0.10005886107683182\n","step: 660, loss: 0.10085632652044296\n","step: 670, loss: 0.10532648861408234\n","step: 680, loss: 0.14252158999443054\n","step: 690, loss: 0.09658423811197281\n","step: 700, loss: 0.05067332461476326\n","step: 710, loss: 0.13786205649375916\n","step: 720, loss: 0.07845460623502731\n","step: 730, loss: 0.09269317984580994\n","step: 740, loss: 0.06276516616344452\n","step: 750, loss: 0.08573117852210999\n","step: 760, loss: 0.2691883444786072\n","step: 770, loss: 0.08958553522825241\n","step: 780, loss: 0.1267603188753128\n","step: 790, loss: 0.04985130578279495\n","step: 800, loss: 0.06249488890171051\n","step: 810, loss: 0.0341629758477211\n","step: 820, loss: 0.03576111048460007\n","step: 830, loss: 0.12079213559627533\n","step: 840, loss: 0.11822885274887085\n","step: 850, loss: 0.07120201736688614\n","step: 860, loss: 0.21986079216003418\n","step: 870, loss: 0.09421814978122711\n","step: 880, loss: 0.1480579376220703\n","step: 890, loss: 0.09256099164485931\n","step: 900, loss: 0.045892275869846344\n","step: 910, loss: 0.13460642099380493\n","step: 920, loss: 0.052457306534051895\n","step: 930, loss: 0.10275845974683762\n","step: 940, loss: 0.14491334557533264\n","step: 950, loss: 0.02113640308380127\n","step: 960, loss: 0.024177568033337593\n","step: 970, loss: 0.06251738965511322\n","step: 980, loss: 0.06846742331981659\n","step: 990, loss: 0.04418667033314705\n","step: 1000, loss: 0.15719905495643616\n","step: 1010, loss: 0.08080369979143143\n","step: 1020, loss: 0.18529576063156128\n","step: 1030, loss: 0.10724323242902756\n","step: 1040, loss: 0.09229831397533417\n","step: 1050, loss: 0.20641271770000458\n","step: 1060, loss: 0.08986767381429672\n","step: 1070, loss: 0.13894005119800568\n","step: 1080, loss: 0.15954230725765228\n","step: 1090, loss: 0.10072237998247147\n","step: 1100, loss: 0.06522595137357712\n","step: 1110, loss: 0.16133832931518555\n","step: 1120, loss: 0.13879191875457764\n","step: 1130, loss: 0.11039195209741592\n","step: 1140, loss: 0.11763671040534973\n","step: 1150, loss: 0.06658076494932175\n","step: 1160, loss: 0.07819438725709915\n","step: 1170, loss: 0.0897347629070282\n","step: 1180, loss: 0.021947283297777176\n","step: 1190, loss: 0.08931408077478409\n","step: 1200, loss: 0.06896384805440903\n","step: 1210, loss: 0.09036725014448166\n","step: 1220, loss: 0.055948372930288315\n","step: 1230, loss: 0.08273272961378098\n","step: 1240, loss: 0.052148301154375076\n","step: 1250, loss: 0.15323059260845184\n","step: 1260, loss: 0.07788635790348053\n","step: 1270, loss: 0.13710308074951172\n","step: 1280, loss: 0.027277659624814987\n","step: 1290, loss: 0.09651704877614975\n","step: 1300, loss: 0.04724057391285896\n","step: 1310, loss: 0.0742054432630539\n","step: 1320, loss: 0.10651876777410507\n","step: 1330, loss: 0.10410656034946442\n","step: 1340, loss: 0.0862249881029129\n","step: 1350, loss: 0.09361982345581055\n","step: 1360, loss: 0.07136496901512146\n","step: 1370, loss: 0.0774477943778038\n","step: 1380, loss: 0.028708677738904953\n","step: 1390, loss: 0.061547573655843735\n","step: 1400, loss: 0.09375736862421036\n","step: 1410, loss: 0.081022709608078\n","step: 1420, loss: 0.10843212157487869\n","step: 1430, loss: 0.12508074939250946\n","step: 1440, loss: 0.10562971979379654\n","step: 1450, loss: 0.05141965299844742\n","step: 1460, loss: 0.056597013026475906\n","step: 1470, loss: 0.04144219309091568\n","step: 1480, loss: 0.0819767415523529\n","step: 1490, loss: 0.0684707909822464\n","step: 1500, loss: 0.19755367934703827\n","step: 1510, loss: 0.030654283240437508\n","step: 1520, loss: 0.1294521987438202\n","step: 1530, loss: 0.04849432036280632\n","step: 1540, loss: 0.06060834974050522\n","step: 1550, loss: 0.24548207223415375\n","step: 1560, loss: 0.24378280341625214\n","step: 1570, loss: 0.1487293690443039\n","step: 1580, loss: 0.04378403350710869\n","step: 1590, loss: 0.11122970283031464\n","step: 1600, loss: 0.10115186870098114\n","step: 1610, loss: 0.16082333028316498\n","step: 1620, loss: 0.17919903993606567\n","step: 1630, loss: 0.1430625170469284\n","step: 1640, loss: 0.09224411845207214\n","step: 1650, loss: 0.1453159898519516\n","step: 1660, loss: 0.07339966297149658\n","step: 1670, loss: 0.06378671526908875\n","step: 1680, loss: 0.11861186474561691\n","step: 1690, loss: 0.11898140609264374\n","step: 1700, loss: 0.11825244873762131\n","step: 1710, loss: 0.0701996460556984\n","step: 1720, loss: 0.15270747244358063\n","step: 1730, loss: 0.121138796210289\n","step: 1740, loss: 0.06823549419641495\n","step: 1750, loss: 0.23443607985973358\n","step: 1760, loss: 0.07444056123495102\n","step: 1770, loss: 0.13812759518623352\n","step: 1780, loss: 0.2234673798084259\n","step: 1790, loss: 0.06095375865697861\n","step: 1800, loss: 0.04121504724025726\n","step: 1810, loss: 0.0890386700630188\n","step: 1820, loss: 0.0897882729768753\n","step: 1830, loss: 0.04980688542127609\n","step: 1840, loss: 0.09806262701749802\n","step: 1850, loss: 0.07134753465652466\n","step: 1860, loss: 0.06747482717037201\n","step: 1870, loss: 0.07584938406944275\n","step: 1880, loss: 0.07292506098747253\n","step: 1890, loss: 0.03844347596168518\n","step: 1900, loss: 0.2592943012714386\n","step: 1910, loss: 0.06678357720375061\n","step: 1920, loss: 0.05148913711309433\n","step: 1930, loss: 0.1260705292224884\n","step: 1940, loss: 0.05851303040981293\n","step: 1950, loss: 0.14303821325302124\n","step: 1960, loss: 0.049604225903749466\n","step: 1970, loss: 0.07597151398658752\n","step: 1980, loss: 0.055438704788684845\n","step: 1990, loss: 0.06472395360469818\n","step: 2000, loss: 0.06821775436401367\n","step: 2010, loss: 0.14155365526676178\n","step: 2020, loss: 0.09820077568292618\n","step: 2030, loss: 0.12758667767047882\n","step: 2040, loss: 0.03682759404182434\n","step: 2050, loss: 0.20315679907798767\n","step: 2060, loss: 0.08649341017007828\n","step: 2070, loss: 0.07862640172243118\n","step: 2080, loss: 0.06111547723412514\n","step: 2090, loss: 0.04405863955616951\n","step: 2100, loss: 0.05975071340799332\n","step: 2110, loss: 0.180500328540802\n","step: 2120, loss: 0.08337506651878357\n","step: 2130, loss: 0.05479982867836952\n","step: 2140, loss: 0.07436651736497879\n","step: 2150, loss: 0.09309350699186325\n","step: 2160, loss: 0.047007933259010315\n","step: 2170, loss: 0.04204588755965233\n","step: 2180, loss: 0.062089934945106506\n","step: 2190, loss: 0.06619708985090256\n","step: 2200, loss: 0.05450328812003136\n","step: 2210, loss: 0.07885415852069855\n","step: 2220, loss: 0.12189196795225143\n","step: 2230, loss: 0.14583131670951843\n","step: 2240, loss: 0.09509933739900589\n","step: 2250, loss: 0.07795354723930359\n","step: 2260, loss: 0.08310742676258087\n","step: 2270, loss: 0.06077193096280098\n","step: 2280, loss: 0.15592172741889954\n","step: 2290, loss: 0.030170323327183723\n","step: 2300, loss: 0.04015602171421051\n","step: 2310, loss: 0.047037746757268906\n","step: 2320, loss: 0.10550542920827866\n","step: 2330, loss: 0.06940720230340958\n","step: 2340, loss: 0.07825466990470886\n","step: 2350, loss: 0.15977102518081665\n","step: 2360, loss: 0.11642652004957199\n","step: 2370, loss: 0.05272580310702324\n","step: 2380, loss: 0.21976563334465027\n","step: 2390, loss: 0.04815692454576492\n","step: 2400, loss: 0.1340695023536682\n","step: 2410, loss: 0.1343306303024292\n","step: 2420, loss: 0.1455845981836319\n","step: 2430, loss: 0.11595194041728973\n","step: 2440, loss: 0.08807485550642014\n","step: 2450, loss: 0.10724274814128876\n","step: 2460, loss: 0.04145020991563797\n","step: 2470, loss: 0.07367538660764694\n","step: 2480, loss: 0.0549577921628952\n","step: 2490, loss: 0.17877337336540222\n","step: 2500, loss: 0.21029126644134521\n","step: 2510, loss: 0.1429334133863449\n","step: 2520, loss: 0.15729162096977234\n","step: 2530, loss: 0.08496377617120743\n","step: 2540, loss: 0.05714244022965431\n","step: 2550, loss: 0.12503716349601746\n","step: 2560, loss: 0.1654348373413086\n","step: 2570, loss: 0.24792636930942535\n","step: 2580, loss: 0.01849912665784359\n","step: 2590, loss: 0.05520695820450783\n","step: 2600, loss: 0.09176810085773468\n","step: 2610, loss: 0.19406233727931976\n","step: 2620, loss: 0.10967221111059189\n","step: 2630, loss: 0.09685110300779343\n","step: 2640, loss: 0.1084885522723198\n","step: 2650, loss: 0.1115955039858818\n","step: 2660, loss: 0.12308517843484879\n","step: 2670, loss: 0.056873831897974014\n","step: 2680, loss: 0.11906035244464874\n","step: 2690, loss: 0.2156890332698822\n","step: 2700, loss: 0.06049490347504616\n","step: 2710, loss: 0.061934925615787506\n","step: 2720, loss: 0.08567822724580765\n","step: 2730, loss: 0.2606249153614044\n","step: 2740, loss: 0.09072811156511307\n","step: 2750, loss: 0.02528996765613556\n","step: 2760, loss: 0.08981655538082123\n","step: 2770, loss: 0.08804754912853241\n","step: 2780, loss: 0.1534935086965561\n","step: 2790, loss: 0.08308067917823792\n","step: 2800, loss: 0.08373498171567917\n","step: 2810, loss: 0.0778016597032547\n","step: 2820, loss: 0.1497056484222412\n","step: 2830, loss: 0.08452144265174866\n","step: 2840, loss: 0.15528252720832825\n","step: 2850, loss: 0.08126630634069443\n","step: 2860, loss: 0.1208660900592804\n","step: 2870, loss: 0.051018912345170975\n","step: 2880, loss: 0.09456778317689896\n","step: 2890, loss: 0.0833863615989685\n","step: 2900, loss: 0.029242394492030144\n","step: 2910, loss: 0.08047671616077423\n","step: 2920, loss: 0.07012078166007996\n","step: 2930, loss: 0.052201367914676666\n","step: 2940, loss: 0.07170611619949341\n","step: 2950, loss: 0.09185291081666946\n","step: 2960, loss: 0.09231793880462646\n","step: 2970, loss: 0.2114802449941635\n","step: 2980, loss: 0.07772880047559738\n","step: 2990, loss: 0.12595517933368683\n","step: 3000, loss: 0.03180515393614769\n","step: 3010, loss: 0.05256510525941849\n","step: 3020, loss: 0.10091190040111542\n","step: 3030, loss: 0.10614128410816193\n","step: 3040, loss: 0.03740396350622177\n","step: 3050, loss: 0.11043915152549744\n","step: 3060, loss: 0.12661829590797424\n","step: 3070, loss: 0.0935409665107727\n","step: 3080, loss: 0.06777551770210266\n","step: 3090, loss: 0.1006581261754036\n","step: 3100, loss: 0.10151775181293488\n","step: 3110, loss: 0.10181408375501633\n","step: 3120, loss: 0.01865386590361595\n","step: 3130, loss: 0.027810726314783096\n","step: 3140, loss: 0.041677895933389664\n","step: 3150, loss: 0.053053464740514755\n","step: 3160, loss: 0.16951514780521393\n","step: 3170, loss: 0.06901470571756363\n","step: 3180, loss: 0.049653198570013046\n","step: 3190, loss: 0.07418486475944519\n","step: 3200, loss: 0.10717467963695526\n","step: 3210, loss: 0.015579422004520893\n","step: 3220, loss: 0.0964401587843895\n","step: 3230, loss: 0.10895783454179764\n","step: 3240, loss: 0.13516736030578613\n","step: 3250, loss: 0.04372898489236832\n","step: 3260, loss: 0.06627154350280762\n","step: 3270, loss: 0.006839616224169731\n","step: 3280, loss: 0.12147394567728043\n","step: 3290, loss: 0.059184323996305466\n","step: 3300, loss: 0.03871282935142517\n","step: 3310, loss: 0.01840643770992756\n","step: 3320, loss: 0.07025771588087082\n","step: 3330, loss: 0.1297571212053299\n","step: 3340, loss: 0.05417635664343834\n","step: 3350, loss: 0.1224687248468399\n","step: 3360, loss: 0.022415906190872192\n","step: 3370, loss: 0.20482581853866577\n","step: 3380, loss: 0.039110735058784485\n","step: 3390, loss: 0.08640199154615402\n","step: 3400, loss: 0.06089707091450691\n","step: 3410, loss: 0.012750883586704731\n","step: 3420, loss: 0.07803905755281448\n","step: 3430, loss: 0.03403659909963608\n","step: 3440, loss: 0.08169915527105331\n","step: 3450, loss: 0.19875983893871307\n","step: 3460, loss: 0.03564603999257088\n","step: 3470, loss: 0.05854257196187973\n","step: 3480, loss: 0.07057897746562958\n","step: 3490, loss: 0.05490947514772415\n","step: 3500, loss: 0.07371729612350464\n","step: 3510, loss: 0.13592734932899475\n","step: 3520, loss: 0.025916047394275665\n","step: 3530, loss: 0.041541535407304764\n","step: 3540, loss: 0.0540674552321434\n","step: 3550, loss: 0.11890656501054764\n","step: 3560, loss: 0.11953535676002502\n","step: 3570, loss: 0.0615205354988575\n","step: 3580, loss: 0.14626124501228333\n","step: 3590, loss: 0.10180336982011795\n","step: 3600, loss: 0.06049013510346413\n","step: 3610, loss: 0.09699741005897522\n","step: 3620, loss: 0.039832912385463715\n","step: 3630, loss: 0.032964326441287994\n","step: 3640, loss: 0.12410114705562592\n","step: 3650, loss: 0.05019025504589081\n","step: 3660, loss: 0.12408290058374405\n","step: 3670, loss: 0.060607071965932846\n","step: 3680, loss: 0.05467737466096878\n","step: 3690, loss: 0.1324814110994339\n","step: 3700, loss: 0.14462071657180786\n","step: 3710, loss: 0.13243332505226135\n","step: 3720, loss: 0.04738684371113777\n","step: 3730, loss: 0.07321266084909439\n","step: 3740, loss: 0.043425146490335464\n","step: 3750, loss: 0.07259906083345413\n","step: 3760, loss: 0.07041041553020477\n","step: 3770, loss: 0.012167111039161682\n","step: 3780, loss: 0.06411129981279373\n","step: 3790, loss: 0.05238615721464157\n","step: 3800, loss: 0.08812632411718369\n","step: 3810, loss: 0.02320459857583046\n","step: 3820, loss: 0.13434894382953644\n","step: 3830, loss: 0.10765193402767181\n","step: 3840, loss: 0.051860541105270386\n","step: 3850, loss: 0.09400836378335953\n","step: 3860, loss: 0.1193082332611084\n","step: 3870, loss: 0.06291548907756805\n","step: 3880, loss: 0.10957175493240356\n","step: 3890, loss: 0.20958827435970306\n","step: 3900, loss: 0.0524391233921051\n","step: 3910, loss: 0.06896300613880157\n","step: 3920, loss: 0.14027798175811768\n","step: 3930, loss: 0.0480828657746315\n","step: 3940, loss: 0.17010119557380676\n","step: 3950, loss: 0.07221817225217819\n","step: 3960, loss: 0.0311150960624218\n","step: 3970, loss: 0.11006943136453629\n","step: 3980, loss: 0.1982874721288681\n","step: 3990, loss: 0.0748143121600151\n","step: 4000, loss: 0.11290021240711212\n","step: 4010, loss: 0.058476630598306656\n","step: 4020, loss: 0.08022413402795792\n","step: 4030, loss: 0.1445545107126236\n","step: 4040, loss: 0.09662993252277374\n","step: 4050, loss: 0.12871460616588593\n","step: 4060, loss: 0.2537047863006592\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.88      0.97      0.93        77\n","           3       1.00      0.79      0.89      1030\n","           4       1.00      0.85      0.92       291\n","           5       0.98      0.84      0.91       294\n","           6       0.95      0.96      0.95      1570\n","           7       0.47      0.95      0.63       186\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.99      0.99       689\n","          10       0.98      0.95      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.94      0.98      0.96        47\n","          13       0.28      0.85      0.42        13\n","          14       0.33      1.00      0.50        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.91      0.82      0.86      1151\n","          17       0.97      0.95      0.96        41\n","          18       0.91      0.97      0.94        32\n","          19       0.82      0.68      0.74        40\n","          20       0.98      1.00      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.75      0.83      4175\n","          23       0.67      0.97      0.80      2253\n","          24       0.00      0.00      0.00        44\n","          25       0.86      0.91      0.88       888\n","          26       0.70      0.78      0.74         9\n","          27       0.93      1.00      0.97        69\n","          28       1.00      0.99      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.94      0.86      0.90      1136\n","          31       0.55      0.84      0.67        19\n","          32       0.83      0.62      0.71         8\n","          33       0.81      0.83      0.82        86\n","          34       0.25      0.56      0.35        32\n","          35       0.98      0.99      0.99       474\n","          36       1.00      0.10      0.18       182\n","          37       0.88      0.96      0.92      1592\n","          38       0.98      0.97      0.97       404\n","          39       0.97      0.96      0.96       485\n","          40       0.91      0.96      0.93       573\n","          41       0.96      0.94      0.95       841\n","          42       0.99      0.98      0.99       575\n","          43       0.96      0.82      0.88       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.83      0.06      0.12        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.80      0.81      0.78     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","Loop 10\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.889564037322998\n","step: 10, loss: 1.879541277885437\n","step: 20, loss: 0.852138102054596\n","step: 30, loss: 0.6019236445426941\n","step: 40, loss: 0.29497015476226807\n","step: 50, loss: 0.2186659574508667\n","step: 60, loss: 0.13997168838977814\n","step: 70, loss: 0.21559494733810425\n","step: 80, loss: 0.14445188641548157\n","step: 90, loss: 0.16802670061588287\n","step: 100, loss: 0.17704999446868896\n","step: 110, loss: 0.09666986763477325\n","step: 120, loss: 0.18809185922145844\n","step: 130, loss: 0.10295765101909637\n","step: 140, loss: 0.12088201195001602\n","step: 150, loss: 0.20180293917655945\n","step: 160, loss: 0.1826397180557251\n","step: 170, loss: 0.11122650653123856\n","step: 180, loss: 0.10047156363725662\n","step: 190, loss: 0.20482635498046875\n","step: 200, loss: 0.2059742510318756\n","step: 210, loss: 0.15915395319461823\n","step: 220, loss: 0.19827055931091309\n","step: 230, loss: 0.13552021980285645\n","step: 240, loss: 0.17386871576309204\n","step: 250, loss: 0.08044538646936417\n","step: 260, loss: 0.12348268181085587\n","step: 270, loss: 0.08928632736206055\n","step: 280, loss: 0.12540002167224884\n","step: 290, loss: 0.12005183845758438\n","step: 300, loss: 0.0330798365175724\n","step: 310, loss: 0.22009249031543732\n","step: 320, loss: 0.1575736403465271\n","step: 330, loss: 0.05795110762119293\n","step: 340, loss: 0.05931124836206436\n","step: 350, loss: 0.15128108859062195\n","step: 360, loss: 0.26602503657341003\n","step: 370, loss: 0.10015183687210083\n","step: 380, loss: 0.10392238944768906\n","step: 390, loss: 0.06515901535749435\n","step: 400, loss: 0.1963050365447998\n","step: 410, loss: 0.06379053741693497\n","step: 420, loss: 0.04273750260472298\n","step: 430, loss: 0.15128548443317413\n","step: 440, loss: 0.24575376510620117\n","step: 450, loss: 0.09290488809347153\n","step: 460, loss: 0.13573019206523895\n","step: 470, loss: 0.10178809612989426\n","step: 480, loss: 0.14562354981899261\n","step: 490, loss: 0.08363845199346542\n","step: 500, loss: 0.111603744328022\n","step: 510, loss: 0.1795545518398285\n","step: 520, loss: 0.09979988634586334\n","step: 530, loss: 0.10159572213888168\n","step: 540, loss: 0.10259868949651718\n","step: 550, loss: 0.1459914892911911\n","step: 560, loss: 0.1769544929265976\n","step: 570, loss: 0.08280880004167557\n","step: 580, loss: 0.09318303316831589\n","step: 590, loss: 0.19804663956165314\n","step: 600, loss: 0.1359202116727829\n","step: 610, loss: 0.10794137418270111\n","step: 620, loss: 0.0618763230741024\n","step: 630, loss: 0.06452619284391403\n","step: 640, loss: 0.09954607486724854\n","step: 650, loss: 0.2781170606613159\n","step: 660, loss: 0.2028271108865738\n","step: 670, loss: 0.12945987284183502\n","step: 680, loss: 0.11371225863695145\n","step: 690, loss: 0.17063646018505096\n","step: 700, loss: 0.17247924208641052\n","step: 710, loss: 0.07243136316537857\n","step: 720, loss: 0.07450450211763382\n","step: 730, loss: 0.13916191458702087\n","step: 740, loss: 0.16816040873527527\n","step: 750, loss: 0.18674878776073456\n","step: 760, loss: 0.06399424374103546\n","step: 770, loss: 0.06020142510533333\n","step: 780, loss: 0.05951670557260513\n","step: 790, loss: 0.07631745934486389\n","step: 800, loss: 0.10283665359020233\n","step: 810, loss: 0.05688997358083725\n","step: 820, loss: 0.05212182551622391\n","step: 830, loss: 0.07012002170085907\n","step: 840, loss: 0.049016840755939484\n","step: 850, loss: 0.13026870787143707\n","step: 860, loss: 0.09517402946949005\n","step: 870, loss: 0.043114498257637024\n","step: 880, loss: 0.12333278357982635\n","step: 890, loss: 0.06760700792074203\n","step: 900, loss: 0.09076157957315445\n","step: 910, loss: 0.1580839306116104\n","step: 920, loss: 0.0418105348944664\n","step: 930, loss: 0.12329771369695663\n","step: 940, loss: 0.17002180218696594\n","step: 950, loss: 0.1115475445985794\n","step: 960, loss: 0.07307744771242142\n","step: 970, loss: 0.06351509690284729\n","step: 980, loss: 0.07097829878330231\n","step: 990, loss: 0.22580395638942719\n","step: 1000, loss: 0.05984290689229965\n","step: 1010, loss: 0.13011285662651062\n","step: 1020, loss: 0.05706308037042618\n","step: 1030, loss: 0.059870559722185135\n","step: 1040, loss: 0.2629563808441162\n","step: 1050, loss: 0.11582297086715698\n","step: 1060, loss: 0.0970647856593132\n","step: 1070, loss: 0.22642584145069122\n","step: 1080, loss: 0.10112888365983963\n","step: 1090, loss: 0.08571495115756989\n","step: 1100, loss: 0.11253447830677032\n","step: 1110, loss: 0.05059189349412918\n","step: 1120, loss: 0.06626034528017044\n","step: 1130, loss: 0.25001290440559387\n","step: 1140, loss: 0.1250079721212387\n","step: 1150, loss: 0.19643010199069977\n","step: 1160, loss: 0.05528832599520683\n","step: 1170, loss: 0.09533552080392838\n","step: 1180, loss: 0.21534627676010132\n","step: 1190, loss: 0.0522400438785553\n","step: 1200, loss: 0.048460669815540314\n","step: 1210, loss: 0.04758865013718605\n","step: 1220, loss: 0.10633661597967148\n","step: 1230, loss: 0.1190786212682724\n","step: 1240, loss: 0.11752597987651825\n","step: 1250, loss: 0.05483029782772064\n","step: 1260, loss: 0.12310110032558441\n","step: 1270, loss: 0.13635043799877167\n","step: 1280, loss: 0.11080615222454071\n","step: 1290, loss: 0.027877332642674446\n","step: 1300, loss: 0.1537923365831375\n","step: 1310, loss: 0.10106021910905838\n","step: 1320, loss: 0.11662592738866806\n","step: 1330, loss: 0.14855286478996277\n","step: 1340, loss: 0.0360499806702137\n","step: 1350, loss: 0.053238287568092346\n","step: 1360, loss: 0.16238155961036682\n","step: 1370, loss: 0.14745710790157318\n","step: 1380, loss: 0.06969571858644485\n","step: 1390, loss: 0.08535322546958923\n","step: 1400, loss: 0.11727222055196762\n","step: 1410, loss: 0.08603481203317642\n","step: 1420, loss: 0.04913690313696861\n","step: 1430, loss: 0.05542556568980217\n","step: 1440, loss: 0.02918050065636635\n","step: 1450, loss: 0.068292535841465\n","step: 1460, loss: 0.1328475922346115\n","step: 1470, loss: 0.0640905573964119\n","step: 1480, loss: 0.12745364010334015\n","step: 1490, loss: 0.11058671027421951\n","step: 1500, loss: 0.03558645397424698\n","step: 1510, loss: 0.07753491401672363\n","step: 1520, loss: 0.14575718343257904\n","step: 1530, loss: 0.159299835562706\n","step: 1540, loss: 0.10916925221681595\n","step: 1550, loss: 0.1676393300294876\n","step: 1560, loss: 0.08550963550806046\n","step: 1570, loss: 0.13744376599788666\n","step: 1580, loss: 0.10865242034196854\n","step: 1590, loss: 0.040882691740989685\n","step: 1600, loss: 0.08118262887001038\n","step: 1610, loss: 0.12540429830551147\n","step: 1620, loss: 0.25928792357444763\n","step: 1630, loss: 0.06836926937103271\n","step: 1640, loss: 0.18116973340511322\n","step: 1650, loss: 0.132989302277565\n","step: 1660, loss: 0.19521640241146088\n","step: 1670, loss: 0.09623079001903534\n","step: 1680, loss: 0.08331677317619324\n","step: 1690, loss: 0.022319814190268517\n","step: 1700, loss: 0.22940374910831451\n","step: 1710, loss: 0.14159941673278809\n","step: 1720, loss: 0.08219192177057266\n","step: 1730, loss: 0.0420617014169693\n","step: 1740, loss: 0.1577150821685791\n","step: 1750, loss: 0.11214248090982437\n","step: 1760, loss: 0.04570387303829193\n","step: 1770, loss: 0.11038989573717117\n","step: 1780, loss: 0.05885152891278267\n","step: 1790, loss: 0.06878995150327682\n","step: 1800, loss: 0.021569354459643364\n","step: 1810, loss: 0.10650414973497391\n","step: 1820, loss: 0.09720951318740845\n","step: 1830, loss: 0.09454480558633804\n","step: 1840, loss: 0.1813695728778839\n","step: 1850, loss: 0.06406733393669128\n","step: 1860, loss: 0.19061851501464844\n","step: 1870, loss: 0.05809018015861511\n","step: 1880, loss: 0.05097511410713196\n","step: 1890, loss: 0.10603165626525879\n","step: 1900, loss: 0.14214210212230682\n","step: 1910, loss: 0.11975046247243881\n","step: 1920, loss: 0.0402538999915123\n","step: 1930, loss: 0.13662086427211761\n","step: 1940, loss: 0.03732733055949211\n","step: 1950, loss: 0.055964887142181396\n","step: 1960, loss: 0.07979561388492584\n","step: 1970, loss: 0.08797749876976013\n","step: 1980, loss: 0.040104638785123825\n","step: 1990, loss: 0.11696323752403259\n","step: 2000, loss: 0.07041149586439133\n","step: 2010, loss: 0.1262873411178589\n","step: 2020, loss: 0.03714660555124283\n","step: 2030, loss: 0.08045226335525513\n","step: 2040, loss: 0.05186012387275696\n","step: 2050, loss: 0.02177795022726059\n","step: 2060, loss: 0.03596026077866554\n","step: 2070, loss: 0.2595233917236328\n","step: 2080, loss: 0.10932964831590652\n","step: 2090, loss: 0.10246769338846207\n","step: 2100, loss: 0.07856588065624237\n","step: 2110, loss: 0.052582159638404846\n","step: 2120, loss: 0.15129534900188446\n","step: 2130, loss: 0.027572020888328552\n","step: 2140, loss: 0.05098068714141846\n","step: 2150, loss: 0.08568302541971207\n","step: 2160, loss: 0.10840238630771637\n","step: 2170, loss: 0.15220613777637482\n","step: 2180, loss: 0.044873662292957306\n","step: 2190, loss: 0.0771256610751152\n","step: 2200, loss: 0.13530947268009186\n","step: 2210, loss: 0.06499604880809784\n","step: 2220, loss: 0.12886831164360046\n","step: 2230, loss: 0.07964282482862473\n","step: 2240, loss: 0.036777786910533905\n","step: 2250, loss: 0.06219659000635147\n","step: 2260, loss: 0.06788217276334763\n","step: 2270, loss: 0.07182516902685165\n","step: 2280, loss: 0.1387563943862915\n","step: 2290, loss: 0.0579770989716053\n","step: 2300, loss: 0.07884310185909271\n","step: 2310, loss: 0.0629940927028656\n","step: 2320, loss: 0.1826290786266327\n","step: 2330, loss: 0.06422348320484161\n","step: 2340, loss: 0.05809980258345604\n","step: 2350, loss: 0.0658775195479393\n","step: 2360, loss: 0.2649981379508972\n","step: 2370, loss: 0.2598060369491577\n","step: 2380, loss: 0.054376691579818726\n","step: 2390, loss: 0.09841364622116089\n","step: 2400, loss: 0.0740833505988121\n","step: 2410, loss: 0.14587344229221344\n","step: 2420, loss: 0.09822840243577957\n","step: 2430, loss: 0.1708781123161316\n","step: 2440, loss: 0.11312226206064224\n","step: 2450, loss: 0.008667019195854664\n","step: 2460, loss: 0.1129297986626625\n","step: 2470, loss: 0.12626127898693085\n","step: 2480, loss: 0.07752541452646255\n","step: 2490, loss: 0.053974371403455734\n","step: 2500, loss: 0.09076960384845734\n","step: 2510, loss: 0.05552082136273384\n","step: 2520, loss: 0.10515537858009338\n","step: 2530, loss: 0.17180122435092926\n","step: 2540, loss: 0.12527993321418762\n","step: 2550, loss: 0.05246254801750183\n","step: 2560, loss: 0.1244853287935257\n","step: 2570, loss: 0.1778649091720581\n","step: 2580, loss: 0.11805485188961029\n","step: 2590, loss: 0.07043611258268356\n","step: 2600, loss: 0.11363092809915543\n","step: 2610, loss: 0.07191047072410583\n","step: 2620, loss: 0.12529192864894867\n","step: 2630, loss: 0.09212536364793777\n","step: 2640, loss: 0.03170151636004448\n","step: 2650, loss: 0.10742124170064926\n","step: 2660, loss: 0.059260979294776917\n","step: 2670, loss: 0.05807923898100853\n","step: 2680, loss: 0.08365210145711899\n","step: 2690, loss: 0.07620320469141006\n","step: 2700, loss: 0.0765131264925003\n","step: 2710, loss: 0.08506067842245102\n","step: 2720, loss: 0.09332410246133804\n","step: 2730, loss: 0.09987698495388031\n","step: 2740, loss: 0.11547277867794037\n","step: 2750, loss: 0.10164106637239456\n","step: 2760, loss: 0.021244702860713005\n","step: 2770, loss: 0.11545371264219284\n","step: 2780, loss: 0.15883642435073853\n","step: 2790, loss: 0.11176736652851105\n","step: 2800, loss: 0.16966158151626587\n","step: 2810, loss: 0.07697074860334396\n","step: 2820, loss: 0.11659165471792221\n","step: 2830, loss: 0.04328898340463638\n","step: 2840, loss: 0.15406052768230438\n","step: 2850, loss: 0.08744104951620102\n","step: 2860, loss: 0.02280604839324951\n","step: 2870, loss: 0.07863225042819977\n","step: 2880, loss: 0.1033492237329483\n","step: 2890, loss: 0.08894072473049164\n","step: 2900, loss: 0.09920188039541245\n","step: 2910, loss: 0.10233759880065918\n","step: 2920, loss: 0.2016380876302719\n","step: 2930, loss: 0.11213812977075577\n","step: 2940, loss: 0.016591643914580345\n","step: 2950, loss: 0.016771050170063972\n","step: 2960, loss: 0.10194677114486694\n","step: 2970, loss: 0.11970384418964386\n","step: 2980, loss: 0.06107437238097191\n","step: 2990, loss: 0.07334761321544647\n","step: 3000, loss: 0.09646636992692947\n","step: 3010, loss: 0.15350045263767242\n","step: 3020, loss: 0.07766274362802505\n","step: 3030, loss: 0.06743112206459045\n","step: 3040, loss: 0.06885246187448502\n","step: 3050, loss: 0.10011190921068192\n","step: 3060, loss: 0.0863303691148758\n","step: 3070, loss: 0.07745785266160965\n","step: 3080, loss: 0.048371341079473495\n","step: 3090, loss: 0.06322978436946869\n","step: 3100, loss: 0.11874434351921082\n","step: 3110, loss: 0.09648474305868149\n","step: 3120, loss: 0.06996773183345795\n","step: 3130, loss: 0.07613635808229446\n","step: 3140, loss: 0.05067334324121475\n","step: 3150, loss: 0.11420156061649323\n","step: 3160, loss: 0.056968897581100464\n","step: 3170, loss: 0.11082872003316879\n","step: 3180, loss: 0.10709793865680695\n","step: 3190, loss: 0.12315542250871658\n","step: 3200, loss: 0.03716371953487396\n","step: 3210, loss: 0.11733512580394745\n","step: 3220, loss: 0.05321134999394417\n","step: 3230, loss: 0.07151737809181213\n","step: 3240, loss: 0.03114297240972519\n","step: 3250, loss: 0.10813099145889282\n","step: 3260, loss: 0.046205177903175354\n","step: 3270, loss: 0.028943026438355446\n","step: 3280, loss: 0.19503875076770782\n","step: 3290, loss: 0.04400550201535225\n","step: 3300, loss: 0.06687553226947784\n","step: 3310, loss: 0.05956221744418144\n","step: 3320, loss: 0.09921401739120483\n","step: 3330, loss: 0.12936802208423615\n","step: 3340, loss: 0.10820198059082031\n","step: 3350, loss: 0.17764128744602203\n","step: 3360, loss: 0.05161405727267265\n","step: 3370, loss: 0.08038361370563507\n","step: 3380, loss: 0.1300690472126007\n","step: 3390, loss: 0.04566475376486778\n","step: 3400, loss: 0.030590573325753212\n","step: 3410, loss: 0.10548032075166702\n","step: 3420, loss: 0.16944515705108643\n","step: 3430, loss: 0.0574839822947979\n","step: 3440, loss: 0.06144255772233009\n","step: 3450, loss: 0.04332830384373665\n","step: 3460, loss: 0.06545577198266983\n","step: 3470, loss: 0.0749688372015953\n","step: 3480, loss: 0.17347851395606995\n","step: 3490, loss: 0.10100188106298447\n","step: 3500, loss: 0.11029838770627975\n","step: 3510, loss: 0.09409109503030777\n","step: 3520, loss: 0.035641588270664215\n","step: 3530, loss: 0.039383482187986374\n","step: 3540, loss: 0.11092743277549744\n","step: 3550, loss: 0.164688840508461\n","step: 3560, loss: 0.15716463327407837\n","step: 3570, loss: 0.05755813419818878\n","step: 3580, loss: 0.056795183569192886\n","step: 3590, loss: 0.04252837225794792\n","step: 3600, loss: 0.101140595972538\n","step: 3610, loss: 0.0766286626458168\n","step: 3620, loss: 0.033178165555000305\n","step: 3630, loss: 0.1719350665807724\n","step: 3640, loss: 0.0857664942741394\n","step: 3650, loss: 0.11182302236557007\n","step: 3660, loss: 0.12056919187307358\n","step: 3670, loss: 0.10099908709526062\n","step: 3680, loss: 0.07848842442035675\n","step: 3690, loss: 0.09046363830566406\n","step: 3700, loss: 0.11911393702030182\n","step: 3710, loss: 0.14290238916873932\n","step: 3720, loss: 0.10282424837350845\n","step: 3730, loss: 0.05080737918615341\n","step: 3740, loss: 0.05055835843086243\n","step: 3750, loss: 0.06939330697059631\n","step: 3760, loss: 0.03722653165459633\n","step: 3770, loss: 0.045890435576438904\n","step: 3780, loss: 0.0569603368639946\n","step: 3790, loss: 0.1182757243514061\n","step: 3800, loss: 0.18323133885860443\n","step: 3810, loss: 0.04817960411310196\n","step: 3820, loss: 0.04132812097668648\n","step: 3830, loss: 0.05709139630198479\n","step: 3840, loss: 0.1173928901553154\n","step: 3850, loss: 0.11008604615926743\n","step: 3860, loss: 0.07064879685640335\n","step: 3870, loss: 0.10416310280561447\n","step: 3880, loss: 0.09015791863203049\n","step: 3890, loss: 0.06773781031370163\n","step: 3900, loss: 0.08290863782167435\n","step: 3910, loss: 0.07938108593225479\n","step: 3920, loss: 0.03057686984539032\n","step: 3930, loss: 0.17472854256629944\n","step: 3940, loss: 0.06669694930315018\n","step: 3950, loss: 0.06617555022239685\n","step: 3960, loss: 0.11680004745721817\n","step: 3970, loss: 0.03549131378531456\n","step: 3980, loss: 0.05583442375063896\n","step: 3990, loss: 0.1012677475810051\n","step: 4000, loss: 0.06828366965055466\n","step: 4010, loss: 0.04748689383268356\n","step: 4020, loss: 0.1475287675857544\n","step: 4030, loss: 0.08616800606250763\n","step: 4040, loss: 0.0458894707262516\n","step: 4050, loss: 0.06348975747823715\n","step: 4060, loss: 0.09971043467521667\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.88      1.00      0.93        35\n","           2       0.93      0.97      0.95        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.99      0.84      0.91       291\n","           5       0.90      0.84      0.87       294\n","           6       0.92      1.00      0.96      1570\n","           7       0.62      0.94      0.74       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.95      0.98      0.96       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.65      0.85      0.73        13\n","          14       0.41      1.00      0.58        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.82      0.86      0.84      1151\n","          17       0.89      0.98      0.93        41\n","          18       0.90      0.84      0.87        32\n","          19       0.96      0.60      0.74        40\n","          20       1.00      0.99      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.94      0.75      0.83      4175\n","          23       0.70      0.98      0.81      2253\n","          24       0.30      0.59      0.40        44\n","          25       0.87      0.90      0.89       888\n","          26       0.90      1.00      0.95         9\n","          27       0.93      1.00      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.98      0.99       344\n","          30       0.96      0.85      0.90      1136\n","          31       0.62      0.53      0.57        19\n","          32       1.00      0.62      0.77         8\n","          33       0.79      0.87      0.83        86\n","          34       0.26      0.59      0.36        32\n","          35       0.99      0.98      0.99       474\n","          36       1.00      0.12      0.22       182\n","          37       0.89      0.92      0.90      1592\n","          38       0.94      0.99      0.96       404\n","          39       0.99      0.85      0.92       485\n","          40       0.94      0.90      0.92       573\n","          41       0.92      0.94      0.93       841\n","          42       0.98      0.99      0.99       575\n","          43       0.94      0.90      0.92       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.96      0.98        82\n","          48       0.75      0.04      0.07        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.83      0.82      0.80     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","Loop 11\n","domain_dev_word_lst 2427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9646313190460205\n","step: 10, loss: 1.97420334815979\n","step: 20, loss: 1.0019079446792603\n","step: 30, loss: 0.38914409279823303\n","step: 40, loss: 0.4883798360824585\n","step: 50, loss: 0.21851304173469543\n","step: 60, loss: 0.15883852541446686\n","step: 70, loss: 0.11100539565086365\n","step: 80, loss: 0.09270802140235901\n","step: 90, loss: 0.1336880475282669\n","step: 100, loss: 0.188099205493927\n","step: 110, loss: 0.24303926527500153\n","step: 120, loss: 0.28486183285713196\n","step: 130, loss: 0.210298553109169\n","step: 140, loss: 0.15324409306049347\n","step: 150, loss: 0.21110299229621887\n","step: 160, loss: 0.18490919470787048\n","step: 170, loss: 0.026357881724834442\n","step: 180, loss: 0.23249347507953644\n","step: 190, loss: 0.1527516096830368\n","step: 200, loss: 0.08726396411657333\n","step: 210, loss: 0.07206195592880249\n","step: 220, loss: 0.21130336821079254\n","step: 230, loss: 0.1278863400220871\n","step: 240, loss: 0.12176171690225601\n","step: 250, loss: 0.0808488205075264\n","step: 260, loss: 0.26033347845077515\n","step: 270, loss: 0.22623968124389648\n","step: 280, loss: 0.21198658645153046\n","step: 290, loss: 0.23232313990592957\n","step: 300, loss: 0.179202139377594\n","step: 310, loss: 0.1169804111123085\n","step: 320, loss: 0.2505186200141907\n","step: 330, loss: 0.09316160529851913\n","step: 340, loss: 0.075220987200737\n","step: 350, loss: 0.08282697200775146\n","step: 360, loss: 0.2150469720363617\n","step: 370, loss: 0.10931342840194702\n","step: 380, loss: 0.08538869768381119\n","step: 390, loss: 0.044139597564935684\n","step: 400, loss: 0.19096699357032776\n","step: 410, loss: 0.07202285528182983\n","step: 420, loss: 0.1334087997674942\n","step: 430, loss: 0.11867975443601608\n","step: 440, loss: 0.10924024134874344\n","step: 450, loss: 0.0845140814781189\n","step: 460, loss: 0.04946783185005188\n","step: 470, loss: 0.10061931610107422\n","step: 480, loss: 0.06277347356081009\n","step: 490, loss: 0.10980875045061111\n","step: 500, loss: 0.08462338894605637\n","step: 510, loss: 0.1733454465866089\n","step: 520, loss: 0.09805820882320404\n","step: 530, loss: 0.07240087538957596\n","step: 540, loss: 0.10521825402975082\n","step: 550, loss: 0.11121009290218353\n","step: 560, loss: 0.1006932333111763\n","step: 570, loss: 0.059406109154224396\n","step: 580, loss: 0.12344444543123245\n","step: 590, loss: 0.10827691107988358\n","step: 600, loss: 0.11520540714263916\n","step: 610, loss: 0.07223115116357803\n","step: 620, loss: 0.15963666141033173\n","step: 630, loss: 0.11683910340070724\n","step: 640, loss: 0.09681074321269989\n","step: 650, loss: 0.08985885232686996\n","step: 660, loss: 0.04631543159484863\n","step: 670, loss: 0.11976027488708496\n","step: 680, loss: 0.1250922679901123\n","step: 690, loss: 0.08710089325904846\n","step: 700, loss: 0.0662609115242958\n","step: 710, loss: 0.16553626954555511\n","step: 720, loss: 0.15621908009052277\n","step: 730, loss: 0.2302618771791458\n","step: 740, loss: 0.043517567217350006\n","step: 750, loss: 0.1017584502696991\n","step: 760, loss: 0.03688178211450577\n","step: 770, loss: 0.11501788347959518\n","step: 780, loss: 0.1179126501083374\n","step: 790, loss: 0.10713931173086166\n","step: 800, loss: 0.13562479615211487\n","step: 810, loss: 0.027197159826755524\n","step: 820, loss: 0.026109468191862106\n","step: 830, loss: 0.16792894899845123\n","step: 840, loss: 0.056732866913080215\n","step: 850, loss: 0.20793083310127258\n","step: 860, loss: 0.18725916743278503\n","step: 870, loss: 0.038793038576841354\n","step: 880, loss: 0.2535288631916046\n","step: 890, loss: 0.18256019055843353\n","step: 900, loss: 0.048469118773937225\n","step: 910, loss: 0.20862722396850586\n","step: 920, loss: 0.08718954026699066\n","step: 930, loss: 0.06376567482948303\n","step: 940, loss: 0.13275933265686035\n","step: 950, loss: 0.13048146665096283\n","step: 960, loss: 0.13901160657405853\n","step: 970, loss: 0.060775239020586014\n","step: 980, loss: 0.0659380555152893\n","step: 990, loss: 0.15012764930725098\n","step: 1000, loss: 0.30861955881118774\n","step: 1010, loss: 0.13609278202056885\n","step: 1020, loss: 0.08101177215576172\n","step: 1030, loss: 0.07055030018091202\n","step: 1040, loss: 0.050926435738801956\n","step: 1050, loss: 0.134646937251091\n","step: 1060, loss: 0.10952640324831009\n","step: 1070, loss: 0.03941595181822777\n","step: 1080, loss: 0.16329865157604218\n","step: 1090, loss: 0.07888233661651611\n","step: 1100, loss: 0.13094502687454224\n","step: 1110, loss: 0.042868223041296005\n","step: 1120, loss: 0.0209739301353693\n","step: 1130, loss: 0.1398046910762787\n","step: 1140, loss: 0.08857563138008118\n","step: 1150, loss: 0.1344689428806305\n","step: 1160, loss: 0.09140455722808838\n","step: 1170, loss: 0.1428612619638443\n","step: 1180, loss: 0.08472979813814163\n","step: 1190, loss: 0.06642284989356995\n","step: 1200, loss: 0.1880514770746231\n","step: 1210, loss: 0.032294102013111115\n","step: 1220, loss: 0.1340964287519455\n","step: 1230, loss: 0.09595023095607758\n","step: 1240, loss: 0.10421331971883774\n","step: 1250, loss: 0.06519270688295364\n","step: 1260, loss: 0.062469225376844406\n","step: 1270, loss: 0.07129139453172684\n","step: 1280, loss: 0.03396685793995857\n","step: 1290, loss: 0.12151213735342026\n","step: 1300, loss: 0.06328082084655762\n","step: 1310, loss: 0.05226028710603714\n","step: 1320, loss: 0.029145605862140656\n","step: 1330, loss: 0.17328111827373505\n","step: 1340, loss: 0.09341597557067871\n","step: 1350, loss: 0.11796647310256958\n","step: 1360, loss: 0.08152762800455093\n","step: 1370, loss: 0.04880691319704056\n","step: 1380, loss: 0.1654423624277115\n","step: 1390, loss: 0.05172538757324219\n","step: 1400, loss: 0.14198923110961914\n","step: 1410, loss: 0.11094406247138977\n","step: 1420, loss: 0.0936979278922081\n","step: 1430, loss: 0.07700943946838379\n","step: 1440, loss: 0.130965456366539\n","step: 1450, loss: 0.06254754960536957\n","step: 1460, loss: 0.11290621757507324\n","step: 1470, loss: 0.0647239089012146\n","step: 1480, loss: 0.145668163895607\n","step: 1490, loss: 0.05298357829451561\n","step: 1500, loss: 0.13536949455738068\n","step: 1510, loss: 0.07166322320699692\n","step: 1520, loss: 0.17696939408779144\n","step: 1530, loss: 0.06689956784248352\n","step: 1540, loss: 0.21756033599376678\n","step: 1550, loss: 0.05550944805145264\n","step: 1560, loss: 0.09842896461486816\n","step: 1570, loss: 0.05011723190546036\n","step: 1580, loss: 0.11122195422649384\n","step: 1590, loss: 0.08368546515703201\n","step: 1600, loss: 0.06473967432975769\n","step: 1610, loss: 0.18612566590309143\n","step: 1620, loss: 0.034896329045295715\n","step: 1630, loss: 0.15207643806934357\n","step: 1640, loss: 0.1564304232597351\n","step: 1650, loss: 0.1253807544708252\n","step: 1660, loss: 0.08760306984186172\n","step: 1670, loss: 0.10002564638853073\n","step: 1680, loss: 0.0348091796040535\n","step: 1690, loss: 0.10382143408060074\n","step: 1700, loss: 0.08161397278308868\n","step: 1710, loss: 0.14251692593097687\n","step: 1720, loss: 0.07720879465341568\n","step: 1730, loss: 0.047841306775808334\n","step: 1740, loss: 0.20973984897136688\n","step: 1750, loss: 0.24192598462104797\n","step: 1760, loss: 0.10953126102685928\n","step: 1770, loss: 0.11558809131383896\n","step: 1780, loss: 0.11448463052511215\n","step: 1790, loss: 0.07242302596569061\n","step: 1800, loss: 0.07618620246648788\n","step: 1810, loss: 0.08874539285898209\n","step: 1820, loss: 0.11062153428792953\n","step: 1830, loss: 0.09102419763803482\n","step: 1840, loss: 0.13878080248832703\n","step: 1850, loss: 0.09328798949718475\n","step: 1860, loss: 0.18934911489486694\n","step: 1870, loss: 0.08355840295553207\n","step: 1880, loss: 0.026873251423239708\n","step: 1890, loss: 0.04149025306105614\n","step: 1900, loss: 0.07115360349416733\n","step: 1910, loss: 0.10260292887687683\n","step: 1920, loss: 0.06453680247068405\n","step: 1930, loss: 0.12378320097923279\n","step: 1940, loss: 0.1881122887134552\n","step: 1950, loss: 0.05063760280609131\n","step: 1960, loss: 0.12642191350460052\n","step: 1970, loss: 0.06037701293826103\n","step: 1980, loss: 0.1274975687265396\n","step: 1990, loss: 0.04506378620862961\n","step: 2000, loss: 0.11680054664611816\n","step: 2010, loss: 0.15549369156360626\n","step: 2020, loss: 0.17619095742702484\n","step: 2030, loss: 0.12122911959886551\n","step: 2040, loss: 0.9603565335273743\n","step: 2050, loss: 0.1284829080104828\n","step: 2060, loss: 0.05590137839317322\n","step: 2070, loss: 0.04641242325305939\n","step: 2080, loss: 0.11552169919013977\n","step: 2090, loss: 0.05287625268101692\n","step: 2100, loss: 0.12362144887447357\n","step: 2110, loss: 0.11280950903892517\n","step: 2120, loss: 0.046544261276721954\n","step: 2130, loss: 0.08850294351577759\n","step: 2140, loss: 0.0874108150601387\n","step: 2150, loss: 0.1032724678516388\n","step: 2160, loss: 0.039121583104133606\n","step: 2170, loss: 0.0202507134526968\n","step: 2180, loss: 0.07041589170694351\n","step: 2190, loss: 0.08101662993431091\n","step: 2200, loss: 0.05242863669991493\n","step: 2210, loss: 0.084622822701931\n","step: 2220, loss: 0.08434167504310608\n","step: 2230, loss: 0.10314615070819855\n","step: 2240, loss: 0.1627878099679947\n","step: 2250, loss: 0.10405562072992325\n","step: 2260, loss: 0.119046151638031\n","step: 2270, loss: 0.04178094491362572\n","step: 2280, loss: 0.12906205654144287\n","step: 2290, loss: 0.07048895955085754\n","step: 2300, loss: 0.14545293152332306\n","step: 2310, loss: 0.07255028933286667\n","step: 2320, loss: 0.10301220417022705\n","step: 2330, loss: 0.08165399730205536\n","step: 2340, loss: 0.15717118978500366\n","step: 2350, loss: 0.16488374769687653\n","step: 2360, loss: 0.08991291373968124\n","step: 2370, loss: 0.28614097833633423\n","step: 2380, loss: 0.023322761058807373\n","step: 2390, loss: 0.2546743154525757\n","step: 2400, loss: 0.1973356008529663\n","step: 2410, loss: 0.10611145198345184\n","step: 2420, loss: 0.11274675279855728\n","step: 2430, loss: 0.2560592293739319\n","step: 2440, loss: 0.03369711712002754\n","step: 2450, loss: 0.10453972220420837\n","step: 2460, loss: 0.06650702655315399\n","step: 2470, loss: 0.054234784096479416\n","step: 2480, loss: 0.0995534136891365\n","step: 2490, loss: 0.07069070637226105\n","step: 2500, loss: 0.0728118047118187\n","step: 2510, loss: 0.09031674265861511\n","step: 2520, loss: 0.04841623082756996\n","step: 2530, loss: 0.09462512284517288\n","step: 2540, loss: 0.08235588669776917\n","step: 2550, loss: 0.08372282236814499\n","step: 2560, loss: 0.0746605172753334\n","step: 2570, loss: 0.1020348072052002\n","step: 2580, loss: 0.10284389555454254\n","step: 2590, loss: 0.12186019122600555\n","step: 2600, loss: 0.13235194981098175\n","step: 2610, loss: 0.14605140686035156\n","step: 2620, loss: 0.09425549954175949\n","step: 2630, loss: 0.045286305248737335\n","step: 2640, loss: 0.1892855316400528\n","step: 2650, loss: 0.1236758679151535\n","step: 2660, loss: 0.029164254665374756\n","step: 2670, loss: 0.09812139719724655\n","step: 2680, loss: 0.0684390738606453\n","step: 2690, loss: 0.03447756543755531\n","step: 2700, loss: 0.010676251724362373\n","step: 2710, loss: 0.13349661231040955\n","step: 2720, loss: 0.13638557493686676\n","step: 2730, loss: 0.15257303416728973\n","step: 2740, loss: 0.07304170727729797\n","step: 2750, loss: 0.1265769749879837\n","step: 2760, loss: 0.07527269423007965\n","step: 2770, loss: 0.020348062738776207\n","step: 2780, loss: 0.01247328519821167\n","step: 2790, loss: 0.062248554080724716\n","step: 2800, loss: 0.15306681394577026\n","step: 2810, loss: 0.11511021852493286\n","step: 2820, loss: 0.09812509268522263\n","step: 2830, loss: 0.07563648372888565\n","step: 2840, loss: 0.15937262773513794\n","step: 2850, loss: 0.08548542112112045\n","step: 2860, loss: 0.1280323714017868\n","step: 2870, loss: 0.1110643818974495\n","step: 2880, loss: 0.060789693146944046\n","step: 2890, loss: 0.027303483337163925\n","step: 2900, loss: 0.07797624915838242\n","step: 2910, loss: 0.11193934082984924\n","step: 2920, loss: 0.05465519055724144\n","step: 2930, loss: 0.029969092458486557\n","step: 2940, loss: 0.12882402539253235\n","step: 2950, loss: 0.06609459221363068\n","step: 2960, loss: 0.11820391565561295\n","step: 2970, loss: 0.0590708926320076\n","step: 2980, loss: 0.1271868199110031\n","step: 2990, loss: 0.17359992861747742\n","step: 3000, loss: 0.06630602478981018\n","step: 3010, loss: 0.1612606942653656\n","step: 3020, loss: 0.0551941841840744\n","step: 3030, loss: 0.03393664211034775\n","step: 3040, loss: 0.027090637013316154\n","step: 3050, loss: 0.09449053555727005\n","step: 3060, loss: 0.13389655947685242\n","step: 3070, loss: 0.1238536611199379\n","step: 3080, loss: 0.1679677516222\n","step: 3090, loss: 0.031723953783512115\n","step: 3100, loss: 0.066807322204113\n","step: 3110, loss: 0.12295988202095032\n","step: 3120, loss: 0.08894289284944534\n","step: 3130, loss: 0.04000178724527359\n","step: 3140, loss: 0.1440584659576416\n","step: 3150, loss: 0.029376482591032982\n","step: 3160, loss: 0.06185935065150261\n","step: 3170, loss: 0.04778135195374489\n","step: 3180, loss: 0.102707639336586\n","step: 3190, loss: 0.07323640584945679\n","step: 3200, loss: 0.20808058977127075\n","step: 3210, loss: 0.08501705527305603\n","step: 3220, loss: 0.07838285714387894\n","step: 3230, loss: 0.1215076670050621\n","step: 3240, loss: 0.01380977500230074\n","step: 3250, loss: 0.11121020466089249\n","step: 3260, loss: 0.1094607412815094\n","step: 3270, loss: 0.11067590117454529\n","step: 3280, loss: 0.031915947794914246\n","step: 3290, loss: 0.04116528481245041\n","step: 3300, loss: 0.01798827014863491\n","step: 3310, loss: 0.08523322641849518\n","step: 3320, loss: 0.1821930855512619\n","step: 3330, loss: 0.03190435841679573\n","step: 3340, loss: 0.1231124997138977\n","step: 3350, loss: 0.13375578820705414\n","step: 3360, loss: 0.04462889954447746\n","step: 3370, loss: 0.07258609682321548\n","step: 3380, loss: 0.2336304783821106\n","step: 3390, loss: 0.18622951209545135\n","step: 3400, loss: 0.07139836251735687\n","step: 3410, loss: 0.1151212602853775\n","step: 3420, loss: 0.08068932592868805\n","step: 3430, loss: 0.0344967283308506\n","step: 3440, loss: 0.047234680503606796\n","step: 3450, loss: 0.06731976568698883\n","step: 3460, loss: 0.094554603099823\n","step: 3470, loss: 0.12817996740341187\n","step: 3480, loss: 0.020973598584532738\n","step: 3490, loss: 0.05871966481208801\n","step: 3500, loss: 0.05419030413031578\n","step: 3510, loss: 0.03791395574808121\n","step: 3520, loss: 0.04409010335803032\n","step: 3530, loss: 0.08295947313308716\n","step: 3540, loss: 0.10188459604978561\n","step: 3550, loss: 0.06873685866594315\n","step: 3560, loss: 0.14126883447170258\n","step: 3570, loss: 0.1050567626953125\n","step: 3580, loss: 0.11741026490926743\n","step: 3590, loss: 0.13757352530956268\n","step: 3600, loss: 0.05950174480676651\n","step: 3610, loss: 0.10086391121149063\n","step: 3620, loss: 0.03216535970568657\n","step: 3630, loss: 0.09883534163236618\n","step: 3640, loss: 0.030348075553774834\n","step: 3650, loss: 0.018081601709127426\n","step: 3660, loss: 0.10236874222755432\n","step: 3670, loss: 0.04863103851675987\n","step: 3680, loss: 0.1255374252796173\n","step: 3690, loss: 0.10674707591533661\n","step: 3700, loss: 0.13258440792560577\n","step: 3710, loss: 0.08707360923290253\n","step: 3720, loss: 0.11196594685316086\n","step: 3730, loss: 0.10000798851251602\n","step: 3740, loss: 0.039905235171318054\n","step: 3750, loss: 0.267345666885376\n","step: 3760, loss: 0.12405675649642944\n","step: 3770, loss: 0.13941557705402374\n","step: 3780, loss: 0.10124063491821289\n","step: 3790, loss: 0.16332051157951355\n","step: 3800, loss: 0.12330619245767593\n","step: 3810, loss: 0.052288588136434555\n","step: 3820, loss: 0.05173932760953903\n","step: 3830, loss: 0.037199974060058594\n","step: 3840, loss: 0.14920268952846527\n","step: 3850, loss: 0.08807448297739029\n","step: 3860, loss: 0.05341176688671112\n","step: 3870, loss: 0.04395196586847305\n","step: 3880, loss: 0.020296193659305573\n","step: 3890, loss: 0.09436171501874924\n","step: 3900, loss: 0.09498455375432968\n","step: 3910, loss: 0.15353362262248993\n","step: 3920, loss: 0.1410013884305954\n","step: 3930, loss: 0.03555070236325264\n","step: 3940, loss: 0.07812472432851791\n","step: 3950, loss: 0.10146544873714447\n","step: 3960, loss: 0.12650692462921143\n","step: 3970, loss: 0.059650104492902756\n","step: 3980, loss: 0.08068004995584488\n","step: 3990, loss: 0.09947946667671204\n","step: 4000, loss: 0.053680453449487686\n","step: 4010, loss: 0.0488925464451313\n","step: 4020, loss: 0.07826779782772064\n","step: 4030, loss: 0.10149399936199188\n","step: 4040, loss: 0.06751083582639694\n","step: 4050, loss: 0.07211591303348541\n","step: 4060, loss: 0.09045927971601486\n","acc=0.90\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.74      1.00      0.85        35\n","           2       0.93      1.00      0.96        77\n","           3       1.00      0.79      0.88      1030\n","           4       0.97      0.83      0.89       291\n","           5       0.79      0.84      0.81       294\n","           6       0.94      0.99      0.96      1570\n","           7       0.59      0.94      0.73       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.94      0.97      0.96       901\n","          11       0.99      1.00      0.99      2111\n","          12       0.96      0.98      0.97        47\n","          13       0.60      0.69      0.64        13\n","          14       0.47      0.98      0.64        43\n","          15       0.94      0.98      0.96      2778\n","          16       0.88      0.83      0.86      1151\n","          17       0.95      0.93      0.94        41\n","          18       0.91      0.97      0.94        32\n","          19       0.92      0.57      0.71        40\n","          20       0.99      0.99      0.99       584\n","          21       0.00      0.00      0.00        52\n","          22       0.95      0.71      0.81      4175\n","          23       0.68      0.97      0.80      2253\n","          24       0.34      0.77      0.48        44\n","          25       0.87      0.89      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.95      1.00      0.97        69\n","          28       1.00      1.00      1.00      1864\n","          29       0.99      0.99      0.99       344\n","          30       0.95      0.86      0.91      1136\n","          31       0.58      0.79      0.67        19\n","          32       1.00      0.62      0.77         8\n","          33       0.75      0.88      0.81        86\n","          34       0.22      0.53      0.31        32\n","          35       0.98      0.99      0.98       474\n","          36       0.87      0.15      0.25       182\n","          37       0.87      0.95      0.91      1592\n","          38       0.94      0.98      0.96       404\n","          39       0.94      0.98      0.96       485\n","          40       0.89      0.94      0.91       573\n","          41       0.94      0.93      0.93       841\n","          42       0.99      0.99      0.99       575\n","          43       0.94      0.91      0.93       152\n","          44       0.86      0.92      0.89        75\n","          46       0.99      0.99      0.99        82\n","          48       0.83      0.06      0.12        79\n","\n","    accuracy                           0.90     28417\n","   macro avg       0.82      0.83      0.80     28417\n","weighted avg       0.92      0.90      0.90     28417\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"g4ZuWMYTojmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"],"metadata":{"id":"jJxcl1cLn7rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"],"metadata":{"id":"6ryA3bq0ot4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"O2DFUhCDorSf","colab":{"base_uri":"https://localhost:8080/","height":562},"executionInfo":{"status":"ok","timestamp":1669231139982,"user_tz":300,"elapsed":9,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"c664fb3c-9b53-46a3-f08a-592598327f88"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"36140bd5-44dc-4362-b2ac-6d0fac1d1f7f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"36140bd5-44dc-4362-b2ac-6d0fac1d1f7f\")) {                    Plotly.newPlot(                        \"36140bd5-44dc-4362-b2ac-6d0fac1d1f7f\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11],\"xaxis\":\"x\",\"y\":[0.7945300660402925,0.8329732888425074,0.7954052801569229,0.7853245766048229,0.7849812161049985,0.8454376919097863,0.8239446772836562,0.8065829101297821,0.8323765897933408,0.8019472184710618,0.83004295763702,0.8220679920279025],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11],\"xaxis\":\"x\",\"y\":[0.8005920263509698,0.8073367361525277,0.8055937448793086,0.8104714423046654,0.7928447765452651,0.8048891136766538,0.7969325826299459,0.8155549031475285,0.8026825638853323,0.8097838680829532,0.817354801054507,0.8282058807258948],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11],\"xaxis\":\"x\",\"y\":[0.779434223557162,0.7883017083378546,0.7765345309726456,0.7805949456105866,0.7754738570381587,0.7947238314757525,0.7797337373903725,0.7832418304982572,0.7816211158065111,0.7775775844157434,0.7993412695892295,0.8020198750703406],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('36140bd5-44dc-4362-b2ac-6d0fac1d1f7f');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"PLs82qPoc_TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2Ngux9I-orPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uEACZ4DaGGRA"},"execution_count":null,"outputs":[]}]}