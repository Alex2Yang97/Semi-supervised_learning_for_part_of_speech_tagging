{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1669867829700,"user_tz":300,"elapsed":28130,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"80e7cac0-eb6d-4fc5-d34c-613b9cdab3fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Collecting boto3\n","  Downloading boto3-1.26.20-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 45.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting botocore<1.30.0,>=1.29.20\n","  Downloading botocore-1.29.20-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 19.8 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.20->boto3->pytorch_pretrained_bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 11.7 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.20->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.20 botocore-1.29.20 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[K     |████████████████████████████████| 512 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.0\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from utils import read_conll_file, read_data\n","\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1669867852787,"user_tz":300,"elapsed":23096,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"18b5635b-9f7c-47db-da7c-2adecdd3728b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"CPKysG3i3nsR","executionInfo":{"status":"ok","timestamp":1669867560048,"user_tz":300,"elapsed":17,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KqectYYC30N","executionInfo":{"status":"ok","timestamp":1669844235056,"user_tz":300,"elapsed":5482,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"ef3b8b14-0709-4cc6-c6a1-3c460110b389"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1669844235057,"user_tz":300,"elapsed":11,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"8a658ea2-d4ef-4933-9239-415d4ce8b59f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL","executionInfo":{"status":"error","timestamp":1669860619611,"user_tz":300,"elapsed":3718,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"a57dbb56-3885-4208-acbd-a4a56517749c"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ed50582c290e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_","executionInfo":{"status":"ok","timestamp":1669844238361,"user_tz":300,"elapsed":1238,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7c3919a-f064-42ed-e856-862834b0fe64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 844408.70B/s]\n"]}]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator, average=\"weighted\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","\n","    pred_lst = []\n","    true_lst = []\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            for s in y_hat.cpu().numpy().tolist():\n","              pred_lst.extend(s)\n","            for s in y.numpy().tolist():\n","              true_lst.extend(s)\n","\n","    precision_value = multiclass_precision(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    recall_value = multiclass_recall(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    f1_value = multiclass_f1_score(\n","            torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","            average=average)   \n","    acc = multiclass_accuracy(\n","        torch.tensor(pred_lst), torch.tensor(true_lst), num_classes=len(wsj_tags), ignore_index=0, \n","        average=average)    \n","\n","\n","    return precision_value, recall_value, f1_value, acc"],"metadata":{"id":"DW4KvG4x6I91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K","executionInfo":{"status":"ok","timestamp":1669844271980,"user_tz":300,"elapsed":33399,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8c2a511-953d-4e9b-cb5f-844f854362ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:22<00:00, 18223503.74B/s]\n"]}]},{"cell_type":"code","source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"],"metadata":{"id":"B0x3gRfi9iyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# torch.save(model.state_dict(), model_file)"],"metadata":{"id":"LtVeE3zd04C8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value = eval(model, test_iter)\n","print(wsj_precision_value, wsj_recall_value, wsj_f1_value, wsj_acc_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAD3Wd574v6Q","executionInfo":{"status":"ok","timestamp":1669844292560,"user_tz":300,"elapsed":20296,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"dd296ea8-017f-4598-ec56-401d0e13e503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9771) tensor(0.9743) tensor(0.9751) tensor(0.9743)\n"]}]},{"cell_type":"markdown","source":["# Self Training"],"metadata":{"id":"reoycWJi5azd"}},{"cell_type":"code","source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"],"metadata":{"id":"agIHM1TmEYl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"mGm3QLNcD8bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain = \"weblogs\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"],"metadata":{"id":"fwvivWyzEHOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHAOs-fdEHMO","executionInfo":{"status":"ok","timestamp":1669844294706,"user_tz":300,"elapsed":2161,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"e9919937-a552-4e3b-9da2-84ed35eb1763"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 1016\n","The number of tags 47\n","The number of samples: 1015\n","The number of tags 49\n","after filter tag 1016\n","after filter tag 974\n"]}]},{"cell_type":"code","source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []\n","domain_acc_value_lst = []"],"metadata":{"id":"3JGnpmRNHHmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)\n","domain_acc_value_lst.append(domain_acc_value)"],"metadata":{"id":"cHb8ZM-VjG-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"S1HrHo0LEhWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","  acc_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          # When calculating the length of sentences, ignore <pad>\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob.tolist())\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","          # Calculate the accuracy for each sentences, ignore 0\n","          batch_acc = multiclass_accuracy(\n","              torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags), \n","              ignore_index=0, average=\"micro\", multidim_average=\"samplewise\")\n","          acc_lst.extend(batch_acc.tolist())\n","          \n","\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob.tolist())\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","          # Calculate the accuracy for each sentences, ignore 0\n","          batch_acc = multiclass_accuracy(\n","              torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags), \n","              ignore_index=0, average=\"micro\", multidim_average=\"samplewise\")\n","          acc_lst.extend(batch_acc.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","  prob_lst = [prob for prob, _ in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn] # The index of topn sentences\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","\n","  new_prob = prob_lst[: topn]\n","  remain_prob = prob_lst[topn: ]\n","  new_acc = [acc_lst[i] for i in select_ind]\n","  remain_acc = [acc_lst[i] for i in not_select_ind]\n","\n","\n","  return new_train_x, new_train_y, remain_train_x, remain_train_y, new_acc, remain_acc, new_prob, remain_prob"],"metadata":{"id":"vzWOF5sCGjgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc_lst = []\n","prob_lst = []\n","\n","factor_list = [1, 2, 5, 10, 20]\n","factor = factor_list[3] #  to be modified\n","topn = round(factor * len(domain_dev_word_lst) / 100)\n","\n","i = 0\n","while len(domain_dev_word_lst) >= topn:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","  print(\"domain_dev_word_lst\", len(domain_dev_word_lst))\n","\n","  if i == 1:\n","    domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=False,\n","                                num_workers=1,\n","                                collate_fn=pad)\n","  else:\n","    domain_dev_dataset = PosDataset_new(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=True,\n","                                num_workers=1,\n","                                collate_fn=pad_new)\n","  \n","  initial = True if i==1 else False\n","  top_words_ids, top_tags_ids, domain_dev_word_lst, domain_dev_tag_lst, new_acc, remain_acc, new_prob, remain_prob = gen_pseudo_data(model, domain_dev_iter, topn, initial)\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    words = []\n","    tags = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        tags.append(tag_ids[k])\n","    top_words.append(words)\n","    top_tags.append(tags)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)\n","  domain_acc_value_lst.append(domain_acc_value)\n","\n","  acc_lst.append(new_acc)\n","  prob_lst.append(new_prob)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zl3VMmnVVD-8","outputId":"8f7b9cef-cec3-4ba9-f119-f3833383a694","executionInfo":{"status":"ok","timestamp":1669848763491,"user_tz":300,"elapsed":4464445,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loop 1\n","domain_dev_word_lst 1016\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-5c104b1a35d7>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags),\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.9304182529449463\n","step: 10, loss: 1.8634181022644043\n","step: 20, loss: 0.9408775568008423\n","step: 30, loss: 0.4345119297504425\n","step: 40, loss: 0.2426508069038391\n","step: 50, loss: 0.2724077105522156\n","step: 60, loss: 0.15933601558208466\n","step: 70, loss: 0.1676059365272522\n","step: 80, loss: 0.21330897510051727\n","step: 90, loss: 0.1347602903842926\n","step: 100, loss: 0.07239922881126404\n","step: 110, loss: 0.10935758799314499\n","step: 120, loss: 0.23317985236644745\n","step: 130, loss: 0.12763506174087524\n","step: 140, loss: 0.11532191932201385\n","step: 150, loss: 0.2229335755109787\n","step: 160, loss: 0.18676137924194336\n","step: 170, loss: 0.0820370465517044\n","step: 180, loss: 0.21269381046295166\n","step: 190, loss: 0.08923698216676712\n","step: 200, loss: 0.18482738733291626\n","step: 210, loss: 0.09691624343395233\n","step: 220, loss: 0.08529931306838989\n","step: 230, loss: 0.06023191660642624\n","step: 240, loss: 0.13161224126815796\n","step: 250, loss: 0.1318320780992508\n","step: 260, loss: 0.1451229453086853\n","step: 270, loss: 0.15536530315876007\n","step: 280, loss: 0.06631124764680862\n","step: 290, loss: 0.1815900355577469\n","step: 300, loss: 0.05517556145787239\n","step: 310, loss: 0.06472542136907578\n","step: 320, loss: 0.062029317021369934\n","step: 330, loss: 0.07146630436182022\n","step: 340, loss: 0.15017250180244446\n","step: 350, loss: 0.13582856953144073\n","step: 360, loss: 0.1032094806432724\n","step: 370, loss: 0.10894709825515747\n","step: 380, loss: 0.09129257500171661\n","step: 390, loss: 0.1029360219836235\n","step: 400, loss: 0.1445814073085785\n","step: 410, loss: 0.07996466755867004\n","step: 420, loss: 0.0707189217209816\n","step: 430, loss: 0.12970927357673645\n","step: 440, loss: 0.11207971721887589\n","step: 450, loss: 0.11666444689035416\n","step: 460, loss: 0.057010870426893234\n","step: 470, loss: 0.0907917246222496\n","step: 480, loss: 0.23516735434532166\n","step: 490, loss: 0.12494345754384995\n","step: 500, loss: 0.12273035198450089\n","step: 510, loss: 0.1029055267572403\n","step: 520, loss: 0.12487642467021942\n","step: 530, loss: 0.17417120933532715\n","step: 540, loss: 0.08924470096826553\n","step: 550, loss: 0.09721962362527847\n","step: 560, loss: 0.10813430696725845\n","step: 570, loss: 0.15517565608024597\n","step: 580, loss: 0.11229453980922699\n","step: 590, loss: 0.034861087799072266\n","step: 600, loss: 0.1062707006931305\n","step: 610, loss: 0.1505289226770401\n","step: 620, loss: 0.07204414904117584\n","step: 630, loss: 0.06919883191585541\n","step: 640, loss: 0.15172022581100464\n","step: 650, loss: 0.11426285654306412\n","step: 660, loss: 0.07807283848524094\n","step: 670, loss: 0.07356731593608856\n","step: 680, loss: 0.13461938500404358\n","step: 690, loss: 0.018771575763821602\n","step: 700, loss: 0.15853680670261383\n","step: 710, loss: 0.0611891932785511\n","step: 720, loss: 0.05052455514669418\n","step: 730, loss: 0.08196195960044861\n","step: 740, loss: 0.07523583620786667\n","step: 750, loss: 0.09615641087293625\n","step: 760, loss: 0.0626872330904007\n","step: 770, loss: 0.17582346498966217\n","step: 780, loss: 0.12520329654216766\n","step: 790, loss: 0.06971770524978638\n","step: 800, loss: 0.10812334716320038\n","step: 810, loss: 0.0298390444368124\n","step: 820, loss: 0.22703836858272552\n","step: 830, loss: 0.08431936055421829\n","step: 840, loss: 0.10184716433286667\n","step: 850, loss: 0.09800086915493011\n","step: 860, loss: 0.1562807708978653\n","step: 870, loss: 0.051707349717617035\n","step: 880, loss: 0.1370227336883545\n","step: 890, loss: 0.11466065049171448\n","step: 900, loss: 0.1298275589942932\n","step: 910, loss: 0.08090728521347046\n","step: 920, loss: 0.09625877439975739\n","step: 930, loss: 0.07312717288732529\n","step: 940, loss: 0.05696370452642441\n","step: 950, loss: 0.2448582947254181\n","step: 960, loss: 0.0631737932562828\n","step: 970, loss: 0.0648726224899292\n","step: 980, loss: 0.05552885681390762\n","step: 990, loss: 0.05257068946957588\n","step: 1000, loss: 0.07629811018705368\n","step: 1010, loss: 0.06887855380773544\n","step: 1020, loss: 0.06772557646036148\n","step: 1030, loss: 0.10205133259296417\n","step: 1040, loss: 0.09768593311309814\n","step: 1050, loss: 0.08992823958396912\n","step: 1060, loss: 0.13423888385295868\n","step: 1070, loss: 0.08674706518650055\n","step: 1080, loss: 0.15031351149082184\n","step: 1090, loss: 0.1639583259820938\n","step: 1100, loss: 0.0734209418296814\n","step: 1110, loss: 0.13423195481300354\n","step: 1120, loss: 0.03985521197319031\n","step: 1130, loss: 0.0985097587108612\n","step: 1140, loss: 0.07373335212469101\n","step: 1150, loss: 0.06888198107481003\n","step: 1160, loss: 0.18189267814159393\n","step: 1170, loss: 0.03592924028635025\n","step: 1180, loss: 0.1355629712343216\n","step: 1190, loss: 0.1222805604338646\n","step: 1200, loss: 0.10492950677871704\n","step: 1210, loss: 0.08542149513959885\n","step: 1220, loss: 0.10283917933702469\n","step: 1230, loss: 0.06666671484708786\n","step: 1240, loss: 0.07743332535028458\n","step: 1250, loss: 0.1242779791355133\n","step: 1260, loss: 0.10951480269432068\n","step: 1270, loss: 0.11352252215147018\n","step: 1280, loss: 0.1477506309747696\n","step: 1290, loss: 0.11877517402172089\n","step: 1300, loss: 0.15043558180332184\n","step: 1310, loss: 0.144226536154747\n","step: 1320, loss: 0.1567414253950119\n","step: 1330, loss: 0.08773647993803024\n","step: 1340, loss: 0.11468158662319183\n","step: 1350, loss: 0.0802350863814354\n","step: 1360, loss: 0.0376482792198658\n","step: 1370, loss: 0.09970453381538391\n","step: 1380, loss: 0.16496215760707855\n","step: 1390, loss: 0.16438934206962585\n","step: 1400, loss: 0.12772545218467712\n","step: 1410, loss: 0.12964783608913422\n","step: 1420, loss: 0.08244960755109787\n","step: 1430, loss: 0.0981702208518982\n","step: 1440, loss: 0.04209773615002632\n","step: 1450, loss: 0.10759634524583817\n","step: 1460, loss: 0.08577919006347656\n","step: 1470, loss: 0.11215181648731232\n","step: 1480, loss: 0.06986025720834732\n","step: 1490, loss: 0.0903857871890068\n","step: 1500, loss: 0.06973610073328018\n","step: 1510, loss: 0.1130453571677208\n","step: 1520, loss: 0.05926717817783356\n","step: 1530, loss: 0.1345812827348709\n","step: 1540, loss: 0.11982817947864532\n","step: 1550, loss: 0.08436400443315506\n","step: 1560, loss: 0.1374378204345703\n","step: 1570, loss: 0.05104336515069008\n","step: 1580, loss: 0.08993072062730789\n","step: 1590, loss: 0.13960473239421844\n","step: 1600, loss: 0.10173127800226212\n","step: 1610, loss: 0.10014054924249649\n","step: 1620, loss: 0.08360616862773895\n","step: 1630, loss: 0.031195146963000298\n","step: 1640, loss: 0.12440884858369827\n","step: 1650, loss: 0.10357169806957245\n","step: 1660, loss: 0.08573251217603683\n","step: 1670, loss: 0.15707845985889435\n","step: 1680, loss: 0.04719274863600731\n","step: 1690, loss: 0.08019570261240005\n","step: 1700, loss: 0.07659532874822617\n","step: 1710, loss: 0.10114412009716034\n","step: 1720, loss: 0.0824364721775055\n","step: 1730, loss: 0.22538310289382935\n","step: 1740, loss: 0.13301537930965424\n","step: 1750, loss: 0.12680742144584656\n","step: 1760, loss: 0.0941891297698021\n","step: 1770, loss: 0.0844900906085968\n","step: 1780, loss: 0.03389143943786621\n","step: 1790, loss: 0.03522597998380661\n","step: 1800, loss: 0.08949664980173111\n","step: 1810, loss: 0.11017400026321411\n","step: 1820, loss: 0.05324586480855942\n","step: 1830, loss: 0.10052159428596497\n","step: 1840, loss: 0.16039232909679413\n","step: 1850, loss: 0.046806931495666504\n","step: 1860, loss: 0.19260895252227783\n","step: 1870, loss: 0.0809473767876625\n","step: 1880, loss: 0.09919474273920059\n","step: 1890, loss: 0.07611917704343796\n","step: 1900, loss: 0.046550292521715164\n","step: 1910, loss: 0.08298573642969131\n","step: 1920, loss: 0.12834809720516205\n","step: 1930, loss: 0.04726997762918472\n","step: 1940, loss: 0.09029259532690048\n","step: 1950, loss: 0.04276599735021591\n","step: 1960, loss: 0.10067196935415268\n","step: 1970, loss: 0.08420389890670776\n","step: 1980, loss: 0.11615318059921265\n","step: 1990, loss: 0.04474419355392456\n","step: 2000, loss: 0.06799152493476868\n","step: 2010, loss: 0.08186257630586624\n","step: 2020, loss: 0.09367642551660538\n","step: 2030, loss: 0.09556543827056885\n","step: 2040, loss: 0.2559623122215271\n","step: 2050, loss: 0.055400650948286057\n","step: 2060, loss: 0.12129659205675125\n","step: 2070, loss: 0.12053119391202927\n","step: 2080, loss: 0.0722937136888504\n","step: 2090, loss: 0.05525801703333855\n","step: 2100, loss: 0.11318795382976532\n","step: 2110, loss: 0.08649015426635742\n","step: 2120, loss: 0.18334317207336426\n","step: 2130, loss: 0.06450022011995316\n","step: 2140, loss: 0.10178694874048233\n","step: 2150, loss: 0.0779028907418251\n","step: 2160, loss: 0.04183158650994301\n","step: 2170, loss: 0.04629460722208023\n","step: 2180, loss: 0.2089914083480835\n","step: 2190, loss: 0.11200498044490814\n","step: 2200, loss: 0.13562580943107605\n","step: 2210, loss: 0.10431984812021255\n","step: 2220, loss: 0.16250230371952057\n","step: 2230, loss: 0.08544564247131348\n","step: 2240, loss: 0.06290219724178314\n","step: 2250, loss: 0.2512426972389221\n","step: 2260, loss: 0.023005856201052666\n","step: 2270, loss: 0.04976154491305351\n","step: 2280, loss: 0.15937767922878265\n","step: 2290, loss: 0.07263855636119843\n","step: 2300, loss: 0.06133231148123741\n","step: 2310, loss: 0.0467037633061409\n","step: 2320, loss: 0.08249547332525253\n","step: 2330, loss: 0.05998755618929863\n","step: 2340, loss: 0.0779600664973259\n","step: 2350, loss: 0.01343982107937336\n","step: 2360, loss: 0.1919643133878708\n","step: 2370, loss: 0.07822824269533157\n","step: 2380, loss: 0.05702200531959534\n","step: 2390, loss: 0.15146127343177795\n","step: 2400, loss: 0.08104550838470459\n","step: 2410, loss: 0.1390695571899414\n","step: 2420, loss: 0.052900444716215134\n","step: 2430, loss: 0.12291871756315231\n","step: 2440, loss: 0.07255355268716812\n","step: 2450, loss: 0.18459424376487732\n","step: 2460, loss: 0.11381266266107559\n","step: 2470, loss: 0.07577395439147949\n","step: 2480, loss: 0.07928121089935303\n","step: 2490, loss: 0.1610472947359085\n","step: 2500, loss: 0.13620173931121826\n","step: 2510, loss: 0.051315244287252426\n","step: 2520, loss: 0.07447010278701782\n","step: 2530, loss: 0.1288440078496933\n","step: 2540, loss: 0.049917060881853104\n","step: 2550, loss: 0.08563688397407532\n","step: 2560, loss: 0.1060866191983223\n","step: 2570, loss: 0.1575641930103302\n","step: 2580, loss: 0.16882717609405518\n","step: 2590, loss: 0.08341357856988907\n","step: 2600, loss: 0.1696542650461197\n","step: 2610, loss: 0.06177351251244545\n","step: 2620, loss: 0.13308319449424744\n","step: 2630, loss: 0.1339024305343628\n","step: 2640, loss: 0.08593534678220749\n","step: 2650, loss: 0.14938439428806305\n","step: 2660, loss: 0.08230265229940414\n","step: 2670, loss: 0.09656941890716553\n","step: 2680, loss: 0.04393918812274933\n","step: 2690, loss: 0.08109401911497116\n","step: 2700, loss: 0.09284249693155289\n","step: 2710, loss: 0.09921778738498688\n","step: 2720, loss: 0.038482666015625\n","step: 2730, loss: 0.20833925902843475\n","step: 2740, loss: 0.10372758656740189\n","step: 2750, loss: 0.11240308731794357\n","step: 2760, loss: 0.06120612099766731\n","step: 2770, loss: 0.07951270788908005\n","step: 2780, loss: 0.0264449380338192\n","step: 2790, loss: 0.09845619648694992\n","step: 2800, loss: 0.10334491729736328\n","step: 2810, loss: 0.07314622402191162\n","step: 2820, loss: 0.09211675077676773\n","step: 2830, loss: 0.0948452576994896\n","step: 2840, loss: 0.06702114641666412\n","step: 2850, loss: 0.06021836772561073\n","step: 2860, loss: 0.19443447887897491\n","step: 2870, loss: 0.11512551456689835\n","step: 2880, loss: 0.16617178916931152\n","step: 2890, loss: 0.06337359547615051\n","step: 2900, loss: 0.022760212421417236\n","step: 2910, loss: 0.07775536179542542\n","step: 2920, loss: 0.020461343228816986\n","step: 2930, loss: 0.1803179681301117\n","step: 2940, loss: 0.1231914609670639\n","step: 2950, loss: 0.17246565222740173\n","step: 2960, loss: 0.12227816134691238\n","step: 2970, loss: 0.07653298228979111\n","step: 2980, loss: 0.07256412506103516\n","step: 2990, loss: 0.10896355658769608\n","step: 3000, loss: 0.10944865643978119\n","step: 3010, loss: 0.19377484917640686\n","step: 3020, loss: 0.11784357577562332\n","step: 3030, loss: 0.028537673875689507\n","step: 3040, loss: 0.08535199612379074\n","step: 3050, loss: 0.07146872580051422\n","step: 3060, loss: 0.07978937774896622\n","step: 3070, loss: 0.122431181371212\n","step: 3080, loss: 0.06998310983181\n","step: 3090, loss: 0.056100137531757355\n","step: 3100, loss: 0.05101996660232544\n","step: 3110, loss: 0.15835851430892944\n","step: 3120, loss: 0.05608215555548668\n","step: 3130, loss: 0.11192676424980164\n","step: 3140, loss: 0.05549502745270729\n","step: 3150, loss: 0.10874871909618378\n","step: 3160, loss: 0.046754494309425354\n","step: 3170, loss: 0.19285225868225098\n","step: 3180, loss: 0.16795529425144196\n","step: 3190, loss: 0.14240434765815735\n","step: 3200, loss: 0.07037036120891571\n","step: 3210, loss: 0.0709104984998703\n","step: 3220, loss: 0.08054942637681961\n","step: 3230, loss: 0.07610451430082321\n","step: 3240, loss: 0.05551271140575409\n","step: 3250, loss: 0.06129969656467438\n","step: 3260, loss: 0.18921586871147156\n","step: 3270, loss: 0.041442085057497025\n","step: 3280, loss: 0.10309285670518875\n","step: 3290, loss: 0.08845160156488419\n","step: 3300, loss: 0.09349900484085083\n","step: 3310, loss: 0.06994956731796265\n","step: 3320, loss: 0.10392095893621445\n","step: 3330, loss: 0.12680019438266754\n","step: 3340, loss: 0.12071933597326279\n","step: 3350, loss: 0.06936629116535187\n","step: 3360, loss: 0.1270103007555008\n","step: 3370, loss: 0.14126920700073242\n","step: 3380, loss: 0.10758902132511139\n","step: 3390, loss: 0.12598516047000885\n","step: 3400, loss: 0.11611809581518173\n","step: 3410, loss: 0.03652282431721687\n","step: 3420, loss: 0.08188295364379883\n","step: 3430, loss: 0.08830688148736954\n","step: 3440, loss: 0.07304620742797852\n","step: 3450, loss: 0.0643625557422638\n","step: 3460, loss: 0.09767352789640427\n","step: 3470, loss: 0.09487288445234299\n","step: 3480, loss: 0.03951149433851242\n","step: 3490, loss: 0.06275345385074615\n","step: 3500, loss: 0.0361848883330822\n","step: 3510, loss: 0.06891033053398132\n","step: 3520, loss: 0.06435883045196533\n","step: 3530, loss: 0.06563372910022736\n","step: 3540, loss: 0.05149690434336662\n","step: 3550, loss: 0.12249193340539932\n","step: 3560, loss: 0.09489816427230835\n","step: 3570, loss: 0.0681808665394783\n","step: 3580, loss: 0.0856265127658844\n","step: 3590, loss: 0.09116889536380768\n","step: 3600, loss: 0.07561196386814117\n","step: 3610, loss: 0.02154461294412613\n","step: 3620, loss: 0.08707551658153534\n","step: 3630, loss: 0.09234317392110825\n","step: 3640, loss: 0.1255587339401245\n","step: 3650, loss: 0.10280028730630875\n","step: 3660, loss: 0.06475511938333511\n","step: 3670, loss: 0.03655371069908142\n","step: 3680, loss: 0.0724279135465622\n","step: 3690, loss: 0.0663432702422142\n","step: 3700, loss: 0.08657670021057129\n","step: 3710, loss: 0.057566218078136444\n","step: 3720, loss: 0.0591345839202404\n","step: 3730, loss: 0.15034180879592896\n","step: 3740, loss: 0.13525037467479706\n","step: 3750, loss: 0.06178807094693184\n","step: 3760, loss: 0.10766083002090454\n","step: 3770, loss: 0.13006538152694702\n","\n","Loop 2\n","domain_dev_word_lst 914\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-5c104b1a35d7>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torch.tensor(y_hat).to(device), torch.tensor(y).to(device), num_classes=len(wsj_tags),\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.947545051574707\n","step: 10, loss: 1.81839120388031\n","step: 20, loss: 0.7410621643066406\n","step: 30, loss: 0.4402027726173401\n","step: 40, loss: 0.3797014653682709\n","step: 50, loss: 0.19948604702949524\n","step: 60, loss: 0.12781402468681335\n","step: 70, loss: 0.28110232949256897\n","step: 80, loss: 0.180606409907341\n","step: 90, loss: 0.12840670347213745\n","step: 100, loss: 0.20387208461761475\n","step: 110, loss: 0.15809954702854156\n","step: 120, loss: 0.12034683674573898\n","step: 130, loss: 0.1773402839899063\n","step: 140, loss: 0.16111023724079132\n","step: 150, loss: 0.2027190774679184\n","step: 160, loss: 0.10519345849752426\n","step: 170, loss: 0.13140225410461426\n","step: 180, loss: 0.16128839552402496\n","step: 190, loss: 0.2089368849992752\n","step: 200, loss: 0.07861389964818954\n","step: 210, loss: 0.09260715544223785\n","step: 220, loss: 0.08267010003328323\n","step: 230, loss: 0.08966845273971558\n","step: 240, loss: 0.16375243663787842\n","step: 250, loss: 0.15278558433055878\n","step: 260, loss: 0.06763710081577301\n","step: 270, loss: 0.07299073785543442\n","step: 280, loss: 0.11143633723258972\n","step: 290, loss: 0.04896228015422821\n","step: 300, loss: 0.13513855636119843\n","step: 310, loss: 0.08463487774133682\n","step: 320, loss: 0.06747503578662872\n","step: 330, loss: 0.07736528664827347\n","step: 340, loss: 0.18828503787517548\n","step: 350, loss: 0.12210649251937866\n","step: 360, loss: 0.12737023830413818\n","step: 370, loss: 0.18382607400417328\n","step: 380, loss: 0.10843035578727722\n","step: 390, loss: 0.11084561794996262\n","step: 400, loss: 0.07660134136676788\n","step: 410, loss: 0.13540542125701904\n","step: 420, loss: 0.2749859392642975\n","step: 430, loss: 0.10888224840164185\n","step: 440, loss: 0.06812652945518494\n","step: 450, loss: 0.13972127437591553\n","step: 460, loss: 0.07649729400873184\n","step: 470, loss: 0.1416415572166443\n","step: 480, loss: 0.10493610799312592\n","step: 490, loss: 0.10576097667217255\n","step: 500, loss: 0.17715777456760406\n","step: 510, loss: 0.13819345831871033\n","step: 520, loss: 0.1461963802576065\n","step: 530, loss: 0.06357679516077042\n","step: 540, loss: 0.05217950418591499\n","step: 550, loss: 0.1396808922290802\n","step: 560, loss: 0.07310838997364044\n","step: 570, loss: 0.05644616484642029\n","step: 580, loss: 0.09150367230176926\n","step: 590, loss: 0.11479554325342178\n","step: 600, loss: 0.08238169550895691\n","step: 610, loss: 0.0946684256196022\n","step: 620, loss: 0.07840399444103241\n","step: 630, loss: 0.10924706608057022\n","step: 640, loss: 0.045008014887571335\n","step: 650, loss: 0.08385348320007324\n","step: 660, loss: 0.057640355080366135\n","step: 670, loss: 0.11529415845870972\n","step: 680, loss: 0.1458074152469635\n","step: 690, loss: 0.13413412868976593\n","step: 700, loss: 0.11410466581583023\n","step: 710, loss: 0.12295865267515182\n","step: 720, loss: 0.16613365709781647\n","step: 730, loss: 0.07832618802785873\n","step: 740, loss: 0.03470351919531822\n","step: 750, loss: 0.03945605456829071\n","step: 760, loss: 0.10426395386457443\n","step: 770, loss: 0.093271903693676\n","step: 780, loss: 0.10221642255783081\n","step: 790, loss: 0.0682884156703949\n","step: 800, loss: 0.1000828742980957\n","step: 810, loss: 0.07990316301584244\n","step: 820, loss: 0.1168084666132927\n","step: 830, loss: 0.031715575605630875\n","step: 840, loss: 0.1625012457370758\n","step: 850, loss: 0.08785656094551086\n","step: 860, loss: 0.08718820661306381\n","step: 870, loss: 0.15103787183761597\n","step: 880, loss: 0.07045252621173859\n","step: 890, loss: 0.11840959638357162\n","step: 900, loss: 0.08650409430265427\n","step: 910, loss: 0.10559000074863434\n","step: 920, loss: 0.09808484464883804\n","step: 930, loss: 0.1398387849330902\n","step: 940, loss: 0.08250094950199127\n","step: 950, loss: 0.05822504684329033\n","step: 960, loss: 0.19313152134418488\n","step: 970, loss: 0.08206024020910263\n","step: 980, loss: 0.13339881598949432\n","step: 990, loss: 0.03606447950005531\n","step: 1000, loss: 0.0991784855723381\n","step: 1010, loss: 0.12294334918260574\n","step: 1020, loss: 0.1110588014125824\n","step: 1030, loss: 0.02833940088748932\n","step: 1040, loss: 0.03273645043373108\n","step: 1050, loss: 0.0661090612411499\n","step: 1060, loss: 0.13503269851207733\n","step: 1070, loss: 0.043797533959150314\n","step: 1080, loss: 0.25299185514450073\n","step: 1090, loss: 0.11481760442256927\n","step: 1100, loss: 0.06721913069486618\n","step: 1110, loss: 0.07675433158874512\n","step: 1120, loss: 0.0878247618675232\n","step: 1130, loss: 0.1755635142326355\n","step: 1140, loss: 0.05886687710881233\n","step: 1150, loss: 0.16156123578548431\n","step: 1160, loss: 0.12955112755298615\n","step: 1170, loss: 0.05002368986606598\n","step: 1180, loss: 0.15438172221183777\n","step: 1190, loss: 0.0886964276432991\n","step: 1200, loss: 0.07038021832704544\n","step: 1210, loss: 0.0461713932454586\n","step: 1220, loss: 0.13272051513195038\n","step: 1230, loss: 0.15944981575012207\n","step: 1240, loss: 0.16312360763549805\n","step: 1250, loss: 0.044311169534921646\n","step: 1260, loss: 0.14331784844398499\n","step: 1270, loss: 0.09681336581707001\n","step: 1280, loss: 0.09710520505905151\n","step: 1290, loss: 0.06449761241674423\n","step: 1300, loss: 0.07630772888660431\n","step: 1310, loss: 0.07772917300462723\n","step: 1320, loss: 0.10088735818862915\n","step: 1330, loss: 0.04368544742465019\n","step: 1340, loss: 0.0765131264925003\n","step: 1350, loss: 0.08140811324119568\n","step: 1360, loss: 0.060568299144506454\n","step: 1370, loss: 0.09514268487691879\n","step: 1380, loss: 0.09307636320590973\n","step: 1390, loss: 0.14073197543621063\n","step: 1400, loss: 0.056538112461566925\n","step: 1410, loss: 0.21017932891845703\n","step: 1420, loss: 0.16397227346897125\n","step: 1430, loss: 0.04535370692610741\n","step: 1440, loss: 0.12235787510871887\n","step: 1450, loss: 0.0767924040555954\n","step: 1460, loss: 0.18232019245624542\n","step: 1470, loss: 0.11569970101118088\n","step: 1480, loss: 0.06810490041971207\n","step: 1490, loss: 0.044295571744441986\n","step: 1500, loss: 0.13109742105007172\n","step: 1510, loss: 0.10783932358026505\n","step: 1520, loss: 0.20670293271541595\n","step: 1530, loss: 0.17220807075500488\n","step: 1540, loss: 0.08994298428297043\n","step: 1550, loss: 0.0879124253988266\n","step: 1560, loss: 0.047566093504428864\n","step: 1570, loss: 0.043139103800058365\n","step: 1580, loss: 0.18591733276844025\n","step: 1590, loss: 0.024932825937867165\n","step: 1600, loss: 0.08178721368312836\n","step: 1610, loss: 0.10457933694124222\n","step: 1620, loss: 0.15283022820949554\n","step: 1630, loss: 0.11453285068273544\n","step: 1640, loss: 0.13464438915252686\n","step: 1650, loss: 0.05314714461565018\n","step: 1660, loss: 0.08466567099094391\n","step: 1670, loss: 0.0840802788734436\n","step: 1680, loss: 0.050708118826150894\n","step: 1690, loss: 0.11999194324016571\n","step: 1700, loss: 0.040884003043174744\n","step: 1710, loss: 0.08881264179944992\n","step: 1720, loss: 0.045985542237758636\n","step: 1730, loss: 0.06842286884784698\n","step: 1740, loss: 0.07464314252138138\n","step: 1750, loss: 0.029364729300141335\n","step: 1760, loss: 0.07545442879199982\n","step: 1770, loss: 0.04770585149526596\n","step: 1780, loss: 0.041894249618053436\n","step: 1790, loss: 0.03292171284556389\n","step: 1800, loss: 0.026227811351418495\n","step: 1810, loss: 0.09543745219707489\n","step: 1820, loss: 0.0614190511405468\n","step: 1830, loss: 0.06756241619586945\n","step: 1840, loss: 0.060172293335199356\n","step: 1850, loss: 0.06232602894306183\n","step: 1860, loss: 0.10907784104347229\n","step: 1870, loss: 0.16251365840435028\n","step: 1880, loss: 0.05783407390117645\n","step: 1890, loss: 0.06972093135118484\n","step: 1900, loss: 0.11228565126657486\n","step: 1910, loss: 0.07670561969280243\n","step: 1920, loss: 0.10930182039737701\n","step: 1930, loss: 0.07046081870794296\n","step: 1940, loss: 0.09535904228687286\n","step: 1950, loss: 0.09838971495628357\n","step: 1960, loss: 0.09216519445180893\n","step: 1970, loss: 0.16146761178970337\n","step: 1980, loss: 0.1054282933473587\n","step: 1990, loss: 0.08684825152158737\n","step: 2000, loss: 0.14767152070999146\n","step: 2010, loss: 0.10137449204921722\n","step: 2020, loss: 0.04131758213043213\n","step: 2030, loss: 0.06152055040001869\n","step: 2040, loss: 0.32998692989349365\n","step: 2050, loss: 0.1315840780735016\n","step: 2060, loss: 0.08696676790714264\n","step: 2070, loss: 0.018131032586097717\n","step: 2080, loss: 0.0880327820777893\n","step: 2090, loss: 0.05417531728744507\n","step: 2100, loss: 0.0937151089310646\n","step: 2110, loss: 0.03977464511990547\n","step: 2120, loss: 0.1746807098388672\n","step: 2130, loss: 0.1399909406900406\n","step: 2140, loss: 0.09623240679502487\n","step: 2150, loss: 0.08653321862220764\n","step: 2160, loss: 0.09259798377752304\n","step: 2170, loss: 0.08720814436674118\n","step: 2180, loss: 0.08346179127693176\n","step: 2190, loss: 0.06980486959218979\n","step: 2200, loss: 0.17424865067005157\n","step: 2210, loss: 0.06463849544525146\n","step: 2220, loss: 0.06375131756067276\n","step: 2230, loss: 0.05718803405761719\n","step: 2240, loss: 0.13754090666770935\n","step: 2250, loss: 0.08207149803638458\n","step: 2260, loss: 0.024861617013812065\n","step: 2270, loss: 0.08171556890010834\n","step: 2280, loss: 0.10065047442913055\n","step: 2290, loss: 0.04507988318800926\n","step: 2300, loss: 0.1367066651582718\n","step: 2310, loss: 0.06843610107898712\n","step: 2320, loss: 0.0775982141494751\n","step: 2330, loss: 0.03767162188887596\n","step: 2340, loss: 0.04357864335179329\n","step: 2350, loss: 0.13684818148612976\n","step: 2360, loss: 0.09934443980455399\n","step: 2370, loss: 0.0541195273399353\n","step: 2380, loss: 0.1255813091993332\n","step: 2390, loss: 0.12828093767166138\n","step: 2400, loss: 0.034939613193273544\n","step: 2410, loss: 0.1608782261610031\n","step: 2420, loss: 0.08068279176950455\n","step: 2430, loss: 0.09750165790319443\n","step: 2440, loss: 0.09061749279499054\n","step: 2450, loss: 0.118220753967762\n","step: 2460, loss: 0.07460558414459229\n","step: 2470, loss: 0.16224560141563416\n","step: 2480, loss: 0.05253268778324127\n","step: 2490, loss: 0.06793837249279022\n","step: 2500, loss: 0.11417881399393082\n","step: 2510, loss: 0.13123351335525513\n","step: 2520, loss: 0.07424382865428925\n","step: 2530, loss: 0.10741918534040451\n","step: 2540, loss: 0.14806842803955078\n","step: 2550, loss: 0.11525484919548035\n","step: 2560, loss: 0.051909804344177246\n","step: 2570, loss: 0.13395662605762482\n","step: 2580, loss: 0.06390473991632462\n","step: 2590, loss: 0.047061409801244736\n","step: 2600, loss: 0.09323014318943024\n","step: 2610, loss: 0.08523101359605789\n","step: 2620, loss: 0.03064926154911518\n","step: 2630, loss: 0.08092021942138672\n","step: 2640, loss: 0.07709886878728867\n","step: 2650, loss: 0.0772814005613327\n","step: 2660, loss: 0.06617733836174011\n","step: 2670, loss: 0.04235217720270157\n","step: 2680, loss: 0.051937200129032135\n","step: 2690, loss: 0.09600644558668137\n","step: 2700, loss: 0.06703483313322067\n","step: 2710, loss: 0.0792543888092041\n","step: 2720, loss: 0.14016926288604736\n","step: 2730, loss: 0.04407776892185211\n","step: 2740, loss: 0.1333359181880951\n","step: 2750, loss: 0.052751678973436356\n","step: 2760, loss: 0.12543217837810516\n","step: 2770, loss: 0.08610176295042038\n","step: 2780, loss: 0.10003155469894409\n","step: 2790, loss: 0.1529293805360794\n","step: 2800, loss: 0.10508711636066437\n","step: 2810, loss: 0.13275791704654694\n","step: 2820, loss: 0.06449192762374878\n","step: 2830, loss: 0.1699410080909729\n","step: 2840, loss: 0.11877409368753433\n","step: 2850, loss: 0.04521651193499565\n","step: 2860, loss: 0.10242794454097748\n","step: 2870, loss: 0.14423424005508423\n","step: 2880, loss: 0.0703677386045456\n","step: 2890, loss: 0.06464409083127975\n","step: 2900, loss: 0.017725499346852303\n","step: 2910, loss: 0.04227253049612045\n","step: 2920, loss: 0.06923762708902359\n","step: 2930, loss: 0.08100811392068863\n","step: 2940, loss: 0.07724364101886749\n","step: 2950, loss: 0.09856265783309937\n","step: 2960, loss: 0.07922409474849701\n","step: 2970, loss: 0.04005524888634682\n","step: 2980, loss: 0.10957738757133484\n","step: 2990, loss: 0.028304599225521088\n","step: 3000, loss: 0.03785783424973488\n","step: 3010, loss: 0.11254578083753586\n","step: 3020, loss: 0.15771028399467468\n","step: 3030, loss: 0.06020832806825638\n","step: 3040, loss: 0.049261219799518585\n","step: 3050, loss: 0.12173717468976974\n","step: 3060, loss: 0.03959168866276741\n","step: 3070, loss: 0.13530689477920532\n","step: 3080, loss: 0.0796547383069992\n","step: 3090, loss: 0.05321148410439491\n","step: 3100, loss: 0.17570826411247253\n","step: 3110, loss: 0.03141017258167267\n","step: 3120, loss: 0.09004265815019608\n","step: 3130, loss: 0.05315575748682022\n","step: 3140, loss: 0.10230131447315216\n","step: 3150, loss: 0.09349998831748962\n","step: 3160, loss: 0.03271820768713951\n","step: 3170, loss: 0.07032814621925354\n","step: 3180, loss: 0.03282901272177696\n","step: 3190, loss: 0.08958840370178223\n","step: 3200, loss: 0.09298290312290192\n","step: 3210, loss: 0.12946447730064392\n","step: 3220, loss: 0.07834729552268982\n","step: 3230, loss: 0.1939743310213089\n","step: 3240, loss: 0.16219602525234222\n","step: 3250, loss: 0.045046616345644\n","step: 3260, loss: 0.10628259927034378\n","step: 3270, loss: 0.06035992130637169\n","step: 3280, loss: 0.1381480097770691\n","step: 3290, loss: 0.09835238754749298\n","step: 3300, loss: 0.04552951827645302\n","step: 3310, loss: 0.10246801376342773\n","step: 3320, loss: 0.11816413700580597\n","step: 3330, loss: 0.040071479976177216\n","step: 3340, loss: 0.11775104701519012\n","step: 3350, loss: 0.08736847341060638\n","step: 3360, loss: 0.061029016971588135\n","step: 3370, loss: 0.10353204607963562\n","step: 3380, loss: 0.03861523047089577\n","step: 3390, loss: 0.014532611705362797\n","step: 3400, loss: 0.14680655300617218\n","step: 3410, loss: 0.07475059479475021\n","step: 3420, loss: 0.050358738750219345\n","step: 3430, loss: 0.07144171744585037\n","step: 3440, loss: 0.07887757569551468\n","step: 3450, loss: 0.1197054535150528\n","step: 3460, loss: 0.0668516680598259\n","step: 3470, loss: 0.12175025790929794\n","step: 3480, loss: 0.04885958880186081\n","step: 3490, loss: 0.05973169580101967\n","step: 3500, loss: 0.04428482428193092\n","step: 3510, loss: 0.11695516854524612\n","step: 3520, loss: 0.10422763973474503\n","step: 3530, loss: 0.06899093836545944\n","step: 3540, loss: 0.06007540598511696\n","step: 3550, loss: 0.08858386427164078\n","step: 3560, loss: 0.0710199847817421\n","step: 3570, loss: 0.06743300706148148\n","step: 3580, loss: 0.09602435678243637\n","step: 3590, loss: 0.12740200757980347\n","step: 3600, loss: 0.07091137766838074\n","step: 3610, loss: 0.08200087398290634\n","step: 3620, loss: 0.1265018880367279\n","step: 3630, loss: 0.009870702400803566\n","step: 3640, loss: 0.11204103380441666\n","step: 3650, loss: 0.06048359349370003\n","step: 3660, loss: 0.07564211636781693\n","step: 3670, loss: 0.12446027249097824\n","step: 3680, loss: 0.05775485932826996\n","step: 3690, loss: 0.11690589785575867\n","step: 3700, loss: 0.12614145874977112\n","step: 3710, loss: 0.07916558533906937\n","step: 3720, loss: 0.1355128437280655\n","step: 3730, loss: 0.030839037150144577\n","step: 3740, loss: 0.08525331318378448\n","step: 3750, loss: 0.03128041326999664\n","step: 3760, loss: 0.024313339963555336\n","step: 3770, loss: 0.04914015159010887\n","\n","Loop 3\n","domain_dev_word_lst 812\n","Train from scratch...\n","step: 0, loss: 3.9698853492736816\n","step: 10, loss: 1.6269224882125854\n","step: 20, loss: 0.7166692614555359\n","step: 30, loss: 0.41896069049835205\n","step: 40, loss: 0.3069407343864441\n","step: 50, loss: 0.25099363923072815\n","step: 60, loss: 0.21046608686447144\n","step: 70, loss: 0.3168674111366272\n","step: 80, loss: 0.2430477738380432\n","step: 90, loss: 0.10376258939504623\n","step: 100, loss: 0.2897758185863495\n","step: 110, loss: 0.20986835658550262\n","step: 120, loss: 0.09029185026884079\n","step: 130, loss: 0.16507205367088318\n","step: 140, loss: 0.13013964891433716\n","step: 150, loss: 0.2408166378736496\n","step: 160, loss: 0.1639164388179779\n","step: 170, loss: 0.2481350153684616\n","step: 180, loss: 0.20531049370765686\n","step: 190, loss: 0.05896078050136566\n","step: 200, loss: 0.10185836255550385\n","step: 210, loss: 0.14562514424324036\n","step: 220, loss: 0.1049276664853096\n","step: 230, loss: 0.10716357082128525\n","step: 240, loss: 0.0822438895702362\n","step: 250, loss: 0.06153799220919609\n","step: 260, loss: 0.07985290139913559\n","step: 270, loss: 0.20582914352416992\n","step: 280, loss: 0.04166347160935402\n","step: 290, loss: 0.10710185766220093\n","step: 300, loss: 0.08495689183473587\n","step: 310, loss: 0.16119009256362915\n","step: 320, loss: 0.08622249215841293\n","step: 330, loss: 0.08739590644836426\n","step: 340, loss: 0.11593606323003769\n","step: 350, loss: 0.060779180377721786\n","step: 360, loss: 0.07054881006479263\n","step: 370, loss: 0.05753042548894882\n","step: 380, loss: 0.04713430628180504\n","step: 390, loss: 0.21180200576782227\n","step: 400, loss: 0.13485969603061676\n","step: 410, loss: 0.112271748483181\n","step: 420, loss: 0.12578587234020233\n","step: 430, loss: 0.06179967150092125\n","step: 440, loss: 0.2428734302520752\n","step: 450, loss: 0.19404470920562744\n","step: 460, loss: 0.09158609062433243\n","step: 470, loss: 0.14797097444534302\n","step: 480, loss: 0.18762944638729095\n","step: 490, loss: 0.16165421903133392\n","step: 500, loss: 0.06523546576499939\n","step: 510, loss: 0.056195441633462906\n","step: 520, loss: 0.21505817770957947\n","step: 530, loss: 0.09751078486442566\n","step: 540, loss: 0.16775673627853394\n","step: 550, loss: 0.17677876353263855\n","step: 560, loss: 0.07090815901756287\n","step: 570, loss: 0.18767298758029938\n","step: 580, loss: 0.12611989676952362\n","step: 590, loss: 0.08012974262237549\n","step: 600, loss: 0.1636788249015808\n","step: 610, loss: 0.10191597789525986\n","step: 620, loss: 0.05779222398996353\n","step: 630, loss: 0.09811084717512131\n","step: 640, loss: 0.14566297829151154\n","step: 650, loss: 0.19055481255054474\n","step: 660, loss: 0.15347114205360413\n","step: 670, loss: 0.09113502502441406\n","step: 680, loss: 0.0983164831995964\n","step: 690, loss: 0.07726988196372986\n","step: 700, loss: 0.05746053531765938\n","step: 710, loss: 0.20216509699821472\n","step: 720, loss: 0.14544802904129028\n","step: 730, loss: 0.051220718771219254\n","step: 740, loss: 0.09711086004972458\n","step: 750, loss: 0.2183428853750229\n","step: 760, loss: 0.0682840347290039\n","step: 770, loss: 0.19175493717193604\n","step: 780, loss: 0.11266332119703293\n","step: 790, loss: 0.13242316246032715\n","step: 800, loss: 0.11902567744255066\n","step: 810, loss: 0.193409264087677\n","step: 820, loss: 0.1440885066986084\n","step: 830, loss: 0.10989779233932495\n","step: 840, loss: 0.10964631289243698\n","step: 850, loss: 0.0554729700088501\n","step: 860, loss: 0.17712722718715668\n","step: 870, loss: 0.2134985327720642\n","step: 880, loss: 0.07894198596477509\n","step: 890, loss: 0.06450623273849487\n","step: 900, loss: 0.0751621201634407\n","step: 910, loss: 0.055731624364852905\n","step: 920, loss: 0.09484270215034485\n","step: 930, loss: 0.047960251569747925\n","step: 940, loss: 0.05243544280529022\n","step: 950, loss: 0.07199236005544662\n","step: 960, loss: 0.0911891907453537\n","step: 970, loss: 0.14204208552837372\n","step: 980, loss: 0.18023119866847992\n","step: 990, loss: 0.1375439167022705\n","step: 1000, loss: 0.12219495326280594\n","step: 1010, loss: 0.0815625861287117\n","step: 1020, loss: 0.13579463958740234\n","step: 1030, loss: 0.09274377673864365\n","step: 1040, loss: 0.04412642493844032\n","step: 1050, loss: 0.08948497474193573\n","step: 1060, loss: 0.1826181858778\n","step: 1070, loss: 0.1157066598534584\n","step: 1080, loss: 0.15117642283439636\n","step: 1090, loss: 0.08509625494480133\n","step: 1100, loss: 0.13122329115867615\n","step: 1110, loss: 0.10210327059030533\n","step: 1120, loss: 0.17749495804309845\n","step: 1130, loss: 0.058890845626592636\n","step: 1140, loss: 0.09965189546346664\n","step: 1150, loss: 0.17383472621440887\n","step: 1160, loss: 0.09864208847284317\n","step: 1170, loss: 0.1449155956506729\n","step: 1180, loss: 0.06576181203126907\n","step: 1190, loss: 0.10971978306770325\n","step: 1200, loss: 0.14057964086532593\n","step: 1210, loss: 0.0972936600446701\n","step: 1220, loss: 0.057400450110435486\n","step: 1230, loss: 0.014581491239368916\n","step: 1240, loss: 0.11479174345731735\n","step: 1250, loss: 0.14744558930397034\n","step: 1260, loss: 0.016215113922953606\n","step: 1270, loss: 0.09887255728244781\n","step: 1280, loss: 0.062127478420734406\n","step: 1290, loss: 0.10922086983919144\n","step: 1300, loss: 0.10787709802389145\n","step: 1310, loss: 0.07527802884578705\n","step: 1320, loss: 0.07320307940244675\n","step: 1330, loss: 0.13462388515472412\n","step: 1340, loss: 0.07377804070711136\n","step: 1350, loss: 0.09430994093418121\n","step: 1360, loss: 0.04962459206581116\n","step: 1370, loss: 0.061699990183115005\n","step: 1380, loss: 0.06614319235086441\n","step: 1390, loss: 0.057584695518016815\n","step: 1400, loss: 0.10848085582256317\n","step: 1410, loss: 0.09554889053106308\n","step: 1420, loss: 0.04249730706214905\n","step: 1430, loss: 0.1777053326368332\n","step: 1440, loss: 0.1394060254096985\n","step: 1450, loss: 0.24453403055667877\n","step: 1460, loss: 0.06016693636775017\n","step: 1470, loss: 0.06607817858457565\n","step: 1480, loss: 0.1245395764708519\n","step: 1490, loss: 0.08110659569501877\n","step: 1500, loss: 0.04037966951727867\n","step: 1510, loss: 0.14160174131393433\n","step: 1520, loss: 0.15412040054798126\n","step: 1530, loss: 0.1012142226099968\n","step: 1540, loss: 0.08680423349142075\n","step: 1550, loss: 0.09082943201065063\n","step: 1560, loss: 0.061659179627895355\n","step: 1570, loss: 0.04513192176818848\n","step: 1580, loss: 0.06434269994497299\n","step: 1590, loss: 0.03618206828832626\n","step: 1600, loss: 0.11132646352052689\n","step: 1610, loss: 0.04121992737054825\n","step: 1620, loss: 0.057219699025154114\n","step: 1630, loss: 0.04707605019211769\n","step: 1640, loss: 0.0549028255045414\n","step: 1650, loss: 0.06723824888467789\n","step: 1660, loss: 0.04645612835884094\n","step: 1670, loss: 0.1242390125989914\n","step: 1680, loss: 0.09891026467084885\n","step: 1690, loss: 0.10720263421535492\n","step: 1700, loss: 0.08038852363824844\n","step: 1710, loss: 0.03931958228349686\n","step: 1720, loss: 0.06076687574386597\n","step: 1730, loss: 0.02419731393456459\n","step: 1740, loss: 0.10906347632408142\n","step: 1750, loss: 0.1710708737373352\n","step: 1760, loss: 0.18668556213378906\n","step: 1770, loss: 0.07662071287631989\n","step: 1780, loss: 0.17919982969760895\n","step: 1790, loss: 0.13689690828323364\n","step: 1800, loss: 0.06583356857299805\n","step: 1810, loss: 0.06181880831718445\n","step: 1820, loss: 0.11736954748630524\n","step: 1830, loss: 0.05574514716863632\n","step: 1840, loss: 0.18175387382507324\n","step: 1850, loss: 0.09184655547142029\n","step: 1860, loss: 0.08089037984609604\n","step: 1870, loss: 0.07554153352975845\n","step: 1880, loss: 0.13643379509449005\n","step: 1890, loss: 0.08462170511484146\n","step: 1900, loss: 0.08439131081104279\n","step: 1910, loss: 0.1194981262087822\n","step: 1920, loss: 0.07282045483589172\n","step: 1930, loss: 0.13316495716571808\n","step: 1940, loss: 0.07285039871931076\n","step: 1950, loss: 0.08361288905143738\n","step: 1960, loss: 0.167643740773201\n","step: 1970, loss: 0.02020983397960663\n","step: 1980, loss: 0.06288119405508041\n","step: 1990, loss: 0.16181762516498566\n","step: 2000, loss: 0.0873948484659195\n","step: 2010, loss: 0.06655373424291611\n","step: 2020, loss: 0.10704858601093292\n","step: 2030, loss: 0.04068474844098091\n","step: 2040, loss: 0.14746490120887756\n","step: 2050, loss: 0.04946550726890564\n","step: 2060, loss: 0.056125983595848083\n","step: 2070, loss: 0.12874099612236023\n","step: 2080, loss: 0.07524381577968597\n","step: 2090, loss: 0.11952730268239975\n","step: 2100, loss: 0.1826167106628418\n","step: 2110, loss: 0.10155342519283295\n","step: 2120, loss: 0.047344453632831573\n","step: 2130, loss: 0.08646921068429947\n","step: 2140, loss: 0.06757797300815582\n","step: 2150, loss: 0.06401800364255905\n","step: 2160, loss: 0.032247789204120636\n","step: 2170, loss: 0.041536130011081696\n","step: 2180, loss: 0.08806969970464706\n","step: 2190, loss: 0.0601656436920166\n","step: 2200, loss: 0.09523528814315796\n","step: 2210, loss: 0.09324195981025696\n","step: 2220, loss: 0.06813706457614899\n","step: 2230, loss: 0.04774883762001991\n","step: 2240, loss: 0.08803468942642212\n","step: 2250, loss: 0.014345262199640274\n","step: 2260, loss: 0.04742478206753731\n","step: 2270, loss: 0.05595982447266579\n","step: 2280, loss: 0.07349782437086105\n","step: 2290, loss: 0.034564319998025894\n","step: 2300, loss: 0.025082051753997803\n","step: 2310, loss: 0.048460837453603745\n","step: 2320, loss: 0.16689817607402802\n","step: 2330, loss: 0.0910634845495224\n","step: 2340, loss: 0.1344868391752243\n","step: 2350, loss: 0.08838311582803726\n","step: 2360, loss: 0.10081783682107925\n","step: 2370, loss: 0.19599375128746033\n","step: 2380, loss: 0.11839830875396729\n","step: 2390, loss: 0.0490824431180954\n","step: 2400, loss: 0.06502345949411392\n","step: 2410, loss: 0.0837879627943039\n","step: 2420, loss: 0.10717086493968964\n","step: 2430, loss: 0.1471794843673706\n","step: 2440, loss: 0.06206388771533966\n","step: 2450, loss: 0.06188089773058891\n","step: 2460, loss: 0.04933531954884529\n","step: 2470, loss: 0.04366691783070564\n","step: 2480, loss: 0.07950586825609207\n","step: 2490, loss: 0.08685575425624847\n","step: 2500, loss: 0.08581295609474182\n","step: 2510, loss: 0.12465158104896545\n","step: 2520, loss: 0.07607950270175934\n","step: 2530, loss: 0.11220666766166687\n","step: 2540, loss: 0.10905498266220093\n","step: 2550, loss: 0.1092405691742897\n","step: 2560, loss: 0.13821230828762054\n","step: 2570, loss: 0.06272269785404205\n","step: 2580, loss: 0.055931590497493744\n","step: 2590, loss: 0.07266128063201904\n","step: 2600, loss: 0.16098569333553314\n","step: 2610, loss: 0.08564434945583344\n","step: 2620, loss: 0.08878245204687119\n","step: 2630, loss: 0.07221797853708267\n","step: 2640, loss: 0.08215110749006271\n","step: 2650, loss: 0.05729514732956886\n","step: 2660, loss: 0.20595361292362213\n","step: 2670, loss: 0.04880592226982117\n","step: 2680, loss: 0.05841134488582611\n","step: 2690, loss: 0.08690805733203888\n","step: 2700, loss: 0.1221340224146843\n","step: 2710, loss: 0.08337505161762238\n","step: 2720, loss: 0.03351442515850067\n","step: 2730, loss: 0.08436399698257446\n","step: 2740, loss: 0.10781268775463104\n","step: 2750, loss: 0.10106346756219864\n","step: 2760, loss: 0.07201431691646576\n","step: 2770, loss: 0.12044256925582886\n","step: 2780, loss: 0.015097553841769695\n","step: 2790, loss: 0.22211316227912903\n","step: 2800, loss: 0.05299924314022064\n","step: 2810, loss: 0.19739152491092682\n","step: 2820, loss: 0.05209742859005928\n","step: 2830, loss: 0.08597671240568161\n","step: 2840, loss: 0.056521281599998474\n","step: 2850, loss: 0.06470175832509995\n","step: 2860, loss: 0.07322533428668976\n","step: 2870, loss: 0.09235342592000961\n","step: 2880, loss: 0.06521054357290268\n","step: 2890, loss: 0.060758303850889206\n","step: 2900, loss: 0.02450195886194706\n","step: 2910, loss: 0.1333286315202713\n","step: 2920, loss: 0.08435200154781342\n","step: 2930, loss: 0.08094838261604309\n","step: 2940, loss: 0.08256235718727112\n","step: 2950, loss: 0.04616706445813179\n","step: 2960, loss: 0.15915261209011078\n","step: 2970, loss: 0.06896999478340149\n","step: 2980, loss: 0.10328364372253418\n","step: 2990, loss: 0.1749458909034729\n","step: 3000, loss: 0.13469132781028748\n","step: 3010, loss: 0.06282664835453033\n","step: 3020, loss: 0.1465211808681488\n","step: 3030, loss: 0.19313837587833405\n","step: 3040, loss: 0.057999808341264725\n","step: 3050, loss: 0.12295139580965042\n","step: 3060, loss: 0.08438221365213394\n","step: 3070, loss: 0.0674421563744545\n","step: 3080, loss: 0.1450367271900177\n","step: 3090, loss: 0.09156909584999084\n","step: 3100, loss: 0.15509597957134247\n","step: 3110, loss: 0.07371304929256439\n","step: 3120, loss: 0.15472599864006042\n","step: 3130, loss: 0.11372734606266022\n","step: 3140, loss: 0.05878216400742531\n","step: 3150, loss: 0.17793190479278564\n","step: 3160, loss: 0.1383131444454193\n","step: 3170, loss: 0.21570278704166412\n","step: 3180, loss: 0.2630709707736969\n","step: 3190, loss: 0.1556822955608368\n","step: 3200, loss: 0.10398036986589432\n","step: 3210, loss: 0.0704188123345375\n","step: 3220, loss: 0.22341135144233704\n","step: 3230, loss: 0.16173037886619568\n","step: 3240, loss: 0.059569764882326126\n","step: 3250, loss: 0.07054495066404343\n","step: 3260, loss: 0.1561267375946045\n","step: 3270, loss: 0.048883046954870224\n","step: 3280, loss: 0.12178978323936462\n","step: 3290, loss: 0.07798608392477036\n","step: 3300, loss: 0.08242188394069672\n","step: 3310, loss: 0.03699328377842903\n","step: 3320, loss: 0.0533437579870224\n","step: 3330, loss: 0.027954628691077232\n","step: 3340, loss: 0.12136310338973999\n","step: 3350, loss: 0.07254719734191895\n","step: 3360, loss: 0.08337699621915817\n","step: 3370, loss: 0.032158371061086655\n","step: 3380, loss: 0.047198183834552765\n","step: 3390, loss: 0.07590503245592117\n","step: 3400, loss: 0.13216133415699005\n","step: 3410, loss: 0.1119760125875473\n","step: 3420, loss: 0.08924753218889236\n","step: 3430, loss: 0.06343042850494385\n","step: 3440, loss: 0.05220703408122063\n","step: 3450, loss: 0.08593598753213882\n","step: 3460, loss: 0.10476529598236084\n","step: 3470, loss: 0.08497323840856552\n","step: 3480, loss: 0.12756721675395966\n","step: 3490, loss: 0.031420089304447174\n","step: 3500, loss: 0.1654132604598999\n","step: 3510, loss: 0.07088232785463333\n","step: 3520, loss: 0.1100258007645607\n","step: 3530, loss: 0.17241066694259644\n","step: 3540, loss: 0.1598162055015564\n","step: 3550, loss: 0.1346464455127716\n","step: 3560, loss: 0.09448336064815521\n","step: 3570, loss: 0.03804413601756096\n","step: 3580, loss: 0.059761036187410355\n","step: 3590, loss: 0.09915973991155624\n","step: 3600, loss: 0.11116870492696762\n","step: 3610, loss: 0.06261487305164337\n","step: 3620, loss: 0.07555384933948517\n","step: 3630, loss: 0.0933835357427597\n","step: 3640, loss: 0.12009167671203613\n","step: 3650, loss: 0.05843322351574898\n","step: 3660, loss: 0.06830782443284988\n","step: 3670, loss: 0.09958409518003464\n","step: 3680, loss: 0.07500004768371582\n","step: 3690, loss: 0.17940513789653778\n","step: 3700, loss: 0.030167434364557266\n","step: 3710, loss: 0.1045457050204277\n","step: 3720, loss: 0.030043909326195717\n","step: 3730, loss: 0.12434137612581253\n","step: 3740, loss: 0.256081223487854\n","step: 3750, loss: 0.08038389682769775\n","step: 3760, loss: 0.11530208587646484\n","step: 3770, loss: 0.25624880194664\n","\n","Loop 4\n","domain_dev_word_lst 710\n","Train from scratch...\n","step: 0, loss: 3.9278616905212402\n","step: 10, loss: 1.6575796604156494\n","step: 20, loss: 0.7751578688621521\n","step: 30, loss: 0.4719984233379364\n","step: 40, loss: 0.27158740162849426\n","step: 50, loss: 0.2347186952829361\n","step: 60, loss: 0.17692598700523376\n","step: 70, loss: 0.19477932155132294\n","step: 80, loss: 0.12686291337013245\n","step: 90, loss: 0.18094348907470703\n","step: 100, loss: 0.20517028868198395\n","step: 110, loss: 0.11451375484466553\n","step: 120, loss: 0.05600139498710632\n","step: 130, loss: 0.08393803238868713\n","step: 140, loss: 0.168010875582695\n","step: 150, loss: 0.20917116105556488\n","step: 160, loss: 0.1373564600944519\n","step: 170, loss: 0.10958511382341385\n","step: 180, loss: 0.11699805408716202\n","step: 190, loss: 0.16279737651348114\n","step: 200, loss: 0.10501376539468765\n","step: 210, loss: 0.1991649568080902\n","step: 220, loss: 0.25317496061325073\n","step: 230, loss: 0.10395541042089462\n","step: 240, loss: 0.21716322004795074\n","step: 250, loss: 0.14727842807769775\n","step: 260, loss: 0.22325214743614197\n","step: 270, loss: 0.04828403517603874\n","step: 280, loss: 0.12877528369426727\n","step: 290, loss: 0.278396874666214\n","step: 300, loss: 0.11431807279586792\n","step: 310, loss: 0.171296626329422\n","step: 320, loss: 0.07662462443113327\n","step: 330, loss: 0.1524309366941452\n","step: 340, loss: 0.12146478146314621\n","step: 350, loss: 0.15652146935462952\n","step: 360, loss: 0.05428079888224602\n","step: 370, loss: 0.043700456619262695\n","step: 380, loss: 0.06835995614528656\n","step: 390, loss: 0.1518697589635849\n","step: 400, loss: 0.1132231056690216\n","step: 410, loss: 0.166660875082016\n","step: 420, loss: 0.10137472301721573\n","step: 430, loss: 0.1398119330406189\n","step: 440, loss: 0.18311533331871033\n","step: 450, loss: 0.17798291146755219\n","step: 460, loss: 0.21027831733226776\n","step: 470, loss: 0.11314455419778824\n","step: 480, loss: 0.08749659359455109\n","step: 490, loss: 0.09428673982620239\n","step: 500, loss: 0.07464718073606491\n","step: 510, loss: 0.14670570194721222\n","step: 520, loss: 0.13132214546203613\n","step: 530, loss: 0.07448693364858627\n","step: 540, loss: 0.11126865446567535\n","step: 550, loss: 0.11458785831928253\n","step: 560, loss: 0.13239870965480804\n","step: 570, loss: 0.08114781975746155\n","step: 580, loss: 0.10371585190296173\n","step: 590, loss: 0.0763881728053093\n","step: 600, loss: 0.0756605789065361\n","step: 610, loss: 0.033752862364053726\n","step: 620, loss: 0.09568796306848526\n","step: 630, loss: 0.12763045728206635\n","step: 640, loss: 0.14976802468299866\n","step: 650, loss: 0.11582296341657639\n","step: 660, loss: 0.119247667491436\n","step: 670, loss: 0.12514476478099823\n","step: 680, loss: 0.12885473668575287\n","step: 690, loss: 0.1451409012079239\n","step: 700, loss: 0.11983714997768402\n","step: 710, loss: 0.11742304265499115\n","step: 720, loss: 0.048423632979393005\n","step: 730, loss: 0.07827859371900558\n","step: 740, loss: 0.0880516767501831\n","step: 750, loss: 0.0790097787976265\n","step: 760, loss: 0.09708189219236374\n","step: 770, loss: 0.08042474091053009\n","step: 780, loss: 0.06914260238409042\n","step: 790, loss: 0.0730961412191391\n","step: 800, loss: 0.06879003345966339\n","step: 810, loss: 0.08656518161296844\n","step: 820, loss: 0.13255921006202698\n","step: 830, loss: 0.11743545532226562\n","step: 840, loss: 0.20916154980659485\n","step: 850, loss: 0.1016877144575119\n","step: 860, loss: 0.08464860171079636\n","step: 870, loss: 0.0826074406504631\n","step: 880, loss: 0.13172440230846405\n","step: 890, loss: 0.04758008196949959\n","step: 900, loss: 0.13326868414878845\n","step: 910, loss: 0.10551488399505615\n","step: 920, loss: 0.09264621883630753\n","step: 930, loss: 0.051238764077425\n","step: 940, loss: 0.11167235672473907\n","step: 950, loss: 0.16096168756484985\n","step: 960, loss: 0.23147685825824738\n","step: 970, loss: 0.05114499479532242\n","step: 980, loss: 0.09475544095039368\n","step: 990, loss: 0.059589169919490814\n","step: 1000, loss: 0.06108834221959114\n","step: 1010, loss: 0.04236265644431114\n","step: 1020, loss: 0.08133541792631149\n","step: 1030, loss: 0.24871350824832916\n","step: 1040, loss: 0.1321088820695877\n","step: 1050, loss: 0.08529046177864075\n","step: 1060, loss: 0.14661960303783417\n","step: 1070, loss: 0.038336627185344696\n","step: 1080, loss: 0.0515507347881794\n","step: 1090, loss: 0.09862270951271057\n","step: 1100, loss: 0.10872110724449158\n","step: 1110, loss: 0.1523803323507309\n","step: 1120, loss: 0.11335789412260056\n","step: 1130, loss: 0.1327441930770874\n","step: 1140, loss: 0.10246248543262482\n","step: 1150, loss: 0.11441224068403244\n","step: 1160, loss: 0.08473092317581177\n","step: 1170, loss: 0.19090361893177032\n","step: 1180, loss: 0.050379205495119095\n","step: 1190, loss: 0.15445692837238312\n","step: 1200, loss: 0.030175963416695595\n","step: 1210, loss: 0.09030542522668839\n","step: 1220, loss: 0.15168635547161102\n","step: 1230, loss: 0.0805206373333931\n","step: 1240, loss: 0.10081693530082703\n","step: 1250, loss: 0.07848501205444336\n","step: 1260, loss: 0.03907538577914238\n","step: 1270, loss: 0.1588410884141922\n","step: 1280, loss: 0.13765785098075867\n","step: 1290, loss: 0.10483081638813019\n","step: 1300, loss: 0.05802459269762039\n","step: 1310, loss: 0.033488839864730835\n","step: 1320, loss: 0.027916988357901573\n","step: 1330, loss: 0.13224412500858307\n","step: 1340, loss: 0.10256250202655792\n","step: 1350, loss: 0.09160187095403671\n","step: 1360, loss: 0.14251761138439178\n","step: 1370, loss: 0.09202999621629715\n","step: 1380, loss: 0.03569173067808151\n","step: 1390, loss: 0.02551122196018696\n","step: 1400, loss: 0.11194299906492233\n","step: 1410, loss: 0.024756938219070435\n","step: 1420, loss: 0.06175950914621353\n","step: 1430, loss: 0.07021711766719818\n","step: 1440, loss: 0.11612847447395325\n","step: 1450, loss: 0.042714256793260574\n","step: 1460, loss: 0.23401476442813873\n","step: 1470, loss: 0.030632449313998222\n","step: 1480, loss: 0.1519278883934021\n","step: 1490, loss: 0.10864976793527603\n","step: 1500, loss: 0.04391203448176384\n","step: 1510, loss: 0.0683329775929451\n","step: 1520, loss: 0.07327509671449661\n","step: 1530, loss: 0.10918378084897995\n","step: 1540, loss: 0.1338939219713211\n","step: 1550, loss: 0.17748408019542694\n","step: 1560, loss: 0.07767348736524582\n","step: 1570, loss: 0.04195094853639603\n","step: 1580, loss: 0.05709570646286011\n","step: 1590, loss: 0.052068885415792465\n","step: 1600, loss: 0.10357377678155899\n","step: 1610, loss: 0.12798932194709778\n","step: 1620, loss: 0.07253958284854889\n","step: 1630, loss: 0.12982924282550812\n","step: 1640, loss: 0.04302595928311348\n","step: 1650, loss: 0.16174106299877167\n","step: 1660, loss: 0.09261401742696762\n","step: 1670, loss: 0.11664658039808273\n","step: 1680, loss: 0.048840176314115524\n","step: 1690, loss: 0.06992492079734802\n","step: 1700, loss: 0.09516718983650208\n","step: 1710, loss: 0.0670228824019432\n","step: 1720, loss: 0.19440984725952148\n","step: 1730, loss: 0.07396479696035385\n","step: 1740, loss: 0.06051846593618393\n","step: 1750, loss: 0.08378307521343231\n","step: 1760, loss: 0.04904062673449516\n","step: 1770, loss: 0.049297578632831573\n","step: 1780, loss: 0.03356720134615898\n","step: 1790, loss: 0.07659641653299332\n","step: 1800, loss: 0.1460326761007309\n","step: 1810, loss: 0.13567109405994415\n","step: 1820, loss: 0.09052164107561111\n","step: 1830, loss: 0.03948875144124031\n","step: 1840, loss: 0.049796514213085175\n","step: 1850, loss: 0.12337027490139008\n","step: 1860, loss: 0.07642993330955505\n","step: 1870, loss: 0.061405666172504425\n","step: 1880, loss: 0.14170801639556885\n","step: 1890, loss: 0.17413102090358734\n","step: 1900, loss: 0.033720433712005615\n","step: 1910, loss: 0.10002198815345764\n","step: 1920, loss: 0.0981614887714386\n","step: 1930, loss: 0.09629815071821213\n","step: 1940, loss: 0.224636048078537\n","step: 1950, loss: 0.040715016424655914\n","step: 1960, loss: 0.09853346645832062\n","step: 1970, loss: 0.14150674641132355\n","step: 1980, loss: 0.07036726921796799\n","step: 1990, loss: 0.10803502053022385\n","step: 2000, loss: 0.06370832771062851\n","step: 2010, loss: 0.11164149641990662\n","step: 2020, loss: 0.08181686699390411\n","step: 2030, loss: 0.11973375082015991\n","step: 2040, loss: 0.13926975429058075\n","step: 2050, loss: 0.07798998057842255\n","step: 2060, loss: 0.11875176429748535\n","step: 2070, loss: 0.09070716798305511\n","step: 2080, loss: 0.04074660688638687\n","step: 2090, loss: 0.12367970496416092\n","step: 2100, loss: 0.06780120730400085\n","step: 2110, loss: 0.09888757020235062\n","step: 2120, loss: 0.1755247563123703\n","step: 2130, loss: 0.06016814708709717\n","step: 2140, loss: 0.19988791644573212\n","step: 2150, loss: 0.0635051280260086\n","step: 2160, loss: 0.03102145530283451\n","step: 2170, loss: 0.07708025723695755\n","step: 2180, loss: 0.10961249470710754\n","step: 2190, loss: 0.026045553386211395\n","step: 2200, loss: 0.045577701181173325\n","step: 2210, loss: 0.13990582525730133\n","step: 2220, loss: 0.10009826719760895\n","step: 2230, loss: 0.09909943491220474\n","step: 2240, loss: 0.043223511427640915\n","step: 2250, loss: 0.036203354597091675\n","step: 2260, loss: 0.09100114554166794\n","step: 2270, loss: 0.11761825531721115\n","step: 2280, loss: 0.13449250161647797\n","step: 2290, loss: 0.10641416162252426\n","step: 2300, loss: 0.1414971500635147\n","step: 2310, loss: 0.0755556970834732\n","step: 2320, loss: 0.0811639279127121\n","step: 2330, loss: 0.1223059818148613\n","step: 2340, loss: 0.1450091451406479\n","step: 2350, loss: 0.038564082235097885\n","step: 2360, loss: 0.07388769835233688\n","step: 2370, loss: 0.105474092066288\n","step: 2380, loss: 0.11703208833932877\n","step: 2390, loss: 0.12403509020805359\n","step: 2400, loss: 0.06825870275497437\n","step: 2410, loss: 0.13979308307170868\n","step: 2420, loss: 0.10874384641647339\n","step: 2430, loss: 0.10546984523534775\n","step: 2440, loss: 0.0314500592648983\n","step: 2450, loss: 0.06664478778839111\n","step: 2460, loss: 0.10834631323814392\n","step: 2470, loss: 0.24393662810325623\n","step: 2480, loss: 0.04838743060827255\n","step: 2490, loss: 0.0782289132475853\n","step: 2500, loss: 0.1036054864525795\n","step: 2510, loss: 0.10027904063463211\n","step: 2520, loss: 0.11706342548131943\n","step: 2530, loss: 0.14917320013046265\n","step: 2540, loss: 0.14705543220043182\n","step: 2550, loss: 0.07223634421825409\n","step: 2560, loss: 0.029880816116929054\n","step: 2570, loss: 0.10260411351919174\n","step: 2580, loss: 0.17031872272491455\n","step: 2590, loss: 0.10609405487775803\n","step: 2600, loss: 0.06417173147201538\n","step: 2610, loss: 0.022390389814972878\n","step: 2620, loss: 0.13628067076206207\n","step: 2630, loss: 0.11427131295204163\n","step: 2640, loss: 0.06807385385036469\n","step: 2650, loss: 0.13609567284584045\n","step: 2660, loss: 0.025196317583322525\n","step: 2670, loss: 0.15657900273799896\n","step: 2680, loss: 0.027592293918132782\n","step: 2690, loss: 0.11669940501451492\n","step: 2700, loss: 0.15631486475467682\n","step: 2710, loss: 0.12981641292572021\n","step: 2720, loss: 0.05267376825213432\n","step: 2730, loss: 0.05993227660655975\n","step: 2740, loss: 0.17805099487304688\n","step: 2750, loss: 0.14114855229854584\n","step: 2760, loss: 0.11716485768556595\n","step: 2770, loss: 0.10459451377391815\n","step: 2780, loss: 0.11199650913476944\n","step: 2790, loss: 0.03764484077692032\n","step: 2800, loss: 0.04848858714103699\n","step: 2810, loss: 0.13314303755760193\n","step: 2820, loss: 0.1396547257900238\n","step: 2830, loss: 0.06755156069993973\n","step: 2840, loss: 0.05665074661374092\n","step: 2850, loss: 0.08389806747436523\n","step: 2860, loss: 0.14396125078201294\n","step: 2870, loss: 0.14267058670520782\n","step: 2880, loss: 0.0720430463552475\n","step: 2890, loss: 0.08344133198261261\n","step: 2900, loss: 0.1837388575077057\n","step: 2910, loss: 0.042429085820913315\n","step: 2920, loss: 0.10579746961593628\n","step: 2930, loss: 0.15208405256271362\n","step: 2940, loss: 0.09018001705408096\n","step: 2950, loss: 0.052474454045295715\n","step: 2960, loss: 0.031928226351737976\n","step: 2970, loss: 0.0875532254576683\n","step: 2980, loss: 0.07709818333387375\n","step: 2990, loss: 0.07190271466970444\n","step: 3000, loss: 0.09256058186292648\n","step: 3010, loss: 0.0767902135848999\n","step: 3020, loss: 0.11473217606544495\n","step: 3030, loss: 0.10798190534114838\n","step: 3040, loss: 0.0821707621216774\n","step: 3050, loss: 0.08453541249036789\n","step: 3060, loss: 0.11032108217477798\n","step: 3070, loss: 0.13138914108276367\n","step: 3080, loss: 0.0759674534201622\n","step: 3090, loss: 0.09983604401350021\n","step: 3100, loss: 0.05369630455970764\n","step: 3110, loss: 0.07564745098352432\n","step: 3120, loss: 0.1362304836511612\n","step: 3130, loss: 0.09244327247142792\n","step: 3140, loss: 0.08057312667369843\n","step: 3150, loss: 0.11314786970615387\n","step: 3160, loss: 0.10121802985668182\n","step: 3170, loss: 0.05227390304207802\n","step: 3180, loss: 0.15297931432724\n","step: 3190, loss: 0.08702988177537918\n","step: 3200, loss: 0.0389346107840538\n","step: 3210, loss: 0.07440586388111115\n","step: 3220, loss: 0.09594016522169113\n","step: 3230, loss: 0.0795721635222435\n","step: 3240, loss: 0.08734339475631714\n","step: 3250, loss: 0.08064951747655869\n","step: 3260, loss: 0.08480315655469894\n","step: 3270, loss: 0.08444418758153915\n","step: 3280, loss: 0.07498074322938919\n","step: 3290, loss: 0.034364502876996994\n","step: 3300, loss: 0.09640426933765411\n","step: 3310, loss: 0.054668113589286804\n","step: 3320, loss: 0.04657554253935814\n","step: 3330, loss: 0.08847930282354355\n","step: 3340, loss: 0.05840354412794113\n","step: 3350, loss: 0.1112753376364708\n","step: 3360, loss: 0.14545801281929016\n","step: 3370, loss: 0.09987734258174896\n","step: 3380, loss: 0.11084795743227005\n","step: 3390, loss: 0.08126447349786758\n","step: 3400, loss: 0.06251242011785507\n","step: 3410, loss: 0.04185541719198227\n","step: 3420, loss: 0.20964331924915314\n","step: 3430, loss: 0.06336039304733276\n","step: 3440, loss: 0.08211144804954529\n","step: 3450, loss: 0.13824224472045898\n","step: 3460, loss: 0.08268968015909195\n","step: 3470, loss: 0.03960219770669937\n","step: 3480, loss: 0.08598712086677551\n","step: 3490, loss: 0.1606614738702774\n","step: 3500, loss: 0.09709413349628448\n","step: 3510, loss: 0.08381713926792145\n","step: 3520, loss: 0.06232353672385216\n","step: 3530, loss: 0.06955952197313309\n","step: 3540, loss: 0.1261756420135498\n","step: 3550, loss: 0.16953039169311523\n","step: 3560, loss: 0.10951875150203705\n","step: 3570, loss: 0.03841180354356766\n","step: 3580, loss: 0.0802527368068695\n","step: 3590, loss: 0.05811484530568123\n","step: 3600, loss: 0.12215901166200638\n","step: 3610, loss: 0.09053275734186172\n","step: 3620, loss: 0.03868219256401062\n","step: 3630, loss: 0.06386636942625046\n","step: 3640, loss: 0.12108790874481201\n","step: 3650, loss: 0.13036125898361206\n","step: 3660, loss: 0.025316761806607246\n","step: 3670, loss: 0.06448572874069214\n","step: 3680, loss: 0.10900188237428665\n","step: 3690, loss: 0.197713240981102\n","step: 3700, loss: 0.19513025879859924\n","step: 3710, loss: 0.13719473779201508\n","step: 3720, loss: 0.09719197452068329\n","step: 3730, loss: 0.130516916513443\n","step: 3740, loss: 0.19558589160442352\n","step: 3750, loss: 0.09603096544742584\n","step: 3760, loss: 0.07928528636693954\n","step: 3770, loss: 0.10933224111795425\n","\n","Loop 5\n","domain_dev_word_lst 608\n","Train from scratch...\n","step: 0, loss: 3.9436895847320557\n","step: 10, loss: 2.03306245803833\n","step: 20, loss: 0.7570062279701233\n","step: 30, loss: 0.40827667713165283\n","step: 40, loss: 0.46740782260894775\n","step: 50, loss: 0.2711937129497528\n","step: 60, loss: 0.18578073382377625\n","step: 70, loss: 0.13350170850753784\n","step: 80, loss: 0.06807824969291687\n","step: 90, loss: 0.2102748602628708\n","step: 100, loss: 0.09147421270608902\n","step: 110, loss: 0.19373567402362823\n","step: 120, loss: 0.1242346540093422\n","step: 130, loss: 0.32889699935913086\n","step: 140, loss: 0.1936015784740448\n","step: 150, loss: 0.11131584644317627\n","step: 160, loss: 0.11035960912704468\n","step: 170, loss: 0.1841324418783188\n","step: 180, loss: 0.20600570738315582\n","step: 190, loss: 0.14379675686359406\n","step: 200, loss: 0.06974458694458008\n","step: 210, loss: 0.17940089106559753\n","step: 220, loss: 0.1322287917137146\n","step: 230, loss: 0.1336427628993988\n","step: 240, loss: 0.16334062814712524\n","step: 250, loss: 0.12666188180446625\n","step: 260, loss: 0.09271442890167236\n","step: 270, loss: 0.05432025343179703\n","step: 280, loss: 0.09743990749120712\n","step: 290, loss: 0.14090906083583832\n","step: 300, loss: 0.06526637077331543\n","step: 310, loss: 0.15522487461566925\n","step: 320, loss: 0.18042202293872833\n","step: 330, loss: 0.11848443001508713\n","step: 340, loss: 0.2027486264705658\n","step: 350, loss: 0.10082342475652695\n","step: 360, loss: 0.14169098436832428\n","step: 370, loss: 0.055456794798374176\n","step: 380, loss: 0.09068567305803299\n","step: 390, loss: 0.1267649233341217\n","step: 400, loss: 0.09572184830904007\n","step: 410, loss: 0.16006751358509064\n","step: 420, loss: 0.1302645206451416\n","step: 430, loss: 0.1313459426164627\n","step: 440, loss: 0.09940033406019211\n","step: 450, loss: 0.10731685906648636\n","step: 460, loss: 0.1415206342935562\n","step: 470, loss: 0.1232672855257988\n","step: 480, loss: 0.1932714879512787\n","step: 490, loss: 0.11916021257638931\n","step: 500, loss: 0.06235082074999809\n","step: 510, loss: 0.11917002499103546\n","step: 520, loss: 0.07629233598709106\n","step: 530, loss: 0.06845533847808838\n","step: 540, loss: 0.04469515383243561\n","step: 550, loss: 0.099022276699543\n","step: 560, loss: 0.15324543416500092\n","step: 570, loss: 0.07043903321027756\n","step: 580, loss: 0.11833540350198746\n","step: 590, loss: 0.07730288058519363\n","step: 600, loss: 0.10291791707277298\n","step: 610, loss: 0.19086694717407227\n","step: 620, loss: 0.05130016431212425\n","step: 630, loss: 0.05534980073571205\n","step: 640, loss: 0.2203945517539978\n","step: 650, loss: 0.07777324318885803\n","step: 660, loss: 0.09770937263965607\n","step: 670, loss: 0.10172834247350693\n","step: 680, loss: 0.026268012821674347\n","step: 690, loss: 0.06889281421899796\n","step: 700, loss: 0.132838174700737\n","step: 710, loss: 0.13706927001476288\n","step: 720, loss: 0.08755766600370407\n","step: 730, loss: 0.18232889473438263\n","step: 740, loss: 0.1782017946243286\n","step: 750, loss: 0.05278641730546951\n","step: 760, loss: 0.09885790944099426\n","step: 770, loss: 0.12531934678554535\n","step: 780, loss: 0.03989652916789055\n","step: 790, loss: 0.0882963016629219\n","step: 800, loss: 0.09192539006471634\n","step: 810, loss: 0.051783181726932526\n","step: 820, loss: 0.1379878669977188\n","step: 830, loss: 0.06764891743659973\n","step: 840, loss: 0.07067735493183136\n","step: 850, loss: 0.14246852695941925\n","step: 860, loss: 0.20505782961845398\n","step: 870, loss: 0.24832256138324738\n","step: 880, loss: 0.05666271597146988\n","step: 890, loss: 0.10578788816928864\n","step: 900, loss: 0.09174694865942001\n","step: 910, loss: 0.14696770906448364\n","step: 920, loss: 0.039631132036447525\n","step: 930, loss: 0.09156954288482666\n","step: 940, loss: 0.05688343197107315\n","step: 950, loss: 0.0799112468957901\n","step: 960, loss: 0.09982995688915253\n","step: 970, loss: 0.06979356706142426\n","step: 980, loss: 0.1657777577638626\n","step: 990, loss: 0.08743495494127274\n","step: 1000, loss: 0.07841917872428894\n","step: 1010, loss: 0.2629714012145996\n","step: 1020, loss: 0.12587125599384308\n","step: 1030, loss: 0.08592429757118225\n","step: 1040, loss: 0.18302610516548157\n","step: 1050, loss: 0.11481090635061264\n","step: 1060, loss: 0.08722483366727829\n","step: 1070, loss: 0.178975909948349\n","step: 1080, loss: 0.09818233549594879\n","step: 1090, loss: 0.08694057166576385\n","step: 1100, loss: 0.024691715836524963\n","step: 1110, loss: 0.22803933918476105\n","step: 1120, loss: 0.14547114074230194\n","step: 1130, loss: 0.04323587566614151\n","step: 1140, loss: 0.09140929579734802\n","step: 1150, loss: 0.10583344101905823\n","step: 1160, loss: 0.04684498533606529\n","step: 1170, loss: 0.05780574306845665\n","step: 1180, loss: 0.0615229606628418\n","step: 1190, loss: 0.10361164808273315\n","step: 1200, loss: 0.08526263386011124\n","step: 1210, loss: 0.04890502244234085\n","step: 1220, loss: 0.11978346109390259\n","step: 1230, loss: 0.1378747820854187\n","step: 1240, loss: 0.12071552872657776\n","step: 1250, loss: 0.02042253315448761\n","step: 1260, loss: 0.07376813143491745\n","step: 1270, loss: 0.24366223812103271\n","step: 1280, loss: 0.0419386588037014\n","step: 1290, loss: 0.12797798216342926\n","step: 1300, loss: 0.14855821430683136\n","step: 1310, loss: 0.12580639123916626\n","step: 1320, loss: 0.11772620677947998\n","step: 1330, loss: 0.06535661965608597\n","step: 1340, loss: 0.12129441648721695\n","step: 1350, loss: 0.08719520270824432\n","step: 1360, loss: 0.148107647895813\n","step: 1370, loss: 0.12489333748817444\n","step: 1380, loss: 0.11237683147192001\n","step: 1390, loss: 0.03371129930019379\n","step: 1400, loss: 0.03693678230047226\n","step: 1410, loss: 0.13048797845840454\n","step: 1420, loss: 0.029341647401452065\n","step: 1430, loss: 0.05134467035531998\n","step: 1440, loss: 0.1434728354215622\n","step: 1450, loss: 0.07091427594423294\n","step: 1460, loss: 0.06516890972852707\n","step: 1470, loss: 0.0657396912574768\n","step: 1480, loss: 0.11246712505817413\n","step: 1490, loss: 0.08857373148202896\n","step: 1500, loss: 0.12860992550849915\n","step: 1510, loss: 0.04078486189246178\n","step: 1520, loss: 0.07333012670278549\n","step: 1530, loss: 0.11280907690525055\n","step: 1540, loss: 0.09081175178289413\n","step: 1550, loss: 0.10656361281871796\n","step: 1560, loss: 0.14329053461551666\n","step: 1570, loss: 0.11078403890132904\n","step: 1580, loss: 0.050463706254959106\n","step: 1590, loss: 0.1553550809621811\n","step: 1600, loss: 0.05590282008051872\n","step: 1610, loss: 0.04105677083134651\n","step: 1620, loss: 0.057068441063165665\n","step: 1630, loss: 0.06780459731817245\n","step: 1640, loss: 0.13118259608745575\n","step: 1650, loss: 0.08556770533323288\n","step: 1660, loss: 0.0680718719959259\n","step: 1670, loss: 0.028379404917359352\n","step: 1680, loss: 0.06517179310321808\n","step: 1690, loss: 0.06790080666542053\n","step: 1700, loss: 0.06619682163000107\n","step: 1710, loss: 0.038839250802993774\n","step: 1720, loss: 0.09169530123472214\n","step: 1730, loss: 0.05378103628754616\n","step: 1740, loss: 0.13441923260688782\n","step: 1750, loss: 0.12405195087194443\n","step: 1760, loss: 0.10443102568387985\n","step: 1770, loss: 0.10285111516714096\n","step: 1780, loss: 0.1717607080936432\n","step: 1790, loss: 0.08031231164932251\n","step: 1800, loss: 0.044052932411432266\n","step: 1810, loss: 0.10381784290075302\n","step: 1820, loss: 0.08653879910707474\n","step: 1830, loss: 0.11668910086154938\n","step: 1840, loss: 0.14375920593738556\n","step: 1850, loss: 0.08023110032081604\n","step: 1860, loss: 0.03994017094373703\n","step: 1870, loss: 0.11642654985189438\n","step: 1880, loss: 0.043751418590545654\n","step: 1890, loss: 0.11105043441057205\n","step: 1900, loss: 0.042684704065322876\n","step: 1910, loss: 0.12891259789466858\n","step: 1920, loss: 0.05300965532660484\n","step: 1930, loss: 0.053731974214315414\n","step: 1940, loss: 0.09033606201410294\n","step: 1950, loss: 0.10750477761030197\n","step: 1960, loss: 0.10749614238739014\n","step: 1970, loss: 0.05782352015376091\n","step: 1980, loss: 0.10853744298219681\n","step: 1990, loss: 0.10355681926012039\n","step: 2000, loss: 0.036076437681913376\n","step: 2010, loss: 0.0648355707526207\n","step: 2020, loss: 0.06679573655128479\n","step: 2030, loss: 0.04663599655032158\n","step: 2040, loss: 0.10704092681407928\n","step: 2050, loss: 0.07448731362819672\n","step: 2060, loss: 0.1063501387834549\n","step: 2070, loss: 0.0392780527472496\n","step: 2080, loss: 0.0786464512348175\n","step: 2090, loss: 0.10512663424015045\n","step: 2100, loss: 0.04735758155584335\n","step: 2110, loss: 0.027084650471806526\n","step: 2120, loss: 0.09518226981163025\n","step: 2130, loss: 0.07220769673585892\n","step: 2140, loss: 0.09234543889760971\n","step: 2150, loss: 0.07667667418718338\n","step: 2160, loss: 0.045116543769836426\n","step: 2170, loss: 0.0635233148932457\n","step: 2180, loss: 0.1582326591014862\n","step: 2190, loss: 0.043768517673015594\n","step: 2200, loss: 0.044693723320961\n","step: 2210, loss: 0.17442116141319275\n","step: 2220, loss: 0.08213502168655396\n","step: 2230, loss: 0.06624434888362885\n","step: 2240, loss: 0.14158472418785095\n","step: 2250, loss: 0.05695895478129387\n","step: 2260, loss: 0.032002225518226624\n","step: 2270, loss: 0.11848673969507217\n","step: 2280, loss: 0.10447615385055542\n","step: 2290, loss: 0.11581163853406906\n","step: 2300, loss: 0.1433127224445343\n","step: 2310, loss: 0.0983833372592926\n","step: 2320, loss: 0.08929573744535446\n","step: 2330, loss: 0.07744238525629044\n","step: 2340, loss: 0.10009539872407913\n","step: 2350, loss: 0.08107040077447891\n","step: 2360, loss: 0.13277235627174377\n","step: 2370, loss: 0.24894548952579498\n","step: 2380, loss: 0.06054604426026344\n","step: 2390, loss: 0.09853256493806839\n","step: 2400, loss: 0.06567852199077606\n","step: 2410, loss: 0.1622287482023239\n","step: 2420, loss: 0.10197404772043228\n","step: 2430, loss: 0.08221519738435745\n","step: 2440, loss: 0.06745199114084244\n","step: 2450, loss: 0.07347141206264496\n","step: 2460, loss: 0.1707184761762619\n","step: 2470, loss: 0.09083899855613708\n","step: 2480, loss: 0.050316475331783295\n","step: 2490, loss: 0.05245165899395943\n","step: 2500, loss: 0.1117914542555809\n","step: 2510, loss: 0.11792926490306854\n","step: 2520, loss: 0.07431045919656754\n","step: 2530, loss: 0.06291012465953827\n","step: 2540, loss: 0.11383671313524246\n","step: 2550, loss: 0.024339577183127403\n","step: 2560, loss: 0.09662607312202454\n","step: 2570, loss: 0.10261932760477066\n","step: 2580, loss: 0.20501913130283356\n","step: 2590, loss: 0.07201205939054489\n","step: 2600, loss: 0.0510745644569397\n","step: 2610, loss: 0.08691175282001495\n","step: 2620, loss: 0.10182332992553711\n","step: 2630, loss: 0.03603037819266319\n","step: 2640, loss: 0.06471988558769226\n","step: 2650, loss: 0.05882367491722107\n","step: 2660, loss: 0.0699109360575676\n","step: 2670, loss: 0.06585481762886047\n","step: 2680, loss: 0.05918605253100395\n","step: 2690, loss: 0.05453367531299591\n","step: 2700, loss: 0.034683678299188614\n","step: 2710, loss: 0.023968270048499107\n","step: 2720, loss: 0.020485199987888336\n","step: 2730, loss: 0.10217887163162231\n","step: 2740, loss: 0.026454271748661995\n","step: 2750, loss: 0.1712905764579773\n","step: 2760, loss: 0.09062313288450241\n","step: 2770, loss: 0.09259288758039474\n","step: 2780, loss: 0.10514548420906067\n","step: 2790, loss: 0.07578840851783752\n","step: 2800, loss: 0.04843675717711449\n","step: 2810, loss: 0.15445341169834137\n","step: 2820, loss: 0.11874409765005112\n","step: 2830, loss: 0.05546266585588455\n","step: 2840, loss: 0.14409813284873962\n","step: 2850, loss: 0.05123718082904816\n","step: 2860, loss: 0.09134615212678909\n","step: 2870, loss: 0.03603881224989891\n","step: 2880, loss: 0.11129041761159897\n","step: 2890, loss: 0.08115358650684357\n","step: 2900, loss: 0.09723787009716034\n","step: 2910, loss: 0.0815901830792427\n","step: 2920, loss: 0.05309231951832771\n","step: 2930, loss: 0.09456228464841843\n","step: 2940, loss: 0.09246061742305756\n","step: 2950, loss: 0.05122094973921776\n","step: 2960, loss: 0.12331519275903702\n","step: 2970, loss: 0.11928601562976837\n","step: 2980, loss: 0.15579386055469513\n","step: 2990, loss: 0.15221759676933289\n","step: 3000, loss: 0.17126300930976868\n","step: 3010, loss: 0.09530837833881378\n","step: 3020, loss: 0.0682147890329361\n","step: 3030, loss: 0.12763211131095886\n","step: 3040, loss: 0.08214237540960312\n","step: 3050, loss: 0.1375984102487564\n","step: 3060, loss: 0.068015918135643\n","step: 3070, loss: 0.04782760515809059\n","step: 3080, loss: 0.11867370456457138\n","step: 3090, loss: 0.05150647833943367\n","step: 3100, loss: 0.0740310475230217\n","step: 3110, loss: 0.06938481330871582\n","step: 3120, loss: 0.08867748826742172\n","step: 3130, loss: 0.1481788456439972\n","step: 3140, loss: 0.08080118894577026\n","step: 3150, loss: 0.06376755982637405\n","step: 3160, loss: 0.07031415402889252\n","step: 3170, loss: 0.0753246620297432\n","step: 3180, loss: 0.09186982363462448\n","step: 3190, loss: 0.10244271904230118\n","step: 3200, loss: 0.17709016799926758\n","step: 3210, loss: 0.044921986758708954\n","step: 3220, loss: 0.1302284598350525\n","step: 3230, loss: 0.02074768766760826\n","step: 3240, loss: 0.07883958518505096\n","step: 3250, loss: 0.06465444713830948\n","step: 3260, loss: 0.11448117345571518\n","step: 3270, loss: 0.14006271958351135\n","step: 3280, loss: 0.1123780608177185\n","step: 3290, loss: 0.037949852645397186\n","step: 3300, loss: 0.07709453999996185\n","step: 3310, loss: 0.09170132130384445\n","step: 3320, loss: 0.05638763681054115\n","step: 3330, loss: 0.12893037497997284\n","step: 3340, loss: 0.0505363792181015\n","step: 3350, loss: 0.1051899716258049\n","step: 3360, loss: 0.07588448375463486\n","step: 3370, loss: 0.08031467348337173\n","step: 3380, loss: 0.10342536121606827\n","step: 3390, loss: 0.1355213224887848\n","step: 3400, loss: 0.1514771580696106\n","step: 3410, loss: 0.1540859490633011\n","step: 3420, loss: 0.09541705250740051\n","step: 3430, loss: 0.0608452633023262\n","step: 3440, loss: 0.17958977818489075\n","step: 3450, loss: 0.07628513127565384\n","step: 3460, loss: 0.16562941670417786\n","step: 3470, loss: 0.03422732651233673\n","step: 3480, loss: 0.05281967669725418\n","step: 3490, loss: 0.10684165358543396\n","step: 3500, loss: 0.06597307324409485\n","step: 3510, loss: 0.03036670945584774\n","step: 3520, loss: 0.11503631621599197\n","step: 3530, loss: 0.04595991224050522\n","step: 3540, loss: 0.08078454434871674\n","step: 3550, loss: 0.11714975535869598\n","step: 3560, loss: 0.09016876667737961\n","step: 3570, loss: 0.10192551463842392\n","step: 3580, loss: 0.0947592481970787\n","step: 3590, loss: 0.055089522153139114\n","step: 3600, loss: 0.09095580130815506\n","step: 3610, loss: 0.10525722056627274\n","step: 3620, loss: 0.16224512457847595\n","step: 3630, loss: 0.10892365127801895\n","step: 3640, loss: 0.14940373599529266\n","step: 3650, loss: 0.022380029782652855\n","step: 3660, loss: 0.02674219198524952\n","step: 3670, loss: 0.048572689294815063\n","step: 3680, loss: 0.08944613486528397\n","step: 3690, loss: 0.07045087963342667\n","step: 3700, loss: 0.0700950026512146\n","step: 3710, loss: 0.09950648248195648\n","step: 3720, loss: 0.14364488422870636\n","step: 3730, loss: 0.07857950031757355\n","step: 3740, loss: 0.11465761065483093\n","step: 3750, loss: 0.04951302334666252\n","step: 3760, loss: 0.11431471258401871\n","step: 3770, loss: 0.05895466357469559\n","\n","Loop 6\n","domain_dev_word_lst 506\n","Train from scratch...\n","step: 0, loss: 3.9511969089508057\n","step: 10, loss: 1.6375643014907837\n","step: 20, loss: 0.7492409944534302\n","step: 30, loss: 0.34949299693107605\n","step: 40, loss: 0.2867520749568939\n","step: 50, loss: 0.4366309642791748\n","step: 60, loss: 0.218751922249794\n","step: 70, loss: 0.17846564948558807\n","step: 80, loss: 0.19361194968223572\n","step: 90, loss: 0.11489406228065491\n","step: 100, loss: 0.12083709239959717\n","step: 110, loss: 0.2689766585826874\n","step: 120, loss: 0.20721961557865143\n","step: 130, loss: 0.11665137112140656\n","step: 140, loss: 0.1280447542667389\n","step: 150, loss: 0.13062222301959991\n","step: 160, loss: 0.10784676671028137\n","step: 170, loss: 0.20528697967529297\n","step: 180, loss: 0.13248410820960999\n","step: 190, loss: 0.22100605070590973\n","step: 200, loss: 0.20805735886096954\n","step: 210, loss: 0.17308984696865082\n","step: 220, loss: 0.11232619732618332\n","step: 230, loss: 0.13828817009925842\n","step: 240, loss: 0.10654040426015854\n","step: 250, loss: 0.17766684293746948\n","step: 260, loss: 0.051662273705005646\n","step: 270, loss: 0.1559125781059265\n","step: 280, loss: 0.11760859191417694\n","step: 290, loss: 0.18310125172138214\n","step: 300, loss: 0.22756560146808624\n","step: 310, loss: 0.10281075537204742\n","step: 320, loss: 0.293867826461792\n","step: 330, loss: 0.14447982609272003\n","step: 340, loss: 0.05423584580421448\n","step: 350, loss: 0.17717942595481873\n","step: 360, loss: 0.14952073991298676\n","step: 370, loss: 0.16121713817119598\n","step: 380, loss: 0.18000231683254242\n","step: 390, loss: 0.1485251933336258\n","step: 400, loss: 0.03522739186882973\n","step: 410, loss: 0.12288899719715118\n","step: 420, loss: 0.23870840668678284\n","step: 430, loss: 0.061516162008047104\n","step: 440, loss: 0.09440391510725021\n","step: 450, loss: 0.1254415065050125\n","step: 460, loss: 0.10351483523845673\n","step: 470, loss: 0.14878201484680176\n","step: 480, loss: 0.056210119277238846\n","step: 490, loss: 0.04155971109867096\n","step: 500, loss: 0.09363510459661484\n","step: 510, loss: 0.10934195667505264\n","step: 520, loss: 0.19039386510849\n","step: 530, loss: 0.10330946743488312\n","step: 540, loss: 0.10516668111085892\n","step: 550, loss: 0.0778309628367424\n","step: 560, loss: 0.08386942744255066\n","step: 570, loss: 0.11872368305921555\n","step: 580, loss: 0.05229981243610382\n","step: 590, loss: 0.14598876237869263\n","step: 600, loss: 0.19656379520893097\n","step: 610, loss: 0.12141809612512589\n","step: 620, loss: 0.07592693716287613\n","step: 630, loss: 0.04146650433540344\n","step: 640, loss: 0.11200855672359467\n","step: 650, loss: 0.09868427366018295\n","step: 660, loss: 0.17728549242019653\n","step: 670, loss: 0.060617443174123764\n","step: 680, loss: 0.21571674942970276\n","step: 690, loss: 0.1316399723291397\n","step: 700, loss: 0.1337699145078659\n","step: 710, loss: 0.17872661352157593\n","step: 720, loss: 0.06564104557037354\n","step: 730, loss: 0.09834293276071548\n","step: 740, loss: 0.036752067506313324\n","step: 750, loss: 0.03658221662044525\n","step: 760, loss: 0.16211771965026855\n","step: 770, loss: 0.05550089478492737\n","step: 780, loss: 0.12055695056915283\n","step: 790, loss: 0.11888327449560165\n","step: 800, loss: 0.048699043691158295\n","step: 810, loss: 0.15692304074764252\n","step: 820, loss: 0.09818742424249649\n","step: 830, loss: 0.06863029301166534\n","step: 840, loss: 0.12597492337226868\n","step: 850, loss: 0.1286730319261551\n","step: 860, loss: 0.1897495985031128\n","step: 870, loss: 0.03239454701542854\n","step: 880, loss: 0.10552605986595154\n","step: 890, loss: 0.12172078341245651\n","step: 900, loss: 0.09802815318107605\n","step: 910, loss: 0.2910500764846802\n","step: 920, loss: 0.10054938495159149\n","step: 930, loss: 0.08066299557685852\n","step: 940, loss: 0.08038351684808731\n","step: 950, loss: 0.03231382369995117\n","step: 960, loss: 0.0641038715839386\n","step: 970, loss: 0.08497078716754913\n","step: 980, loss: 0.07112333923578262\n","step: 990, loss: 0.07781058549880981\n","step: 1000, loss: 0.14265184104442596\n","step: 1010, loss: 0.20156358182430267\n","step: 1020, loss: 0.14277558028697968\n","step: 1030, loss: 0.04149160906672478\n","step: 1040, loss: 0.20844000577926636\n","step: 1050, loss: 0.17602217197418213\n","step: 1060, loss: 0.11608504503965378\n","step: 1070, loss: 0.12029639631509781\n","step: 1080, loss: 0.13022087514400482\n","step: 1090, loss: 0.08152138441801071\n","step: 1100, loss: 0.0830041915178299\n","step: 1110, loss: 0.029224703088402748\n","step: 1120, loss: 0.033218707889318466\n","step: 1130, loss: 0.21462295949459076\n","step: 1140, loss: 0.13217003643512726\n","step: 1150, loss: 0.07293194532394409\n","step: 1160, loss: 0.11155860126018524\n","step: 1170, loss: 0.0982687771320343\n","step: 1180, loss: 0.03972925618290901\n","step: 1190, loss: 0.08651460707187653\n","step: 1200, loss: 0.08736395090818405\n","step: 1210, loss: 0.04983386769890785\n","step: 1220, loss: 0.13250450789928436\n","step: 1230, loss: 0.09644144028425217\n","step: 1240, loss: 0.16192984580993652\n","step: 1250, loss: 0.11595433205366135\n","step: 1260, loss: 0.1387307494878769\n","step: 1270, loss: 0.12347408384084702\n","step: 1280, loss: 0.06704828143119812\n","step: 1290, loss: 0.12286900728940964\n","step: 1300, loss: 0.1727355718612671\n","step: 1310, loss: 0.14904622733592987\n","step: 1320, loss: 0.12754769623279572\n","step: 1330, loss: 0.07736935466527939\n","step: 1340, loss: 0.043723370879888535\n","step: 1350, loss: 0.16278423368930817\n","step: 1360, loss: 0.07360001653432846\n","step: 1370, loss: 0.08631059527397156\n","step: 1380, loss: 0.16882577538490295\n","step: 1390, loss: 0.0807831808924675\n","step: 1400, loss: 0.048462577164173126\n","step: 1410, loss: 0.05184018239378929\n","step: 1420, loss: 0.06611575186252594\n","step: 1430, loss: 0.09000750631093979\n","step: 1440, loss: 0.08039198070764542\n","step: 1450, loss: 0.09805028140544891\n","step: 1460, loss: 0.08538734912872314\n","step: 1470, loss: 0.24221229553222656\n","step: 1480, loss: 0.0842139720916748\n","step: 1490, loss: 0.08482760936021805\n","step: 1500, loss: 0.05352040007710457\n","step: 1510, loss: 0.08435967564582825\n","step: 1520, loss: 0.1106720119714737\n","step: 1530, loss: 0.08594309538602829\n","step: 1540, loss: 0.07335018366575241\n","step: 1550, loss: 0.11965130269527435\n","step: 1560, loss: 0.062376707792282104\n","step: 1570, loss: 0.1087387278676033\n","step: 1580, loss: 0.12482959032058716\n","step: 1590, loss: 0.15224210917949677\n","step: 1600, loss: 0.1516616940498352\n","step: 1610, loss: 0.08875274658203125\n","step: 1620, loss: 0.07784353941679001\n","step: 1630, loss: 0.08386801928281784\n","step: 1640, loss: 0.057731181383132935\n","step: 1650, loss: 0.06293819844722748\n","step: 1660, loss: 0.10762973874807358\n","step: 1670, loss: 0.031237557530403137\n","step: 1680, loss: 0.0703011155128479\n","step: 1690, loss: 0.05370502173900604\n","step: 1700, loss: 0.09378896653652191\n","step: 1710, loss: 0.09155946224927902\n","step: 1720, loss: 0.10121218115091324\n","step: 1730, loss: 0.12312633544206619\n","step: 1740, loss: 0.0764891505241394\n","step: 1750, loss: 0.2359362691640854\n","step: 1760, loss: 0.08246065676212311\n","step: 1770, loss: 0.05935613065958023\n","step: 1780, loss: 0.07457034289836884\n","step: 1790, loss: 0.12825408577919006\n","step: 1800, loss: 0.04471619427204132\n","step: 1810, loss: 0.10026088356971741\n","step: 1820, loss: 0.17056311666965485\n","step: 1830, loss: 0.09043334424495697\n","step: 1840, loss: 0.0947289988398552\n","step: 1850, loss: 0.10981697589159012\n","step: 1860, loss: 0.06553445011377335\n","step: 1870, loss: 0.062112126499414444\n","step: 1880, loss: 0.07917674630880356\n","step: 1890, loss: 0.1303805708885193\n","step: 1900, loss: 0.07531779259443283\n","step: 1910, loss: 0.10746658593416214\n","step: 1920, loss: 0.1560196578502655\n","step: 1930, loss: 0.049578726291656494\n","step: 1940, loss: 0.019799156114459038\n","step: 1950, loss: 0.0892079547047615\n","step: 1960, loss: 0.2676379680633545\n","step: 1970, loss: 0.04496223106980324\n","step: 1980, loss: 0.07932090759277344\n","step: 1990, loss: 0.10279738157987595\n","step: 2000, loss: 0.05795656144618988\n","step: 2010, loss: 0.07800702750682831\n","step: 2020, loss: 0.12433241307735443\n","step: 2030, loss: 0.07248297333717346\n","step: 2040, loss: 0.13246628642082214\n","step: 2050, loss: 0.07298543304204941\n","step: 2060, loss: 0.02157297171652317\n","step: 2070, loss: 0.05944453552365303\n","step: 2080, loss: 0.1220969706773758\n","step: 2090, loss: 0.11933857947587967\n","step: 2100, loss: 0.15647931396961212\n","step: 2110, loss: 0.09926824271678925\n","step: 2120, loss: 0.09504053741693497\n","step: 2130, loss: 0.0636102482676506\n","step: 2140, loss: 0.03427338972687721\n","step: 2150, loss: 0.07832607626914978\n","step: 2160, loss: 0.07653604447841644\n","step: 2170, loss: 0.036620646715164185\n","step: 2180, loss: 0.09814731031656265\n","step: 2190, loss: 0.09098965674638748\n","step: 2200, loss: 0.0843501091003418\n","step: 2210, loss: 0.044158097356557846\n","step: 2220, loss: 0.04530344903469086\n","step: 2230, loss: 0.07899991422891617\n","step: 2240, loss: 0.06154381111264229\n","step: 2250, loss: 0.08618257194757462\n","step: 2260, loss: 0.04516816884279251\n","step: 2270, loss: 0.07648848742246628\n","step: 2280, loss: 0.057641882449388504\n","step: 2290, loss: 0.032581787556409836\n","step: 2300, loss: 0.07907155901193619\n","step: 2310, loss: 0.0813470184803009\n","step: 2320, loss: 0.08819327503442764\n","step: 2330, loss: 0.10464532673358917\n","step: 2340, loss: 0.07018723338842392\n","step: 2350, loss: 0.03635648265480995\n","step: 2360, loss: 0.09628432989120483\n","step: 2370, loss: 0.08577270060777664\n","step: 2380, loss: 0.08151071518659592\n","step: 2390, loss: 0.15683642029762268\n","step: 2400, loss: 0.16066166758537292\n","step: 2410, loss: 0.06754002720117569\n","step: 2420, loss: 0.05091336742043495\n","step: 2430, loss: 0.09336885809898376\n","step: 2440, loss: 0.17925922572612762\n","step: 2450, loss: 0.046533212065696716\n","step: 2460, loss: 0.024552389979362488\n","step: 2470, loss: 0.07283346354961395\n","step: 2480, loss: 0.04879136011004448\n","step: 2490, loss: 0.06857708841562271\n","step: 2500, loss: 0.18479697406291962\n","step: 2510, loss: 0.19299285113811493\n","step: 2520, loss: 0.07821401208639145\n","step: 2530, loss: 0.0747823417186737\n","step: 2540, loss: 0.05568980425596237\n","step: 2550, loss: 0.1063430979847908\n","step: 2560, loss: 0.06639611721038818\n","step: 2570, loss: 0.11948317289352417\n","step: 2580, loss: 0.0716591477394104\n","step: 2590, loss: 0.1318453848361969\n","step: 2600, loss: 0.2574981153011322\n","step: 2610, loss: 0.09102523326873779\n","step: 2620, loss: 0.17477504909038544\n","step: 2630, loss: 0.1303921937942505\n","step: 2640, loss: 0.01578776352107525\n","step: 2650, loss: 0.11389027535915375\n","step: 2660, loss: 0.05219566449522972\n","step: 2670, loss: 0.035286568105220795\n","step: 2680, loss: 0.13239485025405884\n","step: 2690, loss: 0.09142269939184189\n","step: 2700, loss: 0.06140603870153427\n","step: 2710, loss: 0.08242694288492203\n","step: 2720, loss: 0.15242068469524384\n","step: 2730, loss: 0.15421265363693237\n","step: 2740, loss: 0.10773073881864548\n","step: 2750, loss: 0.21674302220344543\n","step: 2760, loss: 0.14207158982753754\n","step: 2770, loss: 0.08180161565542221\n","step: 2780, loss: 0.07824141532182693\n","step: 2790, loss: 0.14431728422641754\n","step: 2800, loss: 0.10557467490434647\n","step: 2810, loss: 0.08546117693185806\n","step: 2820, loss: 0.16378149390220642\n","step: 2830, loss: 0.13206662237644196\n","step: 2840, loss: 0.11010009795427322\n","step: 2850, loss: 0.015480986796319485\n","step: 2860, loss: 0.12432292848825455\n","step: 2870, loss: 0.10357257723808289\n","step: 2880, loss: 0.06855538487434387\n","step: 2890, loss: 0.11730680614709854\n","step: 2900, loss: 0.09470712393522263\n","step: 2910, loss: 0.08344718813896179\n","step: 2920, loss: 0.09110022336244583\n","step: 2930, loss: 0.04608379676938057\n","step: 2940, loss: 0.07104341685771942\n","step: 2950, loss: 0.15085873007774353\n","step: 2960, loss: 0.04683244973421097\n","step: 2970, loss: 0.14014923572540283\n","step: 2980, loss: 0.0911143571138382\n","step: 2990, loss: 0.09936433285474777\n","step: 3000, loss: 0.03621022403240204\n","step: 3010, loss: 0.17835496366024017\n","step: 3020, loss: 0.18632924556732178\n","step: 3030, loss: 0.02929580770432949\n","step: 3040, loss: 0.1315706968307495\n","step: 3050, loss: 0.09226828813552856\n","step: 3060, loss: 0.03684728592634201\n","step: 3070, loss: 0.18616876006126404\n","step: 3080, loss: 0.08016601949930191\n","step: 3090, loss: 0.055853407829999924\n","step: 3100, loss: 0.10981685668230057\n","step: 3110, loss: 0.058269258588552475\n","step: 3120, loss: 0.18871012330055237\n","step: 3130, loss: 0.14491206407546997\n","step: 3140, loss: 0.19969482719898224\n","step: 3150, loss: 0.10700896382331848\n","step: 3160, loss: 0.12672513723373413\n","step: 3170, loss: 0.04245281219482422\n","step: 3180, loss: 0.04805326461791992\n","step: 3190, loss: 0.11588343232870102\n","step: 3200, loss: 0.04592694714665413\n","step: 3210, loss: 0.1594773381948471\n","step: 3220, loss: 0.08070291578769684\n","step: 3230, loss: 0.036343030631542206\n","step: 3240, loss: 0.18204082548618317\n","step: 3250, loss: 0.07840818911790848\n","step: 3260, loss: 0.15177953243255615\n","step: 3270, loss: 0.08798566460609436\n","step: 3280, loss: 0.05454874783754349\n","step: 3290, loss: 0.07399032264947891\n","step: 3300, loss: 0.0825459435582161\n","step: 3310, loss: 0.11982288211584091\n","step: 3320, loss: 0.11374281346797943\n","step: 3330, loss: 0.017474893480539322\n","step: 3340, loss: 0.047274503856897354\n","step: 3350, loss: 0.1041518971323967\n","step: 3360, loss: 0.03694809228181839\n","step: 3370, loss: 0.030217615887522697\n","step: 3380, loss: 0.1336774230003357\n","step: 3390, loss: 0.1633635014295578\n","step: 3400, loss: 0.10772139579057693\n","step: 3410, loss: 0.08062652498483658\n","step: 3420, loss: 0.04386438429355621\n","step: 3430, loss: 0.1257995218038559\n","step: 3440, loss: 0.05008790269494057\n","step: 3450, loss: 0.11462967097759247\n","step: 3460, loss: 0.09181132167577744\n","step: 3470, loss: 0.11208708584308624\n","step: 3480, loss: 0.04893476516008377\n","step: 3490, loss: 0.11890361458063126\n","step: 3500, loss: 0.060002248734235764\n","step: 3510, loss: 0.10858020186424255\n","step: 3520, loss: 0.08387421816587448\n","step: 3530, loss: 0.07950716465711594\n","step: 3540, loss: 0.10736952722072601\n","step: 3550, loss: 0.05120668560266495\n","step: 3560, loss: 0.02874389849603176\n","step: 3570, loss: 0.06601499021053314\n","step: 3580, loss: 0.13753511011600494\n","step: 3590, loss: 0.14745843410491943\n","step: 3600, loss: 0.06520687788724899\n","step: 3610, loss: 0.11929944902658463\n","step: 3620, loss: 0.056647010147571564\n","step: 3630, loss: 0.0648735910654068\n","step: 3640, loss: 0.021082395687699318\n","step: 3650, loss: 0.11980406939983368\n","step: 3660, loss: 0.09671112149953842\n","step: 3670, loss: 0.06950261443853378\n","step: 3680, loss: 0.07022189348936081\n","step: 3690, loss: 0.09448068588972092\n","step: 3700, loss: 0.07623840123414993\n","step: 3710, loss: 0.09675312042236328\n","step: 3720, loss: 0.1250964105129242\n","step: 3730, loss: 0.09654739499092102\n","step: 3740, loss: 0.12481207400560379\n","step: 3750, loss: 0.0626048594713211\n","step: 3760, loss: 0.09937849640846252\n","step: 3770, loss: 0.11771930754184723\n","\n","Loop 7\n","domain_dev_word_lst 404\n","Train from scratch...\n","step: 0, loss: 3.9503440856933594\n","step: 10, loss: 1.9576313495635986\n","step: 20, loss: 0.8360992670059204\n","step: 30, loss: 0.3717743754386902\n","step: 40, loss: 0.30139464139938354\n","step: 50, loss: 0.22677427530288696\n","step: 60, loss: 0.3193505108356476\n","step: 70, loss: 0.1548212617635727\n","step: 80, loss: 0.22606942057609558\n","step: 90, loss: 0.1647513061761856\n","step: 100, loss: 0.062341343611478806\n","step: 110, loss: 0.19265376031398773\n","step: 120, loss: 0.15083910524845123\n","step: 130, loss: 0.1473461389541626\n","step: 140, loss: 0.11710694432258606\n","step: 150, loss: 0.18770231306552887\n","step: 160, loss: 0.13971763849258423\n","step: 170, loss: 0.19377633929252625\n","step: 180, loss: 0.16778084635734558\n","step: 190, loss: 0.36350172758102417\n","step: 200, loss: 0.21638260781764984\n","step: 210, loss: 0.08393218368291855\n","step: 220, loss: 0.16715100407600403\n","step: 230, loss: 0.17543227970600128\n","step: 240, loss: 0.0697450041770935\n","step: 250, loss: 0.1816439926624298\n","step: 260, loss: 0.1714838147163391\n","step: 270, loss: 0.12887042760849\n","step: 280, loss: 0.13459010422229767\n","step: 290, loss: 0.10682754963636398\n","step: 300, loss: 0.129614919424057\n","step: 310, loss: 0.11367719620466232\n","step: 320, loss: 0.07946141809225082\n","step: 330, loss: 0.15540407598018646\n","step: 340, loss: 0.05901004374027252\n","step: 350, loss: 0.1034960001707077\n","step: 360, loss: 0.0819186344742775\n","step: 370, loss: 0.09607988595962524\n","step: 380, loss: 0.10415850579738617\n","step: 390, loss: 0.15569697320461273\n","step: 400, loss: 0.052193768322467804\n","step: 410, loss: 0.10765659064054489\n","step: 420, loss: 0.13903336226940155\n","step: 430, loss: 0.10390204936265945\n","step: 440, loss: 0.0822446271777153\n","step: 450, loss: 0.09020716696977615\n","step: 460, loss: 0.08951764553785324\n","step: 470, loss: 0.05279254913330078\n","step: 480, loss: 0.17726552486419678\n","step: 490, loss: 0.10499091446399689\n","step: 500, loss: 0.12799930572509766\n","step: 510, loss: 0.1767703890800476\n","step: 520, loss: 0.0827779769897461\n","step: 530, loss: 0.11261031776666641\n","step: 540, loss: 0.35740530490875244\n","step: 550, loss: 0.11312448233366013\n","step: 560, loss: 0.11578849703073502\n","step: 570, loss: 0.15865656733512878\n","step: 580, loss: 0.1730269193649292\n","step: 590, loss: 0.12869171798229218\n","step: 600, loss: 0.08372200280427933\n","step: 610, loss: 0.1421058028936386\n","step: 620, loss: 0.0703396275639534\n","step: 630, loss: 0.13505221903324127\n","step: 640, loss: 0.04898636415600777\n","step: 650, loss: 0.12109009176492691\n","step: 660, loss: 0.06204545497894287\n","step: 670, loss: 0.07943538576364517\n","step: 680, loss: 0.14088399708271027\n","step: 690, loss: 0.10392969101667404\n","step: 700, loss: 0.061587441712617874\n","step: 710, loss: 0.07886791229248047\n","step: 720, loss: 0.23771820962429047\n","step: 730, loss: 0.06082404404878616\n","step: 740, loss: 0.11242536455392838\n","step: 750, loss: 0.06807117164134979\n","step: 760, loss: 0.14751020073890686\n","step: 770, loss: 0.119806669652462\n","step: 780, loss: 0.1970958113670349\n","step: 790, loss: 0.10650447010993958\n","step: 800, loss: 0.07785771042108536\n","step: 810, loss: 0.1440027356147766\n","step: 820, loss: 0.1623581349849701\n","step: 830, loss: 0.09817849099636078\n","step: 840, loss: 0.08667869120836258\n","step: 850, loss: 0.11238674074411392\n","step: 860, loss: 0.22419524192810059\n","step: 870, loss: 0.19112814962863922\n","step: 880, loss: 0.06288005411624908\n","step: 890, loss: 0.04674595221877098\n","step: 900, loss: 0.1367531269788742\n","step: 910, loss: 0.14214389026165009\n","step: 920, loss: 0.08963482081890106\n","step: 930, loss: 0.08696421980857849\n","step: 940, loss: 0.04706013947725296\n","step: 950, loss: 0.15200144052505493\n","step: 960, loss: 0.04134424030780792\n","step: 970, loss: 0.1489420235157013\n","step: 980, loss: 0.19730189442634583\n","step: 990, loss: 0.0784369707107544\n","step: 1000, loss: 0.035923756659030914\n","step: 1010, loss: 0.11165639758110046\n","step: 1020, loss: 0.14919981360435486\n","step: 1030, loss: 0.06628303974866867\n","step: 1040, loss: 0.12692822515964508\n","step: 1050, loss: 0.10286543518304825\n","step: 1060, loss: 0.15852871537208557\n","step: 1070, loss: 0.15322592854499817\n","step: 1080, loss: 0.057101503014564514\n","step: 1090, loss: 0.07254280149936676\n","step: 1100, loss: 0.0313766673207283\n","step: 1110, loss: 0.1004779115319252\n","step: 1120, loss: 0.018961217254400253\n","step: 1130, loss: 0.058346185833215714\n","step: 1140, loss: 0.22911201417446136\n","step: 1150, loss: 0.07015674561262131\n","step: 1160, loss: 0.06870204955339432\n","step: 1170, loss: 0.09751316159963608\n","step: 1180, loss: 0.021255407482385635\n","step: 1190, loss: 0.08133132755756378\n","step: 1200, loss: 0.11012397706508636\n","step: 1210, loss: 0.08755671977996826\n","step: 1220, loss: 0.040992699563503265\n","step: 1230, loss: 0.07701285183429718\n","step: 1240, loss: 0.044173602014780045\n","step: 1250, loss: 0.0851420983672142\n","step: 1260, loss: 0.09995375573635101\n","step: 1270, loss: 0.10745828598737717\n","step: 1280, loss: 0.03295164555311203\n","step: 1290, loss: 0.21547037363052368\n","step: 1300, loss: 0.04748956114053726\n","step: 1310, loss: 0.07276884466409683\n","step: 1320, loss: 0.20369745790958405\n","step: 1330, loss: 0.16143250465393066\n","step: 1340, loss: 0.2415468543767929\n","step: 1350, loss: 0.061469342559576035\n","step: 1360, loss: 0.11326707899570465\n","step: 1370, loss: 0.147271066904068\n","step: 1380, loss: 0.04901856184005737\n","step: 1390, loss: 0.15943986177444458\n","step: 1400, loss: 0.04130422696471214\n","step: 1410, loss: 0.09295763820409775\n","step: 1420, loss: 0.10825394839048386\n","step: 1430, loss: 0.17197448015213013\n","step: 1440, loss: 0.1174897626042366\n","step: 1450, loss: 0.035497359931468964\n","step: 1460, loss: 0.14253632724285126\n","step: 1470, loss: 0.058879438787698746\n","step: 1480, loss: 0.04869208112359047\n","step: 1490, loss: 0.13199952244758606\n","step: 1500, loss: 0.03616909310221672\n","step: 1510, loss: 0.12664443254470825\n","step: 1520, loss: 0.06942730396986008\n","step: 1530, loss: 0.09838316589593887\n","step: 1540, loss: 0.17697182297706604\n","step: 1550, loss: 0.06623885035514832\n","step: 1560, loss: 0.31312599778175354\n","step: 1570, loss: 0.17906351387500763\n","step: 1580, loss: 0.17423942685127258\n","step: 1590, loss: 0.09091164171695709\n","step: 1600, loss: 0.06202900409698486\n","step: 1610, loss: 0.04164482653141022\n","step: 1620, loss: 0.08084163814783096\n","step: 1630, loss: 0.14604337513446808\n","step: 1640, loss: 0.07070871442556381\n","step: 1650, loss: 0.08715753257274628\n","step: 1660, loss: 0.0649448111653328\n","step: 1670, loss: 0.1084698736667633\n","step: 1680, loss: 0.07569826394319534\n","step: 1690, loss: 0.15903058648109436\n","step: 1700, loss: 0.06214512884616852\n","step: 1710, loss: 0.07788442075252533\n","step: 1720, loss: 0.06593593955039978\n","step: 1730, loss: 0.09318474680185318\n","step: 1740, loss: 0.1301092803478241\n","step: 1750, loss: 0.17882463335990906\n","step: 1760, loss: 0.10505933314561844\n","step: 1770, loss: 0.10654773563146591\n","step: 1780, loss: 0.05899154022336006\n","step: 1790, loss: 0.09660045057535172\n","step: 1800, loss: 0.16489732265472412\n","step: 1810, loss: 0.18313544988632202\n","step: 1820, loss: 0.14131177961826324\n","step: 1830, loss: 0.07215999066829681\n","step: 1840, loss: 0.04033442959189415\n","step: 1850, loss: 0.04094651713967323\n","step: 1860, loss: 0.13684412837028503\n","step: 1870, loss: 0.08776887506246567\n","step: 1880, loss: 0.1211187019944191\n","step: 1890, loss: 0.06326769292354584\n","step: 1900, loss: 0.09930157661437988\n","step: 1910, loss: 0.10325700789690018\n","step: 1920, loss: 0.16483289003372192\n","step: 1930, loss: 0.13819317519664764\n","step: 1940, loss: 0.07931219041347504\n","step: 1950, loss: 0.09766857326030731\n","step: 1960, loss: 0.03024234250187874\n","step: 1970, loss: 0.07375852763652802\n","step: 1980, loss: 0.16627874970436096\n","step: 1990, loss: 0.05906121805310249\n","step: 2000, loss: 0.16445134580135345\n","step: 2010, loss: 0.08223006129264832\n","step: 2020, loss: 0.09510362148284912\n","step: 2030, loss: 0.03799547255039215\n","step: 2040, loss: 0.1393011063337326\n","step: 2050, loss: 0.10369708389043808\n","step: 2060, loss: 0.07413514703512192\n","step: 2070, loss: 0.11548006534576416\n","step: 2080, loss: 0.05011381208896637\n","step: 2090, loss: 0.060128048062324524\n","step: 2100, loss: 0.07488448917865753\n","step: 2110, loss: 0.061561886221170425\n","step: 2120, loss: 0.06067980080842972\n","step: 2130, loss: 0.10038937628269196\n","step: 2140, loss: 0.10550076514482498\n","step: 2150, loss: 0.2302653193473816\n","step: 2160, loss: 0.08195015788078308\n","step: 2170, loss: 0.029494354501366615\n","step: 2180, loss: 0.19192330539226532\n","step: 2190, loss: 0.08796199411153793\n","step: 2200, loss: 0.1332862675189972\n","step: 2210, loss: 0.05288359150290489\n","step: 2220, loss: 0.0922103077173233\n","step: 2230, loss: 0.11324522644281387\n","step: 2240, loss: 0.09894344955682755\n","step: 2250, loss: 0.05471015349030495\n","step: 2260, loss: 0.13072185218334198\n","step: 2270, loss: 0.04059305414557457\n","step: 2280, loss: 0.04693937674164772\n","step: 2290, loss: 0.108095183968544\n","step: 2300, loss: 0.05303468555212021\n","step: 2310, loss: 0.07781343907117844\n","step: 2320, loss: 0.03885212913155556\n","step: 2330, loss: 0.02285006456077099\n","step: 2340, loss: 0.03299734368920326\n","step: 2350, loss: 0.2354438304901123\n","step: 2360, loss: 0.07673123478889465\n","step: 2370, loss: 0.07987929880619049\n","step: 2380, loss: 0.061509087681770325\n","step: 2390, loss: 0.03448307141661644\n","step: 2400, loss: 0.07589831203222275\n","step: 2410, loss: 0.08327412605285645\n","step: 2420, loss: 0.08238400518894196\n","step: 2430, loss: 0.06325484812259674\n","step: 2440, loss: 0.09885896742343903\n","step: 2450, loss: 0.09067445993423462\n","step: 2460, loss: 0.14146851003170013\n","step: 2470, loss: 0.0795658603310585\n","step: 2480, loss: 0.057967208325862885\n","step: 2490, loss: 0.09299178421497345\n","step: 2500, loss: 0.11241895705461502\n","step: 2510, loss: 0.06504183262586594\n","step: 2520, loss: 0.13478852808475494\n","step: 2530, loss: 0.09553909301757812\n","step: 2540, loss: 0.045191388577222824\n","step: 2550, loss: 0.03382798284292221\n","step: 2560, loss: 0.0643458440899849\n","step: 2570, loss: 0.08557823300361633\n","step: 2580, loss: 0.070784792304039\n","step: 2590, loss: 0.16467978060245514\n","step: 2600, loss: 0.08811355382204056\n","step: 2610, loss: 0.12717825174331665\n","step: 2620, loss: 0.031157122924923897\n","step: 2630, loss: 0.07514846324920654\n","step: 2640, loss: 0.15140877664089203\n","step: 2650, loss: 0.11167225986719131\n","step: 2660, loss: 0.08257865905761719\n","step: 2670, loss: 0.061817511916160583\n","step: 2680, loss: 0.08119289577007294\n","step: 2690, loss: 0.17049150168895721\n","step: 2700, loss: 0.07691498845815659\n","step: 2710, loss: 0.10052499920129776\n","step: 2720, loss: 0.21032506227493286\n","step: 2730, loss: 0.07957365363836288\n","step: 2740, loss: 0.217860609292984\n","step: 2750, loss: 0.05736652761697769\n","step: 2760, loss: 0.1229933574795723\n","step: 2770, loss: 0.04183053597807884\n","step: 2780, loss: 0.011587997898459435\n","step: 2790, loss: 0.08299876004457474\n","step: 2800, loss: 0.052676405757665634\n","step: 2810, loss: 0.11440904438495636\n","step: 2820, loss: 0.021131284534931183\n","step: 2830, loss: 0.07211539149284363\n","step: 2840, loss: 0.05834563076496124\n","step: 2850, loss: 0.04429992288351059\n","step: 2860, loss: 0.04611987620592117\n","step: 2870, loss: 0.0724811851978302\n","step: 2880, loss: 0.13769136369228363\n","step: 2890, loss: 0.12658119201660156\n","step: 2900, loss: 0.143244206905365\n","step: 2910, loss: 0.09959226101636887\n","step: 2920, loss: 0.11031891405582428\n","step: 2930, loss: 0.06447862088680267\n","step: 2940, loss: 0.11799237877130508\n","step: 2950, loss: 0.09185139834880829\n","step: 2960, loss: 0.04447191581130028\n","step: 2970, loss: 0.06002875044941902\n","step: 2980, loss: 0.05400761216878891\n","step: 2990, loss: 0.07503753900527954\n","step: 3000, loss: 0.09725011885166168\n","step: 3010, loss: 0.06180141121149063\n","step: 3020, loss: 0.10879133641719818\n","step: 3030, loss: 0.061478909105062485\n","step: 3040, loss: 0.09359440952539444\n","step: 3050, loss: 0.0534498356282711\n","step: 3060, loss: 0.030067794024944305\n","step: 3070, loss: 0.2418709546327591\n","step: 3080, loss: 0.1914299726486206\n","step: 3090, loss: 0.06723283231258392\n","step: 3100, loss: 0.07324480265378952\n","step: 3110, loss: 0.10696398466825485\n","step: 3120, loss: 0.04930100217461586\n","step: 3130, loss: 0.11716745793819427\n","step: 3140, loss: 0.12564212083816528\n","step: 3150, loss: 0.0834488719701767\n","step: 3160, loss: 0.06800330430269241\n","step: 3170, loss: 0.06099691987037659\n","step: 3180, loss: 0.026723282411694527\n","step: 3190, loss: 0.03394502401351929\n","step: 3200, loss: 0.041168492287397385\n","step: 3210, loss: 0.09505198150873184\n","step: 3220, loss: 0.043932996690273285\n","step: 3230, loss: 0.07265893369913101\n","step: 3240, loss: 0.08102498203516006\n","step: 3250, loss: 0.12112066149711609\n","step: 3260, loss: 0.08081348240375519\n","step: 3270, loss: 0.06968572735786438\n","step: 3280, loss: 0.20100665092468262\n","step: 3290, loss: 0.16731786727905273\n","step: 3300, loss: 0.031391069293022156\n","step: 3310, loss: 0.07324624806642532\n","step: 3320, loss: 0.10077168047428131\n","step: 3330, loss: 0.05395063757896423\n","step: 3340, loss: 0.0660727396607399\n","step: 3350, loss: 0.054224684834480286\n","step: 3360, loss: 0.02798588015139103\n","step: 3370, loss: 0.02502163127064705\n","step: 3380, loss: 0.10849402099847794\n","step: 3390, loss: 0.08420931547880173\n","step: 3400, loss: 0.02503254823386669\n","step: 3410, loss: 0.04788879677653313\n","step: 3420, loss: 0.02278510294854641\n","step: 3430, loss: 0.07486749440431595\n","step: 3440, loss: 0.14734777808189392\n","step: 3450, loss: 0.09655384719371796\n","step: 3460, loss: 0.17569294571876526\n","step: 3470, loss: 0.07436738163232803\n","step: 3480, loss: 0.03752213716506958\n","step: 3490, loss: 0.09741690009832382\n","step: 3500, loss: 0.11035910248756409\n","step: 3510, loss: 0.12287240475416183\n","step: 3520, loss: 0.03917283937335014\n","step: 3530, loss: 0.06507173925638199\n","step: 3540, loss: 0.09030907601118088\n","step: 3550, loss: 0.1502666026353836\n","step: 3560, loss: 0.10307173430919647\n","step: 3570, loss: 0.05362420156598091\n","step: 3580, loss: 0.04819326475262642\n","step: 3590, loss: 0.06940957903862\n","step: 3600, loss: 0.07712919265031815\n","step: 3610, loss: 0.1854393035173416\n","step: 3620, loss: 0.06489922106266022\n","step: 3630, loss: 0.026990994811058044\n","step: 3640, loss: 0.11417324095964432\n","step: 3650, loss: 0.13981416821479797\n","step: 3660, loss: 0.08949282020330429\n","step: 3670, loss: 0.09573519229888916\n","step: 3680, loss: 0.11062613874673843\n","step: 3690, loss: 0.15339061617851257\n","step: 3700, loss: 0.07456546276807785\n","step: 3710, loss: 0.08314725011587143\n","step: 3720, loss: 0.22419632971286774\n","step: 3730, loss: 0.04406758397817612\n","step: 3740, loss: 0.11093810200691223\n","step: 3750, loss: 0.09577491134405136\n","step: 3760, loss: 0.0905173048377037\n","step: 3770, loss: 0.10264822095632553\n","\n","Loop 8\n","domain_dev_word_lst 302\n","Train from scratch...\n","step: 0, loss: 3.92866587638855\n","step: 10, loss: 1.9076998233795166\n","step: 20, loss: 1.0229328870773315\n","step: 30, loss: 0.34063389897346497\n","step: 40, loss: 0.2587597072124481\n","step: 50, loss: 0.21775618195533752\n","step: 60, loss: 0.19363321363925934\n","step: 70, loss: 0.13090071082115173\n","step: 80, loss: 0.15866826474666595\n","step: 90, loss: 0.20988842844963074\n","step: 100, loss: 0.2090121954679489\n","step: 110, loss: 0.11320970952510834\n","step: 120, loss: 0.16082613170146942\n","step: 130, loss: 0.12754499912261963\n","step: 140, loss: 0.056047119200229645\n","step: 150, loss: 0.12524299323558807\n","step: 160, loss: 0.11706530302762985\n","step: 170, loss: 0.135374516248703\n","step: 180, loss: 0.3414621651172638\n","step: 190, loss: 0.3365704119205475\n","step: 200, loss: 0.15547704696655273\n","step: 210, loss: 0.09076961874961853\n","step: 220, loss: 0.13499437272548676\n","step: 230, loss: 0.0851096361875534\n","step: 240, loss: 0.12221776694059372\n","step: 250, loss: 0.15552446246147156\n","step: 260, loss: 0.09234975278377533\n","step: 270, loss: 0.058008816093206406\n","step: 280, loss: 0.04600115492939949\n","step: 290, loss: 0.21308644115924835\n","step: 300, loss: 0.15336404740810394\n","step: 310, loss: 0.10967911034822464\n","step: 320, loss: 0.12572425603866577\n","step: 330, loss: 0.07828021794557571\n","step: 340, loss: 0.046308379620313644\n","step: 350, loss: 0.06970355659723282\n","step: 360, loss: 0.1480037271976471\n","step: 370, loss: 0.07138759642839432\n","step: 380, loss: 0.051840975880622864\n","step: 390, loss: 0.13354501128196716\n","step: 400, loss: 0.12233205139636993\n","step: 410, loss: 0.18477894365787506\n","step: 420, loss: 0.023466594517230988\n","step: 430, loss: 0.055532705038785934\n","step: 440, loss: 0.11826439201831818\n","step: 450, loss: 0.14546284079551697\n","step: 460, loss: 0.061527907848358154\n","step: 470, loss: 0.06374744325876236\n","step: 480, loss: 0.1645369827747345\n","step: 490, loss: 0.06856901198625565\n","step: 500, loss: 0.11032865941524506\n","step: 510, loss: 0.1487579047679901\n","step: 520, loss: 0.27147945761680603\n","step: 530, loss: 0.031568195670843124\n","step: 540, loss: 0.08726007491350174\n","step: 550, loss: 0.05470044165849686\n","step: 560, loss: 0.07011067122220993\n","step: 570, loss: 0.07609884440898895\n","step: 580, loss: 0.078787662088871\n","step: 590, loss: 0.1700521558523178\n","step: 600, loss: 0.09622817486524582\n","step: 610, loss: 0.14308957755565643\n","step: 620, loss: 0.06960826367139816\n","step: 630, loss: 0.09193362295627594\n","step: 640, loss: 0.09026052057743073\n","step: 650, loss: 0.10444910824298859\n","step: 660, loss: 0.1557171493768692\n","step: 670, loss: 0.10522284358739853\n","step: 680, loss: 0.056158218532800674\n","step: 690, loss: 0.16109491884708405\n","step: 700, loss: 0.12110300362110138\n","step: 710, loss: 0.0790085643529892\n","step: 720, loss: 0.17756542563438416\n","step: 730, loss: 0.10716439038515091\n","step: 740, loss: 0.17437998950481415\n","step: 750, loss: 0.17102444171905518\n","step: 760, loss: 0.20583970844745636\n","step: 770, loss: 0.09832683205604553\n","step: 780, loss: 0.234528049826622\n","step: 790, loss: 0.10823699086904526\n","step: 800, loss: 0.06665734201669693\n","step: 810, loss: 0.08124246448278427\n","step: 820, loss: 0.018832892179489136\n","step: 830, loss: 0.030498716980218887\n","step: 840, loss: 0.1139168068766594\n","step: 850, loss: 0.09224127978086472\n","step: 860, loss: 0.09748256206512451\n","step: 870, loss: 0.08058329671621323\n","step: 880, loss: 0.1365356743335724\n","step: 890, loss: 0.07905784249305725\n","step: 900, loss: 0.04860809072852135\n","step: 910, loss: 0.07116584479808807\n","step: 920, loss: 0.09104759991168976\n","step: 930, loss: 0.1036851704120636\n","step: 940, loss: 0.048212599009275436\n","step: 950, loss: 0.03959253802895546\n","step: 960, loss: 0.09398309141397476\n","step: 970, loss: 0.1909109652042389\n","step: 980, loss: 0.13216039538383484\n","step: 990, loss: 0.0670524314045906\n","step: 1000, loss: 0.15029405057430267\n","step: 1010, loss: 0.09465209394693375\n","step: 1020, loss: 0.14379838109016418\n","step: 1030, loss: 0.08435669541358948\n","step: 1040, loss: 0.13847491145133972\n","step: 1050, loss: 0.037850089371204376\n","step: 1060, loss: 0.08945462107658386\n","step: 1070, loss: 0.05275982618331909\n","step: 1080, loss: 0.08805891126394272\n","step: 1090, loss: 0.052813056856393814\n","step: 1100, loss: 0.12415079027414322\n","step: 1110, loss: 0.05099822208285332\n","step: 1120, loss: 0.06417492777109146\n","step: 1130, loss: 0.16445329785346985\n","step: 1140, loss: 0.12934643030166626\n","step: 1150, loss: 0.03736352175474167\n","step: 1160, loss: 0.14897796511650085\n","step: 1170, loss: 0.0650208592414856\n","step: 1180, loss: 0.05759529024362564\n","step: 1190, loss: 0.09058541059494019\n","step: 1200, loss: 0.06462323665618896\n","step: 1210, loss: 0.08316003531217575\n","step: 1220, loss: 0.09357020258903503\n","step: 1230, loss: 0.14210455119609833\n","step: 1240, loss: 0.06437836587429047\n","step: 1250, loss: 0.018975749611854553\n","step: 1260, loss: 0.12137768417596817\n","step: 1270, loss: 0.08661837875843048\n","step: 1280, loss: 0.17518582940101624\n","step: 1290, loss: 0.07332209497690201\n","step: 1300, loss: 0.1696818470954895\n","step: 1310, loss: 0.09212331473827362\n","step: 1320, loss: 0.11532378941774368\n","step: 1330, loss: 0.09016294032335281\n","step: 1340, loss: 0.04015929624438286\n","step: 1350, loss: 0.03604529798030853\n","step: 1360, loss: 0.14538386464118958\n","step: 1370, loss: 0.14963975548744202\n","step: 1380, loss: 0.11143817752599716\n","step: 1390, loss: 0.11867076903581619\n","step: 1400, loss: 0.07822509109973907\n","step: 1410, loss: 0.07144585996866226\n","step: 1420, loss: 0.09249848127365112\n","step: 1430, loss: 0.15631185472011566\n","step: 1440, loss: 0.0330350361764431\n","step: 1450, loss: 0.10305515676736832\n","step: 1460, loss: 0.13356325030326843\n","step: 1470, loss: 0.06603556871414185\n","step: 1480, loss: 0.1550069898366928\n","step: 1490, loss: 0.08295057713985443\n","step: 1500, loss: 0.08430150151252747\n","step: 1510, loss: 0.04699418321251869\n","step: 1520, loss: 0.10954002290964127\n","step: 1530, loss: 0.08193572610616684\n","step: 1540, loss: 0.12162963300943375\n","step: 1550, loss: 0.06419970095157623\n","step: 1560, loss: 0.08864516019821167\n","step: 1570, loss: 0.05042284354567528\n","step: 1580, loss: 0.07807464897632599\n","step: 1590, loss: 0.07383010536432266\n","step: 1600, loss: 0.09263142943382263\n","step: 1610, loss: 0.12636767327785492\n","step: 1620, loss: 0.10805793851613998\n","step: 1630, loss: 0.07894886285066605\n","step: 1640, loss: 0.08772528916597366\n","step: 1650, loss: 0.05513507500290871\n","step: 1660, loss: 0.09941078722476959\n","step: 1670, loss: 0.12270784378051758\n","step: 1680, loss: 0.11581341922283173\n","step: 1690, loss: 0.184946671128273\n","step: 1700, loss: 0.11432785540819168\n","step: 1710, loss: 0.11887117475271225\n","step: 1720, loss: 0.10994480550289154\n","step: 1730, loss: 0.10164406150579453\n","step: 1740, loss: 0.08638878911733627\n","step: 1750, loss: 0.05239124968647957\n","step: 1760, loss: 0.062025319784879684\n","step: 1770, loss: 0.07911436259746552\n","step: 1780, loss: 0.06502571702003479\n","step: 1790, loss: 0.12226814031600952\n","step: 1800, loss: 0.1436588317155838\n","step: 1810, loss: 0.1386515498161316\n","step: 1820, loss: 0.02620876207947731\n","step: 1830, loss: 0.09230823069810867\n","step: 1840, loss: 0.10252215713262558\n","step: 1850, loss: 0.1302248239517212\n","step: 1860, loss: 0.038215283304452896\n","step: 1870, loss: 0.18934093415737152\n","step: 1880, loss: 0.0349142849445343\n","step: 1890, loss: 0.03545621410012245\n","step: 1900, loss: 0.059093132615089417\n","step: 1910, loss: 0.14565448462963104\n","step: 1920, loss: 0.07696995884180069\n","step: 1930, loss: 0.07187950611114502\n","step: 1940, loss: 0.12603417038917542\n","step: 1950, loss: 0.1096266657114029\n","step: 1960, loss: 0.18032938241958618\n","step: 1970, loss: 0.17408092319965363\n","step: 1980, loss: 0.0921853706240654\n","step: 1990, loss: 0.137388214468956\n","step: 2000, loss: 0.13040849566459656\n","step: 2010, loss: 0.10092869400978088\n","step: 2020, loss: 0.1195032075047493\n","step: 2030, loss: 0.08442463725805283\n","step: 2040, loss: 0.03857902064919472\n","step: 2050, loss: 0.08376038819551468\n","step: 2060, loss: 0.07149496674537659\n","step: 2070, loss: 0.07665947079658508\n","step: 2080, loss: 0.15906879305839539\n","step: 2090, loss: 0.08579391241073608\n","step: 2100, loss: 0.1571478247642517\n","step: 2110, loss: 0.06830628216266632\n","step: 2120, loss: 0.11193137615919113\n","step: 2130, loss: 0.07953666895627975\n","step: 2140, loss: 0.11622730642557144\n","step: 2150, loss: 0.10079681873321533\n","step: 2160, loss: 0.0756552666425705\n","step: 2170, loss: 0.04933590069413185\n","step: 2180, loss: 0.029584847390651703\n","step: 2190, loss: 0.11681593954563141\n","step: 2200, loss: 0.11626392602920532\n","step: 2210, loss: 0.07560500502586365\n","step: 2220, loss: 0.08260809630155563\n","step: 2230, loss: 0.0844993069767952\n","step: 2240, loss: 0.02691037207841873\n","step: 2250, loss: 0.13377290964126587\n","step: 2260, loss: 0.13628175854682922\n","step: 2270, loss: 0.08209765702486038\n","step: 2280, loss: 0.05182385444641113\n","step: 2290, loss: 0.04560381919145584\n","step: 2300, loss: 0.04347136989235878\n","step: 2310, loss: 0.1775670349597931\n","step: 2320, loss: 0.06901846081018448\n","step: 2330, loss: 0.10583491623401642\n","step: 2340, loss: 0.08884619176387787\n","step: 2350, loss: 0.04480959102511406\n","step: 2360, loss: 0.09242893755435944\n","step: 2370, loss: 0.1122533455491066\n","step: 2380, loss: 0.02442506141960621\n","step: 2390, loss: 0.03947226330637932\n","step: 2400, loss: 0.163083016872406\n","step: 2410, loss: 0.09356767684221268\n","step: 2420, loss: 0.18078124523162842\n","step: 2430, loss: 0.15511712431907654\n","step: 2440, loss: 0.07221724092960358\n","step: 2450, loss: 0.06509079039096832\n","step: 2460, loss: 0.04745940491557121\n","step: 2470, loss: 0.08093037456274033\n","step: 2480, loss: 0.08579343557357788\n","step: 2490, loss: 0.08672625571489334\n","step: 2500, loss: 0.1295863538980484\n","step: 2510, loss: 0.1064455509185791\n","step: 2520, loss: 0.03381682187318802\n","step: 2530, loss: 0.03835420310497284\n","step: 2540, loss: 0.027731768786907196\n","step: 2550, loss: 0.3421115279197693\n","step: 2560, loss: 0.1638033539056778\n","step: 2570, loss: 0.07036899775266647\n","step: 2580, loss: 0.0824027881026268\n","step: 2590, loss: 0.06847691535949707\n","step: 2600, loss: 0.05596618726849556\n","step: 2610, loss: 0.07317845523357391\n","step: 2620, loss: 0.07541202008724213\n","step: 2630, loss: 0.044166985899209976\n","step: 2640, loss: 0.07618825882673264\n","step: 2650, loss: 0.07457572221755981\n","step: 2660, loss: 0.18955405056476593\n","step: 2670, loss: 0.18486498296260834\n","step: 2680, loss: 0.08998128026723862\n","step: 2690, loss: 0.05881848931312561\n","step: 2700, loss: 0.12930430471897125\n","step: 2710, loss: 0.19045479595661163\n","step: 2720, loss: 0.0914783775806427\n","step: 2730, loss: 0.07648412138223648\n","step: 2740, loss: 0.11205832660198212\n","step: 2750, loss: 0.0580483116209507\n","step: 2760, loss: 0.07288867980241776\n","step: 2770, loss: 0.11764267086982727\n","step: 2780, loss: 0.08315981179475784\n","step: 2790, loss: 0.08035353571176529\n","step: 2800, loss: 0.08832467347383499\n","step: 2810, loss: 0.050183650106191635\n","step: 2820, loss: 0.06991830468177795\n","step: 2830, loss: 0.04115377366542816\n","step: 2840, loss: 0.061466995626688004\n","step: 2850, loss: 0.04241621494293213\n","step: 2860, loss: 0.052889712154865265\n","step: 2870, loss: 0.13444489240646362\n","step: 2880, loss: 0.041455235332250595\n","step: 2890, loss: 0.07400096952915192\n","step: 2900, loss: 0.08105329424142838\n","step: 2910, loss: 0.0623580701649189\n","step: 2920, loss: 0.07787127792835236\n","step: 2930, loss: 0.1091812252998352\n","step: 2940, loss: 0.0883762314915657\n","step: 2950, loss: 0.1129596009850502\n","step: 2960, loss: 0.08365567028522491\n","step: 2970, loss: 0.09087494760751724\n","step: 2980, loss: 0.07401756942272186\n","step: 2990, loss: 0.056148406118154526\n","step: 3000, loss: 0.10914206504821777\n","step: 3010, loss: 0.0719316154718399\n","step: 3020, loss: 0.1365026831626892\n","step: 3030, loss: 0.10941695421934128\n","step: 3040, loss: 0.1373942494392395\n","step: 3050, loss: 0.030698901042342186\n","step: 3060, loss: 0.07253958284854889\n","step: 3070, loss: 0.06345762312412262\n","step: 3080, loss: 0.07124437391757965\n","step: 3090, loss: 0.15288199484348297\n","step: 3100, loss: 0.09960248321294785\n","step: 3110, loss: 0.05056190863251686\n","step: 3120, loss: 0.23805978894233704\n","step: 3130, loss: 0.06206396594643593\n","step: 3140, loss: 0.06738733500242233\n","step: 3150, loss: 0.11876548826694489\n","step: 3160, loss: 0.10240631550550461\n","step: 3170, loss: 0.045667894184589386\n","step: 3180, loss: 0.17029474675655365\n","step: 3190, loss: 0.05318262800574303\n","step: 3200, loss: 0.07992623746395111\n","step: 3210, loss: 0.07719134539365768\n","step: 3220, loss: 0.09908682852983475\n","step: 3230, loss: 0.065639428794384\n","step: 3240, loss: 0.1827332228422165\n","step: 3250, loss: 0.08380196243524551\n","step: 3260, loss: 0.2303030788898468\n","step: 3270, loss: 0.09628423303365707\n","step: 3280, loss: 0.06266326457262039\n","step: 3290, loss: 0.09531436115503311\n","step: 3300, loss: 0.06135939434170723\n","step: 3310, loss: 0.08286510407924652\n","step: 3320, loss: 0.07068798691034317\n","step: 3330, loss: 0.06943317502737045\n","step: 3340, loss: 0.03295932710170746\n","step: 3350, loss: 0.09417416900396347\n","step: 3360, loss: 0.07306250184774399\n","step: 3370, loss: 0.10156606882810593\n","step: 3380, loss: 0.07546490430831909\n","step: 3390, loss: 0.013538510538637638\n","step: 3400, loss: 0.12505578994750977\n","step: 3410, loss: 0.11805747449398041\n","step: 3420, loss: 0.06367747485637665\n","step: 3430, loss: 0.03424878045916557\n","step: 3440, loss: 0.06417770683765411\n","step: 3450, loss: 0.0835360586643219\n","step: 3460, loss: 0.13170036673545837\n","step: 3470, loss: 0.14397267997264862\n","step: 3480, loss: 0.08295486122369766\n","step: 3490, loss: 0.04709337279200554\n","step: 3500, loss: 0.04085470736026764\n","step: 3510, loss: 0.07351323217153549\n","step: 3520, loss: 0.07833141088485718\n","step: 3530, loss: 0.1485937386751175\n","step: 3540, loss: 0.09140272438526154\n","step: 3550, loss: 0.09048309177160263\n","step: 3560, loss: 0.056544531136751175\n","step: 3570, loss: 0.08169281482696533\n","step: 3580, loss: 0.07934404164552689\n","step: 3590, loss: 0.09987416118383408\n","step: 3600, loss: 0.07443977892398834\n","step: 3610, loss: 0.12391330301761627\n","step: 3620, loss: 0.10188689827919006\n","step: 3630, loss: 0.10536465793848038\n","step: 3640, loss: 0.06480500847101212\n","step: 3650, loss: 0.10176531225442886\n","step: 3660, loss: 0.20737001299858093\n","step: 3670, loss: 0.08491349220275879\n","step: 3680, loss: 0.07339590042829514\n","step: 3690, loss: 0.03579702973365784\n","step: 3700, loss: 0.09886211156845093\n","step: 3710, loss: 0.05325505509972572\n","step: 3720, loss: 0.08402186632156372\n","step: 3730, loss: 0.09314171224832535\n","step: 3740, loss: 0.07578356564044952\n","step: 3750, loss: 0.034474581480026245\n","step: 3760, loss: 0.07652577757835388\n","step: 3770, loss: 0.04269075393676758\n","\n","Loop 9\n","domain_dev_word_lst 200\n","Train from scratch...\n","step: 0, loss: 3.9150426387786865\n","step: 10, loss: 2.0551083087921143\n","step: 20, loss: 0.7424584627151489\n","step: 30, loss: 0.5053241848945618\n","step: 40, loss: 0.23055188357830048\n","step: 50, loss: 0.13593678176403046\n","step: 60, loss: 0.16165240108966827\n","step: 70, loss: 0.17974397540092468\n","step: 80, loss: 0.19651347398757935\n","step: 90, loss: 0.12597300112247467\n","step: 100, loss: 0.07433728128671646\n","step: 110, loss: 0.0629681795835495\n","step: 120, loss: 0.1428256630897522\n","step: 130, loss: 0.1509697139263153\n","step: 140, loss: 0.06825165450572968\n","step: 150, loss: 0.0915914997458458\n","step: 160, loss: 0.27416813373565674\n","step: 170, loss: 0.1838466078042984\n","step: 180, loss: 0.10796870291233063\n","step: 190, loss: 0.09411126375198364\n","step: 200, loss: 0.10061585158109665\n","step: 210, loss: 0.17050565779209137\n","step: 220, loss: 0.15716738998889923\n","step: 230, loss: 0.18877869844436646\n","step: 240, loss: 0.06874144077301025\n","step: 250, loss: 0.12706473469734192\n","step: 260, loss: 0.15545937418937683\n","step: 270, loss: 0.1388007402420044\n","step: 280, loss: 0.07723653316497803\n","step: 290, loss: 0.087149478495121\n","step: 300, loss: 0.1292618066072464\n","step: 310, loss: 0.09129618108272552\n","step: 320, loss: 0.18392498791217804\n","step: 330, loss: 0.05148148909211159\n","step: 340, loss: 0.14266526699066162\n","step: 350, loss: 0.10539582371711731\n","step: 360, loss: 0.11769143491983414\n","step: 370, loss: 0.12603537738323212\n","step: 380, loss: 0.11406143754720688\n","step: 390, loss: 0.13521990180015564\n","step: 400, loss: 0.06811204552650452\n","step: 410, loss: 0.12114120274782181\n","step: 420, loss: 0.189663365483284\n","step: 430, loss: 0.1335107386112213\n","step: 440, loss: 0.07109420746564865\n","step: 450, loss: 0.07543813437223434\n","step: 460, loss: 0.06580916792154312\n","step: 470, loss: 0.08349990099668503\n","step: 480, loss: 0.166526660323143\n","step: 490, loss: 0.13964354991912842\n","step: 500, loss: 0.07815318554639816\n","step: 510, loss: 0.07633774727582932\n","step: 520, loss: 0.11508110910654068\n","step: 530, loss: 0.19049306213855743\n","step: 540, loss: 0.057973530143499374\n","step: 550, loss: 0.0935029685497284\n","step: 560, loss: 0.14894317090511322\n","step: 570, loss: 0.08036339282989502\n","step: 580, loss: 0.08967354893684387\n","step: 590, loss: 0.061426423490047455\n","step: 600, loss: 0.14896869659423828\n","step: 610, loss: 0.12785686552524567\n","step: 620, loss: 0.0854756087064743\n","step: 630, loss: 0.17133592069149017\n","step: 640, loss: 0.07686730474233627\n","step: 650, loss: 0.1798185110092163\n","step: 660, loss: 0.08607083559036255\n","step: 670, loss: 0.0810394436120987\n","step: 680, loss: 0.1342959702014923\n","step: 690, loss: 0.04472212493419647\n","step: 700, loss: 0.07396135479211807\n","step: 710, loss: 0.09764543175697327\n","step: 720, loss: 0.14053195714950562\n","step: 730, loss: 0.18486955761909485\n","step: 740, loss: 0.06333625316619873\n","step: 750, loss: 0.16145892441272736\n","step: 760, loss: 0.11076239496469498\n","step: 770, loss: 0.03766069561243057\n","step: 780, loss: 0.08082018792629242\n","step: 790, loss: 0.019304197281599045\n","step: 800, loss: 0.11302857100963593\n","step: 810, loss: 0.1179954782128334\n","step: 820, loss: 0.14953462779521942\n","step: 830, loss: 0.06507153064012527\n","step: 840, loss: 0.0796232670545578\n","step: 850, loss: 0.18159468472003937\n","step: 860, loss: 0.18641625344753265\n","step: 870, loss: 0.1041291281580925\n","step: 880, loss: 0.053292859345674515\n","step: 890, loss: 0.18265321850776672\n","step: 900, loss: 0.07024713605642319\n","step: 910, loss: 0.08164290338754654\n","step: 920, loss: 0.08971904218196869\n","step: 930, loss: 0.09514503180980682\n","step: 940, loss: 0.059018999338150024\n","step: 950, loss: 0.11893238127231598\n","step: 960, loss: 0.059874799102544785\n","step: 970, loss: 0.19493699073791504\n","step: 980, loss: 0.19435086846351624\n","step: 990, loss: 0.1365749090909958\n","step: 1000, loss: 0.1292109340429306\n","step: 1010, loss: 0.04303983971476555\n","step: 1020, loss: 0.07678166031837463\n","step: 1030, loss: 0.1194353848695755\n","step: 1040, loss: 0.14423306286334991\n","step: 1050, loss: 0.12220229208469391\n","step: 1060, loss: 0.046194251626729965\n","step: 1070, loss: 0.09029525518417358\n","step: 1080, loss: 0.06218542158603668\n","step: 1090, loss: 0.2430078238248825\n","step: 1100, loss: 0.07877840846776962\n","step: 1110, loss: 0.06533078849315643\n","step: 1120, loss: 0.12608933448791504\n","step: 1130, loss: 0.06345143914222717\n","step: 1140, loss: 0.06331457942724228\n","step: 1150, loss: 0.11917132139205933\n","step: 1160, loss: 0.0697813332080841\n","step: 1170, loss: 0.07168138772249222\n","step: 1180, loss: 0.2018308788537979\n","step: 1190, loss: 0.067243292927742\n","step: 1200, loss: 0.09214301407337189\n","step: 1210, loss: 0.06750167161226273\n","step: 1220, loss: 0.1004340872168541\n","step: 1230, loss: 0.1062760055065155\n","step: 1240, loss: 0.10482155531644821\n","step: 1250, loss: 0.17521750926971436\n","step: 1260, loss: 0.16191846132278442\n","step: 1270, loss: 0.08085844665765762\n","step: 1280, loss: 0.06885810941457748\n","step: 1290, loss: 0.06525523960590363\n","step: 1300, loss: 0.17206229269504547\n","step: 1310, loss: 0.16562749445438385\n","step: 1320, loss: 0.17492730915546417\n","step: 1330, loss: 0.05112862214446068\n","step: 1340, loss: 0.05401286855340004\n","step: 1350, loss: 0.09206681698560715\n","step: 1360, loss: 0.020003115758299828\n","step: 1370, loss: 0.08753550052642822\n","step: 1380, loss: 0.15006668865680695\n","step: 1390, loss: 0.11117122322320938\n","step: 1400, loss: 0.06930206716060638\n","step: 1410, loss: 0.08390101790428162\n","step: 1420, loss: 0.06618456542491913\n","step: 1430, loss: 0.032554879784584045\n","step: 1440, loss: 0.12640291452407837\n","step: 1450, loss: 0.0894760936498642\n","step: 1460, loss: 0.057608067989349365\n","step: 1470, loss: 0.09025473147630692\n","step: 1480, loss: 0.07718401402235031\n","step: 1490, loss: 0.09632239490747452\n","step: 1500, loss: 0.11928103864192963\n","step: 1510, loss: 0.07144234329462051\n","step: 1520, loss: 0.04651781916618347\n","step: 1530, loss: 0.06540507823228836\n","step: 1540, loss: 0.03364717587828636\n","step: 1550, loss: 0.05780766159296036\n","step: 1560, loss: 0.14027956128120422\n","step: 1570, loss: 0.06411148607730865\n","step: 1580, loss: 0.06362806260585785\n","step: 1590, loss: 0.04416511580348015\n","step: 1600, loss: 0.06371399015188217\n","step: 1610, loss: 0.13019627332687378\n","step: 1620, loss: 0.07204380631446838\n","step: 1630, loss: 0.1903865486383438\n","step: 1640, loss: 0.05928618833422661\n","step: 1650, loss: 0.09214320033788681\n","step: 1660, loss: 0.07764045894145966\n","step: 1670, loss: 0.07962556928396225\n","step: 1680, loss: 0.11580356955528259\n","step: 1690, loss: 0.04920005053281784\n","step: 1700, loss: 0.04269653931260109\n","step: 1710, loss: 0.10065986216068268\n","step: 1720, loss: 0.19680437445640564\n","step: 1730, loss: 0.09310349822044373\n","step: 1740, loss: 0.06424504518508911\n","step: 1750, loss: 0.13495883345603943\n","step: 1760, loss: 0.21942545473575592\n","step: 1770, loss: 0.11403333395719528\n","step: 1780, loss: 0.08713428676128387\n","step: 1790, loss: 0.1522609442472458\n","step: 1800, loss: 0.1437656134366989\n","step: 1810, loss: 0.10023418068885803\n","step: 1820, loss: 0.1035241037607193\n","step: 1830, loss: 0.11556760221719742\n","step: 1840, loss: 0.045342568308115005\n","step: 1850, loss: 0.07079426944255829\n","step: 1860, loss: 0.07417649030685425\n","step: 1870, loss: 0.12043590843677521\n","step: 1880, loss: 0.09609702229499817\n","step: 1890, loss: 0.036420177668333054\n","step: 1900, loss: 0.06780776381492615\n","step: 1910, loss: 0.06608512252569199\n","step: 1920, loss: 0.13154199719429016\n","step: 1930, loss: 0.18275880813598633\n","step: 1940, loss: 0.0681830421090126\n","step: 1950, loss: 0.09178522229194641\n","step: 1960, loss: 0.12633870542049408\n","step: 1970, loss: 0.07950551807880402\n","step: 1980, loss: 0.10456619411706924\n","step: 1990, loss: 0.1779145747423172\n","step: 2000, loss: 0.05559948831796646\n","step: 2010, loss: 0.1627589613199234\n","step: 2020, loss: 0.13778743147850037\n","step: 2030, loss: 0.07349444180727005\n","step: 2040, loss: 0.02780434861779213\n","step: 2050, loss: 0.040112271904945374\n","step: 2060, loss: 0.05822620168328285\n","step: 2070, loss: 0.1588682383298874\n","step: 2080, loss: 0.11835677176713943\n","step: 2090, loss: 0.03646669536828995\n","step: 2100, loss: 0.020894663408398628\n","step: 2110, loss: 0.13226334750652313\n","step: 2120, loss: 0.1287803053855896\n","step: 2130, loss: 0.03659829497337341\n","step: 2140, loss: 0.1341060847043991\n","step: 2150, loss: 0.07235227525234222\n","step: 2160, loss: 0.14092285931110382\n","step: 2170, loss: 0.07433195412158966\n","step: 2180, loss: 0.059228070080280304\n","step: 2190, loss: 0.06454829126596451\n","step: 2200, loss: 0.07799702882766724\n","step: 2210, loss: 0.0930517166852951\n","step: 2220, loss: 0.06584103405475616\n","step: 2230, loss: 0.054703835397958755\n","step: 2240, loss: 0.0911809429526329\n","step: 2250, loss: 0.07260986417531967\n","step: 2260, loss: 0.04877296835184097\n","step: 2270, loss: 0.0908997505903244\n","step: 2280, loss: 0.11335528641939163\n","step: 2290, loss: 0.11615003645420074\n","step: 2300, loss: 0.1397424191236496\n","step: 2310, loss: 0.12167506664991379\n","step: 2320, loss: 0.031969036906957626\n","step: 2330, loss: 0.09010285139083862\n","step: 2340, loss: 0.13601653277873993\n","step: 2350, loss: 0.12355154007673264\n","step: 2360, loss: 0.06577810645103455\n","step: 2370, loss: 0.21238011121749878\n","step: 2380, loss: 0.09834401309490204\n","step: 2390, loss: 0.1469269096851349\n","step: 2400, loss: 0.10211996734142303\n","step: 2410, loss: 0.07526075839996338\n","step: 2420, loss: 0.14247727394104004\n","step: 2430, loss: 0.0884881243109703\n","step: 2440, loss: 0.05664269998669624\n","step: 2450, loss: 0.10152711719274521\n","step: 2460, loss: 0.05644693970680237\n","step: 2470, loss: 0.09995708614587784\n","step: 2480, loss: 0.09461008012294769\n","step: 2490, loss: 0.07070130109786987\n","step: 2500, loss: 0.11527670919895172\n","step: 2510, loss: 0.08293072879314423\n","step: 2520, loss: 0.04604187235236168\n","step: 2530, loss: 0.0665585994720459\n","step: 2540, loss: 0.07472047954797745\n","step: 2550, loss: 0.11012691259384155\n","step: 2560, loss: 0.05929827317595482\n","step: 2570, loss: 0.056764885783195496\n","step: 2580, loss: 0.1448405534029007\n","step: 2590, loss: 0.11070865392684937\n","step: 2600, loss: 0.15438878536224365\n","step: 2610, loss: 0.04280988126993179\n","step: 2620, loss: 0.04541224241256714\n","step: 2630, loss: 0.0865405723452568\n","step: 2640, loss: 0.09989864379167557\n","step: 2650, loss: 0.08043303340673447\n","step: 2660, loss: 0.03979187458753586\n","step: 2670, loss: 0.14193181693553925\n","step: 2680, loss: 0.09906868636608124\n","step: 2690, loss: 0.07132058590650558\n","step: 2700, loss: 0.09386275708675385\n","step: 2710, loss: 0.11015672236680984\n","step: 2720, loss: 0.08190076798200607\n","step: 2730, loss: 0.05357721820473671\n","step: 2740, loss: 0.05534090846776962\n","step: 2750, loss: 0.14569617807865143\n","step: 2760, loss: 0.04728950932621956\n","step: 2770, loss: 0.0958060771226883\n","step: 2780, loss: 0.04551306366920471\n","step: 2790, loss: 0.10598628968000412\n","step: 2800, loss: 0.11212901026010513\n","step: 2810, loss: 0.03730669245123863\n","step: 2820, loss: 0.04611804336309433\n","step: 2830, loss: 0.2437644749879837\n","step: 2840, loss: 0.08994725346565247\n","step: 2850, loss: 0.10236534476280212\n","step: 2860, loss: 0.07579810917377472\n","step: 2870, loss: 0.057914238423109055\n","step: 2880, loss: 0.15295632183551788\n","step: 2890, loss: 0.0651760846376419\n","step: 2900, loss: 0.040091484785079956\n","step: 2910, loss: 0.07408084720373154\n","step: 2920, loss: 0.051580846309661865\n","step: 2930, loss: 0.18915748596191406\n","step: 2940, loss: 0.12772859632968903\n","step: 2950, loss: 0.07008586823940277\n","step: 2960, loss: 0.1148722767829895\n","step: 2970, loss: 0.10885723680257797\n","step: 2980, loss: 0.17664118111133575\n","step: 2990, loss: 0.19917456805706024\n","step: 3000, loss: 0.07288046926259995\n","step: 3010, loss: 0.08849477022886276\n","step: 3020, loss: 0.10432662814855576\n","step: 3030, loss: 0.05931233614683151\n","step: 3040, loss: 0.02999127469956875\n","step: 3050, loss: 0.065520279109478\n","step: 3060, loss: 0.1296631544828415\n","step: 3070, loss: 0.05116834491491318\n","step: 3080, loss: 0.08130856603384018\n","step: 3090, loss: 0.20890672504901886\n","step: 3100, loss: 0.12363374978303909\n","step: 3110, loss: 0.09835842251777649\n","step: 3120, loss: 0.09171266853809357\n","step: 3130, loss: 0.07782226800918579\n","step: 3140, loss: 0.059873197227716446\n","step: 3150, loss: 0.05396202206611633\n","step: 3160, loss: 0.10891907662153244\n","step: 3170, loss: 0.09099726378917694\n","step: 3180, loss: 0.0658661350607872\n","step: 3190, loss: 0.05242517590522766\n","step: 3200, loss: 0.026287944987416267\n","step: 3210, loss: 0.099576935172081\n","step: 3220, loss: 0.15590037405490875\n","step: 3230, loss: 0.04390319064259529\n","step: 3240, loss: 0.07719658315181732\n","step: 3250, loss: 0.05946090817451477\n","step: 3260, loss: 0.11379880458116531\n","step: 3270, loss: 0.07655026018619537\n","step: 3280, loss: 0.06245148554444313\n","step: 3290, loss: 0.12025326490402222\n","step: 3300, loss: 0.13844700157642365\n","step: 3310, loss: 0.09769724309444427\n","step: 3320, loss: 0.10018827766180038\n","step: 3330, loss: 0.09182143211364746\n","step: 3340, loss: 0.09833304584026337\n","step: 3350, loss: 0.11414310336112976\n","step: 3360, loss: 0.08642951399087906\n","step: 3370, loss: 0.14207294583320618\n","step: 3380, loss: 0.06958531588315964\n","step: 3390, loss: 0.06415842473506927\n","step: 3400, loss: 0.09657174348831177\n","step: 3410, loss: 0.10528703778982162\n","step: 3420, loss: 0.07197485864162445\n","step: 3430, loss: 0.09318646043539047\n","step: 3440, loss: 0.050174012780189514\n","step: 3450, loss: 0.056566521525382996\n","step: 3460, loss: 0.06879077106714249\n","step: 3470, loss: 0.07728008925914764\n","step: 3480, loss: 0.08590646833181381\n","step: 3490, loss: 0.1041841134428978\n","step: 3500, loss: 0.10015727579593658\n","step: 3510, loss: 0.14098286628723145\n","step: 3520, loss: 0.10855276137590408\n","step: 3530, loss: 0.07908491045236588\n","step: 3540, loss: 0.031536515802145004\n","step: 3550, loss: 0.045122597366571426\n","step: 3560, loss: 0.05821229889988899\n","step: 3570, loss: 0.07842110842466354\n","step: 3580, loss: 0.07068423181772232\n","step: 3590, loss: 0.06271103024482727\n","step: 3600, loss: 0.05951281264424324\n","step: 3610, loss: 0.06727410852909088\n","step: 3620, loss: 0.013423962518572807\n","step: 3630, loss: 0.03209128603339195\n","step: 3640, loss: 0.10314740240573883\n","step: 3650, loss: 0.06287328898906708\n","step: 3660, loss: 0.15190939605236053\n","step: 3670, loss: 0.07921342551708221\n","step: 3680, loss: 0.04533207044005394\n","step: 3690, loss: 0.11842203140258789\n","step: 3700, loss: 0.16085299849510193\n","step: 3710, loss: 0.07800266891717911\n","step: 3720, loss: 0.062369782477617264\n","step: 3730, loss: 0.05393415689468384\n","step: 3740, loss: 0.12696221470832825\n","step: 3750, loss: 0.09529337286949158\n","step: 3760, loss: 0.06389173865318298\n","step: 3770, loss: 0.0464138463139534\n"]}]},{"cell_type":"code","source":["print(domain_precision_value_lst)\n","print(domain_recall_value_lst)\n","print(domain_f1_value_lst)\n","print(domain_acc_value_lst)\n","\n","print(acc_lst)\n","print(prob_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4XhS4coiznDG","executionInfo":{"status":"ok","timestamp":1669848763492,"user_tz":300,"elapsed":11,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"57e5e4ac-37b2-4921-d9da-07245a9f0c17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor(0.9466), tensor(0.9481), tensor(0.9471), tensor(0.9505), tensor(0.9502), tensor(0.9529), tensor(0.9476), tensor(0.9483), tensor(0.9488), tensor(0.9455)]\n","[tensor(0.9421), tensor(0.9422), tensor(0.9421), tensor(0.9466), tensor(0.9459), tensor(0.9471), tensor(0.9427), tensor(0.9436), tensor(0.9443), tensor(0.9394)]\n","[tensor(0.9392), tensor(0.9397), tensor(0.9402), tensor(0.9441), tensor(0.9432), tensor(0.9450), tensor(0.9403), tensor(0.9416), tensor(0.9427), tensor(0.9363)]\n","[tensor(0.9421), tensor(0.9422), tensor(0.9421), tensor(0.9466), tensor(0.9459), tensor(0.9471), tensor(0.9427), tensor(0.9436), tensor(0.9443), tensor(0.9394)]\n","[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95652174949646, 1.0, 0.8947368264198303, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.931034505367279, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883499145508, 1.0, 1.0, 0.9090909361839294, 1.0, 1.0, 0.9444444179534912, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769276618958, 0.8999999761581421, 1.0, 1.0, 1.0, 1.0, 1.0], [0.9342105388641357, 0.9672130942344666, 0.9473684430122375, 0.9589040875434875, 0.9230769276618958, 0.9726027250289917, 0.9523809552192688, 0.9624999761581421, 0.9411764740943909, 0.9591836929321289, 0.9387755393981934, 0.9577465057373047, 0.9487179517745972, 0.949999988079071, 0.9718309640884399, 0.914893627166748, 0.9523809552192688, 0.9473684430122375, 0.9636363387107849, 0.9411764740943909, 0.9718309640884399, 0.9777777791023254, 0.9710144996643066, 0.9714285731315613, 0.9558823704719543, 0.7692307829856873, 0.9577465057373047, 0.9599999785423279, 0.957446813583374, 0.9365079402923584, 0.9677419066429138, 0.95652174949646, 0.9722222089767456, 0.9142857193946838, 0.9523809552192688, 0.9583333134651184, 0.9487179517745972, 0.9607843160629272, 0.9487179517745972, 0.9750000238418579, 0.9591836929321289, 0.9672130942344666, 0.9642857313156128, 0.9473684430122375, 0.9523809552192688, 0.9523809552192688, 0.9428571462631226, 0.9591836929321289, 0.9599999785423279, 0.9428571462631226, 0.9607843160629272, 0.9399999976158142, 0.9375, 0.9583333134651184, 0.9056603908538818, 0.9487179517745972, 0.9275362491607666, 0.9090909361839294, 0.930232584476471, 0.9636363387107849, 0.9399999976158142, 0.9230769276618958, 0.9607843160629272, 0.8870967626571655, 0.9523809552192688, 0.9583333134651184, 0.9433962106704712, 0.9347826242446899, 0.970588207244873, 0.9487179517745972, 0.9523809552192688, 0.925000011920929, 0.9473684430122375, 0.9344262480735779, 0.9722222089767456, 0.9473684430122375, 0.9594594836235046, 0.9433962106704712, 0.9285714030265808, 0.95652174949646, 0.9534883499145508, 0.9444444179534912, 0.9420289993286133, 0.95652174949646, 0.9682539701461792, 0.9487179517745972, 0.925000011920929, 0.9230769276618958, 0.9402984976768494, 0.9701492786407471, 0.9318181872367859, 0.9454545378684998, 0.9047619104385376, 0.9130434989929199, 0.9111111164093018, 0.930232584476471, 0.9090909361839294, 0.95652174949646, 0.9487179517745972, 0.9285714030265808, 0.9682539701461792, 0.970588207244873], [0.9583333134651184, 0.9285714030265808, 0.9594594836235046, 0.9305555820465088, 0.970588207244873, 0.9666666388511658, 0.9726027250289917, 0.9552238583564758, 0.9759036302566528, 0.9819819927215576, 0.9733333587646484, 0.9642857313156128, 0.9777777791023254, 0.9193548560142517, 0.976190447807312, 0.9642857313156128, 0.9722222089767456, 0.9718309640884399, 0.9677419066429138, 0.9594594836235046, 0.9666666388511658, 0.9759036302566528, 0.9733333587646484, 0.9518072009086609, 0.9111111164093018, 0.9719626307487488, 0.9583333134651184, 0.9714285731315613, 0.954954981803894, 0.8777777552604675, 0.9345794320106506, 0.9583333134651184, 0.9777777791023254, 0.9583333134651184, 0.9726027250289917, 0.9736841917037964, 0.9729729890823364, 0.9722222089767456, 0.9777777791023254, 0.9819819927215576, 0.9622641801834106, 0.9714285731315613, 0.9722222089767456, 0.9583333134651184, 0.9819819927215576, 0.9558823704719543, 0.9729729890823364, 0.970588207244873, 0.9759036302566528, 0.9714285731315613, 0.9264705777168274, 0.9677419066429138, 0.9638554453849792, 0.9750000238418579, 0.9736841917037964, 0.9722222089767456, 0.9813084006309509, 0.9743589758872986, 0.9555555582046509, 0.9599999785423279, 0.9444444179534912, 0.9819819927215576, 0.9682539701461792, 0.9615384340286255, 0.970588207244873, 0.976190447807312, 0.9555555582046509, 0.9719626307487488, 0.9508196711540222, 0.9813084006309509, 0.9719626307487488, 0.9594594836235046, 0.9666666388511658, 0.9714285731315613, 0.9428571462631226, 0.9639639854431152, 0.9719626307487488, 0.9066666960716248, 0.9714285731315613, 0.9404761791229248, 0.9552238583564758, 0.9552238583564758, 0.9666666388511658, 0.9555555582046509, 0.9777777791023254, 0.9404761791229248, 0.9677419066429138, 0.976190447807312, 0.9577465057373047, 0.9583333134651184, 0.9722222089767456, 0.9813084006309509, 0.9722222089767456, 0.9714285731315613, 0.9777777791023254, 0.9743589758872986, 0.9571428298950195, 0.9523809552192688, 0.9729729890823364, 0.9444444179534912, 0.9714285731315613, 0.9743589758872986], [0.9813084006309509, 0.9666666388511658, 0.9599999785423279, 0.9813084006309509, 0.9729729890823364, 0.9729729890823364, 0.9639639854431152, 0.9639639854431152, 0.9375, 0.9626168012619019, 0.976190447807312, 0.5777778029441833, 0.9729729890823364, 0.9439252614974976, 0.9626168012619019, 0.9729729890823364, 0.9729729890823364, 0.9523809552192688, 0.9666666388511658, 0.9639639854431152, 0.9813084006309509, 0.9719626307487488, 0.9666666388511658, 0.9813084006309509, 0.9444444179534912, 0.9819819927215576, 0.9729729890823364, 0.9719626307487488, 0.9639639854431152, 0.773809552192688, 0.954954981803894, 0.9439252614974976, 0.9719626307487488, 0.9666666388511658, 0.9719626307487488, 0.9189189076423645, 0.9624999761581421, 0.9729729890823364, 0.954954981803894, 0.9819819927215576, 0.9555555582046509, 0.954954981803894, 0.9719626307487488, 0.954954981803894, 0.9729729890823364, 0.954954981803894, 0.9333333373069763, 0.9719626307487488, 0.9626168012619019, 0.9333333373069763, 0.9397590160369873, 0.9729729890823364, 0.9626168012619019, 0.9369369149208069, 0.9813084006309509, 0.9626168012619019, 0.9759036302566528, 0.9252336621284485, 0.9666666388511658, 0.9729729890823364, 0.9819819927215576, 0.9719626307487488, 0.9719626307487488, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9605262875556946, 0.9759036302566528, 0.9777777791023254, 0.9158878326416016, 0.9777777791023254, 0.9729729890823364, 0.9666666388511658, 0.9759036302566528, 0.9639639854431152, 0.9555555582046509, 0.9813084006309509, 0.9666666388511658, 0.9639639854431152, 0.9666666388511658, 0.9345794320106506, 0.9813084006309509, 0.9599999785423279, 0.9719626307487488, 0.9719626307487488, 0.9819819927215576, 0.9487179517745972, 0.9439252614974976, 0.9819819927215576, 0.9666666388511658, 0.9626168012619019, 0.9813084006309509, 0.9639639854431152, 0.954954981803894, 0.9819819927215576, 0.976190447807312, 0.9729729890823364, 0.9819819927215576, 0.9444444179534912, 0.9813084006309509, 0.9626168012619019, 0.9555555582046509], [0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 0.9639639854431152, 0.9459459185600281, 0.9639639854431152, 0.9369369149208069, 0.9909909963607788, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9909909963607788, 1.0, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.954954981803894, 0.9729729890823364, 0.9729729890823364, 0.9459459185600281, 0.9099099040031433, 0.9729729890823364, 1.0, 0.9459459185600281, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.9459459185600281, 0.9639639854431152, 1.0, 0.9909909963607788, 0.9819819927215576, 0.9639639854431152, 0.9909909963607788, 0.9909909963607788, 1.0, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9639639854431152, 0.9909909963607788, 1.0, 0.9819819927215576, 0.9459459185600281, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9279279112815857, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9729729890823364, 0.9279279112815857, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9729729890823364, 0.9009009003639221, 0.9279279112815857, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9369369149208069, 0.8918918967247009, 0.9819819927215576, 0.9729729890823364, 1.0, 0.9909909963607788, 0.9729729890823364, 0.9819819927215576, 0.9909909963607788, 0.9099099040031433, 0.9909909963607788, 1.0, 0.9369369149208069, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9279279112815857, 1.0, 0.9729729890823364, 1.0, 0.9189189076423645, 1.0, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9459459185600281, 1.0, 0.9909909963607788, 0.9729729890823364, 0.9459459185600281, 0.9819819927215576, 0.9909909963607788], [0.9279279112815857, 0.9099099040031433, 0.7747747898101807, 0.8288288116455078, 0.7387387156486511, 0.4864864945411682, 0.6486486196517944, 0.5045045018196106, 0.630630612373352, 0.6486486196517944, 0.7837837934494019, 0.5585585832595825, 0.7657657861709595, 0.6216216087341309, 0.7657657861709595, 0.522522509098053, 0.5675675868988037, 0.477477490901947, 0.4954954981803894, 0.6126126050949097, 0.5045045018196106, 0.6216216087341309, 0.5675675868988037, 0.5765765905380249, 0.5585585832595825, 0.6126126050949097, 0.5315315127372742, 0.45045045018196106, 0.4864864945411682, 0.44144144654273987, 0.342342346906662, 0.342342346906662, 0.44144144654273987, 0.5495495200157166, 0.46846845746040344, 0.6216216087341309, 0.45045045018196106, 0.4324324429035187, 0.46846845746040344, 0.37837839126586914, 0.4054054021835327, 0.3963963985443115, 0.477477490901947, 0.4054054021835327, 0.3333333432674408, 0.5135135054588318, 0.38738739490509033, 0.38738739490509033, 0.5585585832595825, 0.3243243098258972, 0.4234234094619751, 0.4864864945411682, 0.37837839126586914, 0.3243243098258972, 0.37837839126586914, 0.36936935782432556, 0.315315306186676, 0.3513513505458832, 0.3513513505458832, 0.36036035418510437, 0.38738739490509033, 0.28828829526901245, 0.45945945382118225, 0.4864864945411682, 0.3243243098258972, 0.3513513505458832, 0.3513513505458832, 0.3333333432674408, 0.342342346906662, 0.29729729890823364, 0.3963963985443115, 0.30630630254745483, 0.29729729890823364, 0.38738739490509033, 0.3243243098258972, 0.4144144058227539, 0.4054054021835327, 0.36936935782432556, 0.342342346906662, 0.3513513505458832, 0.36036035418510437, 0.3513513505458832, 0.4144144058227539, 0.36036035418510437, 0.315315306186676, 0.4054054021835327, 0.342342346906662, 0.3243243098258972, 0.27927929162979126, 0.3513513505458832, 0.38738739490509033, 0.4324324429035187, 0.315315306186676, 0.2702702581882477, 0.36036035418510437, 0.38738739490509033, 0.3333333432674408, 0.315315306186676, 0.3513513505458832, 0.3963963985443115, 0.3243243098258972, 0.36036035418510437], [0.09909909963607788, 0.045045044273138046, 0.06306306272745132, 0.036036036908626556, 0.018018018454313278, 0.027027027681469917, 0.1621621549129486, 0.036036036908626556, 0.171171173453331, 0.2522522509098053, 0.06306306272745132, 0.22522522509098053, 0.10810811072587967, 0.054054055362939835, 0.06306306272745132, 0.11711711436510086, 0.13513512909412384, 0.09009008854627609, 0.21621622145175934, 0.15315315127372742, 0.2432432472705841, 0.0810810774564743, 0.0810810774564743, 0.171171173453331, 0.28828829526901245, 0.21621622145175934, 0.18018017709255219, 0.2522522509098053, 0.09909909963607788, 0.027027027681469917, 0.09009008854627609, 0.22522522509098053, 0.11711711436510086, 0.12612612545490265, 0.10810811072587967, 0.22522522509098053, 0.09909909963607788, 0.09909909963607788, 0.18918919563293457, 0.15315315127372742, 0.22522522509098053, 0.22522522509098053, 0.21621622145175934, 0.12612612545490265, 0.15315315127372742, 0.09009008854627609, 0.07207207381725311, 0.15315315127372742, 0.0810810774564743, 0.21621622145175934, 0.12612612545490265, 0.22522522509098053, 0.09909909963607788, 0.28828829526901245, 0.0810810774564743, 0.09909909963607788, 0.20720720291137695, 0.11711711436510086, 0.21621622145175934, 0.11711711436510086, 0.19819819927215576, 0.12612612545490265, 0.09009008854627609, 0.22522522509098053, 0.054054055362939835, 0.30630630254745483, 0.21621622145175934, 0.171171173453331, 0.13513512909412384, 0.045045044273138046, 0.15315315127372742, 0.1621621549129486, 0.12612612545490265, 0.2702702581882477, 0.20720720291137695, 0.2702702581882477, 0.11711711436510086, 0.0810810774564743, 0.15315315127372742, 0.20720720291137695, 0.23423422873020172, 0.10810811072587967, 0.09009008854627609, 0.20720720291137695, 0.09009008854627609, 0.171171173453331, 0.09009008854627609, 0.12612612545490265, 0.09909909963607788, 0.0810810774564743, 0.23423422873020172, 0.23423422873020172, 0.09909909963607788, 0.15315315127372742, 0.2522522509098053, 0.18918919563293457, 0.1621621549129486, 0.2432432472705841, 0.15315315127372742, 0.15315315127372742, 0.045045044273138046, 0.18918919563293457], [0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.954954981803894, 0.954954981803894, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9639639854431152, 0.9729729890823364, 0.9729729890823364, 0.954954981803894, 0.9639639854431152, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9459459185600281, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9459459185600281, 0.9639639854431152, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9459459185600281, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.954954981803894, 0.9459459185600281, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.954954981803894, 0.9819819927215576, 0.9819819927215576], [0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.954954981803894, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.954954981803894, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9639639854431152, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 1.0, 0.9729729890823364, 0.9909909963607788, 0.9729729890823364]]\n","[[0.999798059463501, 0.9997609257698059, 0.9997243881225586, 0.9996716380119324, 0.9996636509895325, 0.9996532201766968, 0.9995818138122559, 0.9995136857032776, 0.9994621276855469, 0.9994298815727234, 0.9994193911552429, 0.9992927312850952, 0.9991897344589233, 0.999162495136261, 0.9990848898887634, 0.9990408420562744, 0.9990352392196655, 0.9990233182907104, 0.9989814758300781, 0.9989410042762756, 0.9989268779754639, 0.9989258050918579, 0.9989162683486938, 0.9988672733306885, 0.9988577961921692, 0.9988387227058411, 0.9987951517105103, 0.9987403750419617, 0.9985683560371399, 0.9985527992248535, 0.9985092878341675, 0.9984806180000305, 0.9984214305877686, 0.9983848929405212, 0.9983698725700378, 0.9983627200126648, 0.9983537197113037, 0.9983531832695007, 0.9982770681381226, 0.9982223510742188, 0.9982121586799622, 0.9981723427772522, 0.9981693625450134, 0.9981638789176941, 0.9981609582901001, 0.9981470108032227, 0.998144268989563, 0.9981099963188171, 0.9980669617652893, 0.9980605840682983, 0.9980602264404297, 0.9980195760726929, 0.9980186820030212, 0.9980130791664124, 0.9979899525642395, 0.997988760471344, 0.997943103313446, 0.9978846311569214, 0.9978086352348328, 0.9978045225143433, 0.9977808594703674, 0.9977502822875977, 0.9977333545684814, 0.9976773858070374, 0.9976698756217957, 0.9976546168327332, 0.9976219534873962, 0.9976049065589905, 0.9975472688674927, 0.9975295066833496, 0.9974830150604248, 0.9974546432495117, 0.9974456429481506, 0.9974451065063477, 0.997439980506897, 0.9974192976951599, 0.997401237487793, 0.9973815083503723, 0.9973633885383606, 0.9973442554473877, 0.9972391128540039, 0.9971839189529419, 0.9971553087234497, 0.9971469640731812, 0.9971455931663513, 0.9971393346786499, 0.9970966577529907, 0.9970418214797974, 0.9970322251319885, 0.9969974756240845, 0.9969155788421631, 0.9968834519386292, 0.9968796372413635, 0.9968288540840149, 0.996789276599884, 0.9966727495193481, 0.9966233968734741, 0.9966047406196594, 0.996592104434967, 0.9965866804122925, 0.9965119957923889, 0.9965052008628845], [0.9817234873771667, 0.9785272479057312, 0.9776372313499451, 0.9764548540115356, 0.9757897257804871, 0.9756594896316528, 0.9752596020698547, 0.9750471115112305, 0.9744119048118591, 0.9741156697273254, 0.9739365577697754, 0.9731894731521606, 0.9729788303375244, 0.9728564023971558, 0.9725017547607422, 0.9723671674728394, 0.9723227620124817, 0.9722226858139038, 0.9716639518737793, 0.9716222286224365, 0.9715116024017334, 0.9712647199630737, 0.9710476398468018, 0.9707461595535278, 0.9707195162773132, 0.9705784916877747, 0.9702393412590027, 0.9698932766914368, 0.9696681499481201, 0.969649076461792, 0.9693778157234192, 0.9691499471664429, 0.9688546657562256, 0.9686205387115479, 0.9684120416641235, 0.9681845903396606, 0.9675257802009583, 0.9672830700874329, 0.967068612575531, 0.9667770266532898, 0.9667015671730042, 0.9666045308113098, 0.9664918780326843, 0.9664787650108337, 0.966399073600769, 0.9663869142532349, 0.9660936594009399, 0.9659124612808228, 0.965833306312561, 0.9658108353614807, 0.9657067656517029, 0.9656558036804199, 0.9652805328369141, 0.9652679562568665, 0.9649048447608948, 0.9648956060409546, 0.964782178401947, 0.9644887447357178, 0.9644779562950134, 0.9643340110778809, 0.964260458946228, 0.9642398357391357, 0.9641810655593872, 0.9641528129577637, 0.9641153216362, 0.9637807011604309, 0.9636768102645874, 0.9636565446853638, 0.9636304378509521, 0.9635757803916931, 0.9635730981826782, 0.963456928730011, 0.9633830189704895, 0.9632798433303833, 0.9632640480995178, 0.9632226824760437, 0.9632042646408081, 0.9631358981132507, 0.9629944562911987, 0.9629855155944824, 0.962975263595581, 0.9629319310188293, 0.9628963470458984, 0.9627582430839539, 0.9624484181404114, 0.9623445272445679, 0.9622284173965454, 0.962142288684845, 0.9617661237716675, 0.9616733193397522, 0.961666464805603, 0.9614667892456055, 0.9614167809486389, 0.961249828338623, 0.9612241387367249, 0.9611873030662537, 0.9611416459083557, 0.9611138701438904, 0.9610026478767395, 0.9609654545783997, 0.9608424305915833, 0.9607900977134705], [0.9796269536018372, 0.9779704213142395, 0.9765051007270813, 0.9752669930458069, 0.9749463200569153, 0.9747986793518066, 0.9729233980178833, 0.9728026986122131, 0.9721291065216064, 0.9706289768218994, 0.9704241752624512, 0.970364511013031, 0.9701799154281616, 0.9699467420578003, 0.9696168303489685, 0.969573974609375, 0.969325602054596, 0.9691293239593506, 0.9690230488777161, 0.9689098000526428, 0.968425989151001, 0.9683214426040649, 0.9681045413017273, 0.9680607914924622, 0.9678324460983276, 0.9675041437149048, 0.9674469828605652, 0.9670031070709229, 0.9664599299430847, 0.9653291702270508, 0.9652350544929504, 0.9650313258171082, 0.9645279049873352, 0.9645137786865234, 0.9644918441772461, 0.9644035696983337, 0.9642328023910522, 0.9641045331954956, 0.9640594720840454, 0.9640561938285828, 0.9640471935272217, 0.9640272855758667, 0.9639513492584229, 0.9638761878013611, 0.9638166427612305, 0.9636589288711548, 0.9634768962860107, 0.9632647037506104, 0.9631083607673645, 0.9630898833274841, 0.9630668759346008, 0.9629735350608826, 0.9627017974853516, 0.9626386761665344, 0.9625383615493774, 0.9625263214111328, 0.9624972939491272, 0.9623945951461792, 0.9622145891189575, 0.9619469046592712, 0.9619041085243225, 0.961887776851654, 0.9617716073989868, 0.9616421461105347, 0.9616131782531738, 0.9616064429283142, 0.9613900780677795, 0.9612351059913635, 0.9611502885818481, 0.961097240447998, 0.960878312587738, 0.9608434438705444, 0.9605333805084229, 0.9604579210281372, 0.9604002833366394, 0.9603492617607117, 0.960338830947876, 0.9601511359214783, 0.9600471258163452, 0.9598530530929565, 0.9598486423492432, 0.9597331881523132, 0.9597138166427612, 0.9595560431480408, 0.9595353007316589, 0.9594456553459167, 0.9594177603721619, 0.959350049495697, 0.9593121409416199, 0.9591972827911377, 0.9590803980827332, 0.9590479731559753, 0.9590256810188293, 0.9589102864265442, 0.9587776064872742, 0.9587411880493164, 0.9586794376373291, 0.9586426019668579, 0.9585512280464172, 0.9585363864898682, 0.9585250616073608, 0.9584994316101074], [0.982528567314148, 0.9816603660583496, 0.9746264815330505, 0.9739543199539185, 0.9737696051597595, 0.9732950329780579, 0.9727626442909241, 0.9703720808029175, 0.9697786569595337, 0.9694781303405762, 0.9688504338264465, 0.9683973789215088, 0.9683190584182739, 0.9676313400268555, 0.9675401449203491, 0.967525064945221, 0.9673555493354797, 0.966675341129303, 0.9666330218315125, 0.9666058421134949, 0.966178834438324, 0.9659404158592224, 0.9658769369125366, 0.9657328724861145, 0.9653806686401367, 0.9651960730552673, 0.9650385975837708, 0.9646810293197632, 0.9646472930908203, 0.9644439220428467, 0.9641814231872559, 0.9634606242179871, 0.9634435176849365, 0.9633919596672058, 0.9632967710494995, 0.9631830453872681, 0.9630329012870789, 0.9625131487846375, 0.9618176817893982, 0.9616665244102478, 0.961598813533783, 0.9613313674926758, 0.9610714316368103, 0.9609618186950684, 0.9609208703041077, 0.9607269763946533, 0.9607110023498535, 0.9606322646141052, 0.9604658484458923, 0.960253894329071, 0.9596662521362305, 0.9594911932945251, 0.9594327211380005, 0.9592575430870056, 0.9591984152793884, 0.9591659307479858, 0.9591086506843567, 0.9589337706565857, 0.958751916885376, 0.9587112069129944, 0.9586741328239441, 0.9585857391357422, 0.9583753943443298, 0.9582207202911377, 0.9579774141311646, 0.9579231142997742, 0.9578258991241455, 0.9577410817146301, 0.9575625658035278, 0.9573559165000916, 0.9573303461074829, 0.9568030834197998, 0.956739068031311, 0.9565786123275757, 0.956463098526001, 0.9561796188354492, 0.9561334848403931, 0.9559201002120972, 0.955864429473877, 0.9557777643203735, 0.9556957483291626, 0.9556658267974854, 0.9555184841156006, 0.9552361369132996, 0.9550356268882751, 0.9550142288208008, 0.9548836946487427, 0.9546293020248413, 0.9545192122459412, 0.9545065760612488, 0.9544780254364014, 0.9543478488922119, 0.95427405834198, 0.9542123675346375, 0.954182505607605, 0.9540306925773621, 0.9538965821266174, 0.9538132548332214, 0.9537861943244934, 0.9536778926849365, 0.953654408454895, 0.953607439994812], [0.9962618947029114, 0.9962359666824341, 0.9940988421440125, 0.9940952658653259, 0.9939370155334473, 0.9933323860168457, 0.993274450302124, 0.9929867386817932, 0.9929836392402649, 0.9926305413246155, 0.9924961924552917, 0.9921882152557373, 0.9918815493583679, 0.9918420314788818, 0.9918159246444702, 0.9917689561843872, 0.9917600750923157, 0.991748571395874, 0.9916809797286987, 0.9916248321533203, 0.9915640950202942, 0.9915551543235779, 0.991539716720581, 0.9915123581886292, 0.9915103912353516, 0.9914835095405579, 0.9914689064025879, 0.9914180040359497, 0.9914093613624573, 0.991401195526123, 0.9913724660873413, 0.9913482069969177, 0.9913210272789001, 0.9912845492362976, 0.9912844896316528, 0.9912833571434021, 0.991249680519104, 0.9912381768226624, 0.9911937713623047, 0.9911912083625793, 0.9911167621612549, 0.9910718202590942, 0.9910430312156677, 0.9910363554954529, 0.9910348653793335, 0.9910162687301636, 0.9909998774528503, 0.990997314453125, 0.9909651279449463, 0.9909480810165405, 0.9909345507621765, 0.9909231662750244, 0.9909182786941528, 0.9909132122993469, 0.9908819794654846, 0.9908693432807922, 0.9908515810966492, 0.9908245205879211, 0.9908155798912048, 0.9908043146133423, 0.9907421469688416, 0.9907311797142029, 0.9907229542732239, 0.9907078146934509, 0.9906930923461914, 0.9906731843948364, 0.9906308054924011, 0.9906076788902283, 0.9905974864959717, 0.9904988408088684, 0.9904861450195312, 0.9904653429985046, 0.9904295206069946, 0.9904133081436157, 0.9903989434242249, 0.9903278350830078, 0.9902737140655518, 0.9902559518814087, 0.9902554750442505, 0.9902251362800598, 0.9902238845825195, 0.990131139755249, 0.9900739789009094, 0.9900566935539246, 0.9900454878807068, 0.9900386333465576, 0.9899905920028687, 0.9899823069572449, 0.9899600744247437, 0.9899414777755737, 0.9899335503578186, 0.989916980266571, 0.9899077415466309, 0.989899218082428, 0.9898672699928284, 0.9898489117622375, 0.9898424744606018, 0.9897879958152771, 0.9897738695144653, 0.9897618293762207, 0.9897324442863464, 0.9896923303604126], [0.9450442790985107, 0.8900070786476135, 0.8794874548912048, 0.8765034079551697, 0.8563343286514282, 0.8449638485908508, 0.8381963968276978, 0.8345248699188232, 0.8335363864898682, 0.8257083296775818, 0.8199054598808289, 0.8173989057540894, 0.8154930472373962, 0.8099790215492249, 0.8079418540000916, 0.8077139854431152, 0.8076819181442261, 0.8033620715141296, 0.7989152669906616, 0.7922243475914001, 0.7913991808891296, 0.7883793711662292, 0.7875060439109802, 0.7834787368774414, 0.7808229327201843, 0.7797859907150269, 0.7778797149658203, 0.776918351650238, 0.7697858810424805, 0.7682508826255798, 0.7681737542152405, 0.7681737542152405, 0.7671624422073364, 0.764695942401886, 0.763361394405365, 0.7623496055603027, 0.7622898817062378, 0.7612999081611633, 0.7610208988189697, 0.7594681978225708, 0.7554068565368652, 0.755370020866394, 0.7548799514770508, 0.751437783241272, 0.748779296875, 0.7487309575080872, 0.7459191679954529, 0.745694637298584, 0.7422711253166199, 0.7418997287750244, 0.7416740655899048, 0.7397549152374268, 0.7369393110275269, 0.7366416454315186, 0.7343281507492065, 0.7333763241767883, 0.7332356572151184, 0.7331897616386414, 0.7327280640602112, 0.7314372658729553, 0.7310234308242798, 0.7309238910675049, 0.7303101420402527, 0.7302184104919434, 0.7293860912322998, 0.7264994978904724, 0.7264994978904724, 0.7257937788963318, 0.7249041199684143, 0.7248242497444153, 0.724549412727356, 0.7241301536560059, 0.7236127257347107, 0.7235265374183655, 0.722653865814209, 0.7225748896598816, 0.7213932871818542, 0.7210787534713745, 0.720945417881012, 0.7199737429618835, 0.7193097472190857, 0.7188809514045715, 0.7176252007484436, 0.7174767851829529, 0.717387855052948, 0.7173866033554077, 0.7170433402061462, 0.7163699865341187, 0.7156012654304504, 0.7155174612998962, 0.7150073647499084, 0.7148811221122742, 0.7144075632095337, 0.714149534702301, 0.713483989238739, 0.7128716707229614, 0.7126681208610535, 0.7116051316261292, 0.7115867137908936, 0.7115576267242432, 0.711024820804596, 0.7103127241134644], [0.9914625287055969, 0.9913865923881531, 0.9910546541213989, 0.9890826940536499, 0.9881108999252319, 0.9875422716140747, 0.9872554540634155, 0.9870415329933167, 0.9870398640632629, 0.9868173599243164, 0.9865642786026001, 0.9865396618843079, 0.9865391254425049, 0.9863222241401672, 0.9862241148948669, 0.9859433770179749, 0.9858052134513855, 0.9857724905014038, 0.9857711791992188, 0.9857069253921509, 0.9855886697769165, 0.9855689406394958, 0.9855689406394958, 0.9855484366416931, 0.9854944348335266, 0.9854599833488464, 0.9854305982589722, 0.985424816608429, 0.9853605628013611, 0.9850888848304749, 0.9850178360939026, 0.9849340915679932, 0.9848824739456177, 0.9848802089691162, 0.9848375916481018, 0.9848281145095825, 0.9847772121429443, 0.9847543835639954, 0.9847471117973328, 0.98466956615448, 0.9845507740974426, 0.9845468401908875, 0.9845350384712219, 0.9845187664031982, 0.9844965934753418, 0.9844658374786377, 0.9843281507492065, 0.9843224883079529, 0.9843153953552246, 0.9842690229415894, 0.9842603802680969, 0.9842472672462463, 0.9842242002487183, 0.9841914772987366, 0.9841830730438232, 0.9839462637901306, 0.9839314818382263, 0.9839127063751221, 0.9839079976081848, 0.9837791323661804, 0.9837789535522461, 0.9837646484375, 0.9837576150894165, 0.9837507009506226, 0.9837449789047241, 0.9836942553520203, 0.9836573004722595, 0.9836475253105164, 0.9836066961288452, 0.9836016297340393, 0.9835909605026245, 0.9835889935493469, 0.9835307002067566, 0.9834864735603333, 0.9834781289100647, 0.9834458231925964, 0.9834340214729309, 0.9833908677101135, 0.9833354949951172, 0.983235239982605, 0.9832062125205994, 0.9832049012184143, 0.9831846952438354, 0.9831750392913818, 0.9831539988517761, 0.9831278324127197, 0.983002245426178, 0.9829297661781311, 0.9828432202339172, 0.9827500581741333, 0.9827188849449158, 0.9827188849449158, 0.9826750755310059, 0.9826740026473999, 0.9826594591140747, 0.9826410412788391, 0.9826366901397705, 0.9826234579086304, 0.9825868606567383, 0.9825079441070557, 0.982452392578125, 0.9824007153511047], [0.9933387637138367, 0.9932316541671753, 0.9923577308654785, 0.9922049641609192, 0.991722047328949, 0.99170982837677, 0.9915115833282471, 0.9913317561149597, 0.9910642504692078, 0.9908437728881836, 0.9905516505241394, 0.9904618859291077, 0.9900728464126587, 0.9900256991386414, 0.9898050427436829, 0.9897936582565308, 0.9896578192710876, 0.9896389842033386, 0.9896085858345032, 0.9894849061965942, 0.9893649220466614, 0.9893649220466614, 0.9893522262573242, 0.9893458485603333, 0.9892410635948181, 0.9891753196716309, 0.9891207814216614, 0.9891207814216614, 0.9890836477279663, 0.9890491366386414, 0.9890003204345703, 0.9888866543769836, 0.9888659119606018, 0.9888610243797302, 0.9888567328453064, 0.9887605905532837, 0.9886168241500854, 0.9885454773902893, 0.9885188341140747, 0.9884737133979797, 0.9884617328643799, 0.9884608387947083, 0.9883822798728943, 0.9883486032485962, 0.9880001544952393, 0.9879890084266663, 0.9879744648933411, 0.9878502488136292, 0.987819492816925, 0.9876888394355774, 0.9876157641410828, 0.9875303506851196, 0.9875005483627319, 0.9874879121780396, 0.9873949885368347, 0.9873784780502319, 0.9873462319374084, 0.9873121380805969, 0.9872682690620422, 0.9871925115585327, 0.9871401190757751, 0.9871379137039185, 0.9871112704277039, 0.9870783090591431, 0.9870628118515015, 0.9868687987327576, 0.9868627190589905, 0.9868147969245911, 0.9866924285888672, 0.9865671396255493, 0.9865587949752808, 0.9864859580993652, 0.9864835739135742, 0.9864541888237, 0.9864044189453125, 0.9864006638526917, 0.9862088561058044, 0.9861986041069031, 0.9860697984695435, 0.9860434532165527, 0.9860214591026306, 0.9860170483589172, 0.9859792590141296, 0.9859781265258789, 0.9859337210655212, 0.9859290719032288, 0.9859133362770081, 0.9858909845352173, 0.9858599305152893, 0.9858450889587402, 0.9857838749885559, 0.9857778549194336, 0.9857764840126038, 0.985692024230957, 0.985539972782135, 0.9855263233184814, 0.9854715466499329, 0.9853859543800354, 0.9853073954582214, 0.9852293729782104, 0.9852196574211121, 0.9851891994476318], [0.9979798793792725, 0.9979432225227356, 0.9975451827049255, 0.9973692893981934, 0.9971492290496826, 0.9971070289611816, 0.9966923594474792, 0.9965962171554565, 0.9964686036109924, 0.9963430166244507, 0.9963186383247375, 0.9957948327064514, 0.9957292675971985, 0.9956396818161011, 0.9956262111663818, 0.9956238865852356, 0.9955843687057495, 0.9954198598861694, 0.9953758120536804, 0.9953103065490723, 0.9950469136238098, 0.9950356483459473, 0.9949610233306885, 0.9949295520782471, 0.9946702718734741, 0.9946547746658325, 0.9944997429847717, 0.9942606091499329, 0.9940365552902222, 0.993988037109375, 0.993751585483551, 0.993404746055603, 0.9933041930198669, 0.993100643157959, 0.9930046200752258, 0.9929131269454956, 0.9925493001937866, 0.9925011396408081, 0.9924747943878174, 0.9924297332763672, 0.9924280643463135, 0.9920867085456848, 0.9920803904533386, 0.9919462203979492, 0.991753876209259, 0.9917145371437073, 0.9916725754737854, 0.9916725754737854, 0.991528332233429, 0.9915065169334412, 0.9914698004722595, 0.9914343357086182, 0.9913005828857422, 0.99126136302948, 0.9912552237510681, 0.9910513162612915, 0.9909546375274658, 0.9908639788627625, 0.9908109903335571, 0.9906459450721741, 0.9906203746795654, 0.9905369877815247, 0.9904628396034241, 0.9904350638389587, 0.9903453588485718, 0.9902966022491455, 0.9900204539299011, 0.9898535013198853, 0.989772379398346, 0.9897451400756836, 0.9896813035011292, 0.989677906036377, 0.9896506667137146, 0.9896453022956848, 0.9896261096000671, 0.9895188808441162, 0.9895167946815491, 0.9895046949386597, 0.9894501566886902, 0.989448070526123, 0.9893593788146973, 0.9892783164978027, 0.9891482591629028, 0.9890274405479431, 0.9889963269233704, 0.9889777898788452, 0.9888536334037781, 0.9886375665664673, 0.9884473085403442, 0.9884059429168701, 0.9879454374313354, 0.9878860116004944, 0.9877198338508606, 0.9875537753105164, 0.9874809384346008, 0.9874266386032104, 0.9873436093330383, 0.9873360395431519, 0.9873173236846924, 0.9872413277626038, 0.9872055053710938, 0.987018346786499]]\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"g4ZuWMYTojmJ","executionInfo":{"status":"ok","timestamp":1669867854875,"user_tz":300,"elapsed":3,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"],"metadata":{"id":"jJxcl1cLn7rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"EvkSQ18M7p2H","executionInfo":{"status":"ok","timestamp":1669867853608,"user_tz":300,"elapsed":838,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"],"metadata":{"id":"6ryA3bq0ot4b","executionInfo":{"status":"ok","timestamp":1669867854875,"user_tz":300,"elapsed":1268,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"O2DFUhCDorSf","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1669848766350,"user_tz":300,"elapsed":450,"user":{"displayName":"Qiran Li","userId":"18174922520351617568"}},"outputId":"174e79fd-09e6-4c6f-d578-89a24c207799"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"02f7a519-47d3-41f4-8984-8c29d413cdb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"02f7a519-47d3-41f4-8984-8c29d413cdb6\")) {                    Plotly.newPlot(                        \"02f7a519-47d3-41f4-8984-8c29d413cdb6\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9466208219528198,0.9480803608894348,0.9471018314361572,0.9504967927932739,0.9502373337745667,0.9529371857643127,0.9476420879364014,0.9483458399772644,0.9487507343292236,0.9455497860908508],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9420946836471558,0.9421931505203247,0.9420946836471558,0.946575403213501,0.9458860754966736,0.9470678567886353,0.9427347183227539,0.9435718059539795,0.9442611932754517,0.9394357204437256],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9391671419143677,0.9396697878837585,0.9402294158935547,0.9441280364990234,0.9432413578033447,0.9449554681777954,0.940330445766449,0.9416399002075195,0.9427462816238403,0.9363406300544739],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('02f7a519-47d3-41f4-8984-8c29d413cdb6');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["file_name = f\"topn_{factor}%_domain_{domain}\"\n","scratch_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/model\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","test_metric.to_csv(os.path.join(log_model_dir, file_name) + '.csv')\n","\n","torch.save(model.state_dict(), os.path.join(scratch_model_dir, file_name))"],"metadata":{"id":"PLs82qPoc_TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# file_name\n","factor = 10"],"metadata":{"id":"2Ngux9I-orPX","executionInfo":{"status":"ok","timestamp":1669867854875,"user_tz":300,"elapsed":2,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"F4EEvQElcFbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K3loNHB3cFd7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KmwCM9nFcFgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"058IWT0fcFj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C239hLBrcFm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"muJHU49GcFo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CuGf8TP5cFsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cN63klIkcGdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jm0luGYncGf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc_lst = []\n","prob_lst = []\n","\n","factor_list = [1, 2, 5, 10, 20]\n","factor = factor_list[3] #  to be modified\n","topn = round(factor * len(domain_dev_word_lst) / 100)\n","\n","i = 0\n","while len(domain_dev_word_lst) >= topn:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","  print(\"domain_dev_word_lst\", len(domain_dev_word_lst))\n","\n","  if i == 1:\n","    domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=False,\n","                                num_workers=1,\n","                                collate_fn=pad)\n","  else:\n","    domain_dev_dataset = PosDataset_new(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=True,\n","                                num_workers=1,\n","                                collate_fn=pad_new)\n","  \n","  initial = True if i==1 else False\n","  top_words_ids, top_tags_ids, domain_dev_word_lst, domain_dev_tag_lst, new_acc, remain_acc, new_prob, remain_prob = gen_pseudo_data(model, domain_dev_iter, topn, initial)\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    words = []\n","    tags = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        tags.append(tag_ids[k])\n","    top_words.append(words)\n","    top_tags.append(tags)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value, domain_acc_value = eval(model, domain_test_iter)\n","\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)\n","  domain_acc_value_lst.append(domain_acc_value)\n","\n","  acc_lst.append(new_acc)\n","  prob_lst.append(new_prob)\n"],"metadata":{"id":"hA-BNQc_cGje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["factor = 10"],"metadata":{"id":"gCd5L-SDjstc","executionInfo":{"status":"ok","timestamp":1669867643880,"user_tz":300,"elapsed":337,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["domain = \"newsgroups\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","\n","file_name = f\"topn_{factor}%_domain_{domain}\"\n","test_metric = pd.read_csv(os.path.join(log_model_dir, file_name) + '.csv', index_col=0)\n","test_metric['value'] = test_metric.value.apply(lambda x: float(x[-7:-1]))\n","\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"uEACZ4DaGGRA","executionInfo":{"status":"ok","timestamp":1669867720377,"user_tz":300,"elapsed":1581,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"colab":{"base_uri":"https://localhost:8080/","height":542},"outputId":"70c2868a-51af-482e-9ba5-64b1a8276a1d"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"7f97540a-d28b-44a4-b494-774e5f28c329\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7f97540a-d28b-44a4-b494-774e5f28c329\")) {                    Plotly.newPlot(                        \"7f97540a-d28b-44a4-b494-774e5f28c329\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9262,0.941,0.9424,0.9424,0.9374,0.9426,0.939,0.9444,0.9375,0.94,0.9364],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9319,0.9345,0.9358,0.9367,0.9315,0.9371,0.9319,0.9358,0.9356,0.9369,0.9329],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9268,0.9336,0.9346,0.9359,0.931,0.937,0.9307,0.935,0.9337,0.9354,0.9315],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('7f97540a-d28b-44a4-b494-774e5f28c329');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["!ls '/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iyEQwNk3sq0","executionInfo":{"status":"ok","timestamp":1669867708925,"user_tz":300,"elapsed":457,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"04fb1bcf-de1d-4c57-b2f2-71ae122cdadc"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[" topn_10%_domain_answers.csv\t  topn_20%_domain_newsgroups.csv\n"," topn_10%_domain_emails.csv\t  topn_20%_domain_reviews.csv\n"," topn_10%_domain_newsgroups.csv   topn_20%_domain_weblogs.csv\n"," topn_10%_domain_reviews.csv\t  topn_300_domain_answers.csv\n"," topn_10%_domain_weblogs.csv\t  topn_300_domain_emails.csv\n","'topn_20\\%_domain_answers.csv'\t  topn_300_domain_newsgroups.csv\n"," topn_20%_domain_answers.csv\t  topn_300_domain_reviews.csv\n"," topn_20%_domain_emails.csv\t  topn_300_domain_weblogs.csv\n"]}]},{"cell_type":"code","source":["domain = \"answers\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","\n","file_name = f\"topn_{factor}%_domain_{domain}\"\n","test_metric = pd.read_csv(os.path.join(log_model_dir, file_name) + '.csv', index_col=0)\n","test_metric['value'] = test_metric.value.apply(lambda x: float(x[-7:-1]))\n","\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"i7ccPRECjB-o","executionInfo":{"status":"ok","timestamp":1669862326864,"user_tz":300,"elapsed":461,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"76bdaca7-b1df-47cd-c512-6bb14b9db934"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"9a3a653f-03d9-4de7-935a-4b61245fc83f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9a3a653f-03d9-4de7-935a-4b61245fc83f\")) {                    Plotly.newPlot(                        \"9a3a653f-03d9-4de7-935a-4b61245fc83f\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9297,0.9271,0.9295,0.9337,0.9323,0.9232,0.9325,0.9344,0.9336,0.9346,0.9293],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.926,0.921,0.9248,0.9275,0.9257,0.9153,0.9286,0.9306,0.9285,0.9282,0.9216],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9258,0.92,0.9237,0.9288,0.9264,0.9147,0.929,0.9302,0.9277,0.9282,0.9221],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('9a3a653f-03d9-4de7-935a-4b61245fc83f');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["domain = \"newsgroups\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","\n","file_name = f\"topn_{factor}%_domain_{domain}\"\n","test_metric = pd.read_csv(os.path.join(log_model_dir, file_name) + '.csv', index_col=0)\n","test_metric['value'] = test_metric.value.apply(lambda x: float(x[-7:-1]))\n","\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"Ftk4C1FPjUSw","executionInfo":{"status":"ok","timestamp":1669867857805,"user_tz":300,"elapsed":2932,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"ef4c3f3f-7287-4046-c2f8-8294469e434b"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a170ee61-c090-4901-842e-6cc8c1ddc1d5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a170ee61-c090-4901-842e-6cc8c1ddc1d5\")) {                    Plotly.newPlot(                        \"a170ee61-c090-4901-842e-6cc8c1ddc1d5\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9262,0.941,0.9424,0.9424,0.9374,0.9426,0.939,0.9444,0.9375,0.94,0.9364],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9319,0.9345,0.9358,0.9367,0.9315,0.9371,0.9319,0.9358,0.9356,0.9369,0.9329],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9268,0.9336,0.9346,0.9359,0.931,0.937,0.9307,0.935,0.9337,0.9354,0.9315],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('a170ee61-c090-4901-842e-6cc8c1ddc1d5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["domain = \"reviews\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","\n","file_name = f\"topn_{factor}%_domain_{domain}\"\n","test_metric = pd.read_csv(os.path.join(log_model_dir, file_name) + '.csv', index_col=0)\n","test_metric['value'] = test_metric.value.apply(lambda x: float(x[-7:-1]))\n","\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"tlxxqpe_jbeE","executionInfo":{"status":"ok","timestamp":1669862391694,"user_tz":300,"elapsed":594,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"4ef9550e-6de4-4855-f998-f19b243be621"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"b9a22081-7394-4ca0-844d-a6d5160c03f4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b9a22081-7394-4ca0-844d-a6d5160c03f4\")) {                    Plotly.newPlot(                        \"b9a22081-7394-4ca0-844d-a6d5160c03f4\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9309,0.9315,0.9346,0.9368,0.9295,0.9318,0.936,0.9338,0.936,0.9329,0.9296],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9245,0.9269,0.9227,0.9251,0.9201,0.9242,0.9291,0.9177,0.9219,0.9231,0.9209],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9257,0.9275,0.9253,0.9278,0.9219,0.925,0.9303,0.9211,0.9255,0.9256,0.9222],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b9a22081-7394-4ca0-844d-a6d5160c03f4');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["domain = \"weblogs\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","\n","file_name = f\"topn_{factor}%_domain_{domain}\"\n","test_metric = pd.read_csv(os.path.join(log_model_dir, file_name) + '.csv', index_col=0)\n","test_metric['value'] = test_metric.value.apply(lambda x: float(x[-7:-1]))\n","\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"GfT9kwewjieQ","executionInfo":{"status":"ok","timestamp":1669864914253,"user_tz":300,"elapsed":311,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"f983a272-d7e6-4e72-d602-37aa8b384b38"},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"c658f639-de45-448e-af85-8e009b8b2b70\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c658f639-de45-448e-af85-8e009b8b2b70\")) {                    Plotly.newPlot(                        \"c658f639-de45-448e-af85-8e009b8b2b70\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9466,0.9481,0.9471,0.9505,0.9502,0.9529,0.9476,0.9483,0.9488,0.9455],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9421,0.9422,0.9421,0.9466,0.9459,0.9471,0.9427,0.9436,0.9443,0.9394],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9392,0.9397,0.9402,0.9441,0.9432,0.945,0.9403,0.9416,0.9427,0.9363],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c658f639-de45-448e-af85-8e009b8b2b70');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","\n","fig.update_layout(legend=dict(\n","    orientation=\"h\",\n","    yanchor=\"bottom\",\n","    y=1.02,\n","    xanchor=\"right\",\n","    x=1\n","))\n","\n","fig.show(scale=6, width=500, height=500)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"472KdISVtGtx","executionInfo":{"status":"ok","timestamp":1669867989125,"user_tz":300,"elapsed":335,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"54dd7486-75b6-4d25-a714-0413ba26085a"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"058ae4b3-a0a3-4d46-a5af-e306b328f766\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"058ae4b3-a0a3-4d46-a5af-e306b328f766\")) {                    Plotly.newPlot(                        \"058ae4b3-a0a3-4d46-a5af-e306b328f766\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9262,0.941,0.9424,0.9424,0.9374,0.9426,0.939,0.9444,0.9375,0.94,0.9364],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9319,0.9345,0.9358,0.9367,0.9315,0.9371,0.9319,0.9358,0.9356,0.9369,0.9329],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"xaxis\":\"x\",\"y\":[0.9268,0.9336,0.9346,0.9359,0.931,0.937,0.9307,0.935,0.9337,0.9354,0.9315],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0,\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('058ae4b3-a0a3-4d46-a5af-e306b328f766');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["!pip install kaleido\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_M-OjrRc05CV","executionInfo":{"status":"ok","timestamp":1669867801576,"user_tz":300,"elapsed":25156,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"502fa7eb-9cad-43db-dd62-8a732822a650"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[K     |████████████████████████████████| 79.9 MB 79 kB/s \n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.2.1\n"]}]},{"cell_type":"code","source":["import kaleido #required\n","kaleido.__version__ #0.2.1\n","\n","import plotly\n","plotly.__version__ #5.5.0\n","\n","#now this works:\n","import plotly.graph_objects as go\n","\n","# fig = go.Figure()\n","fig.write_image(\"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/tmp.png\", scale=6, width=500, height=500)"],"metadata":{"id":"ebHow3JP1Hnd","executionInfo":{"status":"ok","timestamp":1669867992991,"user_tz":300,"elapsed":637,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"yxZezKetjTKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metric.value[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ESFNTXP3cim0","executionInfo":{"status":"ok","timestamp":1669860769446,"user_tz":300,"elapsed":4,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"2366de70-d334-4edd-cb39-03448e506922"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'tensor(0.9122)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# [tensor(0.9466), tensor(0.9481), tensor(0.9471), tensor(0.9505), tensor(0.9502), tensor(0.9529), tensor(0.9476), tensor(0.9483), tensor(0.9488), tensor(0.9455)]\n","# [tensor(0.9421), tensor(0.9422), tensor(0.9421), tensor(0.9466), tensor(0.9459), tensor(0.9471), tensor(0.9427), tensor(0.9436), tensor(0.9443), tensor(0.9394)]\n","# [tensor(0.9392), tensor(0.9397), tensor(0.9402), tensor(0.9441), tensor(0.9432), tensor(0.9450), tensor(0.9403), tensor(0.9416), tensor(0.9427), tensor(0.9363)]\n","# [tensor(0.9421), tensor(0.9422), tensor(0.9421), tensor(0.9466), tensor(0.9459), tensor(0.9471), tensor(0.9427), tensor(0.9436), tensor(0.9443), tensor(0.9394)]\n","acc = [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95652174949646, 1.0, 0.8947368264198303, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.931034505367279, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9534883499145508, 1.0, 1.0, 0.9090909361839294, 1.0, 1.0, 0.9444444179534912, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769276618958, 0.8999999761581421, 1.0, 1.0, 1.0, 1.0, 1.0], [0.9342105388641357, 0.9672130942344666, 0.9473684430122375, 0.9589040875434875, 0.9230769276618958, 0.9726027250289917, 0.9523809552192688, 0.9624999761581421, 0.9411764740943909, 0.9591836929321289, 0.9387755393981934, 0.9577465057373047, 0.9487179517745972, 0.949999988079071, 0.9718309640884399, 0.914893627166748, 0.9523809552192688, 0.9473684430122375, 0.9636363387107849, 0.9411764740943909, 0.9718309640884399, 0.9777777791023254, 0.9710144996643066, 0.9714285731315613, 0.9558823704719543, 0.7692307829856873, 0.9577465057373047, 0.9599999785423279, 0.957446813583374, 0.9365079402923584, 0.9677419066429138, 0.95652174949646, 0.9722222089767456, 0.9142857193946838, 0.9523809552192688, 0.9583333134651184, 0.9487179517745972, 0.9607843160629272, 0.9487179517745972, 0.9750000238418579, 0.9591836929321289, 0.9672130942344666, 0.9642857313156128, 0.9473684430122375, 0.9523809552192688, 0.9523809552192688, 0.9428571462631226, 0.9591836929321289, 0.9599999785423279, 0.9428571462631226, 0.9607843160629272, 0.9399999976158142, 0.9375, 0.9583333134651184, 0.9056603908538818, 0.9487179517745972, 0.9275362491607666, 0.9090909361839294, 0.930232584476471, 0.9636363387107849, 0.9399999976158142, 0.9230769276618958, 0.9607843160629272, 0.8870967626571655, 0.9523809552192688, 0.9583333134651184, 0.9433962106704712, 0.9347826242446899, 0.970588207244873, 0.9487179517745972, 0.9523809552192688, 0.925000011920929, 0.9473684430122375, 0.9344262480735779, 0.9722222089767456, 0.9473684430122375, 0.9594594836235046, 0.9433962106704712, 0.9285714030265808, 0.95652174949646, 0.9534883499145508, 0.9444444179534912, 0.9420289993286133, 0.95652174949646, 0.9682539701461792, 0.9487179517745972, 0.925000011920929, 0.9230769276618958, 0.9402984976768494, 0.9701492786407471, 0.9318181872367859, 0.9454545378684998, 0.9047619104385376, 0.9130434989929199, 0.9111111164093018, 0.930232584476471, 0.9090909361839294, 0.95652174949646, 0.9487179517745972, 0.9285714030265808, 0.9682539701461792, 0.970588207244873], [0.9583333134651184, 0.9285714030265808, 0.9594594836235046, 0.9305555820465088, 0.970588207244873, 0.9666666388511658, 0.9726027250289917, 0.9552238583564758, 0.9759036302566528, 0.9819819927215576, 0.9733333587646484, 0.9642857313156128, 0.9777777791023254, 0.9193548560142517, 0.976190447807312, 0.9642857313156128, 0.9722222089767456, 0.9718309640884399, 0.9677419066429138, 0.9594594836235046, 0.9666666388511658, 0.9759036302566528, 0.9733333587646484, 0.9518072009086609, 0.9111111164093018, 0.9719626307487488, 0.9583333134651184, 0.9714285731315613, 0.954954981803894, 0.8777777552604675, 0.9345794320106506, 0.9583333134651184, 0.9777777791023254, 0.9583333134651184, 0.9726027250289917, 0.9736841917037964, 0.9729729890823364, 0.9722222089767456, 0.9777777791023254, 0.9819819927215576, 0.9622641801834106, 0.9714285731315613, 0.9722222089767456, 0.9583333134651184, 0.9819819927215576, 0.9558823704719543, 0.9729729890823364, 0.970588207244873, 0.9759036302566528, 0.9714285731315613, 0.9264705777168274, 0.9677419066429138, 0.9638554453849792, 0.9750000238418579, 0.9736841917037964, 0.9722222089767456, 0.9813084006309509, 0.9743589758872986, 0.9555555582046509, 0.9599999785423279, 0.9444444179534912, 0.9819819927215576, 0.9682539701461792, 0.9615384340286255, 0.970588207244873, 0.976190447807312, 0.9555555582046509, 0.9719626307487488, 0.9508196711540222, 0.9813084006309509, 0.9719626307487488, 0.9594594836235046, 0.9666666388511658, 0.9714285731315613, 0.9428571462631226, 0.9639639854431152, 0.9719626307487488, 0.9066666960716248, 0.9714285731315613, 0.9404761791229248, 0.9552238583564758, 0.9552238583564758, 0.9666666388511658, 0.9555555582046509, 0.9777777791023254, 0.9404761791229248, 0.9677419066429138, 0.976190447807312, 0.9577465057373047, 0.9583333134651184, 0.9722222089767456, 0.9813084006309509, 0.9722222089767456, 0.9714285731315613, 0.9777777791023254, 0.9743589758872986, 0.9571428298950195, 0.9523809552192688, 0.9729729890823364, 0.9444444179534912, 0.9714285731315613, 0.9743589758872986], [0.9813084006309509, 0.9666666388511658, 0.9599999785423279, 0.9813084006309509, 0.9729729890823364, 0.9729729890823364, 0.9639639854431152, 0.9639639854431152, 0.9375, 0.9626168012619019, 0.976190447807312, 0.5777778029441833, 0.9729729890823364, 0.9439252614974976, 0.9626168012619019, 0.9729729890823364, 0.9729729890823364, 0.9523809552192688, 0.9666666388511658, 0.9639639854431152, 0.9813084006309509, 0.9719626307487488, 0.9666666388511658, 0.9813084006309509, 0.9444444179534912, 0.9819819927215576, 0.9729729890823364, 0.9719626307487488, 0.9639639854431152, 0.773809552192688, 0.954954981803894, 0.9439252614974976, 0.9719626307487488, 0.9666666388511658, 0.9719626307487488, 0.9189189076423645, 0.9624999761581421, 0.9729729890823364, 0.954954981803894, 0.9819819927215576, 0.9555555582046509, 0.954954981803894, 0.9719626307487488, 0.954954981803894, 0.9729729890823364, 0.954954981803894, 0.9333333373069763, 0.9719626307487488, 0.9626168012619019, 0.9333333373069763, 0.9397590160369873, 0.9729729890823364, 0.9626168012619019, 0.9369369149208069, 0.9813084006309509, 0.9626168012619019, 0.9759036302566528, 0.9252336621284485, 0.9666666388511658, 0.9729729890823364, 0.9819819927215576, 0.9719626307487488, 0.9719626307487488, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9605262875556946, 0.9759036302566528, 0.9777777791023254, 0.9158878326416016, 0.9777777791023254, 0.9729729890823364, 0.9666666388511658, 0.9759036302566528, 0.9639639854431152, 0.9555555582046509, 0.9813084006309509, 0.9666666388511658, 0.9639639854431152, 0.9666666388511658, 0.9345794320106506, 0.9813084006309509, 0.9599999785423279, 0.9719626307487488, 0.9719626307487488, 0.9819819927215576, 0.9487179517745972, 0.9439252614974976, 0.9819819927215576, 0.9666666388511658, 0.9626168012619019, 0.9813084006309509, 0.9639639854431152, 0.954954981803894, 0.9819819927215576, 0.976190447807312, 0.9729729890823364, 0.9819819927215576, 0.9444444179534912, 0.9813084006309509, 0.9626168012619019, 0.9555555582046509], [0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 0.9639639854431152, 0.9459459185600281, 0.9639639854431152, 0.9369369149208069, 0.9909909963607788, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9909909963607788, 1.0, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.954954981803894, 0.9729729890823364, 0.9729729890823364, 0.9459459185600281, 0.9099099040031433, 0.9729729890823364, 1.0, 0.9459459185600281, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.9459459185600281, 0.9639639854431152, 1.0, 0.9909909963607788, 0.9819819927215576, 0.9639639854431152, 0.9909909963607788, 0.9909909963607788, 1.0, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9639639854431152, 0.9909909963607788, 1.0, 0.9819819927215576, 0.9459459185600281, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9279279112815857, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9729729890823364, 0.9279279112815857, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9729729890823364, 0.9009009003639221, 0.9279279112815857, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9369369149208069, 0.8918918967247009, 0.9819819927215576, 0.9729729890823364, 1.0, 0.9909909963607788, 0.9729729890823364, 0.9819819927215576, 0.9909909963607788, 0.9099099040031433, 0.9909909963607788, 1.0, 0.9369369149208069, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9279279112815857, 1.0, 0.9729729890823364, 1.0, 0.9189189076423645, 1.0, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9459459185600281, 1.0, 0.9909909963607788, 0.9729729890823364, 0.9459459185600281, 0.9819819927215576, 0.9909909963607788], [0.9279279112815857, 0.9099099040031433, 0.7747747898101807, 0.8288288116455078, 0.7387387156486511, 0.4864864945411682, 0.6486486196517944, 0.5045045018196106, 0.630630612373352, 0.6486486196517944, 0.7837837934494019, 0.5585585832595825, 0.7657657861709595, 0.6216216087341309, 0.7657657861709595, 0.522522509098053, 0.5675675868988037, 0.477477490901947, 0.4954954981803894, 0.6126126050949097, 0.5045045018196106, 0.6216216087341309, 0.5675675868988037, 0.5765765905380249, 0.5585585832595825, 0.6126126050949097, 0.5315315127372742, 0.45045045018196106, 0.4864864945411682, 0.44144144654273987, 0.342342346906662, 0.342342346906662, 0.44144144654273987, 0.5495495200157166, 0.46846845746040344, 0.6216216087341309, 0.45045045018196106, 0.4324324429035187, 0.46846845746040344, 0.37837839126586914, 0.4054054021835327, 0.3963963985443115, 0.477477490901947, 0.4054054021835327, 0.3333333432674408, 0.5135135054588318, 0.38738739490509033, 0.38738739490509033, 0.5585585832595825, 0.3243243098258972, 0.4234234094619751, 0.4864864945411682, 0.37837839126586914, 0.3243243098258972, 0.37837839126586914, 0.36936935782432556, 0.315315306186676, 0.3513513505458832, 0.3513513505458832, 0.36036035418510437, 0.38738739490509033, 0.28828829526901245, 0.45945945382118225, 0.4864864945411682, 0.3243243098258972, 0.3513513505458832, 0.3513513505458832, 0.3333333432674408, 0.342342346906662, 0.29729729890823364, 0.3963963985443115, 0.30630630254745483, 0.29729729890823364, 0.38738739490509033, 0.3243243098258972, 0.4144144058227539, 0.4054054021835327, 0.36936935782432556, 0.342342346906662, 0.3513513505458832, 0.36036035418510437, 0.3513513505458832, 0.4144144058227539, 0.36036035418510437, 0.315315306186676, 0.4054054021835327, 0.342342346906662, 0.3243243098258972, 0.27927929162979126, 0.3513513505458832, 0.38738739490509033, 0.4324324429035187, 0.315315306186676, 0.2702702581882477, 0.36036035418510437, 0.38738739490509033, 0.3333333432674408, 0.315315306186676, 0.3513513505458832, 0.3963963985443115, 0.3243243098258972, 0.36036035418510437], [0.09909909963607788, 0.045045044273138046, 0.06306306272745132, 0.036036036908626556, 0.018018018454313278, 0.027027027681469917, 0.1621621549129486, 0.036036036908626556, 0.171171173453331, 0.2522522509098053, 0.06306306272745132, 0.22522522509098053, 0.10810811072587967, 0.054054055362939835, 0.06306306272745132, 0.11711711436510086, 0.13513512909412384, 0.09009008854627609, 0.21621622145175934, 0.15315315127372742, 0.2432432472705841, 0.0810810774564743, 0.0810810774564743, 0.171171173453331, 0.28828829526901245, 0.21621622145175934, 0.18018017709255219, 0.2522522509098053, 0.09909909963607788, 0.027027027681469917, 0.09009008854627609, 0.22522522509098053, 0.11711711436510086, 0.12612612545490265, 0.10810811072587967, 0.22522522509098053, 0.09909909963607788, 0.09909909963607788, 0.18918919563293457, 0.15315315127372742, 0.22522522509098053, 0.22522522509098053, 0.21621622145175934, 0.12612612545490265, 0.15315315127372742, 0.09009008854627609, 0.07207207381725311, 0.15315315127372742, 0.0810810774564743, 0.21621622145175934, 0.12612612545490265, 0.22522522509098053, 0.09909909963607788, 0.28828829526901245, 0.0810810774564743, 0.09909909963607788, 0.20720720291137695, 0.11711711436510086, 0.21621622145175934, 0.11711711436510086, 0.19819819927215576, 0.12612612545490265, 0.09009008854627609, 0.22522522509098053, 0.054054055362939835, 0.30630630254745483, 0.21621622145175934, 0.171171173453331, 0.13513512909412384, 0.045045044273138046, 0.15315315127372742, 0.1621621549129486, 0.12612612545490265, 0.2702702581882477, 0.20720720291137695, 0.2702702581882477, 0.11711711436510086, 0.0810810774564743, 0.15315315127372742, 0.20720720291137695, 0.23423422873020172, 0.10810811072587967, 0.09009008854627609, 0.20720720291137695, 0.09009008854627609, 0.171171173453331, 0.09009008854627609, 0.12612612545490265, 0.09909909963607788, 0.0810810774564743, 0.23423422873020172, 0.23423422873020172, 0.09909909963607788, 0.15315315127372742, 0.2522522509098053, 0.18918919563293457, 0.1621621549129486, 0.2432432472705841, 0.15315315127372742, 0.15315315127372742, 0.045045044273138046, 0.18918919563293457], [0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.954954981803894, 0.954954981803894, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9639639854431152, 0.9729729890823364, 0.9729729890823364, 0.954954981803894, 0.9639639854431152, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9459459185600281, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9459459185600281, 0.9639639854431152, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9459459185600281, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.954954981803894, 0.9459459185600281, 0.9729729890823364, 0.9819819927215576, 0.9729729890823364, 0.9819819927215576, 0.954954981803894, 0.9819819927215576, 0.9819819927215576], [0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.9909909963607788, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.954954981803894, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.954954981803894, 0.9819819927215576, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9819819927215576, 0.9909909963607788, 0.954954981803894, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9819819927215576, 0.9639639854431152, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9909909963607788, 0.9909909963607788, 0.954954981803894, 0.9639639854431152, 0.9819819927215576, 0.9729729890823364, 0.9909909963607788, 0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9729729890823364, 0.9729729890823364, 0.9729729890823364, 0.9819819927215576, 0.9819819927215576, 0.9909909963607788, 0.9729729890823364, 0.9909909963607788, 1.0, 0.9729729890823364, 0.9909909963607788, 0.9729729890823364]]\n","prob_list = [[0.999798059463501, 0.9997609257698059, 0.9997243881225586, 0.9996716380119324, 0.9996636509895325, 0.9996532201766968, 0.9995818138122559, 0.9995136857032776, 0.9994621276855469, 0.9994298815727234, 0.9994193911552429, 0.9992927312850952, 0.9991897344589233, 0.999162495136261, 0.9990848898887634, 0.9990408420562744, 0.9990352392196655, 0.9990233182907104, 0.9989814758300781, 0.9989410042762756, 0.9989268779754639, 0.9989258050918579, 0.9989162683486938, 0.9988672733306885, 0.9988577961921692, 0.9988387227058411, 0.9987951517105103, 0.9987403750419617, 0.9985683560371399, 0.9985527992248535, 0.9985092878341675, 0.9984806180000305, 0.9984214305877686, 0.9983848929405212, 0.9983698725700378, 0.9983627200126648, 0.9983537197113037, 0.9983531832695007, 0.9982770681381226, 0.9982223510742188, 0.9982121586799622, 0.9981723427772522, 0.9981693625450134, 0.9981638789176941, 0.9981609582901001, 0.9981470108032227, 0.998144268989563, 0.9981099963188171, 0.9980669617652893, 0.9980605840682983, 0.9980602264404297, 0.9980195760726929, 0.9980186820030212, 0.9980130791664124, 0.9979899525642395, 0.997988760471344, 0.997943103313446, 0.9978846311569214, 0.9978086352348328, 0.9978045225143433, 0.9977808594703674, 0.9977502822875977, 0.9977333545684814, 0.9976773858070374, 0.9976698756217957, 0.9976546168327332, 0.9976219534873962, 0.9976049065589905, 0.9975472688674927, 0.9975295066833496, 0.9974830150604248, 0.9974546432495117, 0.9974456429481506, 0.9974451065063477, 0.997439980506897, 0.9974192976951599, 0.997401237487793, 0.9973815083503723, 0.9973633885383606, 0.9973442554473877, 0.9972391128540039, 0.9971839189529419, 0.9971553087234497, 0.9971469640731812, 0.9971455931663513, 0.9971393346786499, 0.9970966577529907, 0.9970418214797974, 0.9970322251319885, 0.9969974756240845, 0.9969155788421631, 0.9968834519386292, 0.9968796372413635, 0.9968288540840149, 0.996789276599884, 0.9966727495193481, 0.9966233968734741, 0.9966047406196594, 0.996592104434967, 0.9965866804122925, 0.9965119957923889, 0.9965052008628845], [0.9817234873771667, 0.9785272479057312, 0.9776372313499451, 0.9764548540115356, 0.9757897257804871, 0.9756594896316528, 0.9752596020698547, 0.9750471115112305, 0.9744119048118591, 0.9741156697273254, 0.9739365577697754, 0.9731894731521606, 0.9729788303375244, 0.9728564023971558, 0.9725017547607422, 0.9723671674728394, 0.9723227620124817, 0.9722226858139038, 0.9716639518737793, 0.9716222286224365, 0.9715116024017334, 0.9712647199630737, 0.9710476398468018, 0.9707461595535278, 0.9707195162773132, 0.9705784916877747, 0.9702393412590027, 0.9698932766914368, 0.9696681499481201, 0.969649076461792, 0.9693778157234192, 0.9691499471664429, 0.9688546657562256, 0.9686205387115479, 0.9684120416641235, 0.9681845903396606, 0.9675257802009583, 0.9672830700874329, 0.967068612575531, 0.9667770266532898, 0.9667015671730042, 0.9666045308113098, 0.9664918780326843, 0.9664787650108337, 0.966399073600769, 0.9663869142532349, 0.9660936594009399, 0.9659124612808228, 0.965833306312561, 0.9658108353614807, 0.9657067656517029, 0.9656558036804199, 0.9652805328369141, 0.9652679562568665, 0.9649048447608948, 0.9648956060409546, 0.964782178401947, 0.9644887447357178, 0.9644779562950134, 0.9643340110778809, 0.964260458946228, 0.9642398357391357, 0.9641810655593872, 0.9641528129577637, 0.9641153216362, 0.9637807011604309, 0.9636768102645874, 0.9636565446853638, 0.9636304378509521, 0.9635757803916931, 0.9635730981826782, 0.963456928730011, 0.9633830189704895, 0.9632798433303833, 0.9632640480995178, 0.9632226824760437, 0.9632042646408081, 0.9631358981132507, 0.9629944562911987, 0.9629855155944824, 0.962975263595581, 0.9629319310188293, 0.9628963470458984, 0.9627582430839539, 0.9624484181404114, 0.9623445272445679, 0.9622284173965454, 0.962142288684845, 0.9617661237716675, 0.9616733193397522, 0.961666464805603, 0.9614667892456055, 0.9614167809486389, 0.961249828338623, 0.9612241387367249, 0.9611873030662537, 0.9611416459083557, 0.9611138701438904, 0.9610026478767395, 0.9609654545783997, 0.9608424305915833, 0.9607900977134705], [0.9796269536018372, 0.9779704213142395, 0.9765051007270813, 0.9752669930458069, 0.9749463200569153, 0.9747986793518066, 0.9729233980178833, 0.9728026986122131, 0.9721291065216064, 0.9706289768218994, 0.9704241752624512, 0.970364511013031, 0.9701799154281616, 0.9699467420578003, 0.9696168303489685, 0.969573974609375, 0.969325602054596, 0.9691293239593506, 0.9690230488777161, 0.9689098000526428, 0.968425989151001, 0.9683214426040649, 0.9681045413017273, 0.9680607914924622, 0.9678324460983276, 0.9675041437149048, 0.9674469828605652, 0.9670031070709229, 0.9664599299430847, 0.9653291702270508, 0.9652350544929504, 0.9650313258171082, 0.9645279049873352, 0.9645137786865234, 0.9644918441772461, 0.9644035696983337, 0.9642328023910522, 0.9641045331954956, 0.9640594720840454, 0.9640561938285828, 0.9640471935272217, 0.9640272855758667, 0.9639513492584229, 0.9638761878013611, 0.9638166427612305, 0.9636589288711548, 0.9634768962860107, 0.9632647037506104, 0.9631083607673645, 0.9630898833274841, 0.9630668759346008, 0.9629735350608826, 0.9627017974853516, 0.9626386761665344, 0.9625383615493774, 0.9625263214111328, 0.9624972939491272, 0.9623945951461792, 0.9622145891189575, 0.9619469046592712, 0.9619041085243225, 0.961887776851654, 0.9617716073989868, 0.9616421461105347, 0.9616131782531738, 0.9616064429283142, 0.9613900780677795, 0.9612351059913635, 0.9611502885818481, 0.961097240447998, 0.960878312587738, 0.9608434438705444, 0.9605333805084229, 0.9604579210281372, 0.9604002833366394, 0.9603492617607117, 0.960338830947876, 0.9601511359214783, 0.9600471258163452, 0.9598530530929565, 0.9598486423492432, 0.9597331881523132, 0.9597138166427612, 0.9595560431480408, 0.9595353007316589, 0.9594456553459167, 0.9594177603721619, 0.959350049495697, 0.9593121409416199, 0.9591972827911377, 0.9590803980827332, 0.9590479731559753, 0.9590256810188293, 0.9589102864265442, 0.9587776064872742, 0.9587411880493164, 0.9586794376373291, 0.9586426019668579, 0.9585512280464172, 0.9585363864898682, 0.9585250616073608, 0.9584994316101074], [0.982528567314148, 0.9816603660583496, 0.9746264815330505, 0.9739543199539185, 0.9737696051597595, 0.9732950329780579, 0.9727626442909241, 0.9703720808029175, 0.9697786569595337, 0.9694781303405762, 0.9688504338264465, 0.9683973789215088, 0.9683190584182739, 0.9676313400268555, 0.9675401449203491, 0.967525064945221, 0.9673555493354797, 0.966675341129303, 0.9666330218315125, 0.9666058421134949, 0.966178834438324, 0.9659404158592224, 0.9658769369125366, 0.9657328724861145, 0.9653806686401367, 0.9651960730552673, 0.9650385975837708, 0.9646810293197632, 0.9646472930908203, 0.9644439220428467, 0.9641814231872559, 0.9634606242179871, 0.9634435176849365, 0.9633919596672058, 0.9632967710494995, 0.9631830453872681, 0.9630329012870789, 0.9625131487846375, 0.9618176817893982, 0.9616665244102478, 0.961598813533783, 0.9613313674926758, 0.9610714316368103, 0.9609618186950684, 0.9609208703041077, 0.9607269763946533, 0.9607110023498535, 0.9606322646141052, 0.9604658484458923, 0.960253894329071, 0.9596662521362305, 0.9594911932945251, 0.9594327211380005, 0.9592575430870056, 0.9591984152793884, 0.9591659307479858, 0.9591086506843567, 0.9589337706565857, 0.958751916885376, 0.9587112069129944, 0.9586741328239441, 0.9585857391357422, 0.9583753943443298, 0.9582207202911377, 0.9579774141311646, 0.9579231142997742, 0.9578258991241455, 0.9577410817146301, 0.9575625658035278, 0.9573559165000916, 0.9573303461074829, 0.9568030834197998, 0.956739068031311, 0.9565786123275757, 0.956463098526001, 0.9561796188354492, 0.9561334848403931, 0.9559201002120972, 0.955864429473877, 0.9557777643203735, 0.9556957483291626, 0.9556658267974854, 0.9555184841156006, 0.9552361369132996, 0.9550356268882751, 0.9550142288208008, 0.9548836946487427, 0.9546293020248413, 0.9545192122459412, 0.9545065760612488, 0.9544780254364014, 0.9543478488922119, 0.95427405834198, 0.9542123675346375, 0.954182505607605, 0.9540306925773621, 0.9538965821266174, 0.9538132548332214, 0.9537861943244934, 0.9536778926849365, 0.953654408454895, 0.953607439994812], [0.9962618947029114, 0.9962359666824341, 0.9940988421440125, 0.9940952658653259, 0.9939370155334473, 0.9933323860168457, 0.993274450302124, 0.9929867386817932, 0.9929836392402649, 0.9926305413246155, 0.9924961924552917, 0.9921882152557373, 0.9918815493583679, 0.9918420314788818, 0.9918159246444702, 0.9917689561843872, 0.9917600750923157, 0.991748571395874, 0.9916809797286987, 0.9916248321533203, 0.9915640950202942, 0.9915551543235779, 0.991539716720581, 0.9915123581886292, 0.9915103912353516, 0.9914835095405579, 0.9914689064025879, 0.9914180040359497, 0.9914093613624573, 0.991401195526123, 0.9913724660873413, 0.9913482069969177, 0.9913210272789001, 0.9912845492362976, 0.9912844896316528, 0.9912833571434021, 0.991249680519104, 0.9912381768226624, 0.9911937713623047, 0.9911912083625793, 0.9911167621612549, 0.9910718202590942, 0.9910430312156677, 0.9910363554954529, 0.9910348653793335, 0.9910162687301636, 0.9909998774528503, 0.990997314453125, 0.9909651279449463, 0.9909480810165405, 0.9909345507621765, 0.9909231662750244, 0.9909182786941528, 0.9909132122993469, 0.9908819794654846, 0.9908693432807922, 0.9908515810966492, 0.9908245205879211, 0.9908155798912048, 0.9908043146133423, 0.9907421469688416, 0.9907311797142029, 0.9907229542732239, 0.9907078146934509, 0.9906930923461914, 0.9906731843948364, 0.9906308054924011, 0.9906076788902283, 0.9905974864959717, 0.9904988408088684, 0.9904861450195312, 0.9904653429985046, 0.9904295206069946, 0.9904133081436157, 0.9903989434242249, 0.9903278350830078, 0.9902737140655518, 0.9902559518814087, 0.9902554750442505, 0.9902251362800598, 0.9902238845825195, 0.990131139755249, 0.9900739789009094, 0.9900566935539246, 0.9900454878807068, 0.9900386333465576, 0.9899905920028687, 0.9899823069572449, 0.9899600744247437, 0.9899414777755737, 0.9899335503578186, 0.989916980266571, 0.9899077415466309, 0.989899218082428, 0.9898672699928284, 0.9898489117622375, 0.9898424744606018, 0.9897879958152771, 0.9897738695144653, 0.9897618293762207, 0.9897324442863464, 0.9896923303604126], [0.9450442790985107, 0.8900070786476135, 0.8794874548912048, 0.8765034079551697, 0.8563343286514282, 0.8449638485908508, 0.8381963968276978, 0.8345248699188232, 0.8335363864898682, 0.8257083296775818, 0.8199054598808289, 0.8173989057540894, 0.8154930472373962, 0.8099790215492249, 0.8079418540000916, 0.8077139854431152, 0.8076819181442261, 0.8033620715141296, 0.7989152669906616, 0.7922243475914001, 0.7913991808891296, 0.7883793711662292, 0.7875060439109802, 0.7834787368774414, 0.7808229327201843, 0.7797859907150269, 0.7778797149658203, 0.776918351650238, 0.7697858810424805, 0.7682508826255798, 0.7681737542152405, 0.7681737542152405, 0.7671624422073364, 0.764695942401886, 0.763361394405365, 0.7623496055603027, 0.7622898817062378, 0.7612999081611633, 0.7610208988189697, 0.7594681978225708, 0.7554068565368652, 0.755370020866394, 0.7548799514770508, 0.751437783241272, 0.748779296875, 0.7487309575080872, 0.7459191679954529, 0.745694637298584, 0.7422711253166199, 0.7418997287750244, 0.7416740655899048, 0.7397549152374268, 0.7369393110275269, 0.7366416454315186, 0.7343281507492065, 0.7333763241767883, 0.7332356572151184, 0.7331897616386414, 0.7327280640602112, 0.7314372658729553, 0.7310234308242798, 0.7309238910675049, 0.7303101420402527, 0.7302184104919434, 0.7293860912322998, 0.7264994978904724, 0.7264994978904724, 0.7257937788963318, 0.7249041199684143, 0.7248242497444153, 0.724549412727356, 0.7241301536560059, 0.7236127257347107, 0.7235265374183655, 0.722653865814209, 0.7225748896598816, 0.7213932871818542, 0.7210787534713745, 0.720945417881012, 0.7199737429618835, 0.7193097472190857, 0.7188809514045715, 0.7176252007484436, 0.7174767851829529, 0.717387855052948, 0.7173866033554077, 0.7170433402061462, 0.7163699865341187, 0.7156012654304504, 0.7155174612998962, 0.7150073647499084, 0.7148811221122742, 0.7144075632095337, 0.714149534702301, 0.713483989238739, 0.7128716707229614, 0.7126681208610535, 0.7116051316261292, 0.7115867137908936, 0.7115576267242432, 0.711024820804596, 0.7103127241134644], [0.9914625287055969, 0.9913865923881531, 0.9910546541213989, 0.9890826940536499, 0.9881108999252319, 0.9875422716140747, 0.9872554540634155, 0.9870415329933167, 0.9870398640632629, 0.9868173599243164, 0.9865642786026001, 0.9865396618843079, 0.9865391254425049, 0.9863222241401672, 0.9862241148948669, 0.9859433770179749, 0.9858052134513855, 0.9857724905014038, 0.9857711791992188, 0.9857069253921509, 0.9855886697769165, 0.9855689406394958, 0.9855689406394958, 0.9855484366416931, 0.9854944348335266, 0.9854599833488464, 0.9854305982589722, 0.985424816608429, 0.9853605628013611, 0.9850888848304749, 0.9850178360939026, 0.9849340915679932, 0.9848824739456177, 0.9848802089691162, 0.9848375916481018, 0.9848281145095825, 0.9847772121429443, 0.9847543835639954, 0.9847471117973328, 0.98466956615448, 0.9845507740974426, 0.9845468401908875, 0.9845350384712219, 0.9845187664031982, 0.9844965934753418, 0.9844658374786377, 0.9843281507492065, 0.9843224883079529, 0.9843153953552246, 0.9842690229415894, 0.9842603802680969, 0.9842472672462463, 0.9842242002487183, 0.9841914772987366, 0.9841830730438232, 0.9839462637901306, 0.9839314818382263, 0.9839127063751221, 0.9839079976081848, 0.9837791323661804, 0.9837789535522461, 0.9837646484375, 0.9837576150894165, 0.9837507009506226, 0.9837449789047241, 0.9836942553520203, 0.9836573004722595, 0.9836475253105164, 0.9836066961288452, 0.9836016297340393, 0.9835909605026245, 0.9835889935493469, 0.9835307002067566, 0.9834864735603333, 0.9834781289100647, 0.9834458231925964, 0.9834340214729309, 0.9833908677101135, 0.9833354949951172, 0.983235239982605, 0.9832062125205994, 0.9832049012184143, 0.9831846952438354, 0.9831750392913818, 0.9831539988517761, 0.9831278324127197, 0.983002245426178, 0.9829297661781311, 0.9828432202339172, 0.9827500581741333, 0.9827188849449158, 0.9827188849449158, 0.9826750755310059, 0.9826740026473999, 0.9826594591140747, 0.9826410412788391, 0.9826366901397705, 0.9826234579086304, 0.9825868606567383, 0.9825079441070557, 0.982452392578125, 0.9824007153511047], [0.9933387637138367, 0.9932316541671753, 0.9923577308654785, 0.9922049641609192, 0.991722047328949, 0.99170982837677, 0.9915115833282471, 0.9913317561149597, 0.9910642504692078, 0.9908437728881836, 0.9905516505241394, 0.9904618859291077, 0.9900728464126587, 0.9900256991386414, 0.9898050427436829, 0.9897936582565308, 0.9896578192710876, 0.9896389842033386, 0.9896085858345032, 0.9894849061965942, 0.9893649220466614, 0.9893649220466614, 0.9893522262573242, 0.9893458485603333, 0.9892410635948181, 0.9891753196716309, 0.9891207814216614, 0.9891207814216614, 0.9890836477279663, 0.9890491366386414, 0.9890003204345703, 0.9888866543769836, 0.9888659119606018, 0.9888610243797302, 0.9888567328453064, 0.9887605905532837, 0.9886168241500854, 0.9885454773902893, 0.9885188341140747, 0.9884737133979797, 0.9884617328643799, 0.9884608387947083, 0.9883822798728943, 0.9883486032485962, 0.9880001544952393, 0.9879890084266663, 0.9879744648933411, 0.9878502488136292, 0.987819492816925, 0.9876888394355774, 0.9876157641410828, 0.9875303506851196, 0.9875005483627319, 0.9874879121780396, 0.9873949885368347, 0.9873784780502319, 0.9873462319374084, 0.9873121380805969, 0.9872682690620422, 0.9871925115585327, 0.9871401190757751, 0.9871379137039185, 0.9871112704277039, 0.9870783090591431, 0.9870628118515015, 0.9868687987327576, 0.9868627190589905, 0.9868147969245911, 0.9866924285888672, 0.9865671396255493, 0.9865587949752808, 0.9864859580993652, 0.9864835739135742, 0.9864541888237, 0.9864044189453125, 0.9864006638526917, 0.9862088561058044, 0.9861986041069031, 0.9860697984695435, 0.9860434532165527, 0.9860214591026306, 0.9860170483589172, 0.9859792590141296, 0.9859781265258789, 0.9859337210655212, 0.9859290719032288, 0.9859133362770081, 0.9858909845352173, 0.9858599305152893, 0.9858450889587402, 0.9857838749885559, 0.9857778549194336, 0.9857764840126038, 0.985692024230957, 0.985539972782135, 0.9855263233184814, 0.9854715466499329, 0.9853859543800354, 0.9853073954582214, 0.9852293729782104, 0.9852196574211121, 0.9851891994476318], [0.9979798793792725, 0.9979432225227356, 0.9975451827049255, 0.9973692893981934, 0.9971492290496826, 0.9971070289611816, 0.9966923594474792, 0.9965962171554565, 0.9964686036109924, 0.9963430166244507, 0.9963186383247375, 0.9957948327064514, 0.9957292675971985, 0.9956396818161011, 0.9956262111663818, 0.9956238865852356, 0.9955843687057495, 0.9954198598861694, 0.9953758120536804, 0.9953103065490723, 0.9950469136238098, 0.9950356483459473, 0.9949610233306885, 0.9949295520782471, 0.9946702718734741, 0.9946547746658325, 0.9944997429847717, 0.9942606091499329, 0.9940365552902222, 0.993988037109375, 0.993751585483551, 0.993404746055603, 0.9933041930198669, 0.993100643157959, 0.9930046200752258, 0.9929131269454956, 0.9925493001937866, 0.9925011396408081, 0.9924747943878174, 0.9924297332763672, 0.9924280643463135, 0.9920867085456848, 0.9920803904533386, 0.9919462203979492, 0.991753876209259, 0.9917145371437073, 0.9916725754737854, 0.9916725754737854, 0.991528332233429, 0.9915065169334412, 0.9914698004722595, 0.9914343357086182, 0.9913005828857422, 0.99126136302948, 0.9912552237510681, 0.9910513162612915, 0.9909546375274658, 0.9908639788627625, 0.9908109903335571, 0.9906459450721741, 0.9906203746795654, 0.9905369877815247, 0.9904628396034241, 0.9904350638389587, 0.9903453588485718, 0.9902966022491455, 0.9900204539299011, 0.9898535013198853, 0.989772379398346, 0.9897451400756836, 0.9896813035011292, 0.989677906036377, 0.9896506667137146, 0.9896453022956848, 0.9896261096000671, 0.9895188808441162, 0.9895167946815491, 0.9895046949386597, 0.9894501566886902, 0.989448070526123, 0.9893593788146973, 0.9892783164978027, 0.9891482591629028, 0.9890274405479431, 0.9889963269233704, 0.9889777898788452, 0.9888536334037781, 0.9886375665664673, 0.9884473085403442, 0.9884059429168701, 0.9879454374313354, 0.9878860116004944, 0.9877198338508606, 0.9875537753105164, 0.9874809384346008, 0.9874266386032104, 0.9873436093330383, 0.9873360395431519, 0.9873173236846924, 0.9872413277626038, 0.9872055053710938, 0.987018346786499]]"],"metadata":{"id":"btVtiucfdQ_H","executionInfo":{"status":"ok","timestamp":1669862047237,"user_tz":300,"elapsed":345,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["acc_df = pd.DataFrame(acc)\n","acc_df = pd.DataFrame({'mean': acc_df.mean(axis=1), 'max': acc_df.max(axis=1), 'min': acc_df.min(axis=1)})"],"metadata":{"id":"58NqZbhuiLuK","executionInfo":{"status":"ok","timestamp":1669862188852,"user_tz":300,"elapsed":126,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["acc_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"Mxaxq5CciO3x","executionInfo":{"status":"ok","timestamp":1669862188994,"user_tz":300,"elapsed":7,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"10fc40b7-9b9b-4e6f-b15e-b77249a2775d"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       mean       max       min\n","0  0.973712  1.000000  0.000000\n","1  0.946019  0.977778  0.769231\n","2  0.963192  0.981982  0.877778\n","3  0.959116  0.981982  0.577778\n","4  0.972620  1.000000  0.891892\n","5  0.450980  0.927928  0.270270\n","6  0.147677  0.306306  0.018018\n","7  0.974739  0.990991  0.945946\n","8  0.980480  1.000000  0.954955"],"text/html":["\n","  <div id=\"df-b1d7c8d5-9932-468c-ab48-b015082bfc23\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean</th>\n","      <th>max</th>\n","      <th>min</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.973712</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.946019</td>\n","      <td>0.977778</td>\n","      <td>0.769231</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.963192</td>\n","      <td>0.981982</td>\n","      <td>0.877778</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.959116</td>\n","      <td>0.981982</td>\n","      <td>0.577778</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.972620</td>\n","      <td>1.000000</td>\n","      <td>0.891892</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.450980</td>\n","      <td>0.927928</td>\n","      <td>0.270270</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.147677</td>\n","      <td>0.306306</td>\n","      <td>0.018018</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.974739</td>\n","      <td>0.990991</td>\n","      <td>0.945946</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.980480</td>\n","      <td>1.000000</td>\n","      <td>0.954955</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1d7c8d5-9932-468c-ab48-b015082bfc23')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b1d7c8d5-9932-468c-ab48-b015082bfc23 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b1d7c8d5-9932-468c-ab48-b015082bfc23');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"RxAbxD4siSnb","executionInfo":{"status":"ok","timestamp":1669862190430,"user_tz":300,"elapsed":352,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"outputId":"69e44a1b-586c-4b26-8de2-7e0208a4834f"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"b6053115-9630-4c32-9bd5-98f00bad4770\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b6053115-9630-4c32-9bd5-98f00bad4770\")) {                    Plotly.newPlot(                        \"b6053115-9630-4c32-9bd5-98f00bad4770\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9122,0.9181,0.914,0.9216,0.9139,0.9241,0.9104,0.9128,0.9179,0.9168],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9051,0.9039,0.9008,0.9058,0.9005,0.9036,0.8915,0.9018,0.9109,0.9095],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[0.9023,0.9027,0.8995,0.9056,0.8999,0.9043,0.891,0.8999,0.9086,0.9071],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b6053115-9630-4c32-9bd5-98f00bad4770');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"B65RyWkYiurV"},"execution_count":null,"outputs":[]}]}