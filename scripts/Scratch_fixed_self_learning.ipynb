{"cells":[{"cell_type":"markdown","metadata":{"id":"F_LX9XAD32So"},"source":["# Load Data"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1669693996736,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"8YtjMtuScUxg","outputId":"94e5bae4-61af-4357-f7a2-cd90b100ebc6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nTo Do:\\n1. Try different topN and Domain data.\\n2. 存结果（图，模型，log）\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["'''\n","To Do:\n","1. Try different topN and Domain data.\n","2. 存结果（图，模型，log）\n","'''"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2135,"status":"ok","timestamp":1669693998862,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"C3soh1b03deD","outputId":"3ba32c0d-ef80-4fbd-d637-38b2cb66908c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.26.17)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.17 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.17)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.17->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2040,"status":"ok","timestamp":1669694000899,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"jIFha6OOht8L","outputId":"7ab8b926-26c4-44ef-eb2c-5fa755c6b210"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from utils import read_conll_file, read_data\n","\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CPKysG3i3nsR","executionInfo":{"status":"ok","timestamp":1669694000899,"user_tz":300,"elapsed":4,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3069,"status":"ok","timestamp":1669694003965,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"0KqectYYC30N","outputId":"43e2494b-3fa4-4ff9-87a7-f5515d18a9fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}],"source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1007,"status":"ok","timestamp":1669694004969,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"CgySDvNl3xkh","outputId":"03f5e16b-bcb7-4fcb-9d31-e52ee13e8ef6"},"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}],"source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"]},{"cell_type":"markdown","metadata":{"id":"V9yUXS679IFc"},"source":["# Build Model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"7nIm4vqm3xiL","executionInfo":{"status":"ok","timestamp":1669694004970,"user_tz":300,"elapsed":3,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zleK0sd96JRp","executionInfo":{"status":"ok","timestamp":1669694004970,"user_tz":300,"elapsed":3,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6fRrkkC26JP_","executionInfo":{"status":"ok","timestamp":1669694005974,"user_tz":300,"elapsed":1007,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"T9SUpDvLH2vE","executionInfo":{"status":"ok","timestamp":1669694005974,"user_tz":300,"elapsed":7,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# tokens = tokenizer.tokenize(\"mistakenly\")\n","# tokens"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ggEO3eQKH54f","executionInfo":{"status":"ok","timestamp":1669694005974,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# tid = tokenizer.convert_tokens_to_ids(tokens)\n","# tid"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"WRlUglUjIF36","executionInfo":{"status":"ok","timestamp":1669694005975,"user_tz":300,"elapsed":7,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# tokenizer.convert_ids_to_tokens([234,2000,3000,22893])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"RpKgRRbK6JMr","executionInfo":{"status":"ok","timestamp":1669694005975,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"MZ_JndBu6LjA","executionInfo":{"status":"ok","timestamp":1669694005975,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"9mthfoFt6JFJ","executionInfo":{"status":"ok","timestamp":1669694005976,"user_tz":300,"elapsed":7,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["from pytorch_pretrained_bert import BertModel"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"e6ydlTI16JCz","executionInfo":{"status":"ok","timestamp":1669694005976,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DeD_19uq6JAd","executionInfo":{"status":"ok","timestamp":1669694005976,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"DW4KvG4x6I91","executionInfo":{"status":"ok","timestamp":1669694005976,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["def eval(model, iterator, average=\"macro\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n","    precision_value = precision_score(y_true, y_pred, average=average)\n","    recall_value = recall_score(y_true, y_pred, average=average)\n","    f1_value = f1_score(y_true, y_pred, average=average)\n","\n","    return precision_value, recall_value, f1_value"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0ZDK1-UU6I5K","executionInfo":{"status":"ok","timestamp":1669694017418,"user_tz":300,"elapsed":11447,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"f5pQmdTS6I20","executionInfo":{"status":"ok","timestamp":1669694018828,"user_tz":300,"elapsed":1413,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"B0x3gRfi9iyA","executionInfo":{"status":"ok","timestamp":1669694018828,"user_tz":300,"elapsed":3,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# train(model, train_iter, optimizer, criterion)\n","# eval(model, test_iter)"]},{"cell_type":"markdown","metadata":{"id":"6fVp31VJ5U64"},"source":["# Save Model"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"LtVeE3zd04C8","executionInfo":{"status":"ok","timestamp":1669694018828,"user_tz":300,"elapsed":2,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["model_file = os.path.join(model_dir, \"base_model.pt\")\n","# torch.save(model.state_dict(), model_file)"]},{"cell_type":"markdown","metadata":{"id":"cyvpy9QH4sQd"},"source":["## Load Model"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":10431,"status":"ok","timestamp":1669694029257,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"hAD3Wd574v6Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"996e2851-0dd3-4495-bf9c-dceb35da851b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":23}],"source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","# wsj_precision_value, wsj_recall_value, wsj_f1_value = eval(model, test_iter)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"2M6AkbHMD6VJ","executionInfo":{"status":"ok","timestamp":1669694029258,"user_tz":300,"elapsed":8,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# wsj_precision_value, wsj_recall_value, wsj_f1_value"]},{"cell_type":"markdown","metadata":{"id":"reoycWJi5azd"},"source":["# Self Training"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"agIHM1TmEYl3","executionInfo":{"status":"ok","timestamp":1669694029258,"user_tz":300,"elapsed":7,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"mGm3QLNcD8bU","executionInfo":{"status":"ok","timestamp":1669694029258,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"fwvivWyzEHOi","executionInfo":{"status":"ok","timestamp":1669694029258,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["domain = \"answers\"\n","domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669694029259,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"UHAOs-fdEHMO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"932486ec-4fcd-4300-8656-037944f66df0"},"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"]}],"source":["domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"3JGnpmRNHHmz","executionInfo":{"status":"ok","timestamp":1669694029259,"user_tz":300,"elapsed":5,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["domain_precision_value_lst = []\n","domain_recall_value_lst = []\n","domain_f1_value_lst = []"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":7716,"status":"ok","timestamp":1669694036970,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"},"user_tz":300},"id":"cHb8ZM-VjG-7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9d44edf-f69f-458b-9642-376d40de132e"},"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.64      0.88      0.74         8\n","           2       0.82      0.41      0.55        44\n","           3       1.00      0.87      0.93       987\n","           4       0.99      0.98      0.99       108\n","           5       0.97      0.98      0.98       115\n","           6       1.00      0.97      0.99      1600\n","           7       0.20      0.73      0.31        48\n","           8       0.00      0.00      0.00         4\n","           9       0.99      0.98      0.99      1086\n","          10       0.94      0.98      0.96       386\n","          11       0.97      0.99      0.98      2229\n","          12       0.98      0.95      0.97        61\n","          13       0.27      0.50      0.35        18\n","          14       0.36      0.75      0.49        28\n","          15       0.91      0.97      0.94      2566\n","          16       0.92      0.81      0.86      1511\n","          17       0.81      0.89      0.84        98\n","          18       0.78      0.96      0.86        56\n","          19       0.85      0.85      0.85        20\n","          20       1.00      0.94      0.97       717\n","          21       0.00      0.00      0.00        45\n","          22       0.91      0.92      0.91      3623\n","          23       0.69      0.78      0.73       563\n","          24       0.32      0.47      0.38        19\n","          25       0.93      0.95      0.94      1162\n","          26       0.77      0.96      0.86        28\n","          27       0.77      0.87      0.81        53\n","          28       0.99      0.99      0.99      2476\n","          29       0.99      0.94      0.97       563\n","          30       0.90      0.86      0.88      1947\n","          31       0.77      0.53      0.63        45\n","          32       1.00      0.63      0.77        27\n","          33       0.67      0.83      0.74       151\n","          34       0.29      0.50      0.37        34\n","          35       0.93      0.99      0.96       537\n","          36       0.95      0.32      0.48       124\n","          37       0.95      0.94      0.94      1907\n","          38       0.95      0.94      0.94       465\n","          39       0.96      0.94      0.95       522\n","          40       0.79      0.93      0.85       404\n","          41       0.90      0.92      0.91      1089\n","          42       0.98      0.97      0.98       802\n","          43       0.94      0.85      0.89       143\n","          44       0.86      0.99      0.92       108\n","          45       1.00      1.00      1.00         1\n","          46       0.97      0.98      0.98       192\n","          47       0.00      0.00      0.00         2\n","          48       0.00      0.00      0.00        48\n","\n","    accuracy                           0.93     28770\n","   macro avg       0.76      0.78      0.76     28770\n","weighted avg       0.93      0.93      0.93     28770\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["domain_test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","domain_test_iter = data.DataLoader(dataset=domain_test_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","\n","domain_precision_value_lst.append(domain_precision_value)\n","domain_recall_value_lst.append(domain_recall_value)\n","domain_f1_value_lst.append(domain_f1_value)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"S1HrHo0LEhWN","executionInfo":{"status":"ok","timestamp":1669694036971,"user_tz":300,"elapsed":18,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["class PosDataset_new(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        self.word_lst, self.tag_lst = word_lst, tag_lst\n","\n","    def __len__(self):\n","      return len(self.word_lst)\n","\n","    def __getitem__(self, idx):\n","      words, tags = self.word_lst[idx], self.tag_lst[idx] # words, tags: string list\n","      assert len(words)==len(tags)\n","        # seqlen\n","      seqlen = len(words)\n","\n","      return words, tags, seqlen\n","\n","def pad_new(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    tags = f(1)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(0, maxlen)\n","    y = f(1, maxlen)\n","\n","    f = torch.LongTensor\n","\n","    return f(x), f(y), seqlens\n","\n","def train_new(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        x, y, seqlens = batch\n","        \n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"vzWOF5sCGjgC","executionInfo":{"status":"ok","timestamp":1669694036971,"user_tz":300,"elapsed":17,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["def gen_pseudo_data(model, domain_dev_iter, topn=300, initial=True):\n","  model.eval()\n","\n","  LLD = []\n","  MEAN_PROB = []\n","  new_x_lst = []\n","  new_y_lst = []\n","\n","  if initial:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          _, x, _, _, y, _ = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","          \n","          # Rank by LLD\n","          # lld = torch.prod(max_prob, 1)\n","          # LLD.extend(lld)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","  else:\n","    with torch.no_grad():\n","        for i, batch in enumerate(domain_dev_iter):\n","\n","          x, y, seqlens = batch\n","          sen_len = y.bool().sum(axis=1)\n","\n","          logits, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","          # Save prediction as new training dataset\n","          softmax_value = torch.softmax(logits, dim=2)\n","          max_prob = torch.amax(softmax_value, dim=2)\n","\n","          # Rank by mean probability\n","          res_prob = y.bool().to(device) * max_prob.to(device)\n","          sum_prob = res_prob.sum(axis=1)\n","          mean_prob = sum_prob / sen_len.to(device)\n","          MEAN_PROB.extend(mean_prob)\n","          \n","          new_x_lst.extend(x.tolist())\n","          new_y_lst.extend(y_hat.tolist())\n","\n","  ind = list(range(len(MEAN_PROB)))\n","  ind = [x for _, x in sorted(zip(MEAN_PROB, ind), reverse=True)]\n","\n","  select_ind = ind[: topn]\n","  not_select_ind = ind[topn: ]\n","\n","  new_train_x = [new_x_lst[i] for i in select_ind]\n","  new_train_y = [new_y_lst[i] for i in select_ind]\n","\n","  remain_train_x = [new_x_lst[i] for i in not_select_ind]\n","  remain_train_y = [new_y_lst[i] for i in not_select_ind]\n","\n","  return new_train_x, new_train_y, remain_train_x, remain_train_y"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"zl3VMmnVVD-8","executionInfo":{"status":"ok","timestamp":1669695972048,"user_tz":300,"elapsed":1935093,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20eaac2c-f744-4e97-f633-37e62ce53192"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loop 1\n","domain_dev_word_lst 1713\n","Train from scratch...\n","step: 0, loss: 3.873807430267334\n","step: 10, loss: 1.7628498077392578\n","step: 20, loss: 0.8586282730102539\n","step: 30, loss: 0.45573461055755615\n","step: 40, loss: 0.3006611168384552\n","step: 50, loss: 0.23682256042957306\n","step: 60, loss: 0.2827904522418976\n","step: 70, loss: 0.14820179343223572\n","step: 80, loss: 0.2087739259004593\n","step: 90, loss: 0.07022108137607574\n","step: 100, loss: 0.2536206543445587\n","step: 110, loss: 0.16104190051555634\n","step: 120, loss: 0.10658647865056992\n","step: 130, loss: 0.12622720003128052\n","step: 140, loss: 0.19432146847248077\n","step: 150, loss: 0.07479424774646759\n","step: 160, loss: 0.05321161076426506\n","step: 170, loss: 0.07537906616926193\n","step: 180, loss: 0.08082715421915054\n","step: 190, loss: 0.1110253855586052\n","step: 200, loss: 0.18054001033306122\n","step: 210, loss: 0.1799662709236145\n","step: 220, loss: 0.12505507469177246\n","step: 230, loss: 0.13595665991306305\n","step: 240, loss: 0.1348154991865158\n","step: 250, loss: 0.14813962578773499\n","step: 260, loss: 0.13413293659687042\n","step: 270, loss: 0.15350602567195892\n","step: 280, loss: 0.11608336865901947\n","step: 290, loss: 0.06817301362752914\n","step: 300, loss: 0.15287131071090698\n","step: 310, loss: 0.09345486015081406\n","step: 320, loss: 0.07072467356920242\n","step: 330, loss: 0.17787788808345795\n","step: 340, loss: 0.058836642652750015\n","step: 350, loss: 0.07460451871156693\n","step: 360, loss: 0.11863075196743011\n","step: 370, loss: 0.05721680447459221\n","step: 380, loss: 0.1524903029203415\n","step: 390, loss: 0.12610402703285217\n","step: 400, loss: 0.06903841346502304\n","step: 410, loss: 0.13298772275447845\n","step: 420, loss: 0.08499187231063843\n","step: 430, loss: 0.1792413890361786\n","step: 440, loss: 0.06988883763551712\n","step: 450, loss: 0.0850193127989769\n","step: 460, loss: 0.12538942694664001\n","step: 470, loss: 0.07759685069322586\n","step: 480, loss: 0.10874410718679428\n","step: 490, loss: 0.1864868402481079\n","step: 500, loss: 0.03433506190776825\n","step: 510, loss: 0.09373003989458084\n","step: 520, loss: 0.1480613499879837\n","step: 530, loss: 0.13718818128108978\n","step: 540, loss: 0.09939060360193253\n","step: 550, loss: 0.1246020570397377\n","step: 560, loss: 0.06058685481548309\n","step: 570, loss: 0.10201698541641235\n","step: 580, loss: 0.13637231290340424\n","step: 590, loss: 0.035879358649253845\n","step: 600, loss: 0.10172545164823532\n","step: 610, loss: 0.04390668496489525\n","step: 620, loss: 0.1290230005979538\n","step: 630, loss: 0.05421359837055206\n","step: 640, loss: 0.16976989805698395\n","step: 650, loss: 0.15482807159423828\n","step: 660, loss: 0.18301905691623688\n","step: 670, loss: 0.17795860767364502\n","step: 680, loss: 0.07527495175600052\n","step: 690, loss: 0.09634621441364288\n","step: 700, loss: 0.05536559969186783\n","step: 710, loss: 0.053142547607421875\n","step: 720, loss: 0.07688441127538681\n","step: 730, loss: 0.07384607195854187\n","step: 740, loss: 0.08828116953372955\n","step: 750, loss: 0.07366937398910522\n","step: 760, loss: 0.12522658705711365\n","step: 770, loss: 0.06685826927423477\n","step: 780, loss: 0.16907258331775665\n","step: 790, loss: 0.07202322781085968\n","step: 800, loss: 0.15229232609272003\n","step: 810, loss: 0.058768272399902344\n","step: 820, loss: 0.13093163073062897\n","step: 830, loss: 0.11316485702991486\n","step: 840, loss: 0.14034435153007507\n","step: 850, loss: 0.24121998250484467\n","step: 860, loss: 0.09392370283603668\n","step: 870, loss: 0.15444603562355042\n","step: 880, loss: 0.08650828897953033\n","step: 890, loss: 0.035844478756189346\n","step: 900, loss: 0.0950993075966835\n","step: 910, loss: 0.13171538710594177\n","step: 920, loss: 0.0897866040468216\n","step: 930, loss: 0.18176276981830597\n","step: 940, loss: 0.10355411469936371\n","step: 950, loss: 0.0696464404463768\n","step: 960, loss: 0.0524168536067009\n","step: 970, loss: 0.1715538054704666\n","step: 980, loss: 0.046987637877464294\n","step: 990, loss: 0.1577429473400116\n","step: 1000, loss: 0.05948413535952568\n","step: 1010, loss: 0.1448957771062851\n","step: 1020, loss: 0.13526728749275208\n","step: 1030, loss: 0.02284964732825756\n","step: 1040, loss: 0.10053769499063492\n","step: 1050, loss: 0.03548097237944603\n","step: 1060, loss: 0.1095627173781395\n","step: 1070, loss: 0.06811299920082092\n","step: 1080, loss: 0.07675934582948685\n","step: 1090, loss: 0.12201520055532455\n","step: 1100, loss: 0.09544836729764938\n","step: 1110, loss: 0.041738420724868774\n","step: 1120, loss: 0.04051847755908966\n","step: 1130, loss: 0.07996436953544617\n","step: 1140, loss: 0.04519766569137573\n","step: 1150, loss: 0.0946291834115982\n","step: 1160, loss: 0.28889697790145874\n","step: 1170, loss: 0.01743888296186924\n","step: 1180, loss: 0.12051086127758026\n","step: 1190, loss: 0.07491573691368103\n","step: 1200, loss: 0.11095376312732697\n","step: 1210, loss: 0.17549143731594086\n","step: 1220, loss: 0.12682539224624634\n","step: 1230, loss: 0.08370358496904373\n","step: 1240, loss: 0.1283441036939621\n","step: 1250, loss: 0.12981292605400085\n","step: 1260, loss: 0.028050987049937248\n","step: 1270, loss: 0.05183662101626396\n","step: 1280, loss: 0.07739000022411346\n","step: 1290, loss: 0.20397672057151794\n","step: 1300, loss: 0.09059575200080872\n","step: 1310, loss: 0.04953256621956825\n","step: 1320, loss: 0.06512782722711563\n","step: 1330, loss: 0.060919616371393204\n","step: 1340, loss: 0.22150731086730957\n","step: 1350, loss: 0.12095989286899567\n","step: 1360, loss: 0.061720505356788635\n","step: 1370, loss: 0.07823235541582108\n","step: 1380, loss: 0.06765002012252808\n","step: 1390, loss: 0.13314670324325562\n","step: 1400, loss: 0.06097453832626343\n","step: 1410, loss: 0.1211351826786995\n","step: 1420, loss: 0.08607776463031769\n","step: 1430, loss: 0.06383778154850006\n","step: 1440, loss: 0.08489155769348145\n","step: 1450, loss: 0.027846170589327812\n","step: 1460, loss: 0.07787958532571793\n","step: 1470, loss: 0.1440819501876831\n","step: 1480, loss: 0.07141173630952835\n","step: 1490, loss: 0.13886579871177673\n","step: 1500, loss: 0.12152715027332306\n","step: 1510, loss: 0.09275597333908081\n","step: 1520, loss: 0.18167924880981445\n","step: 1530, loss: 0.20391328632831573\n","step: 1540, loss: 0.09333145618438721\n","step: 1550, loss: 0.07151336967945099\n","step: 1560, loss: 0.07743480056524277\n","step: 1570, loss: 0.09548074007034302\n","step: 1580, loss: 0.04773429036140442\n","step: 1590, loss: 0.15489794313907623\n","step: 1600, loss: 0.12004318833351135\n","step: 1610, loss: 0.08348182588815689\n","step: 1620, loss: 0.03394528478384018\n","step: 1630, loss: 0.14715909957885742\n","step: 1640, loss: 0.06373660266399384\n","step: 1650, loss: 0.09247558563947678\n","step: 1660, loss: 0.07785404473543167\n","step: 1670, loss: 0.13006211817264557\n","step: 1680, loss: 0.10780644416809082\n","step: 1690, loss: 0.03335633501410484\n","step: 1700, loss: 0.14789104461669922\n","step: 1710, loss: 0.1406407356262207\n","step: 1720, loss: 0.13838984072208405\n","step: 1730, loss: 0.03548752889037132\n","step: 1740, loss: 0.05184818059206009\n","step: 1750, loss: 0.05557402968406677\n","step: 1760, loss: 0.06874725222587585\n","step: 1770, loss: 0.040223728865385056\n","step: 1780, loss: 0.04761706292629242\n","step: 1790, loss: 0.1529647558927536\n","step: 1800, loss: 0.15833856165409088\n","step: 1810, loss: 0.10251092165708542\n","step: 1820, loss: 0.1177619993686676\n","step: 1830, loss: 0.05719519406557083\n","step: 1840, loss: 0.05965631082653999\n","step: 1850, loss: 0.06750072538852692\n","step: 1860, loss: 0.09594026952981949\n","step: 1870, loss: 0.09081996232271194\n","step: 1880, loss: 0.06234633922576904\n","step: 1890, loss: 0.15072228014469147\n","step: 1900, loss: 0.0510556697845459\n","step: 1910, loss: 0.11848966032266617\n","step: 1920, loss: 0.11682439595460892\n","step: 1930, loss: 0.12430015206336975\n","step: 1940, loss: 0.11631986498832703\n","step: 1950, loss: 0.03855624422430992\n","step: 1960, loss: 0.10711117833852768\n","step: 1970, loss: 0.12324709445238113\n","step: 1980, loss: 0.04188040271401405\n","step: 1990, loss: 0.07302995026111603\n","step: 2000, loss: 0.11808498203754425\n","step: 2010, loss: 0.10087078809738159\n","step: 2020, loss: 0.08162444084882736\n","step: 2030, loss: 0.19784030318260193\n","step: 2040, loss: 0.03329125791788101\n","step: 2050, loss: 0.06690675020217896\n","step: 2060, loss: 0.0829775333404541\n","step: 2070, loss: 0.08518055826425552\n","step: 2080, loss: 0.1285790652036667\n","step: 2090, loss: 0.14493818581104279\n","step: 2100, loss: 0.09015227854251862\n","step: 2110, loss: 0.11873531341552734\n","step: 2120, loss: 0.10391274839639664\n","step: 2130, loss: 0.102094367146492\n","step: 2140, loss: 0.07349957525730133\n","step: 2150, loss: 0.05979970097541809\n","step: 2160, loss: 0.15050654113292694\n","step: 2170, loss: 0.06973966211080551\n","step: 2180, loss: 0.09125109761953354\n","step: 2190, loss: 0.061212051659822464\n","step: 2200, loss: 0.059159524738788605\n","step: 2210, loss: 0.13396140933036804\n","step: 2220, loss: 0.13589760661125183\n","step: 2230, loss: 0.030671920627355576\n","step: 2240, loss: 0.13275600969791412\n","step: 2250, loss: 0.051027119159698486\n","step: 2260, loss: 0.10733776539564133\n","step: 2270, loss: 0.07599835097789764\n","step: 2280, loss: 0.042193010449409485\n","step: 2290, loss: 0.11297082155942917\n","step: 2300, loss: 0.05124755576252937\n","step: 2310, loss: 0.11541139334440231\n","step: 2320, loss: 0.057006459683179855\n","step: 2330, loss: 0.12977354228496552\n","step: 2340, loss: 0.37940874695777893\n","step: 2350, loss: 0.08374547958374023\n","step: 2360, loss: 0.12542353570461273\n","step: 2370, loss: 0.1488770991563797\n","step: 2380, loss: 0.06854134798049927\n","step: 2390, loss: 0.07548905164003372\n","step: 2400, loss: 0.146282359957695\n","step: 2410, loss: 0.07936292141675949\n","step: 2420, loss: 0.09174700081348419\n","step: 2430, loss: 0.09319491684436798\n","step: 2440, loss: 0.06251035630702972\n","step: 2450, loss: 0.14277401566505432\n","step: 2460, loss: 0.2647450864315033\n","step: 2470, loss: 0.0637783482670784\n","step: 2480, loss: 0.1373203545808792\n","step: 2490, loss: 0.0610031932592392\n","step: 2500, loss: 0.11633874475955963\n","step: 2510, loss: 0.11779391020536423\n","step: 2520, loss: 0.06060007959604263\n","step: 2530, loss: 0.03293958306312561\n","step: 2540, loss: 0.07498084008693695\n","step: 2550, loss: 0.09194817394018173\n","step: 2560, loss: 0.17849445343017578\n","step: 2570, loss: 0.11968010663986206\n","step: 2580, loss: 0.03387303277850151\n","step: 2590, loss: 0.04262525215744972\n","step: 2600, loss: 0.1238018199801445\n","step: 2610, loss: 0.0922664999961853\n","step: 2620, loss: 0.03368682786822319\n","step: 2630, loss: 0.0360906608402729\n","step: 2640, loss: 0.10817992687225342\n","step: 2650, loss: 0.09036146849393845\n","step: 2660, loss: 0.041943926364183426\n","step: 2670, loss: 0.05332423746585846\n","step: 2680, loss: 0.06441185623407364\n","step: 2690, loss: 0.058246809989213943\n","step: 2700, loss: 0.10616938024759293\n","step: 2710, loss: 0.07258247584104538\n","step: 2720, loss: 0.09760849177837372\n","step: 2730, loss: 0.030549796298146248\n","step: 2740, loss: 0.11107674241065979\n","step: 2750, loss: 0.13093681633472443\n","step: 2760, loss: 0.06342625617980957\n","step: 2770, loss: 0.11180837452411652\n","step: 2780, loss: 0.16304799914360046\n","step: 2790, loss: 0.09055303037166595\n","step: 2800, loss: 0.05171281099319458\n","step: 2810, loss: 0.09007901698350906\n","step: 2820, loss: 0.028091751039028168\n","step: 2830, loss: 0.2169020026922226\n","step: 2840, loss: 0.15431520342826843\n","step: 2850, loss: 0.0349428653717041\n","step: 2860, loss: 0.09536660462617874\n","step: 2870, loss: 0.22812063992023468\n","step: 2880, loss: 0.01921912282705307\n","step: 2890, loss: 0.026472188532352448\n","step: 2900, loss: 0.14374418556690216\n","step: 2910, loss: 0.086717888712883\n","step: 2920, loss: 0.05336707830429077\n","step: 2930, loss: 0.07751215994358063\n","step: 2940, loss: 0.014152769930660725\n","step: 2950, loss: 0.026681330054998398\n","step: 2960, loss: 0.0545157827436924\n","step: 2970, loss: 0.07920710742473602\n","step: 2980, loss: 0.08897857367992401\n","step: 2990, loss: 0.021122025325894356\n","step: 3000, loss: 0.09506262093782425\n","step: 3010, loss: 0.15000195801258087\n","step: 3020, loss: 0.04195364937186241\n","step: 3030, loss: 0.06368646025657654\n","step: 3040, loss: 0.10626896470785141\n","step: 3050, loss: 0.1508145034313202\n","step: 3060, loss: 0.06849873811006546\n","step: 3070, loss: 0.023601669818162918\n","step: 3080, loss: 0.09564275294542313\n","step: 3090, loss: 0.09408270567655563\n","step: 3100, loss: 0.09138847142457962\n","step: 3110, loss: 0.12290176749229431\n","step: 3120, loss: 0.14080475270748138\n","step: 3130, loss: 0.09363435208797455\n","step: 3140, loss: 0.07115796208381653\n","step: 3150, loss: 0.09000173211097717\n","step: 3160, loss: 0.07733328640460968\n","step: 3170, loss: 0.04084126278758049\n","step: 3180, loss: 0.06622220575809479\n","step: 3190, loss: 0.16040517389774323\n","step: 3200, loss: 0.0997701957821846\n","step: 3210, loss: 0.18305349349975586\n","step: 3220, loss: 0.14155347645282745\n","step: 3230, loss: 0.061173245310783386\n","step: 3240, loss: 0.05867397412657738\n","step: 3250, loss: 0.02280787192285061\n","step: 3260, loss: 0.046312496066093445\n","step: 3270, loss: 0.14267386496067047\n","step: 3280, loss: 0.056871622800827026\n","step: 3290, loss: 0.1147879958152771\n","step: 3300, loss: 0.11872705072164536\n","step: 3310, loss: 0.2267567217350006\n","step: 3320, loss: 0.15368764102458954\n","step: 3330, loss: 0.08021293580532074\n","step: 3340, loss: 0.11709388345479965\n","step: 3350, loss: 0.08901309221982956\n","step: 3360, loss: 0.08372389525175095\n","step: 3370, loss: 0.06793671101331711\n","step: 3380, loss: 0.2021051049232483\n","step: 3390, loss: 0.09795831143856049\n","step: 3400, loss: 0.03408265858888626\n","step: 3410, loss: 0.1167505532503128\n","step: 3420, loss: 0.07053875923156738\n","step: 3430, loss: 0.05399947986006737\n","step: 3440, loss: 0.0638861283659935\n","step: 3450, loss: 0.04854634404182434\n","step: 3460, loss: 0.06553736329078674\n","step: 3470, loss: 0.06094284728169441\n","step: 3480, loss: 0.13914945721626282\n","step: 3490, loss: 0.06147448718547821\n","step: 3500, loss: 0.0839015319943428\n","step: 3510, loss: 0.05835597589612007\n","step: 3520, loss: 0.06376732140779495\n","step: 3530, loss: 0.2044932246208191\n","step: 3540, loss: 0.11642460525035858\n","step: 3550, loss: 0.03316522762179375\n","step: 3560, loss: 0.12696236371994019\n","step: 3570, loss: 0.07234273850917816\n","step: 3580, loss: 0.05720371752977371\n","step: 3590, loss: 0.12291695177555084\n","step: 3600, loss: 0.06618127971887589\n","step: 3610, loss: 0.06293758004903793\n","step: 3620, loss: 0.054513391107320786\n","step: 3630, loss: 0.1266964226961136\n","step: 3640, loss: 0.04246757924556732\n","step: 3650, loss: 0.0722222626209259\n","step: 3660, loss: 0.13001763820648193\n","step: 3670, loss: 0.14959552884101868\n","step: 3680, loss: 0.17945514619350433\n","step: 3690, loss: 0.16343176364898682\n","step: 3700, loss: 0.08710376918315887\n","step: 3710, loss: 0.07018589228391647\n","step: 3720, loss: 0.09504013508558273\n","step: 3730, loss: 0.04274650290608406\n","step: 3740, loss: 0.09348716586828232\n","step: 3750, loss: 0.16378697752952576\n","step: 3760, loss: 0.1369444876909256\n","step: 3770, loss: 0.13803090155124664\n","step: 3780, loss: 0.06390437483787537\n","step: 3790, loss: 0.0247725248336792\n","step: 3800, loss: 0.06957417726516724\n","acc=0.92\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.50      1.00      0.67         8\n","           2       0.78      0.64      0.70        44\n","           3       1.00      0.87      0.93       987\n","           4       0.95      0.98      0.97       108\n","           5       1.00      0.98      0.99       115\n","           6       1.00      0.97      0.98      1600\n","           7       0.19      0.75      0.31        48\n","           8       0.00      0.00      0.00         4\n","           9       0.98      0.98      0.98      1086\n","          10       0.96      0.95      0.95       386\n","          11       0.97      0.99      0.98      2229\n","          12       1.00      0.95      0.97        61\n","          13       0.21      1.00      0.34        18\n","          14       0.40      0.75      0.52        28\n","          15       0.92      0.97      0.95      2566\n","          16       0.89      0.83      0.86      1511\n","          17       0.91      0.84      0.87        98\n","          18       0.84      0.93      0.88        56\n","          19       0.65      1.00      0.78        20\n","          20       0.98      0.97      0.98       717\n","          21       1.00      0.04      0.09        45\n","          22       0.94      0.89      0.91      3623\n","          23       0.68      0.77      0.72       563\n","          24       0.30      0.63      0.41        19\n","          25       0.94      0.94      0.94      1162\n","          26       0.55      0.39      0.46        28\n","          27       1.00      0.55      0.71        53\n","          28       0.98      1.00      0.99      2476\n","          29       1.00      0.93      0.96       563\n","          30       0.90      0.85      0.88      1947\n","          31       0.69      0.73      0.71        45\n","          32       0.94      0.56      0.70        27\n","          33       0.63      0.74      0.68       151\n","          34       0.40      0.91      0.56        34\n","          35       0.97      0.99      0.98       537\n","          36       0.93      0.11      0.20       124\n","          37       0.91      0.95      0.93      1907\n","          38       0.84      0.97      0.90       465\n","          39       0.92      0.99      0.95       522\n","          40       0.76      0.88      0.81       404\n","          41       0.92      0.92      0.92      1089\n","          42       0.99      0.97      0.98       802\n","          43       0.94      0.71      0.81       143\n","          44       0.85      0.98      0.91       108\n","          45       1.00      1.00      1.00         1\n","          46       0.99      0.98      0.99       192\n","          47       0.00      0.00      0.00         2\n","          48       0.50      0.08      0.14        48\n","\n","    accuracy                           0.92     28770\n","   macro avg       0.78      0.79      0.75     28770\n","weighted avg       0.93      0.92      0.92     28770\n","\n","\n","Loop 2\n","domain_dev_word_lst 1370\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 4.006485462188721\n","step: 10, loss: 1.6920629739761353\n","step: 20, loss: 0.5858360528945923\n","step: 30, loss: 0.3194621503353119\n","step: 40, loss: 0.3381424844264984\n","step: 50, loss: 0.36956024169921875\n","step: 60, loss: 0.18898659944534302\n","step: 70, loss: 0.2660520076751709\n","step: 80, loss: 0.21164844930171967\n","step: 90, loss: 0.07743684947490692\n","step: 100, loss: 0.1992431879043579\n","step: 110, loss: 0.17276734113693237\n","step: 120, loss: 0.12371756136417389\n","step: 130, loss: 0.1008838638663292\n","step: 140, loss: 0.18373408913612366\n","step: 150, loss: 0.07199489325284958\n","step: 160, loss: 0.14443683624267578\n","step: 170, loss: 0.15286271274089813\n","step: 180, loss: 0.12981636822223663\n","step: 190, loss: 0.15759624540805817\n","step: 200, loss: 0.12324119359254837\n","step: 210, loss: 0.1274838149547577\n","step: 220, loss: 0.059503719210624695\n","step: 230, loss: 0.06552645564079285\n","step: 240, loss: 0.1769491732120514\n","step: 250, loss: 0.059940073639154434\n","step: 260, loss: 0.1874794363975525\n","step: 270, loss: 0.06329302489757538\n","step: 280, loss: 0.11444775760173798\n","step: 290, loss: 0.06845829635858536\n","step: 300, loss: 0.10549618303775787\n","step: 310, loss: 0.20086519420146942\n","step: 320, loss: 0.11389583349227905\n","step: 330, loss: 0.08877308666706085\n","step: 340, loss: 0.0746639221906662\n","step: 350, loss: 0.1310758888721466\n","step: 360, loss: 0.06964846700429916\n","step: 370, loss: 0.12299304455518723\n","step: 380, loss: 0.11795981228351593\n","step: 390, loss: 0.0999591052532196\n","step: 400, loss: 0.13857679069042206\n","step: 410, loss: 0.05974540114402771\n","step: 420, loss: 0.056390371173620224\n","step: 430, loss: 0.12694665789604187\n","step: 440, loss: 0.23342637717723846\n","step: 450, loss: 0.09964828938245773\n","step: 460, loss: 0.07373973727226257\n","step: 470, loss: 0.17873132228851318\n","step: 480, loss: 0.18670223653316498\n","step: 490, loss: 0.08951417356729507\n","step: 500, loss: 0.2687302231788635\n","step: 510, loss: 0.13246122002601624\n","step: 520, loss: 0.29537898302078247\n","step: 530, loss: 0.1283005028963089\n","step: 540, loss: 0.04996641352772713\n","step: 550, loss: 0.10095640271902084\n","step: 560, loss: 0.08984553813934326\n","step: 570, loss: 0.08673511445522308\n","step: 580, loss: 0.1750916987657547\n","step: 590, loss: 0.10203319042921066\n","step: 600, loss: 0.12214583903551102\n","step: 610, loss: 0.10250110924243927\n","step: 620, loss: 0.09310364723205566\n","step: 630, loss: 0.056352123618125916\n","step: 640, loss: 0.1250094324350357\n","step: 650, loss: 0.07374690473079681\n","step: 660, loss: 0.11512885987758636\n","step: 670, loss: 0.10591444373130798\n","step: 680, loss: 0.09690940380096436\n","step: 690, loss: 0.09305237978696823\n","step: 700, loss: 0.11782576143741608\n","step: 710, loss: 0.050509657710790634\n","step: 720, loss: 0.09052445739507675\n","step: 730, loss: 0.09358031302690506\n","step: 740, loss: 0.07643802464008331\n","step: 750, loss: 0.07480095326900482\n","step: 760, loss: 0.058614786714315414\n","step: 770, loss: 0.1324014514684677\n","step: 780, loss: 0.059938546270132065\n","step: 790, loss: 0.24343502521514893\n","step: 800, loss: 0.10121820122003555\n","step: 810, loss: 0.0898236408829689\n","step: 820, loss: 0.05712290480732918\n","step: 830, loss: 0.051192451268434525\n","step: 840, loss: 0.10910703986883163\n","step: 850, loss: 0.11384066939353943\n","step: 860, loss: 0.05953824520111084\n","step: 870, loss: 0.046351026743650436\n","step: 880, loss: 0.09101704508066177\n","step: 890, loss: 0.07364287972450256\n","step: 900, loss: 0.07487154006958008\n","step: 910, loss: 0.06994412839412689\n","step: 920, loss: 0.06532081961631775\n","step: 930, loss: 0.09098152816295624\n","step: 940, loss: 0.11255203932523727\n","step: 950, loss: 0.14288875460624695\n","step: 960, loss: 0.08577238023281097\n","step: 970, loss: 0.10659998655319214\n","step: 980, loss: 0.06442765146493912\n","step: 990, loss: 0.10741004347801208\n","step: 1000, loss: 0.14655250310897827\n","step: 1010, loss: 0.11552399396896362\n","step: 1020, loss: 0.037743717432022095\n","step: 1030, loss: 0.06553689390420914\n","step: 1040, loss: 0.026994045823812485\n","step: 1050, loss: 0.06616076081991196\n","step: 1060, loss: 0.07073578238487244\n","step: 1070, loss: 0.18048979341983795\n","step: 1080, loss: 0.06746236979961395\n","step: 1090, loss: 0.10098780691623688\n","step: 1100, loss: 0.08623194694519043\n","step: 1110, loss: 0.08026206493377686\n","step: 1120, loss: 0.1311791092157364\n","step: 1130, loss: 0.08165436238050461\n","step: 1140, loss: 0.07656162232160568\n","step: 1150, loss: 0.11426617205142975\n","step: 1160, loss: 0.1943453699350357\n","step: 1170, loss: 0.05361973121762276\n","step: 1180, loss: 0.08037962019443512\n","step: 1190, loss: 0.10879526287317276\n","step: 1200, loss: 0.13655918836593628\n","step: 1210, loss: 0.1270924061536789\n","step: 1220, loss: 0.0473628006875515\n","step: 1230, loss: 0.06568113714456558\n","step: 1240, loss: 0.0669712945818901\n","step: 1250, loss: 0.1145206093788147\n","step: 1260, loss: 0.07979094237089157\n","step: 1270, loss: 0.08307220041751862\n","step: 1280, loss: 0.084825299680233\n","step: 1290, loss: 0.07887061685323715\n","step: 1300, loss: 0.09055046737194061\n","step: 1310, loss: 0.09564050287008286\n","step: 1320, loss: 0.07974167913198471\n","step: 1330, loss: 0.03878586366772652\n","step: 1340, loss: 0.13462968170642853\n","step: 1350, loss: 0.05547202378511429\n","step: 1360, loss: 0.04044702276587486\n","step: 1370, loss: 0.20555007457733154\n","step: 1380, loss: 0.06454970687627792\n","step: 1390, loss: 0.09690865874290466\n","step: 1400, loss: 0.18183916807174683\n","step: 1410, loss: 0.04356132447719574\n","step: 1420, loss: 0.015244521200656891\n","step: 1430, loss: 0.16171059012413025\n","step: 1440, loss: 0.12671484053134918\n","step: 1450, loss: 0.13091833889484406\n","step: 1460, loss: 0.04436191916465759\n","step: 1470, loss: 0.11744101345539093\n","step: 1480, loss: 0.025157978758215904\n","step: 1490, loss: 0.07787478715181351\n","step: 1500, loss: 0.1096465215086937\n","step: 1510, loss: 0.05618232861161232\n","step: 1520, loss: 0.07434013485908508\n","step: 1530, loss: 0.09945568442344666\n","step: 1540, loss: 0.09950310736894608\n","step: 1550, loss: 0.03221278265118599\n","step: 1560, loss: 0.0670202299952507\n","step: 1570, loss: 0.10060597956180573\n","step: 1580, loss: 0.03620769828557968\n","step: 1590, loss: 0.09031364321708679\n","step: 1600, loss: 0.10057669132947922\n","step: 1610, loss: 0.1388135403394699\n","step: 1620, loss: 0.1007298156619072\n","step: 1630, loss: 0.11296845227479935\n","step: 1640, loss: 0.07986682653427124\n","step: 1650, loss: 0.08094025403261185\n","step: 1660, loss: 0.09076933562755585\n","step: 1670, loss: 0.06880490481853485\n","step: 1680, loss: 0.1056961789727211\n","step: 1690, loss: 0.061160583049058914\n","step: 1700, loss: 0.13257619738578796\n","step: 1710, loss: 0.1722331941127777\n","step: 1720, loss: 0.20958177745342255\n","step: 1730, loss: 0.17590664327144623\n","step: 1740, loss: 0.09813234955072403\n","step: 1750, loss: 0.12326169013977051\n","step: 1760, loss: 0.12606483697891235\n","step: 1770, loss: 0.15532226860523224\n","step: 1780, loss: 0.15601570904254913\n","step: 1790, loss: 0.10636372864246368\n","step: 1800, loss: 0.06742677837610245\n","step: 1810, loss: 0.08388122916221619\n","step: 1820, loss: 0.1160915344953537\n","step: 1830, loss: 0.07020115852355957\n","step: 1840, loss: 0.13590042293071747\n","step: 1850, loss: 0.11531893163919449\n","step: 1860, loss: 0.1379212737083435\n","step: 1870, loss: 0.14134150743484497\n","step: 1880, loss: 0.10599826276302338\n","step: 1890, loss: 0.16787950694561005\n","step: 1900, loss: 0.10041044652462006\n","step: 1910, loss: 0.24059826135635376\n","step: 1920, loss: 0.06768203526735306\n","step: 1930, loss: 0.03204994276165962\n","step: 1940, loss: 0.12673044204711914\n","step: 1950, loss: 0.09267274290323257\n","step: 1960, loss: 0.0729222223162651\n","step: 1970, loss: 0.051511626690626144\n","step: 1980, loss: 0.11526670306921005\n","step: 1990, loss: 0.05264650285243988\n","step: 2000, loss: 0.0709834173321724\n","step: 2010, loss: 0.016027983278036118\n","step: 2020, loss: 0.09188652783632278\n","step: 2030, loss: 0.07413244247436523\n","step: 2040, loss: 0.039621803909540176\n","step: 2050, loss: 0.054392021149396896\n","step: 2060, loss: 0.04359131306409836\n","step: 2070, loss: 0.20559664070606232\n","step: 2080, loss: 0.15386587381362915\n","step: 2090, loss: 0.10708355158567429\n","step: 2100, loss: 0.20199725031852722\n","step: 2110, loss: 0.09424037486314774\n","step: 2120, loss: 0.1265656054019928\n","step: 2130, loss: 0.13434173166751862\n","step: 2140, loss: 0.16008156538009644\n","step: 2150, loss: 0.05175744369626045\n","step: 2160, loss: 0.06455150991678238\n","step: 2170, loss: 0.07990187406539917\n","step: 2180, loss: 0.05380243435502052\n","step: 2190, loss: 0.13417983055114746\n","step: 2200, loss: 0.04778878763318062\n","step: 2210, loss: 0.08903128653764725\n","step: 2220, loss: 0.07470575720071793\n","step: 2230, loss: 0.07764396071434021\n","step: 2240, loss: 0.07255296409130096\n","step: 2250, loss: 0.11621441692113876\n","step: 2260, loss: 0.10452082008123398\n","step: 2270, loss: 0.13896648585796356\n","step: 2280, loss: 0.17619900405406952\n","step: 2290, loss: 0.15611039102077484\n","step: 2300, loss: 0.11358419805765152\n","step: 2310, loss: 0.10411985963582993\n","step: 2320, loss: 0.044527750462293625\n","step: 2330, loss: 0.12354853004217148\n","step: 2340, loss: 0.16821494698524475\n","step: 2350, loss: 0.10201945900917053\n","step: 2360, loss: 0.06670314818620682\n","step: 2370, loss: 0.10682894289493561\n","step: 2380, loss: 0.10960566997528076\n","step: 2390, loss: 0.08389974385499954\n","step: 2400, loss: 0.07670759409666061\n","step: 2410, loss: 0.07293891906738281\n","step: 2420, loss: 0.058535248041152954\n","step: 2430, loss: 0.1205209270119667\n","step: 2440, loss: 0.08622969686985016\n","step: 2450, loss: 0.12925969064235687\n","step: 2460, loss: 0.06995408982038498\n","step: 2470, loss: 0.10661013424396515\n","step: 2480, loss: 0.030654391273856163\n","step: 2490, loss: 0.1162954717874527\n","step: 2500, loss: 0.1512206643819809\n","step: 2510, loss: 0.1456097811460495\n","step: 2520, loss: 0.15248101949691772\n","step: 2530, loss: 0.07753563672304153\n","step: 2540, loss: 0.039009418338537216\n","step: 2550, loss: 0.06038215011358261\n","step: 2560, loss: 0.15629777312278748\n","step: 2570, loss: 0.06345947831869125\n","step: 2580, loss: 0.16621500253677368\n","step: 2590, loss: 0.06257978081703186\n","step: 2600, loss: 0.0807153657078743\n","step: 2610, loss: 0.03024281933903694\n","step: 2620, loss: 0.14228516817092896\n","step: 2630, loss: 0.06719677895307541\n","step: 2640, loss: 0.13427434861660004\n","step: 2650, loss: 0.07930757105350494\n","step: 2660, loss: 0.055305927991867065\n","step: 2670, loss: 0.05744102969765663\n","step: 2680, loss: 0.16520391404628754\n","step: 2690, loss: 0.0645170584321022\n","step: 2700, loss: 0.05895552039146423\n","step: 2710, loss: 0.07578814029693604\n","step: 2720, loss: 0.0379609651863575\n","step: 2730, loss: 0.05329093709588051\n","step: 2740, loss: 0.1803608536720276\n","step: 2750, loss: 0.1050872802734375\n","step: 2760, loss: 0.08443613350391388\n","step: 2770, loss: 0.11322985589504242\n","step: 2780, loss: 0.06821395456790924\n","step: 2790, loss: 0.12110130488872528\n","step: 2800, loss: 0.06610088795423508\n","step: 2810, loss: 0.08750245720148087\n","step: 2820, loss: 0.12902921438217163\n","step: 2830, loss: 0.17778544127941132\n","step: 2840, loss: 0.0592072531580925\n","step: 2850, loss: 0.16613918542861938\n","step: 2860, loss: 0.16723209619522095\n","step: 2870, loss: 0.07200635969638824\n","step: 2880, loss: 0.11704162508249283\n","step: 2890, loss: 0.17610718309879303\n","step: 2900, loss: 0.09167235344648361\n","step: 2910, loss: 0.08332129567861557\n","step: 2920, loss: 0.09936058521270752\n","step: 2930, loss: 0.19363437592983246\n","step: 2940, loss: 0.1497587114572525\n","step: 2950, loss: 0.08294795453548431\n","step: 2960, loss: 0.07575129717588425\n","step: 2970, loss: 0.18847544491291046\n","step: 2980, loss: 0.07939952611923218\n","step: 2990, loss: 0.08756960183382034\n","step: 3000, loss: 0.04039672389626503\n","step: 3010, loss: 0.11458268016576767\n","step: 3020, loss: 0.05581137537956238\n","step: 3030, loss: 0.1742071658372879\n","step: 3040, loss: 0.08073043823242188\n","step: 3050, loss: 0.1078326553106308\n","step: 3060, loss: 0.12358634918928146\n","step: 3070, loss: 0.16844667494297028\n","step: 3080, loss: 0.042309895157814026\n","step: 3090, loss: 0.05207166075706482\n","step: 3100, loss: 0.06507419049739838\n","step: 3110, loss: 0.05379805713891983\n","step: 3120, loss: 0.06885191053152084\n","step: 3130, loss: 0.12866246700286865\n","step: 3140, loss: 0.06761045008897781\n","step: 3150, loss: 0.03797730803489685\n","step: 3160, loss: 0.16568751633167267\n","step: 3170, loss: 0.11151749640703201\n","step: 3180, loss: 0.0820690467953682\n","step: 3190, loss: 0.07610815018415451\n","step: 3200, loss: 0.14622651040554047\n","step: 3210, loss: 0.10345196723937988\n","step: 3220, loss: 0.07856076955795288\n","step: 3230, loss: 0.07730815559625626\n","step: 3240, loss: 0.1410658210515976\n","step: 3250, loss: 0.0413682647049427\n","step: 3260, loss: 0.061688438057899475\n","step: 3270, loss: 0.0980922132730484\n","step: 3280, loss: 0.12236133962869644\n","step: 3290, loss: 0.06794117391109467\n","step: 3300, loss: 0.13637949526309967\n","step: 3310, loss: 0.07441630214452744\n","step: 3320, loss: 0.12367630004882812\n","step: 3330, loss: 0.11213748157024384\n","step: 3340, loss: 0.10451453924179077\n","step: 3350, loss: 0.09879088401794434\n","step: 3360, loss: 0.11478949338197708\n","step: 3370, loss: 0.1320095658302307\n","step: 3380, loss: 0.13968749344348907\n","step: 3390, loss: 0.05435255542397499\n","step: 3400, loss: 0.05011213943362236\n","step: 3410, loss: 0.07286670058965683\n","step: 3420, loss: 0.10619047284126282\n","step: 3430, loss: 0.039178188890218735\n","step: 3440, loss: 0.10997197031974792\n","step: 3450, loss: 0.1373547911643982\n","step: 3460, loss: 0.08669436722993851\n","step: 3470, loss: 0.06738490611314774\n","step: 3480, loss: 0.061958394944667816\n","step: 3490, loss: 0.09325605630874634\n","step: 3500, loss: 0.08363225311040878\n","step: 3510, loss: 0.056861814111471176\n","step: 3520, loss: 0.054409224539995193\n","step: 3530, loss: 0.16830888390541077\n","step: 3540, loss: 0.10600976645946503\n","step: 3550, loss: 0.30315887928009033\n","step: 3560, loss: 0.08281135559082031\n","step: 3570, loss: 0.1640044003725052\n","step: 3580, loss: 0.17351466417312622\n","step: 3590, loss: 0.11592800170183182\n","step: 3600, loss: 0.06388238072395325\n","step: 3610, loss: 0.13088563084602356\n","step: 3620, loss: 0.06801868230104446\n","step: 3630, loss: 0.09145213663578033\n","step: 3640, loss: 0.1339077651500702\n","step: 3650, loss: 0.11992707848548889\n","step: 3660, loss: 0.07230696082115173\n","step: 3670, loss: 0.0986311286687851\n","step: 3680, loss: 0.05833612009882927\n","step: 3690, loss: 0.06887247413396835\n","step: 3700, loss: 0.12255848199129105\n","step: 3710, loss: 0.09957139939069748\n","step: 3720, loss: 0.10183152556419373\n","step: 3730, loss: 0.08290375769138336\n","step: 3740, loss: 0.04161151498556137\n","step: 3750, loss: 0.13238048553466797\n","step: 3760, loss: 0.08236689865589142\n","step: 3770, loss: 0.05687982961535454\n","step: 3780, loss: 0.06273846328258514\n","step: 3790, loss: 0.03701730817556381\n","step: 3800, loss: 0.11954688280820847\n","acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.64      0.88      0.74         8\n","           2       0.63      0.93      0.75        44\n","           3       1.00      0.87      0.93       987\n","           4       1.00      0.98      0.99       108\n","           5       1.00      0.98      0.99       115\n","           6       1.00      0.96      0.98      1600\n","           7       0.21      0.90      0.33        48\n","           8       0.00      0.00      0.00         4\n","           9       0.98      0.99      0.99      1086\n","          10       0.96      0.97      0.97       386\n","          11       0.99      0.98      0.98      2229\n","          12       0.98      0.95      0.97        61\n","          13       1.00      1.00      1.00        18\n","          14       0.47      0.75      0.58        28\n","          15       0.94      0.96      0.95      2566\n","          16       0.87      0.86      0.87      1511\n","          17       0.91      0.79      0.84        98\n","          18       0.84      0.84      0.84        56\n","          19       0.70      0.95      0.81        20\n","          20       0.99      0.97      0.98       717\n","          21       0.67      0.04      0.08        45\n","          22       0.92      0.91      0.91      3623\n","          23       0.68      0.75      0.71       563\n","          24       0.38      0.47      0.42        19\n","          25       0.93      0.96      0.94      1162\n","          26       0.76      1.00      0.86        28\n","          27       0.97      0.66      0.79        53\n","          28       0.98      0.99      0.99      2476\n","          29       0.99      0.93      0.96       563\n","          30       0.90      0.85      0.87      1947\n","          31       0.60      0.64      0.62        45\n","          32       0.94      0.63      0.76        27\n","          33       0.54      0.89      0.67       151\n","          34       0.43      1.00      0.60        34\n","          35       0.97      0.99      0.98       537\n","          36       0.90      0.36      0.52       124\n","          37       0.95      0.94      0.94      1907\n","          38       0.92      0.96      0.94       465\n","          39       0.96      0.93      0.94       522\n","          40       0.87      0.86      0.86       404\n","          41       0.91      0.93      0.92      1089\n","          42       0.99      0.98      0.99       802\n","          43       0.88      0.89      0.88       143\n","          44       0.91      0.98      0.95       108\n","          45       1.00      1.00      1.00         1\n","          46       0.98      0.98      0.98       192\n","          47       0.00      0.00      0.00         2\n","          48       0.71      0.10      0.18        48\n","\n","    accuracy                           0.93     28770\n","   macro avg       0.81      0.82      0.79     28770\n","weighted avg       0.94      0.93      0.93     28770\n","\n","\n","Loop 3\n","domain_dev_word_lst 1027\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.854783535003662\n","step: 10, loss: 1.9920305013656616\n","step: 20, loss: 0.8530915379524231\n","step: 30, loss: 0.23796015977859497\n","step: 40, loss: 0.2345523089170456\n","step: 50, loss: 0.19618573784828186\n","step: 60, loss: 0.3359408378601074\n","step: 70, loss: 0.08745300024747849\n","step: 80, loss: 0.2768062949180603\n","step: 90, loss: 0.17534029483795166\n","step: 100, loss: 0.1951582133769989\n","step: 110, loss: 0.22620664536952972\n","step: 120, loss: 0.13534049689769745\n","step: 130, loss: 0.16895554959774017\n","step: 140, loss: 0.10525413602590561\n","step: 150, loss: 0.2207639068365097\n","step: 160, loss: 0.12419281899929047\n","step: 170, loss: 0.11160830408334732\n","step: 180, loss: 0.07408983260393143\n","step: 190, loss: 0.1491713523864746\n","step: 200, loss: 0.13655757904052734\n","step: 210, loss: 0.04508701339364052\n","step: 220, loss: 0.3360525667667389\n","step: 230, loss: 0.25552675127983093\n","step: 240, loss: 0.0710805281996727\n","step: 250, loss: 0.17186522483825684\n","step: 260, loss: 0.0986037626862526\n","step: 270, loss: 0.09350351244211197\n","step: 280, loss: 0.09387444704771042\n","step: 290, loss: 0.11273252964019775\n","step: 300, loss: 0.08310910314321518\n","step: 310, loss: 0.19454820454120636\n","step: 320, loss: 0.1916903704404831\n","step: 330, loss: 0.07840326428413391\n","step: 340, loss: 0.09064551442861557\n","step: 350, loss: 0.17059306800365448\n","step: 360, loss: 0.15919168293476105\n","step: 370, loss: 0.137206569314003\n","step: 380, loss: 0.13841784000396729\n","step: 390, loss: 0.07085324823856354\n","step: 400, loss: 0.1989896148443222\n","step: 410, loss: 0.06155548617243767\n","step: 420, loss: 0.07386767119169235\n","step: 430, loss: 0.10296731442213058\n","step: 440, loss: 0.07092328369617462\n","step: 450, loss: 0.05900564417243004\n","step: 460, loss: 0.11168514937162399\n","step: 470, loss: 0.11178801208734512\n","step: 480, loss: 0.2472708374261856\n","step: 490, loss: 0.03694787248969078\n","step: 500, loss: 0.08982541412115097\n","step: 510, loss: 0.10491684824228287\n","step: 520, loss: 0.08186022192239761\n","step: 530, loss: 0.11195753514766693\n","step: 540, loss: 0.1059240847826004\n","step: 550, loss: 0.07760623842477798\n","step: 560, loss: 0.12527680397033691\n","step: 570, loss: 0.17740266025066376\n","step: 580, loss: 0.2206307053565979\n","step: 590, loss: 0.13309699296951294\n","step: 600, loss: 0.04960419982671738\n","step: 610, loss: 0.11468739062547684\n","step: 620, loss: 0.1582348793745041\n","step: 630, loss: 0.09367284923791885\n","step: 640, loss: 0.08646564185619354\n","step: 650, loss: 0.23921099305152893\n","step: 660, loss: 0.05746626481413841\n","step: 670, loss: 0.1044204831123352\n","step: 680, loss: 0.08346860855817795\n","step: 690, loss: 0.021032430231571198\n","step: 700, loss: 0.11526139080524445\n","step: 710, loss: 0.07485157251358032\n","step: 720, loss: 0.04560082033276558\n","step: 730, loss: 0.09800255298614502\n","step: 740, loss: 0.15189127624034882\n","step: 750, loss: 0.11889995634555817\n","step: 760, loss: 0.08644133806228638\n","step: 770, loss: 0.14182016253471375\n","step: 780, loss: 0.0929039716720581\n","step: 790, loss: 0.10681761056184769\n","step: 800, loss: 0.08557388186454773\n","step: 810, loss: 0.19092051684856415\n","step: 820, loss: 0.06183777377009392\n","step: 830, loss: 0.10241758823394775\n","step: 840, loss: 0.09521690756082535\n","step: 850, loss: 0.07185488194227219\n","step: 860, loss: 0.15952546894550323\n","step: 870, loss: 0.21251487731933594\n","step: 880, loss: 0.061858102679252625\n","step: 890, loss: 0.0739540234208107\n","step: 900, loss: 0.04555690661072731\n","step: 910, loss: 0.08978809416294098\n","step: 920, loss: 0.034762848168611526\n","step: 930, loss: 0.28003302216529846\n","step: 940, loss: 0.08989375084638596\n","step: 950, loss: 0.058657076209783554\n","step: 960, loss: 0.08404143154621124\n","step: 970, loss: 0.11279143393039703\n","step: 980, loss: 0.12075470387935638\n","step: 990, loss: 0.1175801157951355\n","step: 1000, loss: 0.1382109671831131\n","step: 1010, loss: 0.06794102489948273\n","step: 1020, loss: 0.13655056059360504\n","step: 1030, loss: 0.055840227752923965\n","step: 1040, loss: 0.04762331023812294\n","step: 1050, loss: 0.10625991970300674\n","step: 1060, loss: 0.08659487962722778\n","step: 1070, loss: 0.058014389127492905\n","step: 1080, loss: 0.14845938980579376\n","step: 1090, loss: 0.07230474799871445\n","step: 1100, loss: 0.05950389802455902\n","step: 1110, loss: 0.0866699069738388\n","step: 1120, loss: 0.1263655424118042\n","step: 1130, loss: 0.18632686138153076\n","step: 1140, loss: 0.11850878596305847\n","step: 1150, loss: 0.033296387642621994\n","step: 1160, loss: 0.09275607019662857\n","step: 1170, loss: 0.03280279040336609\n","step: 1180, loss: 0.12859104573726654\n","step: 1190, loss: 0.06251028925180435\n","step: 1200, loss: 0.02668611891567707\n","step: 1210, loss: 0.10916576534509659\n","step: 1220, loss: 0.07725938409566879\n","step: 1230, loss: 0.0805080235004425\n","step: 1240, loss: 0.20779643952846527\n","step: 1250, loss: 0.12221425771713257\n","step: 1260, loss: 0.0351903922855854\n","step: 1270, loss: 0.13198909163475037\n","step: 1280, loss: 0.12249932438135147\n","step: 1290, loss: 0.12892499566078186\n","step: 1300, loss: 0.09295400977134705\n","step: 1310, loss: 0.05836736783385277\n","step: 1320, loss: 0.11040309816598892\n","step: 1330, loss: 0.08023743331432343\n","step: 1340, loss: 0.07448453456163406\n","step: 1350, loss: 0.0998590886592865\n","step: 1360, loss: 0.06194771081209183\n","step: 1370, loss: 0.07894790917634964\n","step: 1380, loss: 0.11051991581916809\n","step: 1390, loss: 0.039827313274145126\n","step: 1400, loss: 0.07950331270694733\n","step: 1410, loss: 0.04041668772697449\n","step: 1420, loss: 0.11655878275632858\n","step: 1430, loss: 0.11270897090435028\n","step: 1440, loss: 0.0635140985250473\n","step: 1450, loss: 0.14635922014713287\n","step: 1460, loss: 0.09145445376634598\n","step: 1470, loss: 0.09184448421001434\n","step: 1480, loss: 0.10083115845918655\n","step: 1490, loss: 0.11138977110385895\n","step: 1500, loss: 0.11767306178808212\n","step: 1510, loss: 0.058430641889572144\n","step: 1520, loss: 0.1077374517917633\n","step: 1530, loss: 0.04485141485929489\n","step: 1540, loss: 0.18721391260623932\n","step: 1550, loss: 0.08674781769514084\n","step: 1560, loss: 0.1375577300786972\n","step: 1570, loss: 0.08314897119998932\n","step: 1580, loss: 0.034206051379442215\n","step: 1590, loss: 0.01787557266652584\n","step: 1600, loss: 0.07592805474996567\n","step: 1610, loss: 0.05120186135172844\n","step: 1620, loss: 0.04152017459273338\n","step: 1630, loss: 0.12807142734527588\n","step: 1640, loss: 0.0897262692451477\n","step: 1650, loss: 0.12461987882852554\n","step: 1660, loss: 0.19773833453655243\n","step: 1670, loss: 0.09314920753240585\n","step: 1680, loss: 0.027904126793146133\n","step: 1690, loss: 0.04684297740459442\n","step: 1700, loss: 0.037488438189029694\n","step: 1710, loss: 0.0817410871386528\n","step: 1720, loss: 0.12689848244190216\n","step: 1730, loss: 0.140988290309906\n","step: 1740, loss: 0.1778482049703598\n","step: 1750, loss: 0.09682464599609375\n","step: 1760, loss: 0.09900977462530136\n","step: 1770, loss: 0.08896574378013611\n","step: 1780, loss: 0.08315244317054749\n","step: 1790, loss: 0.12010527402162552\n","step: 1800, loss: 0.063156358897686\n","step: 1810, loss: 0.06017404422163963\n","step: 1820, loss: 0.20583757758140564\n","step: 1830, loss: 0.12533697485923767\n","step: 1840, loss: 0.08367032557725906\n","step: 1850, loss: 0.02545386552810669\n","step: 1860, loss: 0.12438900768756866\n","step: 1870, loss: 0.11430512368679047\n","step: 1880, loss: 0.15849721431732178\n","step: 1890, loss: 0.08700936287641525\n","step: 1900, loss: 0.02248251624405384\n","step: 1910, loss: 0.0668221190571785\n","step: 1920, loss: 0.0807388424873352\n","step: 1930, loss: 0.14939163625240326\n","step: 1940, loss: 0.14993038773536682\n","step: 1950, loss: 0.04485073685646057\n","step: 1960, loss: 0.05976603180170059\n","step: 1970, loss: 0.04808976128697395\n","step: 1980, loss: 0.12874647974967957\n","step: 1990, loss: 0.08318982273340225\n","step: 2000, loss: 0.03998227417469025\n","step: 2010, loss: 0.13061495125293732\n","step: 2020, loss: 0.14551116526126862\n","step: 2030, loss: 0.108528271317482\n","step: 2040, loss: 0.07445099204778671\n","step: 2050, loss: 0.06286340951919556\n","step: 2060, loss: 0.0666455402970314\n","step: 2070, loss: 0.07667114585638046\n","step: 2080, loss: 0.15053215622901917\n","step: 2090, loss: 0.09851368516683578\n","step: 2100, loss: 0.11343175917863846\n","step: 2110, loss: 0.08662691712379456\n","step: 2120, loss: 0.10478975623846054\n","step: 2130, loss: 0.09817685186862946\n","step: 2140, loss: 0.04091108217835426\n","step: 2150, loss: 0.06208990514278412\n","step: 2160, loss: 0.07264843583106995\n","step: 2170, loss: 0.1257883459329605\n","step: 2180, loss: 0.06382644921541214\n","step: 2190, loss: 0.060010336339473724\n","step: 2200, loss: 0.0334354005753994\n","step: 2210, loss: 0.10105074197053909\n","step: 2220, loss: 0.15858681499958038\n","step: 2230, loss: 0.14213480055332184\n","step: 2240, loss: 0.08556084334850311\n","step: 2250, loss: 0.06499196588993073\n","step: 2260, loss: 0.09502848982810974\n","step: 2270, loss: 0.040411125868558884\n","step: 2280, loss: 0.14468827843666077\n","step: 2290, loss: 0.03720354288816452\n","step: 2300, loss: 0.10940688103437424\n","step: 2310, loss: 0.13313846290111542\n","step: 2320, loss: 0.12602181732654572\n","step: 2330, loss: 0.1036156415939331\n","step: 2340, loss: 0.13805265724658966\n","step: 2350, loss: 0.11326206475496292\n","step: 2360, loss: 0.09917917102575302\n","step: 2370, loss: 0.14307433366775513\n","step: 2380, loss: 0.10750166326761246\n","step: 2390, loss: 0.08543184399604797\n","step: 2400, loss: 0.07675519585609436\n","step: 2410, loss: 0.10540339350700378\n","step: 2420, loss: 0.07975848764181137\n","step: 2430, loss: 0.03615448623895645\n","step: 2440, loss: 0.09322904795408249\n","step: 2450, loss: 0.052618805319070816\n","step: 2460, loss: 0.055735621601343155\n","step: 2470, loss: 0.10465747863054276\n","step: 2480, loss: 0.043770745396614075\n","step: 2490, loss: 0.1788301318883896\n","step: 2500, loss: 0.07999380677938461\n","step: 2510, loss: 0.12268345057964325\n","step: 2520, loss: 0.11961839348077774\n","step: 2530, loss: 0.09362170100212097\n","step: 2540, loss: 0.10985147953033447\n","step: 2550, loss: 0.0727197602391243\n","step: 2560, loss: 0.07911067456007004\n","step: 2570, loss: 0.14206446707248688\n","step: 2580, loss: 0.0778823122382164\n","step: 2590, loss: 0.08044061809778214\n","step: 2600, loss: 0.09211677312850952\n","step: 2610, loss: 0.026211408898234367\n","step: 2620, loss: 0.0687221884727478\n","step: 2630, loss: 0.05145486816763878\n","step: 2640, loss: 0.2150678187608719\n","step: 2650, loss: 0.16732186079025269\n","step: 2660, loss: 0.14215590059757233\n","step: 2670, loss: 0.034252703189849854\n","step: 2680, loss: 0.0771995559334755\n","step: 2690, loss: 0.1310385912656784\n","step: 2700, loss: 0.15220743417739868\n","step: 2710, loss: 0.060931019484996796\n","step: 2720, loss: 0.08019769936800003\n","step: 2730, loss: 0.06688748300075531\n","step: 2740, loss: 0.13790008425712585\n","step: 2750, loss: 0.05677823722362518\n","step: 2760, loss: 0.12381529808044434\n","step: 2770, loss: 0.04841136559844017\n","step: 2780, loss: 0.13631656765937805\n","step: 2790, loss: 0.06828999519348145\n","step: 2800, loss: 0.05712614208459854\n","step: 2810, loss: 0.051686953753232956\n","step: 2820, loss: 0.2992677688598633\n","step: 2830, loss: 0.19078445434570312\n","step: 2840, loss: 0.0654992163181305\n","step: 2850, loss: 0.06104695051908493\n","step: 2860, loss: 0.03698379173874855\n","step: 2870, loss: 0.05439048632979393\n","step: 2880, loss: 0.10558079183101654\n","step: 2890, loss: 0.04495881497859955\n","step: 2900, loss: 0.13719581067562103\n","step: 2910, loss: 0.22216324508190155\n","step: 2920, loss: 0.07760979235172272\n","step: 2930, loss: 0.08180500566959381\n","step: 2940, loss: 0.10923473536968231\n","step: 2950, loss: 0.10515681654214859\n","step: 2960, loss: 0.12486089020967484\n","step: 2970, loss: 0.29917898774147034\n","step: 2980, loss: 0.06780337542295456\n","step: 2990, loss: 0.14526128768920898\n","step: 3000, loss: 0.05411675199866295\n","step: 3010, loss: 0.07992077618837357\n","step: 3020, loss: 0.07607194781303406\n","step: 3030, loss: 0.07615097612142563\n","step: 3040, loss: 0.10413722693920135\n","step: 3050, loss: 0.13438297808170319\n","step: 3060, loss: 0.07884518057107925\n","step: 3070, loss: 0.07526347786188126\n","step: 3080, loss: 0.09498775005340576\n","step: 3090, loss: 0.0649312362074852\n","step: 3100, loss: 0.09143655747175217\n","step: 3110, loss: 0.2202284187078476\n","step: 3120, loss: 0.050372812896966934\n","step: 3130, loss: 0.11867386847734451\n","step: 3140, loss: 0.06868762522935867\n","step: 3150, loss: 0.04764741286635399\n","step: 3160, loss: 0.10837063193321228\n","step: 3170, loss: 0.06974971294403076\n","step: 3180, loss: 0.18035075068473816\n","step: 3190, loss: 0.061653949320316315\n","step: 3200, loss: 0.0644158273935318\n","step: 3210, loss: 0.09416661411523819\n","step: 3220, loss: 0.028207963332533836\n","step: 3230, loss: 0.10049044340848923\n","step: 3240, loss: 0.1285676807165146\n","step: 3250, loss: 0.06949660181999207\n","step: 3260, loss: 0.12565961480140686\n","step: 3270, loss: 0.09187662601470947\n","step: 3280, loss: 0.09874456375837326\n","step: 3290, loss: 0.03768230974674225\n","step: 3300, loss: 0.1089189350605011\n","step: 3310, loss: 0.09715976566076279\n","step: 3320, loss: 0.07355966418981552\n","step: 3330, loss: 0.036378562450408936\n","step: 3340, loss: 0.09613462537527084\n","step: 3350, loss: 0.11402767151594162\n","step: 3360, loss: 0.11350276321172714\n","step: 3370, loss: 0.06254995614290237\n","step: 3380, loss: 0.13050594925880432\n","step: 3390, loss: 0.07437669485807419\n","step: 3400, loss: 0.10874253511428833\n","step: 3410, loss: 0.07617893815040588\n","step: 3420, loss: 0.07524389773607254\n","step: 3430, loss: 0.031230704858899117\n","step: 3440, loss: 0.07621834427118301\n","step: 3450, loss: 0.21415618062019348\n","step: 3460, loss: 0.08321578800678253\n","step: 3470, loss: 0.01887257769703865\n","step: 3480, loss: 0.06453726440668106\n","step: 3490, loss: 0.11803896725177765\n","step: 3500, loss: 0.08832250535488129\n","step: 3510, loss: 0.12948410212993622\n","step: 3520, loss: 0.08926784992218018\n","step: 3530, loss: 0.07113479822874069\n","step: 3540, loss: 0.10513482242822647\n","step: 3550, loss: 0.05254619941115379\n","step: 3560, loss: 0.034432303160429\n","step: 3570, loss: 0.1475699245929718\n","step: 3580, loss: 0.11647725850343704\n","step: 3590, loss: 0.12382521480321884\n","step: 3600, loss: 0.09898652881383896\n","step: 3610, loss: 0.08414533734321594\n","step: 3620, loss: 0.10955575853586197\n","step: 3630, loss: 0.046894241124391556\n","step: 3640, loss: 0.0508236363530159\n","step: 3650, loss: 0.062107108533382416\n","step: 3660, loss: 0.09589467197656631\n","step: 3670, loss: 0.03698817640542984\n","step: 3680, loss: 0.13046663999557495\n","step: 3690, loss: 0.1441577821969986\n","step: 3700, loss: 0.1489240527153015\n","step: 3710, loss: 0.05438642576336861\n","step: 3720, loss: 0.18190504610538483\n","step: 3730, loss: 0.05405209958553314\n","step: 3740, loss: 0.04455896094441414\n","step: 3750, loss: 0.13660725951194763\n","step: 3760, loss: 0.13447220623493195\n","step: 3770, loss: 0.08987173438072205\n","step: 3780, loss: 0.09956194460391998\n","step: 3790, loss: 0.04154181852936745\n","step: 3800, loss: 0.08410724997520447\n","acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.80      1.00      0.89         8\n","           2       0.71      0.84      0.77        44\n","           3       1.00      0.87      0.93       987\n","           4       1.00      0.99      1.00       108\n","           5       0.99      0.98      0.99       115\n","           6       1.00      0.96      0.98      1600\n","           7       0.18      0.83      0.29        48\n","           8       0.00      0.00      0.00         4\n","           9       0.99      0.98      0.99      1086\n","          10       0.93      0.98      0.96       386\n","          11       0.98      0.98      0.98      2229\n","          12       0.92      0.95      0.94        61\n","          13       1.00      0.89      0.94        18\n","          14       0.53      0.75      0.62        28\n","          15       0.95      0.97      0.96      2566\n","          16       0.86      0.88      0.87      1511\n","          17       0.76      0.94      0.84        98\n","          18       0.84      0.93      0.88        56\n","          19       0.95      0.95      0.95        20\n","          20       0.98      0.99      0.99       717\n","          21       1.00      0.09      0.16        45\n","          22       0.92      0.90      0.91      3623\n","          23       0.61      0.79      0.69       563\n","          24       0.30      0.37      0.33        19\n","          25       0.94      0.93      0.94      1162\n","          26       0.78      1.00      0.88        28\n","          27       0.94      0.89      0.91        53\n","          28       0.99      0.99      0.99      2476\n","          29       0.99      0.95      0.97       563\n","          30       0.92      0.87      0.90      1947\n","          31       0.79      0.42      0.55        45\n","          32       0.94      0.56      0.70        27\n","          33       0.65      0.77      0.71       151\n","          34       0.43      0.94      0.59        34\n","          35       0.97      0.98      0.98       537\n","          36       0.98      0.47      0.63       124\n","          37       0.95      0.92      0.94      1907\n","          38       0.94      0.94      0.94       465\n","          39       0.94      0.95      0.95       522\n","          40       0.89      0.87      0.88       404\n","          41       0.88      0.93      0.91      1089\n","          42       0.99      0.97      0.98       802\n","          43       0.86      0.85      0.85       143\n","          44       0.86      0.98      0.92       108\n","          45       1.00      1.00      1.00         1\n","          46       0.98      0.98      0.98       192\n","          47       0.00      0.00      0.00         2\n","          48       0.00      0.00      0.00        48\n","\n","    accuracy                           0.93     28770\n","   macro avg       0.81      0.81      0.79     28770\n","weighted avg       0.94      0.93      0.93     28770\n","\n","\n","Loop 4\n","domain_dev_word_lst 684\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Train from scratch...\n","step: 0, loss: 3.8841216564178467\n","step: 10, loss: 2.028573989868164\n","step: 20, loss: 0.7612056732177734\n","step: 30, loss: 0.5227319598197937\n","step: 40, loss: 0.3425378203392029\n","step: 50, loss: 0.2072959989309311\n","step: 60, loss: 0.20723769068717957\n","step: 70, loss: 0.25055497884750366\n","step: 80, loss: 0.21760807931423187\n","step: 90, loss: 0.1872178167104721\n","step: 100, loss: 0.14835122227668762\n","step: 110, loss: 0.2042236477136612\n","step: 120, loss: 0.1292334944009781\n","step: 130, loss: 0.1586228609085083\n","step: 140, loss: 0.08138562738895416\n","step: 150, loss: 0.18242476880550385\n","step: 160, loss: 0.04360223934054375\n","step: 170, loss: 0.12963241338729858\n","step: 180, loss: 0.1031276285648346\n","step: 190, loss: 0.2945893704891205\n","step: 200, loss: 0.16003382205963135\n","step: 210, loss: 0.1792760193347931\n","step: 220, loss: 0.12115529179573059\n","step: 230, loss: 0.08919410407543182\n","step: 240, loss: 0.09236670285463333\n","step: 250, loss: 0.01792966015636921\n","step: 260, loss: 0.1289737969636917\n","step: 270, loss: 0.11174226552248001\n","step: 280, loss: 0.11330616474151611\n","step: 290, loss: 0.07189547270536423\n","step: 300, loss: 0.06660816073417664\n","step: 310, loss: 0.11683723330497742\n","step: 320, loss: 0.17538809776306152\n","step: 330, loss: 0.10658248513936996\n","step: 340, loss: 0.2151302993297577\n","step: 350, loss: 0.06851868331432343\n","step: 360, loss: 0.11662557721138\n","step: 370, loss: 0.13340483605861664\n","step: 380, loss: 0.0881718173623085\n","step: 390, loss: 0.16795293986797333\n","step: 400, loss: 0.15145820379257202\n","step: 410, loss: 0.0857308879494667\n","step: 420, loss: 0.15009713172912598\n","step: 430, loss: 0.08872511237859726\n","step: 440, loss: 0.11587420105934143\n","step: 450, loss: 0.1508960872888565\n","step: 460, loss: 0.07323913276195526\n","step: 470, loss: 0.1286623328924179\n","step: 480, loss: 0.12308430671691895\n","step: 490, loss: 0.1125979945063591\n","step: 500, loss: 0.12743370234966278\n","step: 510, loss: 0.10200279951095581\n","step: 520, loss: 0.04845863953232765\n","step: 530, loss: 0.14412032067775726\n","step: 540, loss: 0.181331604719162\n","step: 550, loss: 0.09961801767349243\n","step: 560, loss: 0.05460391566157341\n","step: 570, loss: 0.17946474254131317\n","step: 580, loss: 0.13387620449066162\n","step: 590, loss: 0.1105651780962944\n","step: 600, loss: 0.06203801929950714\n","step: 610, loss: 0.10482744127511978\n","step: 620, loss: 0.17402492463588715\n","step: 630, loss: 0.08502574265003204\n","step: 640, loss: 0.1064218133687973\n","step: 650, loss: 0.1933593600988388\n","step: 660, loss: 0.1489594280719757\n","step: 670, loss: 0.17663192749023438\n","step: 680, loss: 0.16421768069267273\n","step: 690, loss: 0.10798119753599167\n","step: 700, loss: 0.06848523020744324\n","step: 710, loss: 0.177144393324852\n","step: 720, loss: 0.18779897689819336\n","step: 730, loss: 0.2740374803543091\n","step: 740, loss: 0.056966524571180344\n","step: 750, loss: 0.1979745626449585\n","step: 760, loss: 0.14500214159488678\n","step: 770, loss: 0.13969233632087708\n","step: 780, loss: 0.07947894185781479\n","step: 790, loss: 0.16490775346755981\n","step: 800, loss: 0.05097716301679611\n","step: 810, loss: 0.029040047898888588\n","step: 820, loss: 0.10207158327102661\n","step: 830, loss: 0.10837307572364807\n","step: 840, loss: 0.10974588990211487\n","step: 850, loss: 0.11038379371166229\n","step: 860, loss: 0.06535760313272476\n","step: 870, loss: 0.03235017880797386\n","step: 880, loss: 0.08985330164432526\n","step: 890, loss: 0.10314080119132996\n","step: 900, loss: 0.049425169825553894\n","step: 910, loss: 0.12445542216300964\n","step: 920, loss: 0.0649791955947876\n","step: 930, loss: 0.14858373999595642\n","step: 940, loss: 0.09939081966876984\n","step: 950, loss: 0.10948184132575989\n","step: 960, loss: 0.07326146960258484\n","step: 970, loss: 0.054065849632024765\n","step: 980, loss: 0.1891111433506012\n","step: 990, loss: 0.14268948137760162\n","step: 1000, loss: 0.03385001793503761\n","step: 1010, loss: 0.06327341496944427\n","step: 1020, loss: 0.12505677342414856\n","step: 1030, loss: 0.08189705014228821\n","step: 1040, loss: 0.08257483690977097\n","step: 1050, loss: 0.048741839826107025\n","step: 1060, loss: 0.06912767887115479\n","step: 1070, loss: 0.08902499079704285\n","step: 1080, loss: 0.08079531788825989\n","step: 1090, loss: 0.12030009925365448\n","step: 1100, loss: 0.16666904091835022\n","step: 1110, loss: 0.15167661011219025\n","step: 1120, loss: 0.11850486695766449\n","step: 1130, loss: 0.1279769390821457\n","step: 1140, loss: 0.16291138529777527\n","step: 1150, loss: 0.07107777148485184\n","step: 1160, loss: 0.07229077070951462\n","step: 1170, loss: 0.08183833956718445\n","step: 1180, loss: 0.07211613655090332\n","step: 1190, loss: 0.1733732670545578\n","step: 1200, loss: 0.1198371946811676\n","step: 1210, loss: 0.06746254116296768\n","step: 1220, loss: 0.09338617324829102\n","step: 1230, loss: 0.11043693125247955\n","step: 1240, loss: 0.05012303218245506\n","step: 1250, loss: 0.07439414411783218\n","step: 1260, loss: 0.10306190699338913\n","step: 1270, loss: 0.07690481841564178\n","step: 1280, loss: 0.13970457017421722\n","step: 1290, loss: 0.12001994997262955\n","step: 1300, loss: 0.1262659877538681\n","step: 1310, loss: 0.10020303726196289\n","step: 1320, loss: 0.09096866101026535\n","step: 1330, loss: 0.12748363614082336\n","step: 1340, loss: 0.0873885527253151\n","step: 1350, loss: 0.04775884374976158\n","step: 1360, loss: 0.14131346344947815\n","step: 1370, loss: 0.1186470165848732\n","step: 1380, loss: 0.20768432319164276\n","step: 1390, loss: 0.08223702758550644\n","step: 1400, loss: 0.061206333339214325\n","step: 1410, loss: 0.03737059608101845\n","step: 1420, loss: 0.06741552799940109\n","step: 1430, loss: 0.09384968876838684\n","step: 1440, loss: 0.12248820066452026\n","step: 1450, loss: 0.14961247146129608\n","step: 1460, loss: 0.09228238463401794\n","step: 1470, loss: 0.08611270040273666\n","step: 1480, loss: 0.14725425839424133\n","step: 1490, loss: 0.16541877388954163\n","step: 1500, loss: 0.11174020916223526\n","step: 1510, loss: 0.07891274988651276\n","step: 1520, loss: 0.14090074598789215\n","step: 1530, loss: 0.0739806517958641\n","step: 1540, loss: 0.08915246278047562\n","step: 1550, loss: 0.14905902743339539\n","step: 1560, loss: 0.06533005088567734\n","step: 1570, loss: 0.10443293303251266\n","step: 1580, loss: 0.060926228761672974\n","step: 1590, loss: 0.06422421336174011\n","step: 1600, loss: 0.16093701124191284\n","step: 1610, loss: 0.0983223244547844\n","step: 1620, loss: 0.10563809424638748\n","step: 1630, loss: 0.0890943631529808\n","step: 1640, loss: 0.1890837848186493\n","step: 1650, loss: 0.09160317480564117\n","step: 1660, loss: 0.09189436584711075\n","step: 1670, loss: 0.04455862194299698\n","step: 1680, loss: 0.11227204650640488\n","step: 1690, loss: 0.08443213254213333\n","step: 1700, loss: 0.14640086889266968\n","step: 1710, loss: 0.11825119704008102\n","step: 1720, loss: 0.11474929004907608\n","step: 1730, loss: 0.08545690029859543\n","step: 1740, loss: 0.08743837475776672\n","step: 1750, loss: 0.10512483865022659\n","step: 1760, loss: 0.08729337900876999\n","step: 1770, loss: 0.08993685245513916\n","step: 1780, loss: 0.10758522152900696\n","step: 1790, loss: 0.05527277663350105\n","step: 1800, loss: 0.06025629863142967\n","step: 1810, loss: 0.052457064390182495\n","step: 1820, loss: 0.18219409883022308\n","step: 1830, loss: 0.07376110553741455\n","step: 1840, loss: 0.23883455991744995\n","step: 1850, loss: 0.03384299576282501\n","step: 1860, loss: 0.07021617144346237\n","step: 1870, loss: 0.020187094807624817\n","step: 1880, loss: 0.05916236713528633\n","step: 1890, loss: 0.07464929670095444\n","step: 1900, loss: 0.04425210878252983\n","step: 1910, loss: 0.04627089947462082\n","step: 1920, loss: 0.07736148685216904\n","step: 1930, loss: 0.15914544463157654\n","step: 1940, loss: 0.030189339071512222\n","step: 1950, loss: 0.04162006452679634\n","step: 1960, loss: 0.08723945170640945\n","step: 1970, loss: 0.1331091821193695\n","step: 1980, loss: 0.16650399565696716\n","step: 1990, loss: 0.0743643119931221\n","step: 2000, loss: 0.04766641929745674\n","step: 2010, loss: 0.09396753460168839\n","step: 2020, loss: 0.13647380471229553\n","step: 2030, loss: 0.07669474929571152\n","step: 2040, loss: 0.14117854833602905\n","step: 2050, loss: 0.06311637908220291\n","step: 2060, loss: 0.06158420816063881\n","step: 2070, loss: 0.1092345118522644\n","step: 2080, loss: 0.12971994280815125\n","step: 2090, loss: 0.1429104506969452\n","step: 2100, loss: 0.05959365889430046\n","step: 2110, loss: 0.06034613028168678\n","step: 2120, loss: 0.1452171504497528\n","step: 2130, loss: 0.10791151225566864\n","step: 2140, loss: 0.0720924660563469\n","step: 2150, loss: 0.11575731635093689\n","step: 2160, loss: 0.1723354607820511\n","step: 2170, loss: 0.11207329481840134\n","step: 2180, loss: 0.028977248817682266\n","step: 2190, loss: 0.14194178581237793\n","step: 2200, loss: 0.0957714319229126\n","step: 2210, loss: 0.0442781001329422\n","step: 2220, loss: 0.15049952268600464\n","step: 2230, loss: 0.05539891496300697\n","step: 2240, loss: 0.12330888956785202\n","step: 2250, loss: 0.12324738502502441\n","step: 2260, loss: 0.06305324286222458\n","step: 2270, loss: 0.09244935214519501\n","step: 2280, loss: 0.1039736270904541\n","step: 2290, loss: 0.13023820519447327\n","step: 2300, loss: 0.03866206854581833\n","step: 2310, loss: 0.0712740421295166\n","step: 2320, loss: 0.0486997552216053\n","step: 2330, loss: 0.09422453492879868\n","step: 2340, loss: 0.07494129985570908\n","step: 2350, loss: 0.05297795683145523\n","step: 2360, loss: 0.08011307567358017\n","step: 2370, loss: 0.028312742710113525\n","step: 2380, loss: 0.049731891602277756\n","step: 2390, loss: 0.0692470520734787\n","step: 2400, loss: 0.028870895504951477\n","step: 2410, loss: 0.08833233267068863\n","step: 2420, loss: 0.07319815456867218\n","step: 2430, loss: 0.07834989577531815\n","step: 2440, loss: 0.04805329442024231\n","step: 2450, loss: 0.05137022212147713\n","step: 2460, loss: 0.10524474084377289\n","step: 2470, loss: 0.08790881186723709\n","step: 2480, loss: 0.0896647498011589\n","step: 2490, loss: 0.033888205885887146\n","step: 2500, loss: 0.17633043229579926\n","step: 2510, loss: 0.05519324168562889\n","step: 2520, loss: 0.09626707434654236\n","step: 2530, loss: 0.06607930362224579\n","step: 2540, loss: 0.045614052563905716\n","step: 2550, loss: 0.112973652780056\n","step: 2560, loss: 0.12417047470808029\n","step: 2570, loss: 0.10847203433513641\n","step: 2580, loss: 0.11693303287029266\n","step: 2590, loss: 0.09091304987668991\n","step: 2600, loss: 0.0511995293200016\n","step: 2610, loss: 0.09363267570734024\n","step: 2620, loss: 0.09674126654863358\n","step: 2630, loss: 0.045801687985658646\n","step: 2640, loss: 0.06328201293945312\n","step: 2650, loss: 0.13928040862083435\n","step: 2660, loss: 0.14521095156669617\n","step: 2670, loss: 0.09084657579660416\n","step: 2680, loss: 0.10577628016471863\n","step: 2690, loss: 0.11369936913251877\n","step: 2700, loss: 0.1391613483428955\n","step: 2710, loss: 0.0337863489985466\n","step: 2720, loss: 0.11770320683717728\n","step: 2730, loss: 0.05494367331266403\n","step: 2740, loss: 0.14330987632274628\n","step: 2750, loss: 0.0858624204993248\n","step: 2760, loss: 0.08346948027610779\n","step: 2770, loss: 0.07107571512460709\n","step: 2780, loss: 0.11121712625026703\n","step: 2790, loss: 0.07001123577356339\n","step: 2800, loss: 0.10643482953310013\n","step: 2810, loss: 0.09162109345197678\n","step: 2820, loss: 0.03524116054177284\n","step: 2830, loss: 0.14202998578548431\n","step: 2840, loss: 0.059441257268190384\n","step: 2850, loss: 0.15282590687274933\n","step: 2860, loss: 0.07284180819988251\n","step: 2870, loss: 0.07511135935783386\n","step: 2880, loss: 0.18711797893047333\n","step: 2890, loss: 0.06663785874843597\n","step: 2900, loss: 0.09876923263072968\n","step: 2910, loss: 0.0781220868229866\n","step: 2920, loss: 0.06026538833975792\n","step: 2930, loss: 0.08011632412672043\n","step: 2940, loss: 0.16006843745708466\n","step: 2950, loss: 0.08047576993703842\n","step: 2960, loss: 0.07321769744157791\n","step: 2970, loss: 0.10949154943227768\n","step: 2980, loss: 0.08219384402036667\n","step: 2990, loss: 0.0838666558265686\n","step: 3000, loss: 0.16979168355464935\n","step: 3010, loss: 0.211476132273674\n","step: 3020, loss: 0.10024300962686539\n","step: 3030, loss: 0.09513556212186813\n","step: 3040, loss: 0.08575661480426788\n","step: 3050, loss: 0.09557489305734634\n","step: 3060, loss: 0.05945331230759621\n","step: 3070, loss: 0.03460808843374252\n","step: 3080, loss: 0.09645896404981613\n","step: 3090, loss: 0.12267953902482986\n","step: 3100, loss: 0.12708118557929993\n","step: 3110, loss: 0.1004103347659111\n","step: 3120, loss: 0.07741103321313858\n","step: 3130, loss: 0.06928974390029907\n","step: 3140, loss: 0.1315925568342209\n","step: 3150, loss: 0.049894604831933975\n","step: 3160, loss: 0.09941790252923965\n","step: 3170, loss: 0.16288651525974274\n","step: 3180, loss: 0.09779029339551926\n","step: 3190, loss: 0.032678574323654175\n","step: 3200, loss: 0.10555780678987503\n","step: 3210, loss: 0.12419038265943527\n","step: 3220, loss: 0.03282460570335388\n","step: 3230, loss: 0.06451970338821411\n","step: 3240, loss: 0.048934053629636765\n","step: 3250, loss: 0.046304430812597275\n","step: 3260, loss: 0.08986346423625946\n","step: 3270, loss: 0.0983835756778717\n","step: 3280, loss: 0.13415727019309998\n","step: 3290, loss: 0.03904718905687332\n","step: 3300, loss: 0.0629439428448677\n","step: 3310, loss: 0.09761547297239304\n","step: 3320, loss: 0.134729266166687\n","step: 3330, loss: 0.07882523536682129\n","step: 3340, loss: 0.08980810642242432\n","step: 3350, loss: 0.11417137831449509\n","step: 3360, loss: 0.09260972589254379\n","step: 3370, loss: 0.10396754741668701\n","step: 3380, loss: 0.15194375813007355\n","step: 3390, loss: 0.025219865143299103\n","step: 3400, loss: 0.07745470851659775\n","step: 3410, loss: 0.09454597532749176\n","step: 3420, loss: 0.04509056359529495\n","step: 3430, loss: 0.15275807678699493\n","step: 3440, loss: 0.08795230835676193\n","step: 3450, loss: 0.1315506398677826\n","step: 3460, loss: 0.16970442235469818\n","step: 3470, loss: 0.03463657572865486\n","step: 3480, loss: 0.050987374037504196\n","step: 3490, loss: 0.11132252216339111\n","step: 3500, loss: 0.14531868696212769\n","step: 3510, loss: 0.07994960993528366\n","step: 3520, loss: 0.06174514442682266\n","step: 3530, loss: 0.12507016956806183\n","step: 3540, loss: 0.05122338980436325\n","step: 3550, loss: 0.12127001583576202\n","step: 3560, loss: 0.07084323465824127\n","step: 3570, loss: 0.05148272588849068\n","step: 3580, loss: 0.10930795222520828\n","step: 3590, loss: 0.09187421202659607\n","step: 3600, loss: 0.08747810870409012\n","step: 3610, loss: 0.05533495917916298\n","step: 3620, loss: 0.06815388053655624\n","step: 3630, loss: 0.12760964035987854\n","step: 3640, loss: 0.04298190400004387\n","step: 3650, loss: 0.14077600836753845\n","step: 3660, loss: 0.1025879979133606\n","step: 3670, loss: 0.09551848471164703\n","step: 3680, loss: 0.1097167432308197\n","step: 3690, loss: 0.0733468309044838\n","step: 3700, loss: 0.12645474076271057\n","step: 3710, loss: 0.04841548204421997\n","step: 3720, loss: 0.026022659614682198\n","step: 3730, loss: 0.09745096415281296\n","step: 3740, loss: 0.07667285948991776\n","step: 3750, loss: 0.1432020664215088\n","step: 3760, loss: 0.07669579237699509\n","step: 3770, loss: 0.11548028886318207\n","step: 3780, loss: 0.07046062499284744\n","step: 3790, loss: 0.13755978643894196\n","step: 3800, loss: 0.0732017308473587\n","acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.67      1.00      0.80         8\n","           2       0.85      1.00      0.92        44\n","           3       1.00      0.87      0.93       987\n","           4       0.97      0.99      0.98       108\n","           5       1.00      0.98      0.99       115\n","           6       1.00      0.96      0.98      1600\n","           7       0.20      0.92      0.33        48\n","           8       0.00      0.00      0.00         4\n","           9       0.99      0.98      0.99      1086\n","          10       0.94      0.94      0.94       386\n","          11       0.98      0.98      0.98      2229\n","          12       0.87      0.95      0.91        61\n","          13       0.91      0.56      0.69        18\n","          14       0.72      0.75      0.74        28\n","          15       0.92      0.98      0.95      2566\n","          16       0.85      0.86      0.86      1511\n","          17       0.75      0.95      0.84        98\n","          18       0.91      0.95      0.93        56\n","          19       1.00      0.55      0.71        20\n","          20       0.99      0.99      0.99       717\n","          21       0.00      0.00      0.00        45\n","          22       0.90      0.92      0.91      3623\n","          23       0.68      0.74      0.71       563\n","          24       0.25      0.32      0.28        19\n","          25       0.94      0.94      0.94      1162\n","          26       0.87      0.93      0.90        28\n","          27       0.96      0.83      0.89        53\n","          28       0.99      0.99      0.99      2476\n","          29       0.99      0.95      0.97       563\n","          30       0.92      0.86      0.89      1947\n","          31       0.88      0.47      0.61        45\n","          32       0.95      0.78      0.86        27\n","          33       0.77      0.74      0.75       151\n","          34       0.43      0.91      0.58        34\n","          35       0.98      0.97      0.98       537\n","          36       0.98      0.38      0.55       124\n","          37       0.94      0.96      0.95      1907\n","          38       0.92      0.96      0.94       465\n","          39       0.95      0.94      0.94       522\n","          40       0.87      0.88      0.87       404\n","          41       0.94      0.91      0.93      1089\n","          42       0.98      0.98      0.98       802\n","          43       0.87      0.73      0.79       143\n","          44       0.88      0.97      0.92       108\n","          45       1.00      1.00      1.00         1\n","          46       0.99      0.98      0.99       192\n","          47       0.00      0.00      0.00         2\n","          48       0.75      0.06      0.12        48\n","\n","    accuracy                           0.93     28770\n","   macro avg       0.81      0.80      0.78     28770\n","weighted avg       0.93      0.93      0.93     28770\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["factor_list = [1, 2, 5, 10, 20]\n","factor = factor_list[4] #  to be modified\n","topn = round(factor * len(domain_dev_word_lst) / 100)\n","i = 0\n","while len(domain_dev_word_lst) >= topn:\n","  i += 1\n","  print(\"\\nLoop\", i)\n","  print(\"domain_dev_word_lst\", len(domain_dev_word_lst))\n","\n","  if i == 1:\n","    domain_dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=False,\n","                                num_workers=1,\n","                                collate_fn=pad)\n","  else:\n","    domain_dev_dataset = PosDataset_new(domain_dev_word_lst, domain_dev_tag_lst)\n","\n","    domain_dev_iter = data.DataLoader(dataset=domain_dev_dataset,\n","                                batch_size=8,\n","                                shuffle=True,\n","                                num_workers=1,\n","                                collate_fn=pad_new)\n","  \n","  initial = True if i==1 else False\n","  top_words_ids, top_tags_ids, domain_dev_word_lst, domain_dev_tag_lst = gen_pseudo_data(model, domain_dev_iter, topn, initial)\n","\n","  # Revert ids to words\n","  top_words = []\n","  top_tags = []\n","  for t in range(len(top_words_ids)):\n","    word_ids = tokenizer.convert_ids_to_tokens(top_words_ids[t])\n","    tag_ids = list(map(idx2tag.get, top_tags_ids[t]))\n","    words = []\n","    tags = []\n","    for k, w in enumerate(word_ids):\n","      if w == '[CLS]':\n","        pass\n","      elif w == '[SEP]':\n","        break\n","      else:\n","        words.append(w)\n","        tags.append(tag_ids[k])\n","    top_words.append(words)\n","    top_tags.append(tags)\n","\n","  new_train_dataset = PosDataset(wsj_train_word_lst+top_words, wsj_train_tag_lst+top_tags)\n","  new_train_iter = data.DataLoader(dataset=new_train_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","\n","  print(\"Train from scratch...\")\n","  model = Net(vocab_size=len(tag2idx))\n","  model.to(device)\n","  model = nn.DataParallel(model)\n","\n","  optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","  criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train(model, new_train_iter, optimizer, criterion)\n","\n","  domain_precision_value, domain_recall_value, domain_f1_value = eval(model, domain_test_iter)\n","  domain_precision_value_lst.append(domain_precision_value)\n","  domain_recall_value_lst.append(domain_recall_value)\n","  domain_f1_value_lst.append(domain_f1_value)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"lgFpgbAzQ-7c","executionInfo":{"status":"ok","timestamp":1669695972048,"user_tz":300,"elapsed":19,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# ids = []\n","# for w in domain_dev_word_lst[10]:\n","#   tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","#   xx = tokenizer.convert_tokens_to_ids(tokens)\n","#   ids.append(xx[0])\n","# print(ids)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"B1yxSL2bM__V","executionInfo":{"status":"ok","timestamp":1669695972048,"user_tz":300,"elapsed":5,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# token = tokenizer.convert_ids_to_tokens(ids)\n","# token"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"_GFhUqbrOtrd","executionInfo":{"status":"ok","timestamp":1669695972049,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["# print(new_train_x[11])\n","# print(tokenizer.convert_ids_to_tokens(new_train_x[11]))\n","# print(new_train_y[11])\n","# print(list(map(idx2tag.get, new_train_y[11])))"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"g4ZuWMYTojmJ","executionInfo":{"status":"ok","timestamp":1669695972049,"user_tz":300,"elapsed":5,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"jJxcl1cLn7rb","executionInfo":{"status":"ok","timestamp":1669695972049,"user_tz":300,"elapsed":5,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["test_metric = pd.DataFrame({\n","    \"Loop\": list(range(len(domain_precision_value_lst))) * 3,\n","    \"metric\": [\"precision\"]*len(domain_precision_value_lst) + [\"recall\"]*len(domain_precision_value_lst) + [\"f1\"]*len(domain_precision_value_lst),\n","    \"value\": domain_precision_value_lst + domain_recall_value_lst + domain_f1_value_lst\n","})"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"EvkSQ18M7p2H","executionInfo":{"status":"ok","timestamp":1669695972050,"user_tz":300,"elapsed":6,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"6ryA3bq0ot4b","executionInfo":{"status":"ok","timestamp":1669695974443,"user_tz":300,"elapsed":2399,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"O2DFUhCDorSf","executionInfo":{"status":"ok","timestamp":1669695975982,"user_tz":300,"elapsed":1541,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}},"colab":{"base_uri":"https://localhost:8080/","height":542},"outputId":"65ab5c31-ce54-41d2-82b2-f6685dbd74ec"},"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"87e8ae66-3e68-4157-a5c0-2d9cb55cfb71\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"87e8ae66-3e68-4157-a5c0-2d9cb55cfb71\")) {                    Plotly.newPlot(                        \"87e8ae66-3e68-4157-a5c0-2d9cb55cfb71\",                        [{\"hovertemplate\":\"metric=precision<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4],\"xaxis\":\"x\",\"y\":[0.7623825636123405,0.7834126277989931,0.8069529326436591,0.8089311076074902,0.8143194082176679],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4],\"xaxis\":\"x\",\"y\":[0.7790802505478349,0.7879802246899684,0.8161108149153917,0.812289707979385,0.7969905324808174],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>Loop=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4],\"xaxis\":\"x\",\"y\":[0.7569432801304008,0.7470834977024158,0.7867889197190948,0.7901273018638534,0.7848072616096896],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loop\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"title\":{\"text\":\"topn: 343, domain: answers\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('87e8ae66-3e68-4157-a5c0-2d9cb55cfb71');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}],"source":["fig_title = f\"topn: {topn}, domain: {domain}\"\n","fig = px.line(test_metric, x=\"Loop\", y=\"value\", color='metric', markers=True, title=fig_title)\n","fig.show()"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"PLs82qPoc_TW","executionInfo":{"status":"ok","timestamp":1669695975983,"user_tz":300,"elapsed":3,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["file_name = f\"topn_{factor}\\%_domain_{domain}\"\n","scratch_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/model\"\n","log_model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/scratch_fixed/result\"\n","test_metric.to_csv(os.path.join(log_model_dir, file_name) + '.csv')"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"2Ngux9I-orPX","executionInfo":{"status":"ok","timestamp":1669695978148,"user_tz":300,"elapsed":2167,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"outputs":[],"source":["torch.save(model.state_dict(), os.path.join(scratch_model_dir, file_name))"]},{"cell_type":"code","source":[],"metadata":{"id":"za1Q9pkFfY6i","executionInfo":{"status":"ok","timestamp":1669695978148,"user_tz":300,"elapsed":21,"user":{"displayName":"Xianmeng Wang","userId":"15062833796982544683"}}},"execution_count":43,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}