{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1TJE-QbEh2Il8B6uPq6QRMi15ZzsL35nA","authorship_tag":"ABX9TyOKagRneROWgfabK/lJGDQH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# WSJ data - One domain"],"metadata":{"id":"i2hUK__11lqr"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"oydKmqqCgsGa","executionInfo":{"status":"ok","timestamp":1667702812632,"user_tz":240,"elapsed":881,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"92e6be3e-8c52-4fc3-9109-b5afbb6df594"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","labeled_dir = os.path.join(data_dir, \"unlabeled\")"],"metadata":{"id":"o8W3eRKNhUhy","executionInfo":{"status":"ok","timestamp":1667702812633,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import codecs"],"metadata":{"id":"XqCivSdLhZL-","executionInfo":{"status":"ok","timestamp":1667702812633,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)"],"metadata":{"id":"KPxc_pjZha-0","executionInfo":{"status":"ok","timestamp":1667702812962,"user_tz":240,"elapsed":115,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"D0_psx2Tpicu","executionInfo":{"status":"ok","timestamp":1667702887581,"user_tz":240,"elapsed":133,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst = []\n","wsj_train_tag_lst = []\n","wsj_tags = []\n","for word, tag in read_conll_file(wsj_train_file):\n","  wsj_train_word_lst.append(word)\n","  wsj_train_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj train\", len(wsj_train_word_lst))\n","\n","wsj_dev_word_lst = []\n","wsj_dev_tag_lst = []\n","for word, tag in read_conll_file(wsj_dev_file):\n","  wsj_dev_word_lst.append(word)\n","  wsj_dev_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj dev\", len(wsj_dev_word_lst))\n","\n","wsj_test_word_lst = []\n","wsj_test_tag_lst = []\n","for word, tag in read_conll_file(wsj_test_file):\n","  wsj_test_word_lst.append(word)\n","  wsj_test_tag_lst.append(tag)\n","  wsj_tags.extend(tag)\n","print(\"The number of sentences in wsj test\", len(wsj_test_word_lst))\n","print(\"The number of tags in wsj\", len(set(wsj_tags)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIxiUPn8hchu","executionInfo":{"status":"ok","timestamp":1667702895761,"user_tz":240,"elapsed":2603,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"afd9fb99-93dc-42c1-dd5d-d5b256d7ec3d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of sentences in wsj train 30060\n","The number of sentences in wsj dev 1336\n","The number of tags in wsj 48\n","The number of sentences in wsj test 1640\n","The number of tags in wsj 48\n"]}]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"H21LlmVT1uVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for f_name in file_name_lst:\n","  print(\"\\n\")\n","  print(\"domain:\", f_name)\n","  domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{f_name}\")\n","  answer_dev_file = os.path.join(domain_dir, f\"gweb-{f_name}-dev.conll\")\n","  answer_test_file = os.path.join(domain_dir, f\"gweb-{f_name}-test.conll\")\n","\n","  ans_dev_word_lst = []\n","  ans_dev_tag_lst = []\n","  ans_tags = []\n","  for word, tag in read_conll_file(answer_dev_file):\n","    ans_dev_word_lst.append(word)\n","    ans_dev_tag_lst.append(tag)\n","    ans_tags.extend(tag)\n","  print(\"The number of sentences in answer dev\", len(ans_dev_word_lst))\n","\n","  ans_test_word_lst = []\n","  ans_test_tag_lst = []\n","  for word, tag in read_conll_file(answer_test_file):\n","    ans_test_word_lst.append(word)\n","    ans_test_tag_lst.append(tag)\n","    ans_tags.extend(tag)\n","  print(\"The number of sentences in answer test\", len(ans_test_word_lst))\n","  print(\"The number of tags in ans\", len(set(ans_tags)))"],"metadata":{"id":"Lu4Ix5Nvhfi8","executionInfo":{"status":"ok","timestamp":1667702789993,"user_tz":240,"elapsed":3890,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02b38ac4-e3cc-4ffe-bd84-7027dcd71ce5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","domain: wsj\n","The number of sentences in answer dev 1336\n","The number of sentences in answer test 1640\n","The number of tags in ans 45\n","\n","\n","domain: answers\n","The number of sentences in answer dev 1745\n","The number of sentences in answer test 1744\n","The number of tags in ans 50\n","\n","\n","domain: emails\n","The number of sentences in answer dev 2450\n","The number of sentences in answer test 2450\n","The number of tags in ans 49\n","\n","\n","domain: newsgroups\n","The number of sentences in answer dev 1196\n","The number of sentences in answer test 1195\n","The number of tags in ans 49\n","\n","\n","domain: reviews\n","The number of sentences in answer dev 1907\n","The number of sentences in answer test 1906\n","The number of tags in ans 50\n","\n","\n","domain: weblogs\n","The number of sentences in answer dev 1016\n","The number of sentences in answer test 1015\n","The number of tags in ans 49\n"]}]},{"cell_type":"markdown","source":["# Penn POS data"],"metadata":{"id":"3NwU9Oha1gdT"}},{"cell_type":"code","source":["# import torchtext\n","# from torchtext.legacy import data\n","# from torchtext.legacy import datasets"],"metadata":{"id":"4abBe4avyWJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TEXT = data.Field(lower = True)\n","# UD_TAGS = data.Field(unk_token = None)\n","# PTB_TAGS = data.Field(unk_token = None)\n","\n","# fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))\n","# train_data, valid_data, test_data = datasets.UDPOS.splits(fields)\n","\n","# print(f\"Number of training examples: {len(train_data)}\")\n","# print(f\"Number of validation examples: {len(valid_data)}\")\n","# print(f\"Number of testing examples: {len(test_data)}\")\n","\n","# print(vars(train_data.examples[0]))"],"metadata":{"id":"1EF9yY-EhfNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/PetrochukM/PyTorch-NLP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"GA9BWOI6WD16","executionInfo":{"status":"ok","timestamp":1667611574938,"user_tz":240,"elapsed":5142,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"3bf1e367-05ae-497c-bc27-33c148a4794c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/PetrochukM/PyTorch-NLP.git\n","  Cloning https://github.com/PetrochukM/PyTorch-NLP.git to /tmp/pip-req-build-s2k3gvkx\n","  Running command git clone -q https://github.com/PetrochukM/PyTorch-NLP.git /tmp/pip-req-build-s2k3gvkx\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp==0.5.0) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp==0.5.0) (4.28.1)\n","Building wheels for collected packages: pytorch-nlp\n","  Building wheel for pytorch-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch-nlp: filename=pytorch_nlp-0.5.0-py3-none-any.whl size=88361 sha256=0910dee3e1b4159f65e322fc0b39ac435976346a7ec5195303fa2379e4de95c4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gprk44_g/wheels/84/78/c4/66b0b0a3f3973609c6fdd26a91411257c13314b2445c7d83fc\n","Successfully built pytorch-nlp\n","Installing collected packages: pytorch-nlp\n","Successfully installed pytorch-nlp-0.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torchnlp"]}}},"metadata":{}}]},{"cell_type":"code","source":["# https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/datasets/ud_pos.html\n","\n","import os\n","import io\n","\n","from torchnlp.download import download_file_maybe_extract\n","\n","\n","def ud_pos_dataset(directory='data/',\n","                   train=False,\n","                   dev=False,\n","                   test=False,\n","                   train_filename='en-ud-tag.v2.train.txt',\n","                   dev_filename='en-ud-tag.v2.dev.txt',\n","                   test_filename='en-ud-tag.v2.test.txt',\n","                   extracted_name='en-ud-v2',\n","                   check_files=['en-ud-v2/en-ud-tag.v2.train.txt'],\n","                   url='https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zip'):\n","    \"\"\"\n","    Load the Universal Dependencies - English Dependency Treebank dataset.\n","\n","    Corpus of sentences annotated using Universal Dependencies annotation. The corpus comprises\n","    254,830 words and 16,622 sentences, taken from various web media including weblogs, newsgroups,\n","    emails, reviews, and Yahoo! answers.\n","\n","    References:\n","        * http://universaldependencies.org/\n","        * https://github.com/UniversalDependencies/UD_English\n","\n","    **Citation:**\n","    Natalia Silveira and Timothy Dozat and Marie-Catherine de Marneffe and Samuel Bowman and\n","    Miriam Connor and John Bauer and Christopher D. Manning (2014).\n","    A Gold Standard Dependency Corpus for {E}nglish\n","\n","    Args:\n","        directory (str, optional): Directory to cache the dataset.\n","        train (bool, optional): If to load the training split of the dataset.\n","        dev (bool, optional): If to load the development split of the dataset.\n","        test (bool, optional): If to load the test split of the dataset.\n","        train_filename (str, optional): The filename of the training split.\n","        dev_filename (str, optional): The filename of the development split.\n","        test_filename (str, optional): The filename of the test split.\n","        extracted_name (str, optional): Name of the extracted dataset directory.\n","        check_files (str, optional): Check if these files exist, then this download was successful.\n","        url (str, optional): URL of the dataset `tar.gz` file.\n","\n","    Returns:\n","        :class:`tuple` of :class:`iterable` or :class:`iterable`:\n","        Returns between one and all dataset splits (train, dev and test) depending on if their\n","        respective boolean argument is ``True``.\n","\n","    Example:\n","        >>> from torchnlp.datasets import ud_pos_dataset  # doctest: +SKIP\n","        >>> train = ud_pos_dataset(train=True)  # doctest: +SKIP\n","        >>> train[17]  # doctest: +SKIP\n","        {\n","          'tokens': ['Guerrillas', 'killed', 'an', 'engineer', ',', 'Asi', 'Ali', ',', 'from',\n","                     'Tikrit', '.'],\n","          'ud_tags': ['NOUN', 'VERB', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'ADP',\n","                      'PROPN', 'PUNCT'],\n","          'ptb_tags': ['NNS', 'VBD', 'DT', 'NN', ',', 'NNP', 'NNP', ',', 'IN', 'NNP', '.']\n","        }\n","    \"\"\"\n","    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n","\n","    ret = []\n","    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n","    splits = [f for (requested, f) in splits if requested]\n","    for filename in splits:\n","        full_path = os.path.join(directory, extracted_name, filename)\n","        examples = []\n","        with io.open(full_path, encoding='utf-8') as f:\n","            sentence = {'tokens': [], 'ud_tags': [], 'ptb_tags': []}\n","            for line in f:\n","                line = line.strip()\n","                if line == '' and len(sentence['tokens']) > 0:\n","                    examples.append(sentence)\n","                    sentence = {'tokens': [], 'ud_tags': [], 'ptb_tags': []}\n","                elif line != '':\n","                    token, ud_tag, ptb_tag = tuple(line.split('\\t'))\n","                    sentence['tokens'].append(token)\n","                    sentence['ud_tags'].append(ud_tag)\n","                    sentence['ptb_tags'].append(ptb_tag)\n","        ret.append(examples)\n","\n","    if len(ret) == 1:\n","        return ret[0]\n","    else:\n","        return tuple(ret)"],"metadata":{"id":"DXvIInTcWDzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["penn_train, penn_dev, penn_test = res = ud_pos_dataset(train=True,\n","                                                              dev=True,\n","                                                              test=True)\n","print(\"Train size\", len(penn_train))\n","print(\"Dev size\", len(penn_dev))\n","print(\"Test size\", len(penn_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWHYnCp1apxc","executionInfo":{"status":"ok","timestamp":1667612311141,"user_tz":240,"elapsed":768,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"e0cfb971-b521-4cb8-9c9b-fcda2f5ba7c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["en-ud-v2.zip: 696kB [00:00, 2.95MB/s]                            \n"]}]},{"cell_type":"code","source":["penn_train[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_z13nLpa9cU","executionInfo":{"status":"ok","timestamp":1667612320052,"user_tz":240,"elapsed":351,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"edbbb8c6-a2d0-4c07-e6bd-928524ff3269"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokens': ['[',\n","  'This',\n","  'killing',\n","  'of',\n","  'a',\n","  'respected',\n","  'cleric',\n","  'will',\n","  'be',\n","  'causing',\n","  'us',\n","  'trouble',\n","  'for',\n","  'years',\n","  'to',\n","  'come',\n","  '.',\n","  ']'],\n"," 'ud_tags': ['PUNCT',\n","  'DET',\n","  'NOUN',\n","  'ADP',\n","  'DET',\n","  'ADJ',\n","  'NOUN',\n","  'AUX',\n","  'AUX',\n","  'VERB',\n","  'PRON',\n","  'NOUN',\n","  'ADP',\n","  'NOUN',\n","  'PART',\n","  'VERB',\n","  'PUNCT',\n","  'PUNCT'],\n"," 'ptb_tags': ['-LRB-',\n","  'DT',\n","  'NN',\n","  'IN',\n","  'DT',\n","  'JJ',\n","  'NN',\n","  'MD',\n","  'VB',\n","  'VBG',\n","  'PRP',\n","  'NN',\n","  'IN',\n","  'NNS',\n","  'TO',\n","  'VB',\n","  '.',\n","  '-RRB-']}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":[],"metadata":{"id":"cwXFTl8rbqBX"},"execution_count":null,"outputs":[]}]}