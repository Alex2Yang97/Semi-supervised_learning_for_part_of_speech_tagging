{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12722,"status":"ok","timestamp":1667176991175,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"SlFDyqy3IQcd","outputId":"d3c2656c-f0ae-4c49-e4c9-000d3716c491"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 27.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.25.4-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting botocore<1.29.0,>=1.28.4\n","  Downloading botocore-1.28.4-py3-none-any.whl (9.3 MB)\n","\u001b[K     |████████████████████████████████| 9.3 MB 47.1 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.4->boto3->pytorch_pretrained_bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 53.8 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.4->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 59.4 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.25.4 botocore-1.28.4 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"]}],"source":["! pip install pytorch_pretrained_bert"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1667176991176,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"Fja_Rb6l_Ofu"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667177261336,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"C53hU_785VR2"},"outputs":[],"source":["data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","answer_dir = os.path.join(data_dir, \"pos_file\", \"answers\")\n","wsj_dir = os.path.join(data_dir, \"pos_file\", \"wsj\")\n","labeled_dir = os.path.join(data_dir, \"unlabeled\")"]},{"cell_type":"code","source":["\n","train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")"],"metadata":{"id":"J9vF6jlNgHe4","executionInfo":{"status":"ok","timestamp":1667177152241,"user_tz":240,"elapsed":1,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTDWMnIL93KA"},"outputs":[],"source":["import codecs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9bxLCHU9Zmt"},"outputs":[],"source":["def read_conll_file(file_name, raw=False):\n","    \"\"\"\n","    read in conll file\n","    word1    tag1\n","    ...      ...\n","    wordN    tagN\n","    Sentences MUST be separated by newlines!\n","    :param file_name: file to read in\n","    :param raw: if raw text file (with one sentence per line) -- adds 'DUMMY' label\n","    :return: generator of instances ((list of  words, list of tags) pairs)\n","    \"\"\"\n","    current_words = []\n","    current_tags = []\n","    \n","    for line in codecs.open(file_name, encoding='utf-8'):\n","        #line = line.strip()\n","        line = line[:-1]\n","\n","        if line:\n","            if raw:\n","                current_words = line.split() ## simple splitting by space\n","                current_tags = ['DUMMY' for _ in current_words]\n","                yield (current_words, current_tags)\n","\n","            else:\n","                if len(line.split(\"\\t\")) != 2:\n","                    if len(line.split(\"\\t\")) == 1: # emtpy words in gimpel\n","                        raise IOError(\"Issue with input file - doesn't have a tag or token?\")\n","                    else:\n","                        print(\"erroneous line: {} (line number: {}) \".format(line), file=sys.stderr)\n","                        exit()\n","                else:\n","                    word, tag = line.split('\\t')\n","                current_words.append(word)\n","                current_tags.append(tag)\n","\n","        else:\n","            if current_words and not raw: #skip emtpy lines\n","                yield (current_words, current_tags)\n","            current_words = []\n","            current_tags = []\n","\n","    # check for last one\n","    if current_tags != [] and not raw:\n","        yield (current_words, current_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rnubtddt_Apk"},"outputs":[],"source":["train_word_lst = []\n","train_tag_lst = []\n","tags = []\n","for word, tag in read_conll_file(train_file):\n","  train_word_lst.append(word)\n","  train_tag_lst.append(tag)\n","  tags.extend(tag)\n","\n","dev_word_lst = []\n","dev_tag_lst = []\n","for word, tag in read_conll_file(dev_file):\n","  dev_word_lst.append(word)\n","  dev_tag_lst.append(tag)\n","  tags.extend(tag)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666970681545,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"xSX1cHNKDBTA","outputId":"9ca9c47a-dc47-46b2-e0fc-b22501548722"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30060"]},"metadata":{},"execution_count":7}],"source":["len(train_word_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666970681546,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"QH8wVqxfDDXr","outputId":"05aaf5a2-2923-4925-e967-e1ac6b31e5d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1336"]},"metadata":{},"execution_count":8}],"source":["len(dev_word_lst)"]},{"cell_type":"markdown","source":["Show sentences from training set and testing set"],"metadata":{"id":"BH4z3UO3zm_5"}},{"cell_type":"code","source":["\" \".join(train_word_lst[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"ZSFgwJMVtQm1","executionInfo":{"status":"ok","timestamp":1666970681546,"user_tz":240,"elapsed":11,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"f79ab246-61ce-41cf-d469-4e6b35859e3d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\" \".join(train_tag_lst[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vcS5_w8ItZDN","executionInfo":{"status":"ok","timestamp":1666970682332,"user_tz":240,"elapsed":794,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"fc519405-71e2-43c5-8450-64c2ecc0f66a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"IN DT NNP CD NN IN `` DT NN '' IN NNP POS NNP NNP -LRB- `` VBN NNS VBP DT NN IN NNP NNP , '' NN CC NNS -RRB- , DT NN IN NNP , VBN IN NNP NNP , VBD RB VBN IN NNP NNP .\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["The total number of tags is 48, and we need to add \\<pad\\> as a tagging."],"metadata":{"id":"Cwy9ROf2zvAT"}},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":182,"status":"ok","timestamp":1667185432603,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"nfLFvcq8DMLq"},"outputs":[],"source":["tags = list(set(tags))\n","tags = [\"<pad>\"] + tags"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666970682332,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"kgodOr4R_iDo","outputId":"6cde0e1f-51d9-4a3e-e96e-5e97bfb2bb21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["49"]},"metadata":{},"execution_count":12}],"source":["len(tags)"]},{"cell_type":"code","source":["print(tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmsUN558tgPn","executionInfo":{"status":"ok","timestamp":1666970682332,"user_tz":240,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"3b39cb3d-0b0a-4e1d-b571-f61a04b4f747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<pad>', 'RBR', 'DT', '.', 'PRP$', 'SYM', 'CC', 'JJS', 'WRB', 'RP', 'NNPS', 'PDT', 'MD', 'FW', 'RBS', 'PRP', 'NNP', 'JJR', 'UH', \"''\", 'WP$', 'VBD', 'NFP', 'WP', ':', 'WDT', 'IN', 'POS', 'TO', 'NN', '-RRB-', '``', 'AFX', '$', 'HYPH', 'XX', ',', 'RB', '-LRB-', 'VBG', 'CD', 'NNS', 'JJ', 'VBZ', 'VB', 'EX', 'VBP', 'LS', 'VBN']\n"]}]},{"cell_type":"markdown","source":["Build two dictionaries for tokens and taggings."],"metadata":{"id":"HunRRszM0E4r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rR9mUV1r_wq2"},"outputs":[],"source":["tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(tags)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOZOzw4ND7QZ"},"outputs":[],"source":["import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOI2_9A1ABkQ"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"o90LRMBhEMxD"},"source":["# Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":768,"status":"ok","timestamp":1666970793585,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"cnSHA64MABfl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea4b7894-85b1-4fe1-8ed3-51ba5e598b5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 835754.28B/s]\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"]},{"cell_type":"markdown","source":["**This part is to help us understand the data processing.**\n","\n","- For any sentence, we need to add \\[CLS\\] at the beginning and \\[SEP\\] at the end.\n","- The tagging of \\[CLS\\] and \\[SEP\\] is \\<pad\\>."],"metadata":{"id":"gsDDjoKc0WjB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHVDzpbjFm-n"},"outputs":[],"source":["sents = []\n","tags_li = []\n","for i in range(len(train_word_lst)):\n","  sents.append([\"[CLS]\"] + train_word_lst[i] + [\"[SEP]\"])\n","  tags_li.append([\"<pad>\"] + train_tag_lst[i] + [\"<pad>\"])"]},{"cell_type":"code","source":["words, tags = sents[0], tags_li[0]"],"metadata":{"id":"4b4jy8JE9UNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since the token that BERT used is different from what we have, we need to re-tokenize the sentence. Therefore, we may get different tokens. For those tokens, we only count the first piece of them.\n","\n","- For example, the original token we had is **Oct.**, and the token processed by BERT is two pieces **Oct** and **.**\n","- Therefore, we use **is_head** to represent the tokens that we will use. The taggings with **1** is what we will use, and the taggings with **0** is what will not use"],"metadata":{"id":"jqsNzDbx1C73"}},{"cell_type":"code","source":["# This part is to understand the data processing\n","for i in range(10):\n","  print(\"\\n\")\n","  print(i)\n","  w = words[i]\n","  t = tags[i]\n","  print(\"word:\", w, \"tag:\", t)\n","\n","  tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","  print(\"tokens:\", tokens)\n","  xx = tokenizer.convert_tokens_to_ids(tokens)\n","  print(\"xx:\", xx)\n","\n","  is_head = [1] + [0]*(len(tokens) - 1)\n","  print(\"is_head:\", is_head)\n","\n","  t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","  print(\"t:\", t)\n","  yy = [tag2idx[each] for each in t]  # (T,)\n","  print(\"yy:\", yy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-JPYCgd_2xG","executionInfo":{"status":"ok","timestamp":1666970798647,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"c1b62df9-0389-4917-f1bf-1e0db32ccdd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","0\n","word: [CLS] tag: <pad>\n","tokens: ['[CLS]']\n","xx: [101]\n","is_head: [1]\n","t: ['<pad>']\n","yy: [0]\n","\n","\n","1\n","word: In tag: IN\n","tokens: ['In']\n","xx: [1130]\n","is_head: [1]\n","t: ['IN']\n","yy: [26]\n","\n","\n","2\n","word: an tag: DT\n","tokens: ['an']\n","xx: [1126]\n","is_head: [1]\n","t: ['DT']\n","yy: [2]\n","\n","\n","3\n","word: Oct. tag: NNP\n","tokens: ['Oct', '.']\n","xx: [14125, 119]\n","is_head: [1, 0]\n","t: ['NNP', '<pad>']\n","yy: [16, 0]\n","\n","\n","4\n","word: 19 tag: CD\n","tokens: ['19']\n","xx: [1627]\n","is_head: [1]\n","t: ['CD']\n","yy: [40]\n","\n","\n","5\n","word: review tag: NN\n","tokens: ['review']\n","xx: [3189]\n","is_head: [1]\n","t: ['NN']\n","yy: [29]\n","\n","\n","6\n","word: of tag: IN\n","tokens: ['of']\n","xx: [1104]\n","is_head: [1]\n","t: ['IN']\n","yy: [26]\n","\n","\n","7\n","word: `` tag: ``\n","tokens: ['`', '`']\n","xx: [169, 169]\n","is_head: [1, 0]\n","t: ['``', '<pad>']\n","yy: [31, 0]\n","\n","\n","8\n","word: The tag: DT\n","tokens: ['The']\n","xx: [1109]\n","is_head: [1]\n","t: ['DT']\n","yy: [2]\n","\n","\n","9\n","word: Misanthrope tag: NN\n","tokens: ['Mi', '##san', '##throp', '##e']\n","xx: [12107, 9995, 21850, 1162]\n","is_head: [1, 0, 0, 0]\n","t: ['NN', '<pad>', '<pad>', '<pad>']\n","yy: [29, 0, 0, 0]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztwXjFMNABan"},"outputs":[],"source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5yebs4aABYS"},"outputs":[],"source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"]},{"cell_type":"markdown","metadata":{"id":"z4fRkwI_GeZ2"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrIL_Q38GcFj"},"outputs":[],"source":["from pytorch_pretrained_bert import BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPJB_bKmGcDM"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"]},{"cell_type":"markdown","metadata":{"id":"-0XmcIDEGkxv"},"source":["# Train and evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbW5KBmNGcAl"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","        # print(logits.shape)\n","        # print(logits)\n","        # print(y.shape)\n","        # print(y)\n","        # break\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fkCEOjiGoVx"},"outputs":[],"source":["def eval(model, iterator):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n"]},{"cell_type":"markdown","metadata":{"id":"3LvxQeUeGsSZ"},"source":["# Load model and train"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25848,"status":"ok","timestamp":1666970867408,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"fzxeHBtYGoTd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4275aa63-f33f-4e8c-f776-2641a6d143e0"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:14<00:00, 28113427.12B/s]\n"]}],"source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsjA21qOGoQ2"},"outputs":[],"source":["train_dataset = PosDataset(train_word_lst, train_tag_lst)\n","eval_dataset = PosDataset(dev_word_lst, dev_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"]},{"cell_type":"markdown","source":["**This part is to help me understand the processed data and the model structure**"],"metadata":{"id":"D1GUolSd2JR7"}},{"cell_type":"code","source":["for i, batch_iter in enumerate(train_iter):\n","  break"],"metadata":{"id":"hBJ04jXfj78_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(batch_iter)):\n","  print(f\"batch_iter[{i}][0]:\", batch_iter[i][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4a4ATHWNkYqO","executionInfo":{"status":"ok","timestamp":1666970867688,"user_tz":240,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"43ba5b39-ba73-43eb-ce5a-9c1e7c803880"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch_iter[0][0]: [CLS] The $ 125 - billion - a - year Bay area economy represents one - fourth of the economy of the nation 's most populous state and accounts for 2 % to 3 % of the nation 's total output of goods and services , according to the Center for Continuing Study of the California Economy in Palo Alto . [SEP]\n","batch_iter[1][0]: tensor([  101,  1109,   109,  8347,   118,  3775,   118,   170,   118,  1214,\n","         2410,  1298,  4190,  5149,  1141,   118,  2223,  1104,  1103,  4190,\n","         1104,  1103,  3790,   112,   188,  1211, 22608,  1352,  1105,  5756,\n","         1111,   123,   110,  1106,   124,   110,  1104,  1103,  3790,   112,\n","          188,  1703,  5964,  1104,  4817,  1105,  1826,   117,  2452,  1106,\n","         1103,  1945,  1111, 20699,  8690,  1104,  1103,  1756, 14592,  1107,\n","        19585,  2858, 17762,   119,   102])\n","batch_iter[2][0]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n","batch_iter[3][0]: <pad> DT $ CD HYPH CD HYPH DT HYPH NN NNP NN NN VBZ CD HYPH NN IN DT NN IN DT NN POS RBS JJ NN CC VBZ IN CD NN TO CD NN IN DT NN POS JJ NN IN NNS CC NNS , VBG IN DT NNP IN NNP NNP IN DT NNP NNP IN NNP NNP . <pad>\n","batch_iter[4][0]: tensor([ 0,  2, 33, 40, 34, 40, 34,  2, 34, 29, 16, 29, 29, 43, 40, 34, 29, 26,\n","         2, 29, 26,  2, 29, 27,  0, 14, 42, 29,  6, 43, 26, 40, 29, 28, 40, 29,\n","        26,  2, 29, 27,  0, 42, 29, 26, 41,  6, 41, 36, 39, 26,  2, 16, 26, 16,\n","        16, 26,  2, 16, 16, 26, 16,  0, 16,  3,  0])\n","batch_iter[5][0]: 65\n"]}]},{"cell_type":"code","source":["for i in range(len(batch_iter)):\n","  print(f\"batch_iter[{i}][1]:\", batch_iter[i][1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4XzfCCD2h3o","executionInfo":{"status":"ok","timestamp":1666970867688,"user_tz":240,"elapsed":4,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"5cbdcef1-a685-4343-9303-ff3bcaf0903c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch_iter[0][1]: [CLS] So far as we can see only two persons are behaving with a dignity recognizing the seriousness of the issues : Mr. Lawson and Sir Alan Walters , the counterpoint of the Chancellor 's difficulties , who also resigned as personal adviser to Mrs. Thatcher . [SEP]\n","batch_iter[1][1]: tensor([  101,  1573,  1677,  1112,  1195,  1169,  1267,  1178,  1160,  4983,\n","         1132,  1129, 22300,  1114,   170, 14931, 17344,  1103,  3021,  1757,\n","         1104,  1103,  2492,   131,  1828,   119, 15205,  1105,  2203,  4258,\n","        20789,   117,  1103,  4073,  7587,  1104,  1103,  8861,   112,   188,\n","         7866,   117,  1150,  1145,  4603,  1112,  2357, 14269,  1106,  2823,\n","          119, 23300,   119,   102,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0])\n","batch_iter[2][1]: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n","batch_iter[3][1]: <pad> RB RB IN PRP MD VB RB CD NNS VBP VBG IN DT NN VBG DT NN IN DT NNS : NNP NNP CC NNP NNP NNP , DT NN IN DT NNP POS NNS , WP RB VBD IN JJ NN IN NNP NNP . <pad>\n","batch_iter[4][1]: tensor([ 0, 37, 37, 26, 15, 12, 44, 37, 40, 41, 46, 39,  0, 26,  2, 29, 39,  2,\n","        29,  0, 26,  2, 41, 24, 16,  0, 16,  6, 16, 16, 16, 36,  2, 29,  0, 26,\n","         2, 16, 27,  0, 41, 36, 23, 37, 21, 26, 42, 29, 26, 16,  0, 16,  3,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n","batch_iter[5][1]: 54\n"]}]},{"cell_type":"code","source":["x = batch_iter[1]\n","y = batch_iter[-2]\n","logits, y, _ = model(x, y)"],"metadata":{"id":"i3csAKnfk00S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Output: 8 sentences * 58 tokens * 49 taggings\n","logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5SMZm3v32-E","executionInfo":{"status":"ok","timestamp":1666970870307,"user_tz":240,"elapsed":8,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"999154ad-e8ef-4d81-8e96-7269a0d87590"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 65, 49])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["logits[0, 0, :]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyvazzJzEksU","executionInfo":{"status":"ok","timestamp":1666970870307,"user_tz":240,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"ce6df015-aef4-4a7e-9c0a-0ea414dfe066"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.8223, -0.0341,  0.2307,  0.3083,  0.2271, -0.4868, -0.5868, -0.3462,\n","         0.3355, -0.1309, -0.3546,  0.5737, -0.0997, -0.1102, -0.8688, -0.1514,\n","         0.1021, -0.3319, -0.0531,  0.5775,  0.1398,  0.1345,  0.1572,  0.1084,\n","        -0.5019, -0.1154,  0.1645, -0.1370, -0.0526, -0.1087, -0.1527,  0.0240,\n","        -0.5115,  0.1530, -0.1388, -0.5213, -0.0163, -0.0054,  0.3151, -0.1519,\n","        -0.5824, -0.0512, -0.1611,  0.2870, -0.0377,  0.4715, -0.6076, -0.2004,\n","         0.2832], device='cuda:0', grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","y = y.view(-1)  # (N*T,)"],"metadata":{"id":"oQEVVcYwmyxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IV9O9p3hmyuj","executionInfo":{"status":"ok","timestamp":1666970870308,"user_tz":240,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"1c0f8094-16f9-4df1-b25c-a3ad79a929fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([520, 49])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maZj0Vl7myrb","executionInfo":{"status":"ok","timestamp":1666970871071,"user_tz":240,"elapsed":767,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"c3e0efbc-404c-4d8f-8139-5c45876c0f80"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([520])"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["logits[:, 1].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg5hy68wnMSU","executionInfo":{"status":"ok","timestamp":1666970871071,"user_tz":240,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"7a364f8a-a7e9-49c5-e786-dd6af1f9f2d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([520])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["loss = criterion(logits, y)"],"metadata":{"id":"937iqcQHnTUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-Cb_kuQnULf","executionInfo":{"status":"ok","timestamp":1666970871072,"user_tz":240,"elapsed":4,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"dd7e873e-0ee2-4a76-f6d3-ccf2c748142d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.8318, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":464530,"status":"ok","timestamp":1666971335599,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"9yQ-qPkPGoOw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6eeae99-2d69-40f4-8af1-115da9b43d02"},"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 3.8127729892730713\n","step: 10, loss: 1.9694442749023438\n","step: 20, loss: 0.879882276058197\n","step: 30, loss: 0.30797073245048523\n","step: 40, loss: 0.3897354304790497\n","step: 50, loss: 0.2547544538974762\n","step: 60, loss: 0.14905545115470886\n","step: 70, loss: 0.18233917653560638\n","step: 80, loss: 0.1623125970363617\n","step: 90, loss: 0.1022351086139679\n","step: 100, loss: 0.22179630398750305\n","step: 110, loss: 0.0652894452214241\n","step: 120, loss: 0.1724962443113327\n","step: 130, loss: 0.18637429177761078\n","step: 140, loss: 0.22088506817817688\n","step: 150, loss: 0.25937268137931824\n","step: 160, loss: 0.18992236256599426\n","step: 170, loss: 0.1468413919210434\n","step: 180, loss: 0.10014870017766953\n","step: 190, loss: 0.14707069098949432\n","step: 200, loss: 0.12163648009300232\n","step: 210, loss: 0.166069895029068\n","step: 220, loss: 0.07133134454488754\n","step: 230, loss: 0.05981002002954483\n","step: 240, loss: 0.16793608665466309\n","step: 250, loss: 0.1492682546377182\n","step: 260, loss: 0.10050888359546661\n","step: 270, loss: 0.1977836936712265\n","step: 280, loss: 0.1065964549779892\n","step: 290, loss: 0.08773518353700638\n","step: 300, loss: 0.11492165923118591\n","step: 310, loss: 0.10752097517251968\n","step: 320, loss: 0.08210337907075882\n","step: 330, loss: 0.10443001240491867\n","step: 340, loss: 0.07493607699871063\n","step: 350, loss: 0.0705207958817482\n","step: 360, loss: 0.06529203802347183\n","step: 370, loss: 0.08479960262775421\n","step: 380, loss: 0.09010225534439087\n","step: 390, loss: 0.06179497018456459\n","step: 400, loss: 0.1108366921544075\n","step: 410, loss: 0.03635425120592117\n","step: 420, loss: 0.1392987072467804\n","step: 430, loss: 0.06312322616577148\n","step: 440, loss: 0.07116254419088364\n","step: 450, loss: 0.09643588215112686\n","step: 460, loss: 0.10926727950572968\n","step: 470, loss: 0.20215368270874023\n","step: 480, loss: 0.0788838267326355\n","step: 490, loss: 0.11714911460876465\n","step: 500, loss: 0.09610234946012497\n","step: 510, loss: 0.08385957032442093\n","step: 520, loss: 0.07998859137296677\n","step: 530, loss: 0.027271589264273643\n","step: 540, loss: 0.0457603894174099\n","step: 550, loss: 0.05238591879606247\n","step: 560, loss: 0.07382382452487946\n","step: 570, loss: 0.13709615170955658\n","step: 580, loss: 0.18421702086925507\n","step: 590, loss: 0.09090787917375565\n","step: 600, loss: 0.08232608437538147\n","step: 610, loss: 0.07738029211759567\n","step: 620, loss: 0.025600532069802284\n","step: 630, loss: 0.08996908366680145\n","step: 640, loss: 0.04146111384034157\n","step: 650, loss: 0.03865814954042435\n","step: 660, loss: 0.17321409285068512\n","step: 670, loss: 0.09256136417388916\n","step: 680, loss: 0.10306692868471146\n","step: 690, loss: 0.220808744430542\n","step: 700, loss: 0.16408565640449524\n","step: 710, loss: 0.10687080770730972\n","step: 720, loss: 0.059822190552949905\n","step: 730, loss: 0.19946874678134918\n","step: 740, loss: 0.05430573970079422\n","step: 750, loss: 0.13386137783527374\n","step: 760, loss: 0.1007370725274086\n","step: 770, loss: 0.12004005163908005\n","step: 780, loss: 0.09237783402204514\n","step: 790, loss: 0.1410209685564041\n","step: 800, loss: 0.07671701908111572\n","step: 810, loss: 0.07936874032020569\n","step: 820, loss: 0.09194396436214447\n","step: 830, loss: 0.10631325095891953\n","step: 840, loss: 0.10842352360486984\n","step: 850, loss: 0.032278284430503845\n","step: 860, loss: 0.0647617056965828\n","step: 870, loss: 0.06154457479715347\n","step: 880, loss: 0.05253380537033081\n","step: 890, loss: 0.10403077304363251\n","step: 900, loss: 0.060902707278728485\n","step: 910, loss: 0.06954668462276459\n","step: 920, loss: 0.023739267140626907\n","step: 930, loss: 0.09892016649246216\n","step: 940, loss: 0.038595814257860184\n","step: 950, loss: 0.026496613398194313\n","step: 960, loss: 0.10417364537715912\n","step: 970, loss: 0.05653474107384682\n","step: 980, loss: 0.10857824236154556\n","step: 990, loss: 0.12262420356273651\n","step: 1000, loss: 0.039072003215551376\n","step: 1010, loss: 0.1388186365365982\n","step: 1020, loss: 0.07624844461679459\n","step: 1030, loss: 0.04709121212363243\n","step: 1040, loss: 0.0977349579334259\n","step: 1050, loss: 0.06208265945315361\n","step: 1060, loss: 0.09821604937314987\n","step: 1070, loss: 0.09220591932535172\n","step: 1080, loss: 0.03722963482141495\n","step: 1090, loss: 0.06765076518058777\n","step: 1100, loss: 0.07665105909109116\n","step: 1110, loss: 0.1436111330986023\n","step: 1120, loss: 0.07090824097394943\n","step: 1130, loss: 0.05838015303015709\n","step: 1140, loss: 0.18848992884159088\n","step: 1150, loss: 0.11065708100795746\n","step: 1160, loss: 0.09054279327392578\n","step: 1170, loss: 0.049816347658634186\n","step: 1180, loss: 0.062093909829854965\n","step: 1190, loss: 0.0486440472304821\n","step: 1200, loss: 0.10051440447568893\n","step: 1210, loss: 0.037362728267908096\n","step: 1220, loss: 0.14262835681438446\n","step: 1230, loss: 0.05734562873840332\n","step: 1240, loss: 0.032334014773368835\n","step: 1250, loss: 0.03151094540953636\n","step: 1260, loss: 0.06407385319471359\n","step: 1270, loss: 0.09196215122938156\n","step: 1280, loss: 0.0958499163389206\n","step: 1290, loss: 0.1389082968235016\n","step: 1300, loss: 0.10709885507822037\n","step: 1310, loss: 0.0953783467411995\n","step: 1320, loss: 0.07127530127763748\n","step: 1330, loss: 0.07994138449430466\n","step: 1340, loss: 0.03614908084273338\n","step: 1350, loss: 0.034055739641189575\n","step: 1360, loss: 0.08682078868150711\n","step: 1370, loss: 0.0957387313246727\n","step: 1380, loss: 0.0641079768538475\n","step: 1390, loss: 0.049720313400030136\n","step: 1400, loss: 0.08973218500614166\n","step: 1410, loss: 0.08324923366308212\n","step: 1420, loss: 0.026615535840392113\n","step: 1430, loss: 0.060213446617126465\n","step: 1440, loss: 0.09158831834793091\n","step: 1450, loss: 0.11575164645910263\n","step: 1460, loss: 0.045203596353530884\n","step: 1470, loss: 0.09936197102069855\n","step: 1480, loss: 0.14965535700321198\n","step: 1490, loss: 0.05910337716341019\n","step: 1500, loss: 0.05271293967962265\n","step: 1510, loss: 0.09078779816627502\n","step: 1520, loss: 0.12243712693452835\n","step: 1530, loss: 0.05419411510229111\n","step: 1540, loss: 0.08434416353702545\n","step: 1550, loss: 0.15178149938583374\n","step: 1560, loss: 0.08980190753936768\n","step: 1570, loss: 0.03297232836484909\n","step: 1580, loss: 0.1601359248161316\n","step: 1590, loss: 0.12564247846603394\n","step: 1600, loss: 0.01930312067270279\n","step: 1610, loss: 0.05628366395831108\n","step: 1620, loss: 0.18698297441005707\n","step: 1630, loss: 0.08727187663316727\n","step: 1640, loss: 0.058415163308382034\n","step: 1650, loss: 0.1326625943183899\n","step: 1660, loss: 0.04335089772939682\n","step: 1670, loss: 0.0782618522644043\n","step: 1680, loss: 0.15936864912509918\n","step: 1690, loss: 0.05964677035808563\n","step: 1700, loss: 0.13342702388763428\n","step: 1710, loss: 0.14459100365638733\n","step: 1720, loss: 0.14552395045757294\n","step: 1730, loss: 0.08414033055305481\n","step: 1740, loss: 0.053054329007864\n","step: 1750, loss: 0.06497734040021896\n","step: 1760, loss: 0.11364949494600296\n","step: 1770, loss: 0.0653388649225235\n","step: 1780, loss: 0.07363606244325638\n","step: 1790, loss: 0.05758314207196236\n","step: 1800, loss: 0.12380701303482056\n","step: 1810, loss: 0.05829130858182907\n","step: 1820, loss: 0.028510045260190964\n","step: 1830, loss: 0.055724795907735825\n","step: 1840, loss: 0.0809638574719429\n","step: 1850, loss: 0.06746091693639755\n","step: 1860, loss: 0.12142118811607361\n","step: 1870, loss: 0.04831129312515259\n","step: 1880, loss: 0.07467906922101974\n","step: 1890, loss: 0.06802943348884583\n","step: 1900, loss: 0.09261958301067352\n","step: 1910, loss: 0.016349252313375473\n","step: 1920, loss: 0.05117454752326012\n","step: 1930, loss: 0.030892452225089073\n","step: 1940, loss: 0.10620898753404617\n","step: 1950, loss: 0.11632225662469864\n","step: 1960, loss: 0.11324996501207352\n","step: 1970, loss: 0.13627038896083832\n","step: 1980, loss: 0.09230051189661026\n","step: 1990, loss: 0.06544949114322662\n","step: 2000, loss: 0.03937416896224022\n","step: 2010, loss: 0.0543137788772583\n","step: 2020, loss: 0.052991706877946854\n","step: 2030, loss: 0.04115082323551178\n","step: 2040, loss: 0.0497383214533329\n","step: 2050, loss: 0.05913572013378143\n","step: 2060, loss: 0.06483978778123856\n","step: 2070, loss: 0.13194569945335388\n","step: 2080, loss: 0.05159426108002663\n","step: 2090, loss: 0.07623545080423355\n","step: 2100, loss: 0.0658775195479393\n","step: 2110, loss: 0.06969566643238068\n","step: 2120, loss: 0.016355641186237335\n","step: 2130, loss: 0.04590536281466484\n","step: 2140, loss: 0.11756113171577454\n","step: 2150, loss: 0.08249026536941528\n","step: 2160, loss: 0.09543686360120773\n","step: 2170, loss: 0.07277093827724457\n","step: 2180, loss: 0.04057753458619118\n","step: 2190, loss: 0.10997671633958817\n","step: 2200, loss: 0.10746457427740097\n","step: 2210, loss: 0.05211034417152405\n","step: 2220, loss: 0.07459899038076401\n","step: 2230, loss: 0.19307534396648407\n","step: 2240, loss: 0.09962359070777893\n","step: 2250, loss: 0.20132751762866974\n","step: 2260, loss: 0.23224873840808868\n","step: 2270, loss: 0.11057161539793015\n","step: 2280, loss: 0.0474177822470665\n","step: 2290, loss: 0.10963175445795059\n","step: 2300, loss: 0.23952533304691315\n","step: 2310, loss: 0.06035415083169937\n","step: 2320, loss: 0.037714362144470215\n","step: 2330, loss: 0.05290117859840393\n","step: 2340, loss: 0.04643014073371887\n","step: 2350, loss: 0.09129756689071655\n","step: 2360, loss: 0.04229589179158211\n","step: 2370, loss: 0.07195302098989487\n","step: 2380, loss: 0.04452109709382057\n","step: 2390, loss: 0.09582231938838959\n","step: 2400, loss: 0.14120495319366455\n","step: 2410, loss: 0.10158681124448776\n","step: 2420, loss: 0.060583338141441345\n","step: 2430, loss: 0.06597951054573059\n","step: 2440, loss: 0.09844893217086792\n","step: 2450, loss: 0.04849504679441452\n","step: 2460, loss: 0.16948455572128296\n","step: 2470, loss: 0.07187443226575851\n","step: 2480, loss: 0.11065673828125\n","step: 2490, loss: 0.15262165665626526\n","step: 2500, loss: 0.09620730578899384\n","step: 2510, loss: 0.06539037078619003\n","step: 2520, loss: 0.14182108640670776\n","step: 2530, loss: 0.1424717754125595\n","step: 2540, loss: 0.08238200843334198\n","step: 2550, loss: 0.05062765255570412\n","step: 2560, loss: 0.04228675737977028\n","step: 2570, loss: 0.15279969573020935\n","step: 2580, loss: 0.08647317439317703\n","step: 2590, loss: 0.10938294976949692\n","step: 2600, loss: 0.06611843407154083\n","step: 2610, loss: 0.0920204445719719\n","step: 2620, loss: 0.0639166310429573\n","step: 2630, loss: 0.04637816548347473\n","step: 2640, loss: 0.12134084105491638\n","step: 2650, loss: 0.09868352115154266\n","step: 2660, loss: 0.022925619035959244\n","step: 2670, loss: 0.09959527850151062\n","step: 2680, loss: 0.05410223454236984\n","step: 2690, loss: 0.10049661993980408\n","step: 2700, loss: 0.08117372542619705\n","step: 2710, loss: 0.018693506717681885\n","step: 2720, loss: 0.03248133882880211\n","step: 2730, loss: 0.07898133248090744\n","step: 2740, loss: 0.050280556082725525\n","step: 2750, loss: 0.09519963711500168\n","step: 2760, loss: 0.024998513981699944\n","step: 2770, loss: 0.08011051267385483\n","step: 2780, loss: 0.06769764423370361\n","step: 2790, loss: 0.12326531112194061\n","step: 2800, loss: 0.10738828778266907\n","step: 2810, loss: 0.05572447180747986\n","step: 2820, loss: 0.059119898825883865\n","step: 2830, loss: 0.10663152486085892\n","step: 2840, loss: 0.093233622610569\n","step: 2850, loss: 0.12918734550476074\n","step: 2860, loss: 0.09892736375331879\n","step: 2870, loss: 0.07185723632574081\n","step: 2880, loss: 0.060134340077638626\n","step: 2890, loss: 0.03203136473894119\n","step: 2900, loss: 0.26653599739074707\n","step: 2910, loss: 0.07359711825847626\n","step: 2920, loss: 0.1873551309108734\n","step: 2930, loss: 0.19095440208911896\n","step: 2940, loss: 0.11822763830423355\n","step: 2950, loss: 0.08978572487831116\n","step: 2960, loss: 0.04833110049366951\n","step: 2970, loss: 0.1049470528960228\n","step: 2980, loss: 0.04802286997437477\n","step: 2990, loss: 0.11661890894174576\n","step: 3000, loss: 0.08763036131858826\n","step: 3010, loss: 0.06189711391925812\n","step: 3020, loss: 0.0814029797911644\n","step: 3030, loss: 0.05955995246767998\n","step: 3040, loss: 0.05892520025372505\n","step: 3050, loss: 0.09228210896253586\n","step: 3060, loss: 0.15044192969799042\n","step: 3070, loss: 0.07249461114406586\n","step: 3080, loss: 0.13343101739883423\n","step: 3090, loss: 0.08260835707187653\n","step: 3100, loss: 0.0856437236070633\n","step: 3110, loss: 0.07514600455760956\n","step: 3120, loss: 0.03431808203458786\n","step: 3130, loss: 0.09699631482362747\n","step: 3140, loss: 0.07826913148164749\n","step: 3150, loss: 0.08922962099313736\n","step: 3160, loss: 0.05342728644609451\n","step: 3170, loss: 0.059703607112169266\n","step: 3180, loss: 0.05282209813594818\n","step: 3190, loss: 0.19299234449863434\n","step: 3200, loss: 0.11749741435050964\n","step: 3210, loss: 0.15259526669979095\n","step: 3220, loss: 0.08759237825870514\n","step: 3230, loss: 0.04847206920385361\n","step: 3240, loss: 0.10850152373313904\n","step: 3250, loss: 0.054180484265089035\n","step: 3260, loss: 0.11560126394033432\n","step: 3270, loss: 0.08587203174829483\n","step: 3280, loss: 0.19893091917037964\n","step: 3290, loss: 0.07686176896095276\n","step: 3300, loss: 0.11549761891365051\n","step: 3310, loss: 0.03674422949552536\n","step: 3320, loss: 0.09697367250919342\n","step: 3330, loss: 0.09779521822929382\n","step: 3340, loss: 0.07588733732700348\n","step: 3350, loss: 0.06988324224948883\n","step: 3360, loss: 0.15539537370204926\n","step: 3370, loss: 0.10745269060134888\n","step: 3380, loss: 0.04726927727460861\n","step: 3390, loss: 0.1135735809803009\n","step: 3400, loss: 0.12210584431886673\n","step: 3410, loss: 0.03136369213461876\n","step: 3420, loss: 0.08224153518676758\n","step: 3430, loss: 0.10806265473365784\n","step: 3440, loss: 0.03533513844013214\n","step: 3450, loss: 0.040566541254520416\n","step: 3460, loss: 0.11661364883184433\n","step: 3470, loss: 0.06731420010328293\n","step: 3480, loss: 0.04307188466191292\n","step: 3490, loss: 0.10549415647983551\n","step: 3500, loss: 0.028622537851333618\n","step: 3510, loss: 0.05933578312397003\n","step: 3520, loss: 0.06401017308235168\n","step: 3530, loss: 0.06328774243593216\n","step: 3540, loss: 0.08007185161113739\n","step: 3550, loss: 0.1529131978750229\n","step: 3560, loss: 0.06260295957326889\n","step: 3570, loss: 0.05587257072329521\n","step: 3580, loss: 0.15413708984851837\n","step: 3590, loss: 0.12669162452220917\n","step: 3600, loss: 0.09943893551826477\n","step: 3610, loss: 0.10494600981473923\n","step: 3620, loss: 0.07615898549556732\n","step: 3630, loss: 0.03636980801820755\n","step: 3640, loss: 0.05042361468076706\n","step: 3650, loss: 0.08612870424985886\n","step: 3660, loss: 0.06517890095710754\n","step: 3670, loss: 0.1020626649260521\n","step: 3680, loss: 0.04558849707245827\n","step: 3690, loss: 0.03876957669854164\n","step: 3700, loss: 0.11402472108602524\n","step: 3710, loss: 0.04512273520231247\n","step: 3720, loss: 0.033958397805690765\n","step: 3730, loss: 0.053947024047374725\n","step: 3740, loss: 0.08882895857095718\n","step: 3750, loss: 0.1125701367855072\n"]}],"source":["train(model, train_iter, optimizer, criterion)"]},{"cell_type":"code","source":["eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHMczCaFv21Y","executionInfo":{"status":"ok","timestamp":1666971341913,"user_tz":240,"elapsed":6317,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"5fa3e9b9-d520-4cc8-a308-d2acbeaf505e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1666971341914,"user":{"displayName":"Alex Y","userId":"02188660656026482944"},"user_tz":240},"id":"6VTPNx3aGoMI","outputId":"3e5a8302-1d67-45ae-e7ae-892c1a39e214"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Influential JJ JJ',\n"," 'members NNS NNS',\n"," 'of IN IN',\n"," 'the DT DT',\n"," 'House NNP NNP',\n"," 'Ways NNP NNPS',\n"," 'and CC CC',\n"," 'Means NNP NNPS',\n"," 'Committee NNP NNP',\n"," 'introduced VBD VBD',\n"," 'legislation NN NN',\n"," 'that WDT WDT',\n"," 'would MD MD',\n"," 'restrict VB VB',\n"," 'how WRB WRB',\n"," 'the DT DT',\n"," 'new JJ JJ',\n"," 'savings NNS NNS',\n"," '- HYPH HYPH',\n"," 'and CC CC',\n"," '- HYPH HYPH',\n"," 'loan NN NN',\n"," 'bailout NN NN',\n"," 'agency NN NN',\n"," 'can MD MD',\n"," 'raise VB VB',\n"," 'capital NN NN',\n"," ', , ,',\n"," 'creating VBG VBG',\n"," 'another DT DT',\n"," 'potential JJ JJ',\n"," 'obstacle NN NN',\n"," 'to IN IN',\n"," 'the DT DT',\n"," 'government NN NN',\n"," \"'s POS POS\",\n"," 'sale NN NN',\n"," 'of IN IN',\n"," 'sick JJ JJ',\n"," 'thrifts NNS NNS',\n"," '. . .',\n"," '',\n"," 'The DT DT',\n"," 'bill NN NN',\n"," ', , ,',\n"," 'whose WP$ WP$',\n"," 'backers NNS NNS',\n"," 'include VBP VBP',\n"," 'Chairman NNP NNP',\n"," 'Dan NNP NNP',\n"," 'Rostenkowski NNP NNP',\n"," '-LRB- -LRB- -LRB-',\n"," 'D. NNP NNP',\n"," ', , ,',\n"," 'Ill. NNP NNP',\n"," '-RRB- -RRB- -RRB-',\n"," ', , ,',\n"," 'would MD MD',\n"," 'prevent VB VB',\n"," 'the DT DT',\n"," 'Resolution NNP NNP',\n"," 'Trust NNP NNP',\n"," 'Corp. NNP NNP',\n"," 'from IN IN',\n"," 'raising VBG VBG',\n"," 'temporary JJ JJ',\n"," 'working VBG JJ',\n"," 'capital NN NN',\n"," 'by IN IN',\n"," 'having VBG VBG',\n"," 'an DT DT',\n"," 'RTC NNP NNP',\n"," '- HYPH HYPH',\n"," 'owned VBN VBN',\n"," 'bank NN NN',\n"," 'or CC CC',\n"," 'thrift NN NN',\n"," 'issue NN NN',\n"," 'debt NN NN',\n"," 'that WDT WDT',\n"," 'would MD MD',\n"," \"n't RB RB\",\n"," 'be VB VB',\n"," 'counted VBN VBN',\n"," 'on IN IN',\n"," 'the DT DT',\n"," 'federal JJ JJ',\n"," 'budget NN NN',\n"," '. . .',\n"," '',\n"," 'The DT DT',\n"," 'bill NN NN',\n"," 'intends VBZ VBZ',\n"," 'to TO TO',\n"," 'restrict VB VB',\n"," 'the DT DT',\n"," 'RTC NNP NNP',\n"," 'to IN IN',\n"," 'Treasury NNP NNP',\n"," 'borrowings NNS NNS']"]},"metadata":{},"execution_count":43}],"source":["open('result', 'r').read().splitlines()[:100]"]},{"cell_type":"markdown","source":["# Other metrics"],"metadata":{"id":"dxJjI0qmOTnI"}},{"cell_type":"code","source":["train_dataset = PosDataset(train_word_lst, train_tag_lst)\n","eval_dataset = PosDataset(dev_word_lst, dev_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"XqzI_-xpPMMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","with torch.no_grad():\n","    for i, batch in enumerate(test_iter):\n","        words, x, is_heads, tags, y, seqlens = batch\n","\n","        _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","        Words.extend(words)\n","        Is_heads.extend(is_heads)\n","        Tags.extend(tags)\n","        Y.extend(y.numpy().tolist())\n","        Y_hat.extend(y_hat.cpu().numpy().tolist())"],"metadata":{"id":"O0Vr_szQPXcI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dev_word_lst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZvmsCYyPjKu","executionInfo":{"status":"ok","timestamp":1666971539225,"user_tz":240,"elapsed":225,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"ec689f8c-1058-4afb-8f40-200fc2f1a1ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1336"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["len(Y_hat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZfAKuOaPxUL","executionInfo":{"status":"ok","timestamp":1666971550197,"user_tz":240,"elapsed":2,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"7987f2c1-57ba-45ac-eec4-638eb9f9c192"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1336"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["y_hat.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0t2gYcHP2GK","executionInfo":{"status":"ok","timestamp":1666971600157,"user_tz":240,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"6df1180e-acb5-425f-ab29-154aa1c24573"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 63])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])"],"metadata":{"id":"x5kaVjtuQapS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aT-sDEyNQcDe","executionInfo":{"status":"ok","timestamp":1666971729152,"user_tz":240,"elapsed":2,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"f4bb03f9-4bdf-44f1-802d-57e736c610f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32092,)"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["y_pred.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aB-6vFyQhaK","executionInfo":{"status":"ok","timestamp":1666971741899,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"cb21d6f3-2cf9-47df-db59-54b1b3f209f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32092,)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score"],"metadata":{"id":"eCKfgHUIQu7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## calc metric\n","y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","print(\"acc=%.2f\"%acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4N7YsjIPgsj","executionInfo":{"status":"ok","timestamp":1666971842019,"user_tz":240,"elapsed":3,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"c2cd3d78-6de8-4227-9813-b4a7a3907770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.97\n"]}]},{"cell_type":"code","source":["# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n","print(\"micro precision\", precision_score(y_true, y_pred, average=\"micro\"))\n","# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n","print(\"macro precision\", precision_score(y_true, y_pred, average=\"macro\"))\n","# Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n","# This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n","print(\"weighted precision\", precision_score(y_true, y_pred, average=\"weighted\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7ykMxR-Q1nI","executionInfo":{"status":"ok","timestamp":1666972175010,"user_tz":240,"elapsed":9,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"2f9d3246-3af2-4bf4-a5c1-4260650cfe9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["micro precision 0.9731085628817151\n","macro precision 0.9073238140324092\n","weighted precision 0.9743623300289095\n"]}]},{"cell_type":"code","source":["print(\"micro recall\", recall_score(y_true, y_pred, average=\"micro\"))\n","print(\"macro recall\", recall_score(y_true, y_pred, average=\"macro\"))\n","print(\"weighted recall\", recall_score(y_true, y_pred, average=\"weighted\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zCx_4lOR3TG","executionInfo":{"status":"ok","timestamp":1666972175011,"user_tz":240,"elapsed":7,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"8ec9c3b3-ab33-41a5-d4d6-2c3d6d3b6dd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["micro recall 0.9731085628817151\n","macro recall 0.9151197467279802\n","weighted recall 0.9731085628817151\n"]}]},{"cell_type":"code","source":["print(\"micro f1\", f1_score(y_true, y_pred, average=\"micro\"))\n","print(\"macro f1\", f1_score(y_true, y_pred, average=\"macro\"))\n","print(\"weighted f1\", f1_score(y_true, y_pred, average=\"weighted\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyou7v9mSFLD","executionInfo":{"status":"ok","timestamp":1666972176069,"user_tz":240,"elapsed":2,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"3f54cfce-c141-47ea-e77f-e8d5c19b9a77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["micro f1 0.9731085628817152\n","macro f1 0.9043129765781763\n","weighted f1 0.973577776244746\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rznKBd1r_010"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1biYNQqzBOY9ELx4lSgdv4cnvGOK70DoU","authorship_tag":"ABX9TyNE3g8xV3Th/hxMmh6NCO4n"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}