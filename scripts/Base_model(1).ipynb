{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1CKPVg3uLbnOI0YnI2aD4l33yXFkYqvqf","authorship_tag":"ABX9TyPW6b41GbYTotqZIA+CN3qc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","source":["! pip install pytorch_pretrained_bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e4oNQYEdk4U","executionInfo":{"status":"ok","timestamp":1668007615239,"user_tz":300,"elapsed":12973,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"b57a5c53-418b-43b4-d097-8c35bec9b979"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 32.0 MB/s \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.26.5-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.8 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting botocore<1.30.0,>=1.29.5\n","  Downloading botocore-1.29.5-py3-none-any.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 10.0 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.5->boto3->pytorch_pretrained_bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.5->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 53.6 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.5 botocore-1.29.5 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1668007632543,"user_tz":300,"elapsed":17311,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"0e049c4b-bd7b-49de-b1fc-24ee479f9f61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","\n","import os\n","\n","from utils import read_conll_file, read_data\n","\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/data/gweb_sancl\"\n","wsj_dir = os.path.join(data_dir, \"pos_fine\", \"wsj\")\n","labeled_dir = os.path.join(data_dir, \"unlabeled\")\n","model_dir = \"/content/drive/MyDrive/Colab Notebooks/Capstone/model\""]},{"cell_type":"code","source":["wsj_train_file = os.path.join(wsj_dir, \"gweb-wsj-train.conll\")\n","wsj_dev_file = os.path.join(wsj_dir, \"gweb-wsj-dev.conll\")\n","wsj_test_file = os.path.join(wsj_dir, \"gweb-wsj-test.conll\")"],"metadata":{"id":"bOIJvUda3xt5","executionInfo":{"status":"ok","timestamp":1668007632544,"user_tz":300,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["wsj_train_word_lst, wsj_train_tag_lst, wsj_train_tag_set = read_data(wsj_train_file)\n","wsj_dev_word_lst, wsj_dev_tag_lst, wsj_dev_tag_set = read_data(wsj_dev_file)\n","wsj_test_word_lst, wsj_test_tag_lst, wsj_test_tag_set = read_data(wsj_test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRWJRko38sKr","executionInfo":{"status":"ok","timestamp":1668007635402,"user_tz":300,"elapsed":2863,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"ca1cfc88-0391-44b0-e972-822c161c271a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["# import random\n","\n","# random.seed(0)\n","# random.shuffle(wsj_train_word_lst)\n","# random.seed(0)\n","# random.shuffle(wsj_train_tag_lst)\n","\n","# labeled_train_words = wsj_train_word_lst[:10000]\n","# labeled_train_tags = wsj_train_tag_lst[:10000]\n","# unlabeled_words = wsj_train_word_lst[10000:]\n","# unlabeled_tags = wsj_train_tag_lst[10000:]\n","\n","# print(len(labeled_train_words))\n","# print(len(unlabeled_words))"],"metadata":{"id":"9N8fDl0u72zK","executionInfo":{"status":"ok","timestamp":1668007635403,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["wsj_tags = wsj_train_tag_set + wsj_dev_tag_set + wsj_test_tag_set\n","wsj_tags = sorted(list(set(wsj_tags)))\n","wsj_tags = [\"<pad>\"] + wsj_tags\n","tag2idx = {tag:idx for idx, tag in enumerate(wsj_tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(wsj_tags)}\n","print(len(wsj_tags))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgySDvNl3xkh","executionInfo":{"status":"ok","timestamp":1668007635403,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"9bf1e6f6-25fe-4cfe-98b6-0fd98f78c418"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["49\n"]}]},{"cell_type":"markdown","source":["# Build Model"],"metadata":{"id":"V9yUXS679IFc"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer"],"metadata":{"id":"7nIm4vqm3xiL","executionInfo":{"status":"ok","timestamp":1668007639261,"user_tz":300,"elapsed":3862,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"zleK0sd96JRp","executionInfo":{"status":"ok","timestamp":1668007639397,"user_tz":300,"elapsed":142,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"6fRrkkC26JP_","executionInfo":{"status":"ok","timestamp":1668007639664,"user_tz":300,"elapsed":270,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a85b7219-b38c-40ea-a8dc-c5e87115044d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 20770577.19B/s]\n"]}]},{"cell_type":"code","source":["class PosDataset(data.Dataset):\n","    def __init__(self, word_lst, tag_lst):\n","        sents, tags_li = [], [] # list of lists\n","        for i in range(len(word_lst)):\n","            sents.append([\"[CLS]\"] + word_lst[i] + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tag_lst[i] + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n","\n","        # We give credits only to the first piece.\n","        x, y = [], [] # list of ids\n","        is_heads = [] # list. 1: the token is the first piece of a word\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n","            yy = [tag2idx[each] for each in t]  # (T,)\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        # seqlen\n","        seqlen = len(y)\n","\n","        # to string\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen\n"],"metadata":{"id":"RpKgRRbK6JMr","executionInfo":{"status":"ok","timestamp":1668007639893,"user_tz":300,"elapsed":9,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    '''Pads to the longest sample'''\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"MZ_JndBu6LjA","executionInfo":{"status":"ok","timestamp":1668007639893,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"9mthfoFt6JFJ","executionInfo":{"status":"ok","timestamp":1668007639894,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","        '''\n","        x: (N, T). int64\n","        y: (N, T). int64\n","        '''\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"e6ydlTI16JCz","executionInfo":{"status":"ok","timestamp":1668007639894,"user_tz":300,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y # for monitoring\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n","\n","        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","        y = y.view(-1)  # (N*T,)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0: # monitoring\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"DeD_19uq6JAd","executionInfo":{"status":"ok","timestamp":1668007639894,"user_tz":300,"elapsed":5,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator, average=\"macro\"):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    ## gets results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    ## calc metric\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n","\n","    print(\"acc=%.2f\"%acc)\n","    print(\"classification_report\", classification_report(y_true, y_pred))\n","    precision_value = precision_score(y_true, y_pred, average=average)\n","    recall_value = recall_score(y_true, y_pred, average=average)\n","    f1_value = f1_score(y_true, y_pred, average=average)\n","\n","    return precision_value, recall_value, f1_value"],"metadata":{"id":"DW4KvG4x6I91","executionInfo":{"status":"ok","timestamp":1668007640066,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"0ZDK1-UU6I5K","executionInfo":{"status":"ok","timestamp":1668007665074,"user_tz":300,"elapsed":25013,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8277b1a-c0a7-4389-83f4-c0f323aea3f7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 404400730/404400730 [00:14<00:00, 28816658.77B/s]\n"]}]},{"cell_type":"code","source":["train_dataset = PosDataset(wsj_train_word_lst, wsj_train_tag_lst)\n","eval_dataset = PosDataset(wsj_test_word_lst, wsj_test_tag_lst)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"f5pQmdTS6I20","executionInfo":{"status":"ok","timestamp":1668007665287,"user_tz":300,"elapsed":215,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["train(model, train_iter, optimizer, criterion)\n","eval(model, test_iter)"],"metadata":{"id":"B0x3gRfi9iyA","executionInfo":{"status":"ok","timestamp":1667838394050,"user_tz":300,"elapsed":474354,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6abb4796-00fa-4d50-e6a6-6da753cb9232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 3.9347734451293945\n","step: 10, loss: 1.762121558189392\n","step: 20, loss: 0.7181916236877441\n","step: 30, loss: 0.5616925358772278\n","step: 40, loss: 0.3763507902622223\n","step: 50, loss: 0.2633183002471924\n","step: 60, loss: 0.33471694588661194\n","step: 70, loss: 0.23860099911689758\n","step: 80, loss: 0.17271754145622253\n","step: 90, loss: 0.1582895666360855\n","step: 100, loss: 0.21905650198459625\n","step: 110, loss: 0.10516443848609924\n","step: 120, loss: 0.08706970512866974\n","step: 130, loss: 0.10375358909368515\n","step: 140, loss: 0.07010554522275925\n","step: 150, loss: 0.11012985557317734\n","step: 160, loss: 0.1452144980430603\n","step: 170, loss: 0.1952163577079773\n","step: 180, loss: 0.12142522633075714\n","step: 190, loss: 0.12944729626178741\n","step: 200, loss: 0.19011715054512024\n","step: 210, loss: 0.13491018116474152\n","step: 220, loss: 0.1767224371433258\n","step: 230, loss: 0.14211967587471008\n","step: 240, loss: 0.13849107921123505\n","step: 250, loss: 0.1435754895210266\n","step: 260, loss: 0.11594511568546295\n","step: 270, loss: 0.10954378545284271\n","step: 280, loss: 0.04325617477297783\n","step: 290, loss: 0.13439597189426422\n","step: 300, loss: 0.1249300017952919\n","step: 310, loss: 0.2415233999490738\n","step: 320, loss: 0.14598718285560608\n","step: 330, loss: 0.04608522728085518\n","step: 340, loss: 0.10865508019924164\n","step: 350, loss: 0.12506960332393646\n","step: 360, loss: 0.10812211036682129\n","step: 370, loss: 0.14449918270111084\n","step: 380, loss: 0.0840618908405304\n","step: 390, loss: 0.09097260981798172\n","step: 400, loss: 0.12471240758895874\n","step: 410, loss: 0.14399680495262146\n","step: 420, loss: 0.08527946472167969\n","step: 430, loss: 0.1480165719985962\n","step: 440, loss: 0.07925064116716385\n","step: 450, loss: 0.15331119298934937\n","step: 460, loss: 0.12331808358430862\n","step: 470, loss: 0.18316562473773956\n","step: 480, loss: 0.04555908963084221\n","step: 490, loss: 0.18600857257843018\n","step: 500, loss: 0.11877996474504471\n","step: 510, loss: 0.1144290342926979\n","step: 520, loss: 0.22074474394321442\n","step: 530, loss: 0.1045498326420784\n","step: 540, loss: 0.08797936141490936\n","step: 550, loss: 0.06968444585800171\n","step: 560, loss: 0.07502097636461258\n","step: 570, loss: 0.09907069802284241\n","step: 580, loss: 0.10822264850139618\n","step: 590, loss: 0.15577822923660278\n","step: 600, loss: 0.07083644717931747\n","step: 610, loss: 0.08790042251348495\n","step: 620, loss: 0.17596003413200378\n","step: 630, loss: 0.13081513345241547\n","step: 640, loss: 0.05193636938929558\n","step: 650, loss: 0.08126023411750793\n","step: 660, loss: 0.10191832482814789\n","step: 670, loss: 0.14566673338413239\n","step: 680, loss: 0.11038347333669662\n","step: 690, loss: 0.04891978204250336\n","step: 700, loss: 0.11388082802295685\n","step: 710, loss: 0.05924658477306366\n","step: 720, loss: 0.08883921056985855\n","step: 730, loss: 0.05091794207692146\n","step: 740, loss: 0.04858468845486641\n","step: 750, loss: 0.10871725529432297\n","step: 760, loss: 0.12496943026781082\n","step: 770, loss: 0.0752897784113884\n","step: 780, loss: 0.0808822438120842\n","step: 790, loss: 0.08891157805919647\n","step: 800, loss: 0.11005541682243347\n","step: 810, loss: 0.024954766035079956\n","step: 820, loss: 0.10499420017004013\n","step: 830, loss: 0.2087043970823288\n","step: 840, loss: 0.1654040813446045\n","step: 850, loss: 0.0787990540266037\n","step: 860, loss: 0.04885045811533928\n","step: 870, loss: 0.06272757053375244\n","step: 880, loss: 0.0976957455277443\n","step: 890, loss: 0.07105634361505508\n","step: 900, loss: 0.11017155647277832\n","step: 910, loss: 0.055873241275548935\n","step: 920, loss: 0.09266079217195511\n","step: 930, loss: 0.23628021776676178\n","step: 940, loss: 0.15234750509262085\n","step: 950, loss: 0.06093187630176544\n","step: 960, loss: 0.08967112749814987\n","step: 970, loss: 0.19316503405570984\n","step: 980, loss: 0.1363896131515503\n","step: 990, loss: 0.22384148836135864\n","step: 1000, loss: 0.17291271686553955\n","step: 1010, loss: 0.05934424698352814\n","step: 1020, loss: 0.04038166627287865\n","step: 1030, loss: 0.11119915544986725\n","step: 1040, loss: 0.14217509329319\n","step: 1050, loss: 0.16445770859718323\n","step: 1060, loss: 0.16542789340019226\n","step: 1070, loss: 0.03346756473183632\n","step: 1080, loss: 0.09962013363838196\n","step: 1090, loss: 0.05283885821700096\n","step: 1100, loss: 0.11964205652475357\n","step: 1110, loss: 0.1204352080821991\n","step: 1120, loss: 0.10263742506504059\n","step: 1130, loss: 0.09386057406663895\n","step: 1140, loss: 0.1262611299753189\n","step: 1150, loss: 0.0835929661989212\n","step: 1160, loss: 0.02002311497926712\n","step: 1170, loss: 0.0937616303563118\n","step: 1180, loss: 0.07082069665193558\n","step: 1190, loss: 0.08795412629842758\n","step: 1200, loss: 0.08003178238868713\n","step: 1210, loss: 0.03826805204153061\n","step: 1220, loss: 0.07491198182106018\n","step: 1230, loss: 0.08111722022294998\n","step: 1240, loss: 0.050871312618255615\n","step: 1250, loss: 0.08201299607753754\n","step: 1260, loss: 0.06120416522026062\n","step: 1270, loss: 0.22081515192985535\n","step: 1280, loss: 0.05227242782711983\n","step: 1290, loss: 0.07120131701231003\n","step: 1300, loss: 0.09013351052999496\n","step: 1310, loss: 0.11459986120462418\n","step: 1320, loss: 0.11032316088676453\n","step: 1330, loss: 0.1299237459897995\n","step: 1340, loss: 0.034874964505434036\n","step: 1350, loss: 0.06549418717622757\n","step: 1360, loss: 0.18900293111801147\n","step: 1370, loss: 0.08060279488563538\n","step: 1380, loss: 0.04818674176931381\n","step: 1390, loss: 0.11282683908939362\n","step: 1400, loss: 0.07765097916126251\n","step: 1410, loss: 0.14146088063716888\n","step: 1420, loss: 0.07106109708547592\n","step: 1430, loss: 0.12280601263046265\n","step: 1440, loss: 0.04818948358297348\n","step: 1450, loss: 0.233370840549469\n","step: 1460, loss: 0.1385727822780609\n","step: 1470, loss: 0.1184249296784401\n","step: 1480, loss: 0.07036065310239792\n","step: 1490, loss: 0.03340067341923714\n","step: 1500, loss: 0.14329485595226288\n","step: 1510, loss: 0.1069813147187233\n","step: 1520, loss: 0.12930478155612946\n","step: 1530, loss: 0.10355311632156372\n","step: 1540, loss: 0.0986616387963295\n","step: 1550, loss: 0.13415013253688812\n","step: 1560, loss: 0.16055117547512054\n","step: 1570, loss: 0.05199141055345535\n","step: 1580, loss: 0.10724950581789017\n","step: 1590, loss: 0.08632444590330124\n","step: 1600, loss: 0.09997431188821793\n","step: 1610, loss: 0.05608255788683891\n","step: 1620, loss: 0.1489144116640091\n","step: 1630, loss: 0.07890757918357849\n","step: 1640, loss: 0.1064915582537651\n","step: 1650, loss: 0.17617280781269073\n","step: 1660, loss: 0.15584833920001984\n","step: 1670, loss: 0.10977426171302795\n","step: 1680, loss: 0.07006947696208954\n","step: 1690, loss: 0.15138298273086548\n","step: 1700, loss: 0.12578916549682617\n","step: 1710, loss: 0.0339093878865242\n","step: 1720, loss: 0.04295794293284416\n","step: 1730, loss: 0.1148025393486023\n","step: 1740, loss: 0.058110930025577545\n","step: 1750, loss: 0.07443968951702118\n","step: 1760, loss: 0.06530831754207611\n","step: 1770, loss: 0.13356009125709534\n","step: 1780, loss: 0.09892067313194275\n","step: 1790, loss: 0.07812757045030594\n","step: 1800, loss: 0.10447946935892105\n","step: 1810, loss: 0.060105759650468826\n","step: 1820, loss: 0.11771285533905029\n","step: 1830, loss: 0.06471879780292511\n","step: 1840, loss: 0.06858497858047485\n","step: 1850, loss: 0.058613136410713196\n","step: 1860, loss: 0.04672091454267502\n","step: 1870, loss: 0.16856001317501068\n","step: 1880, loss: 0.025503309443593025\n","step: 1890, loss: 0.043287333101034164\n","step: 1900, loss: 0.3081347644329071\n","step: 1910, loss: 0.14971795678138733\n","step: 1920, loss: 0.04691416770219803\n","step: 1930, loss: 0.07932568341493607\n","step: 1940, loss: 0.07364959269762039\n","step: 1950, loss: 0.17198698222637177\n","step: 1960, loss: 0.0735836774110794\n","step: 1970, loss: 0.11903398483991623\n","step: 1980, loss: 0.08647533506155014\n","step: 1990, loss: 0.09023435413837433\n","step: 2000, loss: 0.06794423609972\n","step: 2010, loss: 0.07459954917430878\n","step: 2020, loss: 0.12173973023891449\n","step: 2030, loss: 0.09784328937530518\n","step: 2040, loss: 0.04470057785511017\n","step: 2050, loss: 0.06914980709552765\n","step: 2060, loss: 0.10696356743574142\n","step: 2070, loss: 0.059839826077222824\n","step: 2080, loss: 0.07889476418495178\n","step: 2090, loss: 0.06925255805253983\n","step: 2100, loss: 0.19163943827152252\n","step: 2110, loss: 0.03666632995009422\n","step: 2120, loss: 0.10628198087215424\n","step: 2130, loss: 0.10362399369478226\n","step: 2140, loss: 0.03932953253388405\n","step: 2150, loss: 0.1317085325717926\n","step: 2160, loss: 0.10296711325645447\n","step: 2170, loss: 0.2889048159122467\n","step: 2180, loss: 0.1575394868850708\n","step: 2190, loss: 0.1111433133482933\n","step: 2200, loss: 0.12043438106775284\n","step: 2210, loss: 0.10286283493041992\n","step: 2220, loss: 0.10573320090770721\n","step: 2230, loss: 0.09444862604141235\n","step: 2240, loss: 0.07292072474956512\n","step: 2250, loss: 0.11548105627298355\n","step: 2260, loss: 0.08219592273235321\n","step: 2270, loss: 0.0713895708322525\n","step: 2280, loss: 0.060570452362298965\n","step: 2290, loss: 0.09630664438009262\n","step: 2300, loss: 0.07912863045930862\n","step: 2310, loss: 0.11167933791875839\n","step: 2320, loss: 0.1282733827829361\n","step: 2330, loss: 0.06353811919689178\n","step: 2340, loss: 0.08829335868358612\n","step: 2350, loss: 0.046702709048986435\n","step: 2360, loss: 0.10056573152542114\n","step: 2370, loss: 0.0471440926194191\n","step: 2380, loss: 0.11148837953805923\n","step: 2390, loss: 0.10946526378393173\n","step: 2400, loss: 0.07940449565649033\n","step: 2410, loss: 0.08386136591434479\n","step: 2420, loss: 0.10758282989263535\n","step: 2430, loss: 0.07081154733896255\n","step: 2440, loss: 0.02061517722904682\n","step: 2450, loss: 0.04573008418083191\n","step: 2460, loss: 0.07313648611307144\n","step: 2470, loss: 0.13201037049293518\n","step: 2480, loss: 0.05334782972931862\n","step: 2490, loss: 0.10530495643615723\n","step: 2500, loss: 0.1074836477637291\n","step: 2510, loss: 0.12460608780384064\n","step: 2520, loss: 0.07541795819997787\n","step: 2530, loss: 0.053852908313274384\n","step: 2540, loss: 0.057132475078105927\n","step: 2550, loss: 0.21002452075481415\n","step: 2560, loss: 0.06380987912416458\n","step: 2570, loss: 0.0885709747672081\n","step: 2580, loss: 0.17317897081375122\n","step: 2590, loss: 0.08900593966245651\n","step: 2600, loss: 0.059050772339105606\n","step: 2610, loss: 0.12240622192621231\n","step: 2620, loss: 0.054232072085142136\n","step: 2630, loss: 0.052945576608181\n","step: 2640, loss: 0.06882020831108093\n","step: 2650, loss: 0.1607448309659958\n","step: 2660, loss: 0.0327993743121624\n","step: 2670, loss: 0.11475828289985657\n","step: 2680, loss: 0.09017907828092575\n","step: 2690, loss: 0.151533842086792\n","step: 2700, loss: 0.11257609724998474\n","step: 2710, loss: 0.12900398671627045\n","step: 2720, loss: 0.13372835516929626\n","step: 2730, loss: 0.08208589255809784\n","step: 2740, loss: 0.07869749516248703\n","step: 2750, loss: 0.053708046674728394\n","step: 2760, loss: 0.12066615372896194\n","step: 2770, loss: 0.0823027640581131\n","step: 2780, loss: 0.0710839107632637\n","step: 2790, loss: 0.059084802865982056\n","step: 2800, loss: 0.11253593862056732\n","step: 2810, loss: 0.055219616740942\n","step: 2820, loss: 0.02004251815378666\n","step: 2830, loss: 0.12378241121768951\n","step: 2840, loss: 0.10732226073741913\n","step: 2850, loss: 0.0534496009349823\n","step: 2860, loss: 0.05080858990550041\n","step: 2870, loss: 0.1212293803691864\n","step: 2880, loss: 0.13130472600460052\n","step: 2890, loss: 0.11996906250715256\n","step: 2900, loss: 0.04332927241921425\n","step: 2910, loss: 0.03332225978374481\n","step: 2920, loss: 0.03511831536889076\n","step: 2930, loss: 0.17964929342269897\n","step: 2940, loss: 0.11133318394422531\n","step: 2950, loss: 0.17463456094264984\n","step: 2960, loss: 0.07770334184169769\n","step: 2970, loss: 0.07309811562299728\n","step: 2980, loss: 0.13021937012672424\n","step: 2990, loss: 0.08378198742866516\n","step: 3000, loss: 0.06652640551328659\n","step: 3010, loss: 0.12621381878852844\n","step: 3020, loss: 0.08589436113834381\n","step: 3030, loss: 0.0919315293431282\n","step: 3040, loss: 0.06998059153556824\n","step: 3050, loss: 0.09755067527294159\n","step: 3060, loss: 0.10923995077610016\n","step: 3070, loss: 0.11973558366298676\n","step: 3080, loss: 0.06093764677643776\n","step: 3090, loss: 0.04258957877755165\n","step: 3100, loss: 0.06215762719511986\n","step: 3110, loss: 0.06135273352265358\n","step: 3120, loss: 0.07709277421236038\n","step: 3130, loss: 0.05343717336654663\n","step: 3140, loss: 0.13513708114624023\n","step: 3150, loss: 0.12421389669179916\n","step: 3160, loss: 0.09023518860340118\n","step: 3170, loss: 0.09123700857162476\n","step: 3180, loss: 0.08910343050956726\n","step: 3190, loss: 0.03767601028084755\n","step: 3200, loss: 0.04099302366375923\n","step: 3210, loss: 0.09805579483509064\n","step: 3220, loss: 0.02121104672551155\n","step: 3230, loss: 0.08549460768699646\n","step: 3240, loss: 0.06248778849840164\n","step: 3250, loss: 0.058360930532217026\n","step: 3260, loss: 0.10318510979413986\n","step: 3270, loss: 0.11440858989953995\n","step: 3280, loss: 0.07023336738348007\n","step: 3290, loss: 0.14470605552196503\n","step: 3300, loss: 0.0740031898021698\n","step: 3310, loss: 0.05752909928560257\n","step: 3320, loss: 0.07717503607273102\n","step: 3330, loss: 0.07393357157707214\n","step: 3340, loss: 0.07802633941173553\n","step: 3350, loss: 0.034209948033094406\n","step: 3360, loss: 0.11067809909582138\n","step: 3370, loss: 0.11205030232667923\n","step: 3380, loss: 0.10685285925865173\n","step: 3390, loss: 0.09235747158527374\n","step: 3400, loss: 0.07875963300466537\n","step: 3410, loss: 0.15301412343978882\n","step: 3420, loss: 0.07854396849870682\n","step: 3430, loss: 0.08315891772508621\n","step: 3440, loss: 0.04894467070698738\n","step: 3450, loss: 0.06060725450515747\n","step: 3460, loss: 0.09318017214536667\n","step: 3470, loss: 0.1825995147228241\n","step: 3480, loss: 0.04412369430065155\n","step: 3490, loss: 0.10599538683891296\n","step: 3500, loss: 0.0861503854393959\n","step: 3510, loss: 0.10434409230947495\n","step: 3520, loss: 0.09161418676376343\n","step: 3530, loss: 0.10042116045951843\n","step: 3540, loss: 0.08002711832523346\n","step: 3550, loss: 0.16333742439746857\n","step: 3560, loss: 0.11002717912197113\n","step: 3570, loss: 0.017872372642159462\n","step: 3580, loss: 0.025283079594373703\n","step: 3590, loss: 0.10258489102125168\n","step: 3600, loss: 0.07824643701314926\n","step: 3610, loss: 0.08853793144226074\n","step: 3620, loss: 0.18687890470027924\n","step: 3630, loss: 0.14033573865890503\n","step: 3640, loss: 0.04821082577109337\n","step: 3650, loss: 0.16694246232509613\n","step: 3660, loss: 0.05320550128817558\n","step: 3670, loss: 0.08047446608543396\n","step: 3680, loss: 0.08079887926578522\n","step: 3690, loss: 0.059294577687978745\n","step: 3700, loss: 0.12127149850130081\n","step: 3710, loss: 0.13386070728302002\n","step: 3720, loss: 0.12834101915359497\n","step: 3730, loss: 0.0529269278049469\n","step: 3740, loss: 0.05132045969367027\n","step: 3750, loss: 0.09433736652135849\n","acc=0.98\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       178\n","           2       1.00      1.00      1.00       352\n","           3       1.00      1.00      1.00      2000\n","           4       1.00      1.00      1.00        60\n","           5       1.00      1.00      1.00        60\n","           6       1.00      1.00      1.00      1613\n","           7       1.00      1.00      1.00       223\n","           9       1.00      0.99      1.00       935\n","          10       0.99      1.00      0.99      1266\n","          11       1.00      1.00      1.00      3309\n","          12       1.00      1.00      1.00        46\n","          13       1.00      0.20      0.33        20\n","          14       1.00      1.00      1.00       511\n","          15       0.98      0.99      0.99      4250\n","          16       0.95      0.94      0.94      2423\n","          17       0.94      0.95      0.94       139\n","          18       0.92      0.96      0.94        73\n","          19       1.00      1.00      1.00         4\n","          20       1.00      1.00      1.00       413\n","          22       0.98      0.98      0.98      5545\n","          23       0.98      0.95      0.97      4133\n","          24       0.21      0.80      0.33        45\n","          25       0.99      0.99      0.99      2316\n","          26       0.70      0.93      0.80        15\n","          27       1.00      0.99      0.99       373\n","          28       0.98      1.00      0.99       766\n","          29       1.00      0.97      0.98       357\n","          30       0.96      0.93      0.95      1405\n","          31       0.91      0.87      0.89        82\n","          32       0.92      0.89      0.90        37\n","          33       0.91      0.82      0.86       126\n","          34       0.85      1.00      0.92        11\n","          35       0.98      0.96      0.97       588\n","          36       0.50      0.29      0.36         7\n","          37       0.99      0.96      0.97      1124\n","          38       0.99      0.97      0.98      1162\n","          39       0.93      0.96      0.95       608\n","          40       0.90      0.96      0.93       829\n","          41       0.96      0.98      0.97       563\n","          42       0.98      0.99      0.99       873\n","          43       0.92      0.98      0.95       191\n","          44       0.99      1.00      0.99        90\n","          45       1.00      1.00      1.00        14\n","          46       1.00      0.99      0.99        89\n","          48       1.00      1.00      1.00       366\n","\n","    accuracy                           0.98     39590\n","   macro avg       0.94      0.94      0.93     39590\n","weighted avg       0.98      0.98      0.98     39590\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.9397482533696433, 0.9371770472723233, 0.9273419086238314)"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"6fVp31VJ5U64"}},{"cell_type":"code","source":["model_file = os.path.join(model_dir, \"base_model.pt\")"],"metadata":{"id":"LtVeE3zd04C8","executionInfo":{"status":"ok","timestamp":1668007746001,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# torch.save(model.state_dict(), model_file)"],"metadata":{"id":"MgMq01rh9ism","executionInfo":{"status":"ok","timestamp":1668007748443,"user_tz":300,"elapsed":881,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"cyvpy9QH4sQd"}},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(model_file))\n","wsj_precision_value, wsj_recall_value, wsj_f1_value = eval(model, test_iter)"],"metadata":{"id":"hAD3Wd574v6Q","executionInfo":{"status":"ok","timestamp":1668007808585,"user_tz":300,"elapsed":21848,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0805f5a3-3479-4ba3-f0a4-d6f04b8fb3b1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["acc=0.98\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00       178\n","           2       1.00      1.00      1.00       352\n","           3       1.00      1.00      1.00      2000\n","           4       1.00      1.00      1.00        60\n","           5       1.00      1.00      1.00        60\n","           6       1.00      1.00      1.00      1613\n","           7       1.00      1.00      1.00       223\n","           9       1.00      0.99      1.00       935\n","          10       0.99      1.00      0.99      1266\n","          11       1.00      1.00      1.00      3309\n","          12       1.00      1.00      1.00        46\n","          13       1.00      0.20      0.33        20\n","          14       1.00      1.00      1.00       511\n","          15       0.98      0.99      0.99      4250\n","          16       0.95      0.94      0.94      2423\n","          17       0.94      0.95      0.94       139\n","          18       0.92      0.96      0.94        73\n","          19       1.00      1.00      1.00         4\n","          20       1.00      1.00      1.00       413\n","          22       0.98      0.98      0.98      5545\n","          23       0.98      0.95      0.97      4133\n","          24       0.21      0.80      0.33        45\n","          25       0.99      0.99      0.99      2316\n","          26       0.70      0.93      0.80        15\n","          27       1.00      0.99      0.99       373\n","          28       0.98      1.00      0.99       766\n","          29       1.00      0.97      0.98       357\n","          30       0.96      0.93      0.95      1405\n","          31       0.91      0.87      0.89        82\n","          32       0.92      0.89      0.90        37\n","          33       0.91      0.82      0.86       126\n","          34       0.85      1.00      0.92        11\n","          35       0.98      0.96      0.97       588\n","          36       0.50      0.29      0.36         7\n","          37       0.99      0.96      0.97      1124\n","          38       0.99      0.97      0.98      1162\n","          39       0.93      0.96      0.95       608\n","          40       0.90      0.96      0.93       829\n","          41       0.96      0.98      0.97       563\n","          42       0.98      0.99      0.99       873\n","          43       0.92      0.98      0.95       191\n","          44       0.99      1.00      0.99        90\n","          45       1.00      1.00      1.00        14\n","          46       1.00      0.99      0.99        89\n","          48       1.00      1.00      1.00       366\n","\n","    accuracy                           0.98     39590\n","   macro avg       0.94      0.94      0.93     39590\n","weighted avg       0.98      0.98      0.98     39590\n","\n"]}]},{"cell_type":"code","source":["print(wsj_precision_value, wsj_recall_value, wsj_f1_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4pfi5lqA3G9","executionInfo":{"status":"ok","timestamp":1668007829417,"user_tz":300,"elapsed":110,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"d2d6e1f0-bc14-493c-85e6-8914c52d862a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9397482533696433 0.9371770472723233 0.9273419086238314\n"]}]},{"cell_type":"code","source":["file_name_lst = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"ND978DPG6Ngm","executionInfo":{"status":"ok","timestamp":1668009304712,"user_tz":300,"elapsed":120,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def filter_tag(process_words, process_tags, label_tags_set=wsj_tags):\n","  new_words = []\n","  new_tags = []\n","  for words, tags in zip(process_words, process_tags):\n","    w_lst = []\n","    t_lst = []\n","    for i, t in enumerate(tags):\n","      if t in label_tags_set:\n","        w_lst.append(words[i])\n","        t_lst.append(tags[i])\n","\n","    if w_lst:\n","      new_words.append(w_lst)\n","      new_tags.append(t_lst)\n","  print(\"after filter tag\", len(new_words))\n","  return new_words, new_tags"],"metadata":{"id":"UIt5ataG_2vc","executionInfo":{"status":"ok","timestamp":1668009377177,"user_tz":300,"elapsed":128,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["dev_precision_value_lst = []\n","dev_recall_value_lst = []\n","dev_f1_value_lst = []\n","\n","test_precision_value_lst = []\n","test_recall_value_lst = []\n","test_f1_value_lst = []\n","\n","for domain in file_name_lst:\n","  print(\"\\n\")\n","  print(\"Domain\", domain)\n","  domain_dir = os.path.join(data_dir, \"pos_fine\", f\"{domain}\")\n","  domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","  domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")\n","\n","  domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","  domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","\n","  domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","  domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","  test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_iter = data.DataLoader(dataset=dev_dataset,\n","                              batch_size=8,\n","                              shuffle=True,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","  test_iter = data.DataLoader(dataset=test_dataset,\n","                              batch_size=8,\n","                              shuffle=False,\n","                              num_workers=1,\n","                              collate_fn=pad)\n","  \n","  dev_precision_value, dev_recall_value, dev_f1_value = eval(model, dev_iter)\n","  test_precision_value, test_recall_value, test_f1_value = eval(model, test_iter)\n","\n","  dev_precision_value_lst.append(dev_precision_value)\n","  dev_recall_value_lst.append(dev_recall_value)\n","  dev_f1_value_lst.append(dev_f1_value)\n","\n","  test_precision_value_lst.append(test_precision_value)\n","  test_recall_value_lst.append(test_recall_value)\n","  test_f1_value_lst.append(test_f1_value)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJ0Vh14U97K5","executionInfo":{"status":"ok","timestamp":1668009442116,"user_tz":300,"elapsed":61436,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"4633851e-3f87-4b31-82a4-9658db40b816"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Domain answers\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n","acc=0.92\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.71      0.94      0.81        16\n","           2       0.61      0.96      0.74        68\n","           3       1.00      0.82      0.90       881\n","           4       0.87      0.98      0.92        81\n","           5       0.98      0.97      0.97        90\n","           6       1.00      0.97      0.98      1445\n","           7       0.21      0.89      0.34        53\n","           8       0.00      0.00      0.00        11\n","           9       0.99      0.98      0.99       870\n","          10       0.86      0.97      0.91       295\n","          11       0.97      0.99      0.98      2022\n","          12       0.92      0.93      0.93        87\n","          13       0.94      0.71      0.81        24\n","          14       0.59      0.96      0.73        70\n","          15       0.95      0.98      0.96      2384\n","          16       0.88      0.87      0.87      1415\n","          17       0.93      0.93      0.93       106\n","          18       0.88      0.97      0.92        86\n","          19       0.41      0.89      0.56        18\n","          20       0.98      0.98      0.98       440\n","          21       0.70      0.10      0.17        73\n","          22       0.86      0.92      0.89      3103\n","          23       0.77      0.78      0.78      1356\n","          24       0.74      0.63      0.68        71\n","          25       0.90      0.97      0.93       980\n","          26       1.00      0.94      0.97        18\n","          27       0.96      0.75      0.84        68\n","          28       0.99      0.97      0.98      1793\n","          29       0.98      0.93      0.95       320\n","          30       0.91      0.83      0.87      1524\n","          31       0.78      0.76      0.77        37\n","          32       1.00      0.52      0.69        21\n","          33       0.77      0.91      0.83       104\n","          34       0.18      0.44      0.25        16\n","          35       0.98      0.98      0.98       388\n","          36       0.92      0.06      0.12       172\n","          37       0.95      0.90      0.92      1340\n","          38       0.96      0.96      0.96       430\n","          39       0.97      0.94      0.95       371\n","          40       0.91      0.92      0.92       330\n","          41       0.93      0.94      0.94       953\n","          42       0.98      0.98      0.98       699\n","          43       0.96      0.85      0.91       124\n","          44       0.88      0.97      0.92       126\n","          46       0.99      0.98      0.98       164\n","          47       0.00      0.00      0.00         2\n","          48       1.00      0.01      0.03        73\n","\n","    accuracy                           0.92     25118\n","   macro avg       0.82      0.80      0.78     25118\n","weighted avg       0.92      0.92      0.91     25118\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["acc=0.92\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.62      1.00      0.76         8\n","           2       0.52      0.98      0.68        44\n","           3       1.00      0.87      0.93       987\n","           4       1.00      0.98      0.99       108\n","           5       0.99      0.98      0.99       115\n","           6       1.00      0.97      0.98      1600\n","           7       0.19      0.69      0.29        48\n","           8       0.00      0.00      0.00         4\n","           9       0.99      0.98      0.99      1086\n","          10       0.90      0.97      0.94       386\n","          11       0.97      0.99      0.98      2229\n","          12       0.95      0.95      0.95        61\n","          13       0.94      0.89      0.91        18\n","          14       0.33      0.75      0.46        28\n","          15       0.95      0.97      0.96      2566\n","          16       0.87      0.88      0.87      1511\n","          17       0.84      0.89      0.86        98\n","          18       0.85      0.93      0.89        56\n","          19       0.40      0.95      0.57        20\n","          20       0.99      0.99      0.99       717\n","          21       0.75      0.07      0.12        45\n","          22       0.89      0.93      0.91      3623\n","          23       0.64      0.76      0.69       563\n","          24       0.43      0.47      0.45        19\n","          25       0.92      0.96      0.94      1162\n","          26       0.80      1.00      0.89        28\n","          27       1.00      0.62      0.77        53\n","          28       0.98      0.96      0.97      2476\n","          29       1.00      0.90      0.95       563\n","          30       0.91      0.85      0.88      1947\n","          31       0.71      0.56      0.63        45\n","          32       0.94      0.63      0.76        27\n","          33       0.67      0.79      0.73       151\n","          34       0.41      0.91      0.57        34\n","          35       0.98      0.97      0.97       537\n","          36       1.00      0.06      0.11       124\n","          37       0.95      0.92      0.93      1907\n","          38       0.96      0.93      0.95       465\n","          39       0.97      0.92      0.95       522\n","          40       0.88      0.88      0.88       404\n","          41       0.92      0.90      0.91      1089\n","          42       0.99      0.97      0.98       802\n","          43       0.89      0.86      0.88       143\n","          44       0.88      0.98      0.93       108\n","          45       1.00      1.00      1.00         1\n","          46       0.99      0.98      0.99       192\n","          47       0.00      0.00      0.00         2\n","          48       1.00      0.17      0.29        48\n","\n","    accuracy                           0.92     28770\n","   macro avg       0.81      0.80      0.77     28770\n","weighted avg       0.93      0.92      0.92     28770\n","\n","\n","\n","Domain emails\n","The number of samples: 2450\n","The number of tags 49\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["The number of samples: 2450\n","The number of tags 48\n","after filter tag 2427\n","after filter tag 2402\n","acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.66      1.00      0.80        55\n","           2       0.40      0.90      0.55        73\n","           3       1.00      0.84      0.91      1166\n","           4       0.90      0.78      0.83       233\n","           5       0.93      0.79      0.85       234\n","           6       0.99      0.99      0.99      1566\n","           7       0.65      0.96      0.77       203\n","           8       0.00      0.00      0.00         8\n","           9       1.00      0.98      0.99       751\n","          10       0.95      0.97      0.96       870\n","          11       0.98      1.00      0.99      2062\n","          12       1.00      1.00      1.00        37\n","          13       1.00      0.20      0.33        15\n","          14       0.41      1.00      0.58        54\n","          15       0.97      0.98      0.97      2830\n","          16       0.87      0.87      0.87      1195\n","          17       0.89      0.94      0.92        36\n","          18       0.98      1.00      0.99        43\n","          19       0.29      0.86      0.44        14\n","          20       0.99      1.00      1.00       596\n","          21       0.69      0.22      0.33       102\n","          22       0.93      0.79      0.85      4122\n","          23       0.74      0.95      0.83      2420\n","          24       0.56      0.62      0.59        64\n","          25       0.87      0.96      0.91      1108\n","          26       0.95      0.95      0.95        22\n","          27       0.98      0.97      0.97        94\n","          28       0.99      0.99      0.99      1785\n","          29       1.00      0.94      0.97       381\n","          30       0.92      0.87      0.89      1155\n","          31       0.84      0.70      0.76        23\n","          32       0.88      0.78      0.82         9\n","          33       0.72      0.85      0.78        97\n","          34       0.25      0.68      0.37        34\n","          35       0.99      0.98      0.98       519\n","          36       1.00      0.04      0.07       184\n","          37       0.90      0.95      0.92      1586\n","          38       0.97      0.96      0.97       480\n","          39       0.96      0.94      0.95       447\n","          40       0.93      0.91      0.92       582\n","          41       0.95      0.93      0.94       756\n","          42       0.97      0.98      0.98       514\n","          43       0.91      0.90      0.91       104\n","          44       0.89      0.98      0.93        86\n","          45       1.00      1.00      1.00         1\n","          46       1.00      0.97      0.99       108\n","          48       0.86      0.32      0.47        74\n","\n","    accuracy                           0.91     28898\n","   macro avg       0.84      0.83      0.80     28898\n","weighted avg       0.93      0.91      0.91     28898\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["acc=0.91\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.67      1.00      0.80        35\n","           2       0.42      0.94      0.58        77\n","           3       1.00      0.79      0.89      1030\n","           4       0.90      0.85      0.87       291\n","           5       0.94      0.84      0.89       294\n","           6       0.99      0.98      0.98      1570\n","           7       0.60      0.94      0.73       186\n","           8       0.00      0.00      0.00        11\n","           9       1.00      0.99      0.99       689\n","          10       0.96      0.98      0.97       901\n","          11       0.98      1.00      0.99      2111\n","          12       0.98      0.98      0.98        47\n","          13       0.75      0.69      0.72        13\n","          14       0.30      1.00      0.46        43\n","          15       0.98      0.98      0.98      2778\n","          16       0.85      0.87      0.86      1151\n","          17       0.93      0.98      0.95        41\n","          18       0.91      1.00      0.96        32\n","          19       0.50      0.80      0.62        40\n","          20       0.99      0.99      0.99       584\n","          21       0.36      0.10      0.15        52\n","          22       0.93      0.75      0.83      4175\n","          23       0.69      0.97      0.80      2253\n","          24       0.36      0.68      0.47        44\n","          25       0.84      0.93      0.88       888\n","          26       1.00      1.00      1.00         9\n","          27       0.99      0.97      0.98        69\n","          28       0.99      0.98      0.99      1864\n","          29       1.00      0.95      0.97       344\n","          30       0.94      0.87      0.90      1136\n","          31       0.57      0.63      0.60        19\n","          32       1.00      0.62      0.77         8\n","          33       0.71      0.92      0.80        86\n","          34       0.25      0.56      0.35        32\n","          35       0.98      0.97      0.98       474\n","          36       1.00      0.04      0.08       182\n","          37       0.89      0.93      0.91      1592\n","          38       0.98      0.96      0.97       404\n","          39       0.98      0.94      0.96       485\n","          40       0.93      0.88      0.90       573\n","          41       0.94      0.93      0.93       841\n","          42       0.98      0.99      0.99       575\n","          43       0.96      0.93      0.95       152\n","          44       0.90      0.92      0.91        75\n","          46       1.00      0.98      0.99        82\n","          48       0.67      0.05      0.09        79\n","\n","    accuracy                           0.91     28417\n","   macro avg       0.81      0.83      0.79     28417\n","weighted avg       0.92      0.91      0.90     28417\n","\n","\n","\n","Domain newsgroups\n","The number of samples: 1196\n","The number of tags 49\n","The number of samples: 1195\n","The number of tags 49"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","after filter tag 1190\n","after filter tag 1180\n","acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.88      0.99      0.93        97\n","           2       0.66      0.94      0.77       171\n","           3       1.00      0.87      0.93       994\n","           4       0.94      0.81      0.87        96\n","           5       0.93      0.82      0.87       101\n","           6       1.00      0.99      0.99       927\n","           7       0.70      0.94      0.81       316\n","           8       0.00      0.00      0.00         2\n","           9       0.99      0.99      0.99       585\n","          10       0.98      0.77      0.86       760\n","          11       0.99      0.99      0.99      1917\n","          12       1.00      1.00      1.00        26\n","          13       1.00      0.88      0.93         8\n","          14       0.75      0.98      0.85       128\n","          15       0.97      0.98      0.98      2551\n","          16       0.91      0.88      0.89      1242\n","          17       0.87      0.90      0.88        51\n","          18       0.94      0.97      0.95        30\n","          19       0.04      1.00      0.07         5\n","          20       0.99      0.99      0.99       238\n","          21       0.82      0.23      0.36        61\n","          22       0.92      0.90      0.91      3175\n","          23       0.82      0.92      0.87      1823\n","          24       0.51      0.83      0.63        89\n","          25       0.94      0.96      0.95      1140\n","          26       0.88      1.00      0.93        14\n","          27       0.97      0.98      0.98       120\n","          28       0.98      0.99      0.98       735\n","          29       1.00      0.95      0.98       273\n","          30       0.96      0.86      0.91       782\n","          31       0.75      0.75      0.75        20\n","          32       0.93      0.87      0.90        15\n","          33       0.63      0.84      0.72        51\n","          34       0.12      0.25      0.16         8\n","          35       0.97      0.96      0.97       310\n","          36       1.00      0.06      0.11        34\n","          37       0.95      0.93      0.94       784\n","          38       0.98      0.96      0.97       526\n","          39       0.96      0.95      0.96       345\n","          40       0.89      0.93      0.91       448\n","          41       0.95      0.95      0.95       404\n","          42       0.99      0.98      0.99       418\n","          43       0.89      0.98      0.93       122\n","          44       0.98      0.97      0.97       120\n","          45       1.00      1.00      1.00         6\n","          46       1.00      0.85      0.92        87\n","          48       1.00      0.28      0.43       173\n","\n","    accuracy                           0.93     22328\n","   macro avg       0.86      0.85      0.82     22328\n","weighted avg       0.94      0.93      0.93     22328\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        16\n","           2       0.49      0.83      0.61       137\n","           3       1.00      0.90      0.94       993\n","           4       0.77      0.59      0.67       113\n","           5       0.97      0.60      0.74       112\n","           6       0.99      0.99      0.99       906\n","           7       0.38      0.85      0.52        89\n","           8       0.00      0.00      0.00         2\n","           9       1.00      0.97      0.99       636\n","          10       0.95      0.98      0.97       413\n","          11       0.98      1.00      0.99      1804\n","          12       0.96      1.00      0.98        25\n","          13       0.75      0.12      0.21        25\n","          14       0.64      0.98      0.77        87\n","          15       0.97      0.99      0.98      2256\n","          16       0.90      0.92      0.91      1285\n","          17       0.85      0.92      0.88        50\n","          18       0.86      1.00      0.92        30\n","          19       0.64      1.00      0.78         7\n","          20       1.00      1.00      1.00       325\n","          21       0.35      0.07      0.12        99\n","          22       0.92      0.92      0.92      2755\n","          23       0.85      0.91      0.88      1608\n","          24       0.53      0.88      0.66        34\n","          25       0.95      0.98      0.97       899\n","          26       1.00      0.90      0.95        21\n","          27       0.98      0.99      0.98        82\n","          28       0.99      0.99      0.99       779\n","          29       1.00      0.97      0.98       325\n","          30       0.96      0.89      0.93       830\n","          31       0.86      0.82      0.84        44\n","          32       0.94      0.85      0.89        20\n","          33       0.75      0.89      0.82        47\n","          34       0.48      0.83      0.61        12\n","          35       0.97      0.97      0.97       299\n","          36       1.00      0.12      0.21        50\n","          37       0.97      0.92      0.94       855\n","          38       0.97      0.97      0.97       362\n","          39       0.97      0.96      0.96       360\n","          40       0.94      0.90      0.92       454\n","          41       0.94      0.96      0.95       436\n","          42       0.99      0.97      0.98       533\n","          43       0.92      0.95      0.93        94\n","          44       0.93      1.00      0.96        78\n","          45       1.00      0.50      0.67         2\n","          46       0.98      0.98      0.98        60\n","          48       1.00      0.32      0.48       139\n","\n","    accuracy                           0.93     20588\n","   macro avg       0.86      0.83      0.82     20588\n","weighted avg       0.94      0.93      0.93     20588\n","\n","\n","\n","Domain reviews\n","The number of samples: 1907\n","The number of tags 47\n","The number of samples: 1906\n","The number of tags 50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["after filter tag 1905\n","after filter tag 1906\n","acc=0.94\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.93      0.96      0.95        27\n","           2       0.56      0.82      0.67        44\n","           3       1.00      0.88      0.94       902\n","           4       1.00      1.00      1.00        59\n","           5       0.97      1.00      0.98        60\n","           6       0.99      0.98      0.99      1700\n","           7       0.19      0.72      0.30        36\n","           8       0.00      0.00      0.00         5\n","           9       0.99      0.99      0.99      1096\n","          10       0.94      0.99      0.97       311\n","          11       0.98      0.99      0.99      2181\n","          12       0.91      0.96      0.93        45\n","          13       0.67      0.33      0.44         6\n","          14       0.55      0.82      0.66        61\n","          15       0.96      0.98      0.97      2308\n","          16       0.92      0.89      0.91      2013\n","          17       0.85      0.86      0.86        72\n","          18       0.89      0.90      0.89       105\n","          19       0.00      0.00      0.00         0\n","          20       0.99      0.99      0.99       353\n","          21       1.00      0.25      0.40        24\n","          22       0.92      0.93      0.93      3748\n","          23       0.73      0.86      0.79       928\n","          24       0.57      0.59      0.58        41\n","          25       0.92      0.97      0.94       934\n","          26       0.91      0.78      0.84        40\n","          27       0.96      0.85      0.90        55\n","          28       0.99      0.99      0.99      2151\n","          29       1.00      0.95      0.98       547\n","          30       0.94      0.89      0.92      2020\n","          31       0.67      0.56      0.61        32\n","          32       0.95      0.67      0.78        30\n","          33       0.65      0.87      0.75       103\n","          34       0.41      0.48      0.44        23\n","          35       0.99      0.97      0.98       429\n","          36       0.82      0.15      0.26        59\n","          37       0.95      0.91      0.93      1121\n","          38       0.98      0.95      0.96      1276\n","          39       0.94      0.94      0.94       351\n","          40       0.85      0.92      0.89       498\n","          41       0.92      0.92      0.92       779\n","          42       0.99      0.98      0.99       567\n","          43       0.95      0.89      0.92       100\n","          44       0.90      0.99      0.94        82\n","          46       0.97      0.98      0.98       118\n","          48       0.50      0.02      0.04        48\n","\n","    accuracy                           0.94     27488\n","   macro avg       0.82      0.79      0.78     27488\n","weighted avg       0.94      0.94      0.94     27488\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["acc=0.93\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.83      0.97      0.89        39\n","           2       0.54      0.94      0.69        49\n","           3       1.00      0.86      0.92       961\n","           4       0.99      0.99      0.99        84\n","           5       0.98      0.99      0.98        93\n","           6       1.00      0.98      0.99      1717\n","           7       0.14      0.91      0.25        23\n","           8       0.00      0.00      0.00         4\n","           9       1.00      0.98      0.99      1130\n","          10       0.92      0.99      0.96       330\n","          11       0.97      0.99      0.98      2258\n","          12       0.91      0.89      0.90        47\n","          13       1.00      0.71      0.83         7\n","          14       0.56      0.92      0.70        49\n","          15       0.95      0.96      0.96      2440\n","          16       0.92      0.87      0.90      2019\n","          17       0.86      0.83      0.85        66\n","          18       0.85      0.90      0.88       110\n","          19       0.41      0.50      0.45        14\n","          20       0.99      0.95      0.97       357\n","          21       1.00      0.06      0.11        17\n","          22       0.91      0.92      0.92      3741\n","          23       0.60      0.82      0.69       862\n","          24       0.62      0.56      0.59        32\n","          25       0.87      0.96      0.92      1031\n","          26       0.85      0.88      0.87        26\n","          27       0.95      0.74      0.83        54\n","          28       0.99      0.97      0.98      2148\n","          29       1.00      0.93      0.97       521\n","          30       0.94      0.84      0.88      2017\n","          31       0.83      0.66      0.73        44\n","          32       1.00      0.42      0.59        26\n","          33       0.69      0.87      0.77       130\n","          34       0.32      0.56      0.41        18\n","          35       0.99      0.98      0.99       470\n","          36       1.00      0.17      0.29        77\n","          37       0.95      0.91      0.93      1188\n","          38       0.96      0.96      0.96      1261\n","          39       0.97      0.92      0.95       392\n","          40       0.84      0.93      0.88       498\n","          41       0.91      0.90      0.91       781\n","          42       0.97      0.97      0.97       594\n","          43       0.94      0.94      0.94       102\n","          44       0.97      0.95      0.96        88\n","          45       1.00      1.00      1.00         1\n","          46       0.98      0.99      0.99       103\n","          47       0.00      0.00      0.00         1\n","          48       1.00      0.04      0.08        49\n","\n","    accuracy                           0.93     28069\n","   macro avg       0.83      0.79      0.77     28069\n","weighted avg       0.93      0.93      0.93     28069\n","\n","\n","\n","Domain weblogs\n","The number of samples: 1016\n","The number of tags 47\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["The number of samples: 1015\n","The number of tags 49\n","after filter tag 1016\n","after filter tag 974\n","acc=0.96\n","classification_report               precision    recall  f1-score   support\n","\n","           1       0.43      1.00      0.60         6\n","           2       0.59      0.91      0.71       161\n","           3       1.00      0.95      0.98      1126\n","           4       0.97      0.95      0.96        63\n","           5       0.92      0.95      0.94        63\n","           6       1.00      1.00      1.00       952\n","           7       0.42      0.90      0.57        29\n","           8       0.00      0.00      0.00         8\n","           9       1.00      0.97      0.99       784\n","          10       0.95      0.96      0.96       388\n","          11       0.98      1.00      0.99      2261\n","          12       0.92      1.00      0.96        55\n","          13       0.50      0.18      0.27        11\n","          14       0.91      0.98      0.94       169\n","          15       0.98      0.99      0.99      2970\n","          16       0.93      0.91      0.92      1685\n","          17       0.85      0.98      0.91        45\n","          18       0.76      1.00      0.87        26\n","          19       0.00      0.00      0.00         4\n","          20       1.00      1.00      1.00       216\n","          21       0.75      0.23      0.35        13\n","          22       0.96      0.95      0.95      2766\n","          23       0.92      0.97      0.94      2540\n","          24       0.77      0.78      0.77       138\n","          25       0.95      0.98      0.96      1221\n","          26       0.92      1.00      0.96        22\n","          27       0.97      0.99      0.98       124\n","          28       0.98      0.99      0.99       644\n","          29       1.00      0.97      0.98       218\n","          30       0.97      0.92      0.94      1017\n","          31       0.83      0.74      0.78        27\n","          32       1.00      0.67      0.80        21\n","          33       0.72      0.89      0.80        55\n","          34       0.82      1.00      0.90         9\n","          35       0.98      0.98      0.98       322\n","          36       1.00      0.17      0.29         6\n","          37       0.98      0.95      0.96       691\n","          38       0.99      0.98      0.98       633\n","          39       0.98      0.97      0.97       420\n","          40       0.93      0.95      0.94       630\n","          41       0.96      0.98      0.97       441\n","          42       0.99      0.99      0.99       584\n","          43       0.91      0.98      0.95       123\n","          44       0.99      0.99      0.99       107\n","          45       1.00      1.00      1.00         2\n","          46       0.98      0.90      0.94        67\n","          48       1.00      0.07      0.13       162\n","\n","    accuracy                           0.96     24025\n","   macro avg       0.86      0.84      0.82     24025\n","weighted avg       0.96      0.96      0.95     24025\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]},{"output_type":"stream","name":"stdout","text":["acc=0.95\n","classification_report               precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00         2\n","           2       0.58      0.89      0.70       138\n","           3       1.00      0.88      0.94       938\n","           4       0.95      0.94      0.94        77\n","           5       0.97      0.94      0.96        80\n","           6       0.99      0.98      0.99       887\n","           7       0.41      0.94      0.57        89\n","           8       0.00      0.00      0.00         2\n","           9       0.99      0.98      0.99       597\n","          10       0.97      1.00      0.98       259\n","          11       0.99      1.00      0.99      1860\n","          12       0.94      1.00      0.97        31\n","          13       0.80      0.44      0.57         9\n","          14       0.90      0.99      0.94       169\n","          15       0.98      0.99      0.98      2324\n","          16       0.92      0.90      0.91      1296\n","          17       0.80      0.90      0.85        31\n","          18       0.81      1.00      0.90        30\n","          19       0.43      1.00      0.60         3\n","          20       0.99      0.98      0.98       226\n","          21       0.67      0.13      0.22        15\n","          22       0.93      0.96      0.95      2371\n","          23       0.92      0.94      0.93      1908\n","          24       0.75      0.81      0.78       106\n","          25       0.95      0.97      0.96       895\n","          26       0.87      0.93      0.90        14\n","          27       0.95      1.00      0.97       122\n","          28       0.99      0.99      0.99       733\n","          29       1.00      0.96      0.98       220\n","          30       0.95      0.88      0.92       983\n","          31       0.81      0.71      0.76        31\n","          32       1.00      0.74      0.85        31\n","          33       0.67      0.93      0.77        80\n","          34       0.90      1.00      0.95         9\n","          35       0.99      0.98      0.98       271\n","          36       1.00      0.17      0.29        35\n","          37       0.98      0.95      0.96       661\n","          38       0.99      0.99      0.99       610\n","          39       0.96      0.94      0.95       363\n","          40       0.95      0.93      0.94       471\n","          41       0.95      0.96      0.95       380\n","          42       0.99      0.99      0.99       513\n","          43       0.93      0.95      0.94       101\n","          44       0.96      0.99      0.97        96\n","          45       1.00      0.50      0.67         4\n","          46       1.00      0.99      0.99        91\n","          48       0.96      0.16      0.28       147\n","\n","    accuracy                           0.95     20309\n","   macro avg       0.88      0.86      0.84     20309\n","weighted avg       0.95      0.95      0.95     20309\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n","\n","Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"BbYmR48fKfCz","executionInfo":{"status":"ok","timestamp":1668009442116,"user_tz":300,"elapsed":13,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["dev_precision_value_lst"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH2_uvbsBIpe","executionInfo":{"status":"ok","timestamp":1668009442116,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"9a780995-ff78-4292-b09b-1d1373e367e8"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.8222879010077336,\n"," 0.840899106124663,\n"," 0.8581409423386496,\n"," 0.8194049426170883,\n"," 0.8585879743872378]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# dev_metric = pd.DataFrame({\n","#     \"domain\": file_name_lst * 3,\n","#     \"metric\": [\"precision\"]*5 + [\"recall\"]*5 + [\"f1\"]*5,\n","#     \"value\": dev_precision_value_lst + dev_recall_value_lst + dev_f1_value_lst,\n","# })\n","\n","test_metric = pd.DataFrame({\n","    \"domain\": ([\"wsj_test\"] + file_name_lst) * 3,\n","    \"metric\": [\"precision\"]*6 + [\"recall\"]*6 + [\"f1\"]*6,\n","    \"value\": [wsj_precision_value] + test_precision_value_lst + [wsj_recall_value] + test_recall_value_lst + [wsj_f1_value] + test_f1_value_lst,\n","})"],"metadata":{"id":"a3labrZjKdqC","executionInfo":{"status":"ok","timestamp":1668009442117,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go"],"metadata":{"id":"dPKsw1gY6NeC","executionInfo":{"status":"ok","timestamp":1668009442117,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# fig = px.line(dev_metric, x=\"domain\", y=\"value\", color='metric', markers=True)\n","# fig.show()"],"metadata":{"id":"HpKyTf7p6NWM","executionInfo":{"status":"ok","timestamp":1668009442117,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["fig = px.line(test_metric, x=\"domain\", y=\"value\", color='metric', markers=True)\n","fig.show()"],"metadata":{"id":"l9sQ8lGr7p4a","colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"status":"ok","timestamp":1668009469672,"user_tz":300,"elapsed":156,"user":{"displayName":"Alex Y","userId":"02188660656026482944"}},"outputId":"93e9b31a-a21e-4740-8ce4-07bbaff52820"},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"d37a4ccc-89ac-45bc-b803-65de02282226\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d37a4ccc-89ac-45bc-b803-65de02282226\")) {                    Plotly.newPlot(                        \"d37a4ccc-89ac-45bc-b803-65de02282226\",                        [{\"hovertemplate\":\"metric=precision<br>domain=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"precision\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"precision\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"wsj_test\",\"answers\",\"emails\",\"newsgroups\",\"reviews\",\"weblogs\"],\"xaxis\":\"x\",\"y\":[0.9397482533696433,0.8075327471086554,0.8149770917992446,0.8559982172016384,0.8310378568131189,0.8814285552576873],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=recall<br>domain=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"recall\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"recall\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"wsj_test\",\"answers\",\"emails\",\"newsgroups\",\"reviews\",\"weblogs\"],\"xaxis\":\"x\",\"y\":[0.9371770472723233,0.8034837579643055,0.8263669527156633,0.8312875198371481,0.7920887748209792,0.8556524615808957],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"metric=f1<br>domain=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"f1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"f1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"wsj_test\",\"answers\",\"emails\",\"newsgroups\",\"reviews\",\"weblogs\"],\"xaxis\":\"x\",\"y\":[0.9273419086238314,0.7706166853043731,0.7903634316049043,0.8155874078330301,0.7744221497192484,0.8427362574888253],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"domain\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"metric\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('d37a4ccc-89ac-45bc-b803-65de02282226');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"EvkSQ18M7p2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0moH7FPjB0Sb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2DEMIx2r3xf2"},"execution_count":null,"outputs":[]}]}