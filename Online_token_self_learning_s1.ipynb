{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"F_LX9XAD32So"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3soh1b03deD","executionInfo":{"status":"ok","timestamp":1673622011320,"user_tz":300,"elapsed":35905,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"}},"outputId":"4bb08ed1-087c-4f77-b40e-203da9d79f16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n","Collecting boto3\n","  Downloading boto3-1.26.49-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n","Collecting botocore<1.30.0,>=1.29.49\n","  Downloading botocore-1.29.49-py3-none-any.whl (10.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.49->boto3->pytorch_pretrained_bert) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.26.49 botocore-1.29.49 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.0 urllib3-1.26.14\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.2.1\n"]}],"source":["! pip install pytorch_pretrained_bert\n","! pip install torchmetrics\n","! pip install -U kaleido"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Capstone')\n","from utils import read_conll_file, read_data, filter_tag, create_sub_dir, read_unlabeled_data\n","from utils import TAG2IDX, IDX2TAG, DATA_DIR, POS_FINE_DIR, UNLABELED_DIR\n","from utils import MODEL_DIR, INT_RESULT_DIR, METRICS_DIR, RESULT_DIR, PLOT_TAGS_DIR\n","# from utils import wsj_train_word_lst, wsj_train_tag_lst, wsj_test_word_lst, wsj_test_tag_lst\n","\n","from build_model import PosDataset, Net, DEVICE, TOKENIZER\n","from build_model import pad, train_one_epoch, eval\n","\n","from analysis import save_sns_fig, analysis_output, make_plot_metric\n","\n","from create_pseudo_data_by_tokens import gen_pseudo_data_for_token_by_unlabel\n","\n","import os\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n","from tqdm import tqdm_notebook as tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer, BertModel\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_precision, multiclass_recall, multiclass_accuracy\n","\n","torch.manual_seed(0)\n","\n","import time\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIFha6OOht8L","executionInfo":{"status":"ok","timestamp":1673622056322,"user_tz":300,"elapsed":45013,"user":{"displayName":"Wenxin Zhang","userId":"10737419063992196839"}},"outputId":"bb9a606d-e874-471b-b17d-dfbcce8b49eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 213450/213450 [00:00<00:00, 321364.48B/s]\n"]},{"output_type":"stream","name":"stdout","text":["The number of samples: 30060\n","The number of tags 48\n","The number of samples: 1336\n","The number of tags 45\n","The number of samples: 1640\n","The number of tags 45\n"]}]},{"cell_type":"code","source":["def run_online_token(domain, token_top_percent, lr=0.000001):\n","\n","  print(\"=========================================================\")\n","  print(\"Create directories\")\n","  (sub_model_dir, sub_metrics_dir, sub_result_dir, \n","    sub_int_res_dir) = create_sub_dir(domain, method_name=\"Online_token_self_learning\")\n","\n","  print(\"=========================================================\")\n","  print(\"Load data\")\n","  time1 = time.time()\n","\n","  ul_domain_file = os.path.join(UNLABELED_DIR, f\"gweb-{domain}.unlabeled.txt\")\n","\n","  domain_dir = os.path.join(POS_FINE_DIR, f\"{domain}\")\n","  domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","  domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")\n","\n","  domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","  domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","\n","  domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","  domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","  test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","  dev_iter = data.DataLoader(\n","      dataset=dev_dataset,\n","      batch_size=8,\n","      shuffle=True,\n","      num_workers=1,\n","      collate_fn=pad)\n","  test_iter = data.DataLoader(\n","      dataset=test_dataset,\n","      batch_size=8,\n","      shuffle=False,\n","      num_workers=1,\n","      collate_fn=pad)\n","\n","  time2 = time.time()\n","  print(\" Running time:\", time2 - time1)\n","\n","  # =========================================================\n","  avg_domain_prec_lst = []\n","  avg_domain_rec_lst = []\n","  avg_domain_f1_lst = []\n","  avg_domain_acc_lst = []\n","\n","  micro_domain_prec_lst = []\n","  micro_domain_rec_lst = []\n","  micro_domain_f1_lst = []\n","  micro_domain_acc_lst = []\n","\n","  macro_domain_prec_lst = []\n","  macro_domain_rec_lst = []\n","  macro_domain_f1_lst = []\n","  macro_domain_acc_lst = []\n","\n","  print(\"=========================================================\")\n","  print(\"Start Self-training\")\n","\n","  loop_i = 0\n","\n","  domain_unlabeled_data = read_unlabeled_data(ul_domain_file, max_unlabeled=100_000)\n","\n","  time3 = time.time()\n","  print(\"  Running time:\", time3 - time2)\n","\n","  print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","  cnt_non_ignored_tokens = sum([len(s) for s in domain_unlabeled_data])\n","  token_topn = int(token_top_percent * cnt_non_ignored_tokens)\n","  print(\"  The number of non_ignored_tokens\", cnt_non_ignored_tokens)\n","  print(\"  The number of token_topn\", token_topn)\n","\n","  token_prob_lst = []\n","  while cnt_non_ignored_tokens >= token_topn:\n","    \n","    time4 = time.time()\n","\n","    loop_i += 1\n","    print(\"\\nLoop\", loop_i)\n","\n","    # =========================================================\n","    # Load model\n","    if loop_i == 1:\n","      model_name = [name for name in os.listdir(MODEL_DIR) if \"base_model_\" in name][0]\n","      model_file = os.path.join(MODEL_DIR, model_name)\n","    else:\n","      model_name = [name for name in os.listdir(sub_model_dir) if f\"model-top{token_top_percent}-loop{loop_i-1}-lr{lr}\" in name][0]\n","      model_file = os.path.join(sub_model_dir, model_name)\n","    \n","    print(model_file)\n","\n","    model = Net(vocab_size=len(TAG2IDX))\n","    model.to(DEVICE)\n","    model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(model_file))\n","\n","    # =========================================================\n","    # Performance on test dataset\n","\n","    output_res_file = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}-lr{lr}.txt\")\n","    \n","    (test_prec_avg, test_rec_avg, test_f1_avg, acc_avg, \n","      test_prec_micro, test_rec_micro, test_f1_micro, test_acc_micro, \n","      test_prec_macro, test_rec_macro, test_f1_macro, test_acc_macro) = eval(\n","        model, test_iter, save_output=True, output_file=output_res_file)\n","    \n","    time5 = time.time()\n","    print(\" Running time:\", time5 - time4)\n","\n","    avg_domain_prec_lst.append(test_prec_avg.item())\n","    avg_domain_rec_lst.append(test_rec_avg.item())\n","    avg_domain_f1_lst.append(test_f1_avg.item())\n","    avg_domain_acc_lst.append(acc_avg.item())\n","\n","    micro_domain_prec_lst.append(test_prec_micro.item())\n","    micro_domain_rec_lst.append(test_rec_micro.item())\n","    micro_domain_f1_lst.append(test_f1_micro.item())\n","    micro_domain_acc_lst.append(test_acc_micro.item())\n","\n","    macro_domain_prec_lst.append(test_prec_macro.item())\n","    macro_domain_rec_lst.append(test_rec_macro.item())\n","    macro_domain_f1_lst.append(test_f1_macro.item())\n","    macro_domain_acc_lst.append(test_acc_macro.item())\n","\n","    csv_file_name = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}-lr{lr}.csv\")\n","    output_plot_name = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}-lr{lr}.png\")\n","\n","    _ = analysis_output(\n","        output_res_file, csvsave=True, pngsave=True, \n","        csv_file_name=csv_file_name, output_plot_name=output_plot_name, \n","        figtitle=f\"{domain}-top{token_top_percent}-loop{loop_i} Test: Accuracy for each tag\")\n","\n","\n","    print(\"=========================================================\")\n","    print(\"Generate new train dataset\")\n","\n","    print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","    if loop_i ==1:\n","      fake_tags = None\n","\n","    ul_domain_dataset = PosDataset(domain_unlabeled_data, fake_tags)\n","    unlabel_iter = data.DataLoader(\n","        dataset=ul_domain_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad\n","        )\n","\n","    # Save analysis outputs for intermediate results\n","    output_int_res_file = os.path.join(sub_int_res_dir, f\"top{token_top_percent}-loop{loop_i}-lr{lr}.txt\")\n","\n","    time6 = time.time()\n","    print(\" Running time:\", time6 - time5)\n","\n","    (top_words, top_tags, top_pseudo_tags, top_prob_pseudo_tags, top_prob, \n","    _, _, _, _, _)= gen_pseudo_data_for_token_by_unlabel(\n","          model, unlabel_iter, len(domain_unlabeled_data), save_output=True, output_file=output_int_res_file)\n","    \n","    print(\"  Total sentences\", len(top_prob_pseudo_tags))\n","    assert len(top_words)==len(top_pseudo_tags)==len(top_prob_pseudo_tags)\n","\n","    cnt_all_words = sum([len(s) for s in top_words])\n","    cnt_all_tags = sum([len(s) for s in top_tags])\n","    cnt_all_pseudo_tags = sum([len(s) for s in top_pseudo_tags])\n","    cnt_all_probs = sum([len(s) for s in top_prob_pseudo_tags])\n","    print(\"  Total tokens\", cnt_all_probs)\n","    assert cnt_all_words==cnt_all_tags==cnt_all_pseudo_tags==cnt_all_probs\n","\n","    time7 = time.time()\n","    print(\" Running time:\", time7 - time6)\n","\n","    print(\"=========================================================\")\n","    print(\"Select tokens by probability\")\n","    \n","    non_ignored_tokens = []\n","    for tokens_i in top_tags:\n","      for token_i in tokens_i:\n","        if token_i != '<pad>':\n","          non_ignored_tokens.append(token_i)\n","    cnt_non_ignored_tokens = len(non_ignored_tokens)\n","    print(\"  Total non_ignored tokens\", cnt_non_ignored_tokens)\n","\n","    if cnt_non_ignored_tokens <= 100:\n","      print(\"  The number of non-ignored tokens is too few. Stop training\")\n","      break\n","\n","    print(\"  Calculate threshold probability\") \n","    all_pseudo_tags_prob_lst = []\n","    for top_tags_i, top_prob_pseudo_tags_i in zip(top_tags, top_prob_pseudo_tags):\n","      for tags_i, prob_pseudo_tags_i in zip(top_tags_i, top_prob_pseudo_tags_i):\n","        if tags_i != '<pad>':\n","          all_pseudo_tags_prob_lst.append(prob_pseudo_tags_i)\n","    all_pseudo_tags_prob_lst.sort(reverse=True)\n","\n","    if token_topn >= len(all_pseudo_tags_prob_lst):\n","      threshold_prob = all_pseudo_tags_prob_lst[-1]\n","      token_prob_lst.append(all_pseudo_tags_prob_lst)\n","    else:\n","      threshold_prob = all_pseudo_tags_prob_lst[token_topn]\n","      token_prob_lst.append(\n","          all_pseudo_tags_prob_lst[: token_topn+1]\n","          )\n","    print(\"  Threshold probability\", threshold_prob)\n","\n","    print(\"  Filter tags by probability\")\n","\n","    new_training_tags = []\n","    fake_tags = []\n","    for t, p in zip(top_pseudo_tags, top_prob_pseudo_tags):\n","      t_lst = []\n","      remain_t_lst = []\n","      for t_i, p_i in zip(t, p):\n","        if t_i == '<pad>':\n","          t_lst.append('<pad>')\n","          remain_t_lst.append('<pad>')\n","        else:\n","          if p_i >= threshold_prob:\n","            t_lst.append(t_i)\n","            remain_t_lst.append('<pad>')\n","          else:\n","            t_lst.append('<pad>')\n","            remain_t_lst.append(t_i)\n","\n","      new_training_tags.append(t_lst)\n","      fake_tags.append(remain_t_lst)\n","\n","    assert len(top_words)==len(new_training_tags)==len(fake_tags)\n","\n","    time8 = time.time()\n","    print(\" Running time:\", time8 - time7)\n","\n","    print(\"=========================================================\")\n","    print(\"Generate new train dataset\")\n","\n","    domain_unlabeled_data = top_words\n","    new_train_dataset = PosDataset(domain_unlabeled_data, new_training_tags)\n","    new_train_iter = data.DataLoader(\n","        dataset=new_train_dataset,\n","        batch_size=8,\n","        shuffle=True,\n","        num_workers=1,\n","        collate_fn=pad)\n","\n","    print(\"=========================================================\")\n","    print(\"Self training for epochs\")\n","    \n","    epoch_number = 0\n","    EPOCHS = 3\n","\n","    best_vloss = 1_000_000.\n","\n","    optimizer = optim.Adam(model.parameters(), lr = lr)\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    for epoch in range(EPOCHS):\n","        print('  EPOCH {}:'.format(epoch_number + 1))\n","\n","        model.train(True)\n","        avg_loss = train_one_epoch(model, new_train_iter, optimizer, loss_fn, epoch_number)\n","\n","        model.train(False)\n","\n","        running_vloss = 0.0\n","        for i, vbatch in enumerate(dev_iter):\n","          words, x, is_heads, tags, y, seqlens = vbatch\n","\n","          logits, y, _ = model(x, y)\n","          logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","          y = y.view(-1)  # (N*T,)\n","          \n","          vloss = loss_fn(logits, y)\n","          running_vloss += vloss\n","\n","        avg_vloss = running_vloss / (i + 1)\n","        print('  LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = os.path.join(sub_model_dir, f'model-top{token_top_percent}-loop{loop_i}-lr{lr}')\n","            torch.save(model.state_dict(), model_path)\n","\n","        epoch_number += 1\n","    \n","  time9 = time.time()\n","  print(\" Running time:\", time9 - time8)\n","\n","  print(\"=========================================================\")\n","  print(\"Save metrics and probability list\")\n","\n","  metrics_df = pd.DataFrame({\n","      \"avg_domain_prec_lst\": avg_domain_prec_lst,\n","      \"avg_domain_rec_lst\": avg_domain_rec_lst,\n","      \"avg_domain_f1_lst\": avg_domain_f1_lst,\n","      \"avg_domain_acc_lst\": avg_domain_acc_lst,\n","\n","      \"micro_domain_prec_lst\": micro_domain_prec_lst,\n","      \"micro_domain_rec_lst\": micro_domain_rec_lst,\n","      \"micro_domain_f1_lst\": micro_domain_f1_lst,\n","      \"micro_domain_acc_lst\": micro_domain_acc_lst,\n","\n","      \"macro_domain_prec_lst\": macro_domain_prec_lst,\n","      \"macro_domain_rec_lst\": macro_domain_rec_lst,\n","      \"macro_domain_f1_lst\": macro_domain_f1_lst,\n","      \"macro_domain_acc_lst\": macro_domain_acc_lst\n","\n","  })\n","\n","  metrics_df.to_csv(os.path.join(sub_metrics_dir, f\"metrics_df-top{token_top_percent}-lr{lr}.csv\"), index=False)\n","  make_plot_metric(metrics_df, sub_metrics_dir, name=f\"top{token_top_percent}-lr{lr}\")\n","\n","  prob_df = pd.DataFrame({})\n","  fix_len = len(token_prob_lst[0])\n","  for i in range(len(token_prob_lst)):\n","    prob_df[i+1] = token_prob_lst[i] + [None] * (fix_len - len(token_prob_lst[i]))\n","\n","  prob_df.to_csv(os.path.join(sub_metrics_dir, f\"prob_df-top{token_top_percent}-lr{lr}.csv\"), index=False)\n","\n","  time10 = time.time()\n","  print(\" Running time:\", time10 - time9)\n","\n"],"metadata":{"id":"0ZDK1-UU6I5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_top_percent_lst = [0.05, 0.1, 0.2] # 0.01, 0.02\n","lr_lst = [0.000001, 0.00001]\n","DOMAIN_LST = [\"answers\", \"emails\", \"newsgroups\", \"reviews\", \"weblogs\"]"],"metadata":{"id":"3H4y3NgsQCTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for token_top_percent in token_top_percent_lst[::-1]:\n","  for domain in DOMAIN_LST:\n","    for lr in lr_lst:\n","      print(f\"\\n$$$ Run {domain}, token_top_percent {token_top_percent}, lr {lr}\")\n","      # run_online_token(domain, top_percent, lr=lr)\n","\n","      sub_metrics_dir = os.path.join(METRICS_DIR, \"Online_token_self_learning\", domain)\n","      if os.path.exists(os.path.join(sub_metrics_dir, f\"metrics_df-top{token_top_percent}-lr{lr}.csv\")):\n","        print(\"Already run\")\n","      else:\n","        run_online_token(domain, token_top_percent, lr=lr)\n"],"metadata":{"id":"Tuo_C2vBQEXp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a467c16-e37e-4eba-9803-6107a0ae92e3"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","$$$ Run answers, token_top_percent 0.2, lr 1e-06\n","=========================================================\n","Create directories\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 2.99457049369812\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n","  Running time: 12.431578159332275\n","  The number of unlabeled data 27260\n","  The number of non_ignored_tokens 419790\n","  The number of token_topn 83958\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 404400730/404400730 [00:40<00:00, 10107081.65B/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Running time: 72.09998083114624\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 4.8411781787872314\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 116.6079113483429\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 419790\n","  Calculate threshold probability\n","  Threshold probability 0.9997041821479797\n","  Filter tags by probability\n"," Running time: 0.7445340156555176\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.00014909412298584357\n","  batch 1000 loss: 8.323495500371791e-05\n","  batch 1500 loss: 0.00011769830303092022\n","  batch 2000 loss: 4.911161337440717e-05\n","  batch 2500 loss: 4.1620492043875857e-05\n","  batch 3000 loss: 3.469905576275778e-05\n","  LOSS train 3.469905576275778e-05 valid 0.4126460552215576\n","  EPOCH 2:\n","  batch 500 loss: 2.691794295606087e-05\n","  batch 1000 loss: 2.3143121472458006e-05\n","  batch 1500 loss: 1.984399577486329e-05\n","  batch 2000 loss: 1.7004430106680955e-05\n","  batch 2500 loss: 1.7066879043341033e-05\n","  batch 3000 loss: 1.3137922230271214e-05\n","  LOSS train 1.3137922230271214e-05 valid 0.43072786927223206\n","  EPOCH 3:\n","  batch 500 loss: 1.0754684070889197e-05\n","  batch 1000 loss: 9.622575556932133e-06\n","  batch 1500 loss: 8.54475577125413e-06\n","  batch 2000 loss: 7.551167478595744e-06\n","  batch 2500 loss: 6.663788351033872e-06\n","  batch 3000 loss: 5.880843403247127e-06\n","  LOSS train 5.880843403247127e-06 valid 0.462392657995224\n","\n","Loop 2\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Online_token_self_learning/answers/model-top0.2-loop1-lr1e-06\n"," Running time: 13.807124614715576\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 2.419206142425537\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 118.16876339912415\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 335811\n","  Calculate threshold probability\n","  Threshold probability 0.9997945427894592\n","  Filter tags by probability\n"," Running time: 0.4306182861328125\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 4.3627666083921215e-05\n","  batch 1000 loss: 0.00015092225044645603\n","  batch 1500 loss: 2.8314601280726493e-05\n","  batch 2000 loss: 5.502332434298296e-05\n","  batch 2500 loss: 1.9350396405570792e-05\n","  batch 3000 loss: 4.1353287815582004e-05\n","  LOSS train 4.1353287815582004e-05 valid 0.44330862164497375\n","  EPOCH 2:\n","  batch 500 loss: 1.5192039305475192e-05\n","  batch 1000 loss: 1.4071078211600251e-05\n","  batch 1500 loss: 1.288896724508959e-05\n","  batch 2000 loss: 1.1980027652498393e-05\n","  batch 2500 loss: 1.0943178801426256e-05\n","  batch 3000 loss: 1.022964243657043e-05\n","  LOSS train 1.022964243657043e-05 valid 0.4575173258781433\n","  EPOCH 3:\n","  batch 500 loss: 8.243175978350336e-06\n","  batch 1000 loss: 7.722654615463398e-06\n","  batch 1500 loss: 7.119537003291043e-06\n","  batch 2000 loss: 6.236266518499178e-06\n","  batch 2500 loss: 5.579149053119181e-06\n","  batch 3000 loss: 5.04770480984007e-06\n","  LOSS train 5.04770480984007e-06 valid 0.49329522252082825\n","\n","Loop 3\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Online_token_self_learning/answers/model-top0.2-loop2-lr1e-06\n"," Running time: 13.636536359786987\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.8255889415740967\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 118.1428894996643\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 251849\n","  Calculate threshold probability\n","  Threshold probability 0.9985394477844238\n","  Filter tags by probability\n"," Running time: 0.4085693359375\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.00015759709263511468\n","  batch 1000 loss: 0.00010193888700450771\n","  batch 1500 loss: 0.00012001757968391758\n","  batch 2000 loss: 0.00011048266086072544\n","  batch 2500 loss: 0.00012892115955401095\n","  batch 3000 loss: 9.055054265263606e-05\n","  LOSS train 9.055054265263606e-05 valid 0.4970865845680237\n","  EPOCH 2:\n","  batch 500 loss: 9.262821710217395e-05\n","  batch 1000 loss: 0.00020073144688285537\n","  batch 1500 loss: 6.937414917047136e-05\n","  batch 2000 loss: 0.00010494258718608762\n","  batch 2500 loss: 0.000229708858543745\n","  batch 3000 loss: 0.00011818625126761617\n","  LOSS train 0.00011818625126761617 valid 0.5210139751434326\n","  EPOCH 3:\n","  batch 500 loss: 0.0001416853705286485\n","  batch 1000 loss: 3.3394781028619036e-05\n","  batch 1500 loss: 0.00014838417443752404\n","  batch 2000 loss: 3.496993770022527e-05\n","  batch 2500 loss: 4.2837707116632375e-05\n","  batch 3000 loss: 0.00024379162810146226\n","  LOSS train 0.00024379162810146226 valid 0.5303047895431519\n","\n","Loop 4\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Online_token_self_learning/answers/model-top0.2-loop3-lr1e-06\n"," Running time: 14.188310623168945\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 1.8662981986999512\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 117.81299591064453\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 167890\n","  Calculate threshold probability\n","  Threshold probability 0.9907739758491516\n","  Filter tags by probability\n"," Running time: 0.35732269287109375\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.001337332732437062\n","  batch 1000 loss: 0.0010312304998515173\n","  batch 1500 loss: 0.0011442948378244183\n","  batch 2000 loss: 0.0009225421466544503\n","  batch 2500 loss: 0.0006242133290797938\n","  batch 3000 loss: 0.0007932623944216175\n","  LOSS train 0.0007932623944216175 valid 0.5617725253105164\n","  EPOCH 2:\n","  batch 500 loss: 0.0009032873483665753\n","  batch 1000 loss: 0.0007238823328953003\n","  batch 1500 loss: 0.0004582083750719903\n","  batch 2000 loss: 0.0007426242153305793\n","  batch 2500 loss: 0.0005820267634699121\n","  batch 3000 loss: 0.00047705632129509465\n","  LOSS train 0.00047705632129509465 valid 0.5717052817344666\n","  EPOCH 3:\n","  batch 500 loss: 0.0005940166118816705\n","  batch 1000 loss: 0.0006740547587323817\n","  batch 1500 loss: 0.0005160016079462366\n","  batch 2000 loss: 0.0007576852716956637\n","  batch 2500 loss: 0.000410909490303311\n","  batch 3000 loss: 0.0006573306003483595\n","  LOSS train 0.0006573306003483595 valid 0.5958850979804993\n","\n","Loop 5\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Online_token_self_learning/answers/model-top0.2-loop4-lr1e-06\n"," Running time: 12.851698160171509\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.6178829669952393\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 117.11507487297058\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 83936\n","  Calculate threshold probability\n","  Threshold probability 0.11144426465034485\n","  Filter tags by probability\n"," Running time: 0.28244829177856445\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 0.03756506197305862\n","  batch 1000 loss: 0.03501345693366602\n","  batch 1500 loss: 0.034390525524038824\n","  batch 2000 loss: 0.03156947170593776\n","  batch 2500 loss: 0.03291560588753782\n","  batch 3000 loss: 0.0315753404838033\n","  LOSS train 0.0315753404838033 valid 0.5669230818748474\n","  EPOCH 2:\n","  batch 500 loss: 0.03003549846902024\n","  batch 1000 loss: 0.02873860913515091\n","  batch 1500 loss: 0.029545098553644494\n","  batch 2000 loss: 0.029228808644693346\n","  batch 2500 loss: 0.027972962256055327\n","  batch 3000 loss: 0.028186649405397474\n","  LOSS train 0.028186649405397474 valid 0.5918976068496704\n","  EPOCH 3:\n","  batch 500 loss: 0.026570211106620265\n","  batch 1000 loss: 0.025504831173224374\n","  batch 1500 loss: 0.02498395155603066\n","  batch 2000 loss: 0.025111337908601854\n","  batch 2500 loss: 0.026555810509598814\n","  batch 3000 loss: 0.026516659661661834\n","  LOSS train 0.026516659661661834 valid 0.5991678833961487\n"," Running time: 1349.812416791916\n","=========================================================\n","Save metrics and probability list\n"," Running time: 4.888578414916992\n","\n","$$$ Run answers, token_top_percent 0.2, lr 1e-05\n","=========================================================\n","Create directories\n","=========================================================\n","Load data\n","The number of samples: 1745\n","The number of tags 49\n","The number of samples: 1744\n","The number of tags 50\n","after filter tag 1713\n","after filter tag 1723\n"," Running time: 0.16769862174987793\n","=========================================================\n","Start Self-training\n","Loaded... 27260 unlabeled instances\n","  Running time: 7.190104961395264\n","  The number of unlabeled data 27260\n","  The number of non_ignored_tokens 419790\n","  The number of token_topn 83958\n","\n","Loop 1\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/base_model_20230105_184923_0\n"," Running time: 14.577746868133545\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5257489681243896\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 118.25278782844543\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 419790\n","  Calculate threshold probability\n","  Threshold probability 0.9997045397758484\n","  Filter tags by probability\n"," Running time: 0.9388034343719482\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n","  batch 500 loss: 3.3942933652724606e-05\n","  batch 1000 loss: 0.0002746755093294269\n","  batch 1500 loss: 0.0001035035498898651\n","  batch 2000 loss: 5.217584307956713e-06\n","  batch 2500 loss: 3.042928366085107e-06\n","  batch 3000 loss: 0.0004087770367881376\n","  LOSS train 0.0004087770367881376 valid 0.503710150718689\n","  EPOCH 2:\n","  batch 500 loss: 2.5560035984426576e-06\n","  batch 1000 loss: 2.1359416290351873e-06\n","  batch 1500 loss: 1.2033181515107571e-06\n","  batch 2000 loss: 1.045254361656589e-06\n","  batch 2500 loss: 8.109566780376554e-07\n","  batch 3000 loss: 6.879085543971542e-07\n","  LOSS train 6.879085543971542e-07 valid 0.5428502559661865\n","  EPOCH 3:\n","  batch 500 loss: 4.674137657332267e-07\n","  batch 1000 loss: 3.924613058501336e-07\n","  batch 1500 loss: 3.659878908024439e-07\n","  batch 2000 loss: 2.7676808166887665e-07\n","  batch 2500 loss: 2.423817006231843e-07\n","  batch 3000 loss: 1.8134530306213037e-07\n","  LOSS train 1.8134530306213037e-07 valid 0.5840645432472229\n","\n","Loop 2\n","/content/drive/.shortcut-targets-by-id/14c0k_vTOqyJvw3jILduI00xS9-yQkgb8/Capstone/model/Online_token_self_learning/answers/model-top0.2-loop1-lr1e-05\n"," Running time: 13.096262693405151\n","=========================================================\n","Generate new train dataset\n","  The number of unlabeled data 27260\n"," Running time: 0.5804831981658936\n","  Total sentences 27260\n","  Total tokens 419790\n"," Running time: 117.703697681427\n","=========================================================\n","Select tokens by probability\n","  Total non_ignored tokens 335811\n","  Calculate threshold probability\n","  Threshold probability 0.9999909400939941\n","  Filter tags by probability\n"," Running time: 0.4188241958618164\n","=========================================================\n","Generate new train dataset\n","=========================================================\n","Self training for epochs\n","  EPOCH 1:\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P-tg-dx-ucte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4dLvQJPUucrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VT1iNxziucoh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# domain = \"answers\"\n","# token_top_percent = 0.2"],"metadata":{"id":"so2ClVX4ABW3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# print(\"=========================================================\")\n","# print(\"Create directories\")\n","# (sub_model_dir, sub_metrics_dir, sub_result_dir, \n","#   sub_int_res_dir) = create_sub_dir(domain, method_name=\"Online_token_self_learning\")\n","\n","# print(\"=========================================================\")\n","# print(\"Load data\")\n","# time1 = time.time()\n","\n","# ul_domain_file = os.path.join(UNLABELED_DIR, f\"gweb-{domain}.unlabeled.txt\")\n","\n","# domain_dir = os.path.join(POS_FINE_DIR, f\"{domain}\")\n","# domain_dev_file = os.path.join(domain_dir, f\"gweb-{domain}-dev.conll\")\n","# domain_test_file = os.path.join(domain_dir, f\"gweb-{domain}-test.conll\")\n","\n","# domain_dev_word_lst, domain_dev_tag_lst, domain_dev_tag_set = read_data(domain_dev_file)\n","# domain_test_word_lst, domain_test_tag_lst, domain_test_tag_set = read_data(domain_test_file)\n","\n","# domain_dev_word_lst, domain_dev_tag_lst = filter_tag(domain_dev_word_lst, domain_dev_tag_lst)  \n","# domain_test_word_lst, domain_test_tag_lst = filter_tag(domain_test_word_lst, domain_test_tag_lst)\n","\n","# dev_dataset = PosDataset(domain_dev_word_lst, domain_dev_tag_lst)\n","# test_dataset = PosDataset(domain_test_word_lst, domain_test_tag_lst)\n","\n","# dev_iter = data.DataLoader(\n","#     dataset=dev_dataset,\n","#     batch_size=8,\n","#     shuffle=True,\n","#     num_workers=1,\n","#     collate_fn=pad)\n","# test_iter = data.DataLoader(\n","#     dataset=test_dataset,\n","#     batch_size=8,\n","#     shuffle=False,\n","#     num_workers=1,\n","#     collate_fn=pad)\n","\n","# time2 = time.time()\n","# print(\" Running time:\", time2 - time1)\n","\n","# # =========================================================\n","# avg_domain_prec_lst = []\n","# avg_domain_rec_lst = []\n","# avg_domain_f1_lst = []\n","# avg_domain_acc_lst = []\n","\n","# micro_domain_prec_lst = []\n","# micro_domain_rec_lst = []\n","# micro_domain_f1_lst = []\n","# micro_domain_acc_lst = []\n","\n","# macro_domain_prec_lst = []\n","# macro_domain_rec_lst = []\n","# macro_domain_f1_lst = []\n","# macro_domain_acc_lst = []\n","\n","# print(\"=========================================================\")\n","# print(\"Start Self-training\")\n","\n","# loop_i = 0\n","\n","# domain_unlabeled_data = read_unlabeled_data(ul_domain_file, max_unlabeled=100_000)\n","\n","# time3 = time.time()\n","# print(\"  Running time:\", time3 - time2)\n","\n","# print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","# cnt_non_ignored_tokens = sum([len(s) for s in domain_unlabeled_data])\n","# token_topn = int(token_top_percent * cnt_non_ignored_tokens)\n","# print(\"  The number of non_ignored_tokens\", cnt_non_ignored_tokens)\n","# print(\"  The number of token_topn\", token_topn)\n","\n","# token_prob_lst = []\n","# while cnt_non_ignored_tokens >= token_topn:\n","  \n","#   time4 = time.time()\n","\n","#   loop_i += 1\n","#   print(\"\\nLoop\", loop_i)\n","\n","#   # =========================================================\n","#   # Load model\n","#   if loop_i == 1:\n","#     model_name = [name for name in os.listdir(MODEL_DIR) if \"base_model_\" in name][0]\n","#     model_file = os.path.join(MODEL_DIR, model_name)\n","#   else:\n","#     model_name = [name for name in os.listdir(sub_model_dir) if f\"model-top{token_top_percent}-loop{loop_i-1}\" in name][0]\n","#     model_file = os.path.join(sub_model_dir, model_name)\n","  \n","#   print(model_file)\n","\n","#   model = Net(vocab_size=len(TAG2IDX))\n","#   model.to(DEVICE)\n","#   model = nn.DataParallel(model)\n","#   model.load_state_dict(torch.load(model_file))\n","\n","#   # =========================================================\n","#   # Performance on test dataset\n","\n","#   output_res_file = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}.txt\")\n","  \n","#   (test_prec_avg, test_rec_avg, test_f1_avg, acc_avg, \n","#     test_prec_micro, test_rec_micro, test_f1_micro, test_acc_micro, \n","#     test_prec_macro, test_rec_macro, test_f1_macro, test_acc_macro) = eval(\n","#       model, test_iter, save_output=True, output_file=output_res_file)\n","  \n","#   time5 = time.time()\n","#   print(\" Running time:\", time5 - time4)\n","\n","#   avg_domain_prec_lst.append(test_prec_avg.item())\n","#   avg_domain_rec_lst.append(test_rec_avg.item())\n","#   avg_domain_f1_lst.append(test_f1_avg.item())\n","#   avg_domain_acc_lst.append(acc_avg.item())\n","\n","#   micro_domain_prec_lst.append(test_prec_micro.item())\n","#   micro_domain_rec_lst.append(test_rec_micro.item())\n","#   micro_domain_f1_lst.append(test_f1_micro.item())\n","#   micro_domain_acc_lst.append(test_acc_micro.item())\n","\n","#   macro_domain_prec_lst.append(test_prec_macro.item())\n","#   macro_domain_rec_lst.append(test_rec_macro.item())\n","#   macro_domain_f1_lst.append(test_f1_macro.item())\n","#   macro_domain_acc_lst.append(test_acc_macro.item())\n","\n","#   csv_file_name = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}.csv\")\n","#   output_plot_name = os.path.join(sub_result_dir, f\"top{token_top_percent}-loop{loop_i}.png\")\n","\n","#   _ = analysis_output(\n","#       output_res_file, csvsave=True, pngsave=True, \n","#       csv_file_name=csv_file_name, output_plot_name=output_plot_name, \n","#       figtitle=f\"{domain}-top{token_top_percent}-loop{loop_i} Test: Accuracy for each tag\")\n","\n","\n","#   print(\"=========================================================\")\n","#   print(\"Generate new train dataset\")\n","\n","#   print(\"  The number of unlabeled data\", len(domain_unlabeled_data))\n","\n","#   if loop_i ==1:\n","#     fake_tags = None\n","\n","#   ul_domain_dataset = PosDataset(domain_unlabeled_data, fake_tags)\n","#   unlabel_iter = data.DataLoader(\n","#       dataset=ul_domain_dataset,\n","#       batch_size=8,\n","#       shuffle=True,\n","#       num_workers=1,\n","#       collate_fn=pad\n","#       )\n","\n","#   # Save analysis outputs for intermediate results\n","#   output_int_res_file = os.path.join(sub_int_res_dir, f\"top{token_top_percent}-loop{loop_i}.txt\")\n","\n","#   time6 = time.time()\n","#   print(\" Running time:\", time6 - time5)\n","\n","#   (top_words, top_tags, top_pseudo_tags, top_prob_pseudo_tags, top_prob, \n","#   _, _, _, _, _)= gen_pseudo_data_for_token_by_unlabel(\n","#         model, unlabel_iter, len(domain_unlabeled_data), save_output=True, output_file=output_int_res_file)\n","  \n","#   print(\"  Total sentences\", len(top_prob_pseudo_tags))\n","#   assert len(top_words)==len(top_pseudo_tags)==len(top_prob_pseudo_tags)\n","\n","#   cnt_all_words = sum([len(s) for s in top_words])\n","#   cnt_all_tags = sum([len(s) for s in top_tags])\n","#   cnt_all_pseudo_tags = sum([len(s) for s in top_pseudo_tags])\n","#   cnt_all_probs = sum([len(s) for s in top_prob_pseudo_tags])\n","#   print(\"  Total tokens\", cnt_all_probs)\n","#   assert cnt_all_words==cnt_all_tags==cnt_all_pseudo_tags==cnt_all_probs\n","\n","#   time7 = time.time()\n","#   print(\" Running time:\", time7 - time6)\n","\n","#   print(\"=========================================================\")\n","#   print(\"Select tokens by probability\")\n","  \n","#   non_ignored_tokens = []\n","#   for tokens_i in top_tags:\n","#     for token_i in tokens_i:\n","#       if token_i != '<pad>':\n","#         non_ignored_tokens.append(token_i)\n","#   cnt_non_ignored_tokens = len(non_ignored_tokens)\n","#   print(\"  Total non_ignored tokens\", cnt_non_ignored_tokens)\n","\n","#   if cnt_non_ignored_tokens <= 100:\n","#     print(\"  The number of non-ignored tokens is too few. Stop training\")\n","#     break\n","\n","#   print(\"  Calculate threshold probability\") \n","#   all_pseudo_tags_prob_lst = []\n","#   for top_tags_i, top_prob_pseudo_tags_i in zip(top_tags, top_prob_pseudo_tags):\n","#     for tags_i, prob_pseudo_tags_i in zip(top_tags_i, top_prob_pseudo_tags_i):\n","#       if tags_i != '<pad>':\n","#         all_pseudo_tags_prob_lst.append(prob_pseudo_tags_i)\n","#   all_pseudo_tags_prob_lst.sort(reverse=True)\n","\n","#   if token_topn >= len(all_pseudo_tags_prob_lst):\n","#     threshold_prob = all_pseudo_tags_prob_lst[-1]\n","#     token_prob_lst.append(all_pseudo_tags_prob_lst)\n","#   else:\n","#     threshold_prob = all_pseudo_tags_prob_lst[token_topn]\n","#     token_prob_lst.append(\n","#         all_pseudo_tags_prob_lst[: token_topn+1]\n","#         )\n","#   print(\"  Threshold probability\", threshold_prob)\n","\n","#   print(\"  Filter tags by probability\")\n","\n","#   ckeck_tags = []\n","#   new_training_tags = []\n","#   fake_tags = []\n","#   for t, p in zip(top_pseudo_tags, top_prob_pseudo_tags):\n","#     t_lst = []\n","#     remain_t_lst = []\n","#     for t_i, p_i in zip(t, p):\n","#       if t_i == '<pad>':\n","#         t_lst.append('<pad>')\n","#         remain_t_lst.append('<pad>')\n","#       else:\n","#         if p_i >= threshold_prob:\n","#           t_lst.append(t_i)\n","#           remain_t_lst.append('<pad>')\n","#           ckeck_tags.append(t_i)\n","#         else:\n","#           t_lst.append('<pad>')\n","#           remain_t_lst.append(t_i)\n","\n","#     new_training_tags.append(t_lst)\n","#     fake_tags.append(remain_t_lst)\n","\n","#   assert len(top_words)==len(new_training_tags)==len(fake_tags)\n","\n","#   cnt = Counter(ckeck_tags)\n","#   print(cnt)\n","\n","#   time8 = time.time()\n","#   print(\" Running time:\", time8 - time7)\n","\n","#   print(\"=========================================================\")\n","#   print(\"Generate new train dataset\")\n","\n","#   domain_unlabeled_data = top_words\n","#   new_train_dataset = PosDataset(domain_unlabeled_data, new_training_tags)\n","#   new_train_iter = data.DataLoader(\n","#       dataset=new_train_dataset,\n","#       batch_size=8,\n","#       shuffle=True,\n","#       num_workers=1,\n","#       collate_fn=pad)\n","\n","#   print(\"=========================================================\")\n","#   print(\"Self training for epochs\")\n","  \n","#   epoch_number = 0\n","#   EPOCHS = 1\n","\n","#   best_vloss = 1_000_000.\n","\n","#   optimizer = optim.Adam(model.parameters(), lr = 0.00001)\n","#   loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n","\n","#   for epoch in range(EPOCHS):\n","#       print('  EPOCH {}:'.format(epoch_number + 1))\n","\n","#       model.train(True)\n","#       avg_loss = train_one_epoch(model, new_train_iter, optimizer, loss_fn, epoch_number)\n","\n","#       model.train(False)\n","\n","#       running_vloss = 0.0\n","#       for i, vbatch in enumerate(dev_iter):\n","#         words, x, is_heads, tags, y, seqlens = vbatch\n","\n","#         logits, y, _ = model(x, y)\n","#         logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n","#         y = y.view(-1)  # (N*T,)\n","        \n","#         vloss = loss_fn(logits, y)\n","#         running_vloss += vloss\n","\n","#       avg_vloss = running_vloss / (i + 1)\n","#       print('  LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","#       # Track best performance, and save the model's state\n","#       if avg_vloss < best_vloss:\n","#           best_vloss = avg_vloss\n","#           model_path = os.path.join(sub_model_dir, f'model-top{token_top_percent}-loop{loop_i}')\n","#           torch.save(model.state_dict(), model_path)\n","\n","#       epoch_number += 1\n","    \n","  \n","# time9 = time.time()\n","# print(\" Running time:\", time9 - time8)\n","\n","# print(\"=========================================================\")\n","# print(\"Save metrics and probability list\")\n","\n","# metrics_df = pd.DataFrame({\n","#     \"avg_domain_prec_lst\": avg_domain_prec_lst,\n","#     \"avg_domain_rec_lst\": avg_domain_rec_lst,\n","#     \"avg_domain_f1_lst\": avg_domain_f1_lst,\n","#     \"avg_domain_acc_lst\": avg_domain_acc_lst,\n","\n","#     \"micro_domain_prec_lst\": micro_domain_prec_lst,\n","#     \"micro_domain_rec_lst\": micro_domain_rec_lst,\n","#     \"micro_domain_f1_lst\": micro_domain_f1_lst,\n","#     \"micro_domain_acc_lst\": micro_domain_acc_lst,\n","\n","#     \"macro_domain_prec_lst\": macro_domain_prec_lst,\n","#     \"macro_domain_rec_lst\": macro_domain_rec_lst,\n","#     \"macro_domain_f1_lst\": macro_domain_f1_lst,\n","#     \"macro_domain_acc_lst\": macro_domain_acc_lst\n","\n","# })\n","\n","# metrics_df.to_csv(os.path.join(sub_metrics_dir, f\"metrics_df-top{token_top_percent}.csv\"), index=False)\n","# make_plot_metric(metrics_df, sub_metrics_dir, name=f\"top{token_top_percent}\")\n","\n","# prob_df = pd.DataFrame({})\n","# fix_len = len(token_prob_lst[0])\n","# for i in range(len(token_prob_lst)):\n","#   prob_df[i+1] = token_prob_lst[i] + [None] * (fix_len - len(token_prob_lst[i]))\n","\n","# prob_df.to_csv(os.path.join(sub_metrics_dir, f\"prob_df-top{token_top_percent}.csv\"), index=False)\n","\n","# time10 = time.time()\n","# print(\" Running time:\", time10 - time9)\n","\n"],"metadata":{"id":"N_6jMFYtQANP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for i in range(200):\n","#   print(\"\\n\")\n","#   print(domain_unlabeled_data[i])\n","#   print(new_training_tags[i])\n","#   print(fake_tags[i])"],"metadata":{"id":"lBHQ8DyoGxXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c36IaHc2GxT4"},"execution_count":null,"outputs":[]}]}